{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"WELCOME TO \ud83e\uddec FUSIONBENCH","text":""},{"location":"#install-fusionbench","title":"Install FusionBench","text":"<p>Prerequisites:</p> <ul> <li>Python 3.10 or later (some features may not work as expected for earlier versions)</li> </ul> Install from GitHub repositoryInstall from PyPi <p>Install the latest version of <code>fusion-bench</code> from GitHub repository and install it in editable mode by passing the <code>-e</code> flag to pip.</p> <pre><code>git clone https://github.com/tanganke/fusion_bench.git\ncd fusion_bench\n\n# checkout to use a specific version. for example, v0.1.6\n# git checkout v0.1.6\n\npip install -e . # install the package in editable mode\n</code></pre> <p><code>fusion-bench</code> can also be installed from PyPI as a library and toolkit for deep model fusion.</p> <pre><code>pip install fusion-bench\n\n# you can also install a specific version\n# pip install fusion-bench==0.1.6\n</code></pre> <p>Installing <code>fusion-bench</code> will also install the latest stable PyTorch if you don't have it already.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p>Learn More about FusionBench</p> <p>Learn the basic concepts of FusionBench and the command line interface (CLI) as well as the programmatic usage of FusionBench.</p> <p> Read More</p> </li> <li> <p>Learn More About Deep Model Fusion</p> <p>Read an introduction to deep model fusion and learn about its key concepts, techniques, and applications.</p> <p> Read More</p> </li> </ul> <p>Contributing to FusionBench</p> <ul> <li>Any questions or comments can be directed to the GitHub Issues page for this project.</li> <li>Any contributions or pull requests are welcome. If you find any mistakes or have suggestions for improvements, please feel free to raise an issue or submit a pull request.</li> </ul> <p>Introduction to Deep Model Fusion (The Learn From Model Paradigm)</p> <p>Deep model fusion is a technique that merges, ensemble, or fuse multiple deep neural networks to obtain a unified model. It can be used to improve the performance and robustness of model or to combine the strengths of different models, such as fuse multiple task-specific models to create a multi-task model. For a more detailed introduction to deep model fusion, you can refer to W. Li, 2023, 'Deep Model Fusion: A Survey'.  In this benchmark, we evaluate the performance of different fusion methods on a variety of datasets and tasks. ...</p> <p> Read More</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find this benchmark useful, please consider citing our work:</p> <pre><code>@article{tang2024fusionbench,\n  title={Fusionbench: A comprehensive benchmark of deep model fusion},\n  author={Tang, Anke and Shen, Li and Luo, Yong and Hu, Han and Du, Bo and Tao, Dacheng},\n  journal={arXiv preprint arXiv:2406.03280},\n  year={2024}\n}\n</code></pre>"},{"location":"introduction_to_model_fusion/","title":"Introduction to Deep Model Fusion (The Learn From Model Paradigm)","text":"<p>Deep model fusion is a technique that merges, ensemble, or fuse multiple deep neural networks to obtain a unified model. It can be used to improve the performance and robustness of model or to combine the strengths of different models, such as fuse multiple task-specific models to create a multi-task model. For a more detailed introduction to deep model fusion, you can refer to W. Li, 2023, 'Deep Model Fusion: A Survey'. In this benchmark, we evaluate the performance of different fusion methods on a variety of datasets and tasks.</p>"},{"location":"introduction_to_model_fusion/#background","title":"Background","text":"<p>The traditional approach of training deep networks with supervised learning requires:</p> <ol> <li>gathering a large dataset relevant to the problem at hand. </li> <li>Once the dataset is collected, manual labeling may be necessary to provide supervision.</li> <li>After the dataset is prepared, Feature Engineering can be conducted to select and transform variables that are most relevant to the predictive model. This step is optional but can significantly improve the accuracy of the model.</li> <li>The next step is Model Training, in which the design model architecture is developed and an optimization algorithm is chosen.</li> </ol> <p>We call this the \"learn from data\" paradigm. This approach has been very successful in many applications, and the size of both the dataset and the model has been increasing rapidly. </p> Hongling Zheng, Li Shen, et al. \"Learn from model beyond fine-tuning: A survey.\"  arXiv:2310.08184 (2023). <p>However, in the past few years, it is obvious that the learning paradigm of deep neural networks has undergone a significant shift. The traditional approach of training deep networks with supervised learning has been complemented with emerging techniques that transfer knowledge from existing models, edit existing models, or leverage unlabeled data to tune the models. Many data-efficient approaches have been proposed, such as model tuning, model distillation, model ensemble, model pruning, model editing and so on.  We can even achieve satisfactory results with just prompt design in a few minutes, without any data or model training.</p> <p>This transformation is driven by the increasing complexity of the models and the increasing cost of data labeling. We call this the \"learn from model\" paradigm.</p> <p>So why do we need to learn from models? What are the advantages of learning from models? By learning from model, we aim to enhance model performance, reduce training costs, improve generalization...</p> <ul> <li>Knowledge transfer across models (accelerate training, reuse sunk training cost)  </li> <li>Knowledge transfer across tasks (Facilitating learning from smaller datasets)</li> <li>Task regularization improve generalization and reduce overfitting</li> <li>Reduced model/parameter maintenance costs</li> <li>Improve the efficiency of the inference phase (one forward inference to obtain multiple task predictions)</li> <li>...</li> </ul>"},{"location":"introduction_to_model_fusion/#applications-of-deep-model-fusion","title":"Applications of Deep Model Fusion","text":""},{"location":"introduction_to_model_fusion/#accelerating-model-training","title":"Accelerating Model Training","text":"<p>Leveraging existing models provides a significant advantage in accelerating the development of new models.  Training a model from scratch is often both costly and time-consuming, so utilizing pre-existing models can substantially speed up this process.  By harnessing the knowledge embedded in these models, we can enhance the training efficiency for new models.</p>"},{"location":"introduction_to_model_fusion/#multi-task-learning","title":"Multi-Task Learning","text":"<p>Another popular application of deep model fusion is to construct multi-task models (FusionBench provides a comprehensive benchmark for multi-task model fusion). Where we aim to collaboratively train one single model with data from multiple tasks to facilitate knowledge transfer. Unlike traditional multi-task learning approaches that train a single model with data from multiple tasks, the multi-task model fusion approach reuse the task-specific models to construct a multi-task model.  This approach can be more data-efficient.</p> <p>Similar to single-task model training, train a multi-task model needs three steps: </p> <ol> <li>Data collection and preparation.</li> <li>Model design and algorithm selection.</li> <li>Model training and evaluation.</li> </ol>  Core steps of multi-task model training  <p>However, the traditional model training strategy has limitations in certain applications.  Particularly when the training data for each task is private and cannot be shared.  In other cases, we may only have access to the models trained on the private data, but not the data itself. In these cases, we need to develop a new training strategy that can train a multi-task model without sharing the private data.  And training a multi-task model from scratch can be very expensive.</p> <p>Model fusion without accessing training data in machine learning has attracted increasing interest due to the practical resource-saving and data privacy issues.  Creating a multi-task model by merging models for distinct tasks has proven to be an economical andscalable approach. By fuse deep neural networks, transfer knowledge, addressing task interference, and utilizing combined knowledge can be achieved without the need for accessing labeled datasets. This is scalable to large models and a wide array of tasks. </p> The general framework of multi-task model fusion"},{"location":"introduction_to_model_fusion/#improving-model-performance","title":"Improving Model Performance","text":"<p>Moreover, the knowledge from existing models can be used to create new models.  For instance, if we have multiple models, we can combine their knowledge through techniques like model ensembling or weight manipulation.  This approach allows us to develop more sophisticated models by integrating the strengths of several pre-trained models.</p>"},{"location":"introduction_to_model_fusion/#scaling-up-models","title":"Scaling Up Models","text":"<p>Additionally, we can upscale existing models to create larger, more powerful models.  This can be achieved by mixing the weights of existing models or by adding new layers to them.  One of the more popular methods for scaling up models recently is the use of Mixture of Experts (MoE) methods.  These methods allow for the scaling of models to very large sizes while maintaining a manageable inference cost.</p>"},{"location":"introduction_to_model_fusion/#formal-definition-and-taxonomies-of-deep-model-fusion","title":"Formal Definition and Taxonomies of Deep Model Fusion","text":"<p>In this section, we provide a formal definition of deep model fusion and introduce several taxonomies to categorize different fusion methods. We define deep model fusion as the process of combining multiple deep neural networks to create a unified model that leverages the knowledge and capabilities of the individual models. We taxonomize these fusion methods based on their underlying principles and mechanisms, providing a structured framework for understanding and comparing different fusion techniques. We categorize deep model fusion methods into three main types: Model Ensemble, Model Merging, and Model Mixing.</p> <p>The initial diagram presented below provides a visual representation of the Model Ensemble technique.  This method involves the training of several independent models, each generating its own set of predictions.  These individual predictions are then aggregated to produce a final, more accurate prediction. The Model Ensemble technique is frequently employed to enhance both the performance and reliability of models.  It achieves this by leveraging the collective intelligence of multiple models, thereby reducing the likelihood of errors that could arise from a single model's predictions. However, it's important to note that this technique does come with its own set of challenges.  The necessity to train and maintain multiple models can lead to increased computational costs.  This is due to the additional resources required for the training process, as well as the storage needed for each model.  Therefore, while the Model Ensemble technique can lead to improved accuracy, it's essential to consider the potential trade-off in terms of computational expense.</p> <p>Moving on, the second diagram represents Model Merging. This technique involves combining multiple models into a single model of the same architecture. By merging these models, we can create a more robust and comprehensive model that leverages the strengths of its constituent models.  This can lead to improved performance and generalization, especially in scenarios where the individual models excel in different areas.</p> <p>Lastly, the third diagram depicts Model Mixing, a technique where the weights of multiple models are combined to form a new model.  The new model typically has a different architecture from the original models, such as a larger size or more complex structure.</p>"},{"location":"supported_algorithms/","title":"Supported algorithms","text":"<p>Here is a table of supported algorithms in the benchmark:</p> Model Merging AlgorithmsModel Mixing AlgorithmsModel Ensemble AlgorithmsOthers Algorithm Name Class Path <code>dummy</code> <code>clip_finetune</code> <code>.classification.clip_finetune.ImageClassificationFineTuningForCLIP</code> <code>TaskVectorCosSimilarity</code> <code>.analysis.task_vector_cos_similarity.TaskVectorCosSimilarity</code> <code>simple_ensemble</code> <code>.ensemble.EnsembleAlgorithm</code> <code>weighted_ensemble</code> <code>.ensemble.WeightedEnsembleAlgorithm</code> <code>max_model_predictor</code> <code>.ensemble.MaxModelPredictorAlgorithm</code> <code>simple_average</code> <code>.simple_average.SimpleAverageAlgorithm</code> <code>weighted_average</code> <code>.weighted_average.weighted_average.WeightedAverageAlgorithm</code> <code>weighted_average_for_llama</code> <code>.weighted_average.llama.WeightedAverageForLLama</code> <code>clip_fisher_merging</code> <code>.fisher_merging.clip_fisher_merging.FisherMergingAlgorithmForCLIP</code> <code>gpt2_fisher_merging</code> <code>.fisher_merging.gpt2_fisher_merging.FisherMergingAlgorithmForGPT2</code> <code>clip_regmean</code> <code>.regmean.clip_regmean.RegMeanAlgorithmForCLIP</code> <code>gpt2_regmean</code> <code>.regmean.gpt2_regmean.RegMeanAlgorithmForGPT2</code> <code>task_arithmetic</code> <code>.task_arithmetic.TaskArithmeticAlgorithm</code> <code>ties_merging</code> <code>.ties_merging.ties_merging.TiesMergingAlgorithm</code> <code>clip_task_wise_adamerging</code> <code>.adamerging.clip_task_wise_adamerging.CLIPTaskWiseAdaMergingAlgorithm</code> <code>clip_layer_wise_adamerging</code> <code>.adamerging.clip_layer_wise_adamerging.CLIPLayerWiseAdaMergingAlgorithm</code> <code>singular_projection_merging</code> <code>fusion_bench.method.smile_upscaling.singular_projection_merging.SingularProjectionMergingAlgorithm</code> <code>pwe_moe_ls_for_clip</code> <code>.pwe_moe.clip_pwe_moe.PWEMoELinearScalarizationForCLIP</code> <code>pwe_moe_epo_for_clip</code> <code>.pwe_moe.clip_pwe_moe.PWEMoExactParetoOptimalForCLIP</code> <code>clip_concrete_task_arithmetic</code> <code>.concrete_subspace.clip_concrete_task_arithmetic.ConcreteTaskArithmeticAlgorithmForCLIP</code> <code>clip_concrete_task_wise_adamerging</code> <code>.concrete_subspace.clip_concrete_adamerging.ConcreteTaskWiseAdaMergingForCLIP</code> <code>clip_concrete_layer_wise_adamerging</code> <code>.concrete_subspace.clip_concrete_adamerging.ConcreteLayerWiseAdaMergingForCLIP</code> <code>mixtral_moe_merging</code> <code>.mixture_of_experts.mixtral_merging.MixtralMoEMergingAlgorithm</code> <code>mixtral_for_causal_lm_merging</code> <code>.mixture_of_experts.mixtral_merging.MixtralForCausalLMMergingAlgorithm</code> <code>clip_weight_ensembling_moe</code> <code>.we_moe.clip_we_moe.CLIPWeightEnsemblingMoEAlgorithm</code> Algorithm Name Class Description Depth Upscaling DepthUpscalingAlgorithm Depth upscaling algorithm that concatenates the layers of model.  MoE Upscaling MixtralUpscalingAlgorithm, MixtralForCausalLMUpscalingAlgorithm Mixture of Experts upscaling algorithm that merges the models.  SMILE Upscaling SmileUpscalingAlgorithm Upscale models to sparse low-rank MoE model.  Algorithm Name Class Description Simple Ensemble SimpleEnsembleAlgorithm Simple ensemble algorithm that averages the predictions of multiple models. Weighted Ensemble WeightedEnsembleAlgorithm Ensemble algorithm that averages the predictions of multiple models with given weights. <p>These algorithms are not directly related to model fusion, but they are used in the benchmark for other purposes.</p> Algorithm Name Class Description Dummy Algorithm DummyAlgorithm Return model as it is. <p>You can find the implementation of these algorithms in the corresponding files.</p>"},{"location":"algorithms/","title":"Algorithm Module","text":"<p>The Algorithm module is a core component of FusionBench, dedicated to the implementation and execution of various model fusion techniques. This module provides the mechanisms necessary to combine multiple models from model pools, enabling sophisticated and optimized model merging operations.</p>"},{"location":"algorithms/#algorithm-configuration-structure","title":"Algorithm Configuration Structure","text":"<p>Algorithms use Hydra-based configuration with the <code>_target_</code> field:</p>"},{"location":"algorithms/#basic-algorithm-configuration","title":"Basic Algorithm Configuration","text":"<pre><code># Simple algorithm with no parameters\n_target_: fusion_bench.method.SimpleAverageAlgorithm\n</code></pre>"},{"location":"algorithms/#parameterized-algorithm-configuration","title":"Parameterized Algorithm Configuration","text":"<pre><code># Algorithm with parameters\n_target_: fusion_bench.method.TaskArithmeticAlgorithm\nscaling_factor: 0.3\n</code></pre>"},{"location":"algorithms/#advanced-algorithm-configuration","title":"Advanced Algorithm Configuration","text":"<pre><code># Complex algorithm with multiple parameters\n_target_: fusion_bench.method.MoreAdvancedAlgorithm\nweights_initial: [0.3, 0.3, 0.4]  \nlayer_wise_weight: false\nentropy_k: 1\nentropy_regularization_weight: 0.001\ntest_time_adaptation_steps: 100\n</code></pre>"},{"location":"algorithms/#implementation-architecture","title":"Implementation Architecture","text":"<p>All fusion algorithms inherit from <code>BaseAlgorithm</code>:</p> <pre><code>from fusion_bench.method import BaseAlgorithm\nfrom fusion_bench.modelpool import BaseModelPool\n\nclass CustomAlgorithm(BaseAlgorithm):\n    \"\"\"\n    Custom model fusion algorithm implementation.\n    \"\"\"\n\n    # Configuration mapping for YAML serialization\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"custom_param\": \"custom_param\",\n        \"another_param\": \"another_param\",\n    }\n\n    def __init__(self, custom_param: float = 0.5, another_param: bool = True, **kwargs):\n        \"\"\"Initialize the algorithm with custom parameters.\"\"\"\n        self.custom_param = custom_param\n        self.another_param = another_param\n        super().__init__(**kwargs)\n\n    def run(self, modelpool: BaseModelPool):\n        \"\"\"\n        Execute the fusion algorithm.\n\n        Args:\n            modelpool: Pool of models to fuse\n\n        Returns:\n            Fused model (torch.nn.Module)\n        \"\"\"\n        # Your fusion logic here\n        pretrained_model = modelpool.load_pretrained_model()\n        models = [modelpool.load_model(name) for name in modelpool.model_names]\n\n        # Implement your fusion strategy\n        merged_model = self.merge_models(pretrained_model, models)\n        return merged_model\n</code></pre>"},{"location":"algorithms/#usage-examples","title":"Usage Examples","text":""},{"location":"algorithms/#direct-instantiation","title":"Direct Instantiation","text":"<pre><code>from fusion_bench.method import TaskArithmeticAlgorithm\nfrom fusion_bench.modelpool import BaseModelPool\n\n# Create algorithm directly\nalgorithm = TaskArithmeticAlgorithm(scaling_factor=0.3)\n\n# Apply to your model pool\nmerged_model = algorithm.run(your_modelpool)\n</code></pre>"},{"location":"algorithms/#configuration-based-usage","title":"Configuration-Based Usage","text":"<pre><code>from fusion_bench.utils import instantiate\nfrom omegaconf import OmegaConf\n\n# Load from configuration\nconfig = OmegaConf.load(\"config/method/task_arithmetic.yaml\")\nalgorithm = instantiate(config)\n\n# Execute fusion\nmerged_model = algorithm.run(modelpool)\n</code></pre>"},{"location":"algorithms/#integration-with-programs","title":"Integration with Programs","text":"<p>The most common usage is through FusionBench programs:</p> <pre><code>from fusion_bench.programs import FabricModelFusionProgram\n\n# Full workflow using program\nprogram = FabricModelFusionProgram(\n    method=method_config,\n    modelpool=modelpool_config, \n    taskpool=taskpool_config\n)\n\n# This runs: algorithm.run(modelpool) + evaluation\nprogram.run()\n</code></pre>"},{"location":"algorithms/#command-line-usage","title":"Command Line Usage","text":"<pre><code>fusion_bench \\\n    method=task_arithmetic \\\n    method.scaling_factor=0.3 \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre>"},{"location":"algorithms/#advanced-features","title":"Advanced Features","text":""},{"location":"algorithms/#profiling-support","title":"Profiling Support","text":"<p>Many algorithms support profiling through <code>SimpleProfilerMixin</code>:</p> <pre><code>from fusion_bench.method import BaseAlgorithm\nfrom fusion_bench.mixins import SimpleProfilerMixin\n\nclass ProfilingAlgorithm(BaseAlgorithm, SimpleProfilerMixin):\n\n    def run(self, modelpool):\n        with self.profile(\"initialization\"):\n            # Initialization code\n            pass\n\n        with self.profile(\"model_merging\"):\n            # Merging logic\n            pass\n\n        # Print timing summary\n        self.print_profile_summary()\n</code></pre>"},{"location":"algorithms/#lightning-fabric-integration","title":"Lightning Fabric Integration","text":"<p>For distributed and accelerated computing:</p> <pre><code>from fusion_bench.method import BaseAlgorithm\nfrom fusion_bench.mixins import LightningFabricMixin\n\nclass DistributedAlgorithm(BaseAlgorithm, LightningFabricMixin):\n\n    def run(self, modelpool):\n        # Access fabric for distributed operations\n        if hasattr(self, 'fabric'):\n            # Use self.fabric for distributed operations\n            pass\n\n        # Algorithm implementation\n        merged_model = self.merge_models(modelpool)\n        return merged_model\n</code></pre>"},{"location":"algorithms/#integration-with-taskpools","title":"Integration with TaskPools","text":"<p>Algorithms can access taskpools for evaluation during fusion:</p> <pre><code>class AdaptiveAlgorithm(BaseAlgorithm):\n\n    def run(self, modelpool):\n        # Access taskpool if available through program\n        if hasattr(self, '_program') and self._program.taskpool:\n            # Use taskpool for adaptive fusion\n            taskpool = self._program.taskpool\n\n            for step in range(self.adaptation_steps):\n                merged_model = self.merge_step(modelpool)\n                results = taskpool.evaluate(merged_model)\n                self.update_weights(results)\n\n        return merged_model\n</code></pre>"},{"location":"algorithms/#migration-from-v01x","title":"Migration from v0.1.x","text":"<p>If you're migrating from v0.1.x, note these key changes:</p> <ol> <li>Base Class: Use <code>BaseAlgorithm</code> instead of <code>ModelFusionAlgorithm</code></li> <li>Configuration: Use <code>_target_</code> fields instead of string-based algorithm names  </li> <li>Instantiation: Use <code>instantiate(config)</code> instead of factory methods</li> <li>Parameters: Pass parameters to <code>__init__</code> instead of through config dict</li> </ol>"},{"location":"algorithms/#migration-example","title":"Migration Example","text":"<pre><code># Old (v0.1.x, deprecated)\nfrom fusion_bench.compat.method import ModelFusionAlgorithm, AlgorithmFactory\n\nclass OldAlgorithm(ModelFusionAlgorithm):\n    def __init__(self, algorithm_config):\n        super().__init__(algorithm_config)\n        self.param = algorithm_config.get('param', 0.5)\n\nalgorithm = AlgorithmFactory.create_algorithm(config)\n\n# New (v0.2+)\nfrom fusion_bench.method import BaseAlgorithm\n\nclass NewAlgorithm(BaseAlgorithm):\n    def __init__(self, param: float = 0.5, **kwargs):\n        self.param = param\n        super().__init__(**kwargs)\n\nalgorithm = instantiate(config)  # or direct instantiation\n</code></pre> <p>For backward compatibility, v0.1.x style configurations and factory methods are still supported through the <code>fusion_bench.compat</code> module, but new implementations should use the v0.2+ style.</p>"},{"location":"algorithms/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.BaseAlgorithm</li> </ul>"},{"location":"algorithms/adamerging/","title":"AdaMerging","text":"Task Vector, Task Arithmetic, and AdaMerging. Credit to <sup>1</sup> <p>In the complex landscape of multi-task learning, AdaMerging has emerged as a potent method for adaptively merging model parameters to optimize performance across tasks. Unlike traditional fixed-coefficient methods, AdaMerging autonomously learns merging coefficients, offering a more refined and responsive approach<sup>1</sup>. </p> <p>Task Vectors. Similar to Task Arithmetic, AdaMerging begins by computing task vectors for each fine-tuned model:</p> \\[\\tau_i = \\theta_i - \\theta_0\\] <p>where \\(\\theta_i\\) represents the parameters of the model fine-tuned for task \\(i\\), and \\(\\theta_0\\) denotes the parameters of the pre-trained model.</p> <p>Adaptive Coefficient Learning. The cornerstone of AdaMerging lies in its adaptive nature, where it learns the coefficients for merging either on a task-wise or layer-wise basis. This adaptability is driven by an entropy minimization strategy applied to unlabeled test samples as a surrogate objective function, which serves to refine the merging coefficients for optimal performance.</p> <p>The optimization objective for AdaMerging is:</p> \\[\\min_{\\lambda} \\mathbb{E}_{x \\sim \\mathcal{D}_{test}} [H(p(y|x; \\theta_{\\lambda}))]\\] <p>where \\(H(\\cdot)\\) denotes the entropy function, \\(p(y|x; \\theta_{\\lambda})\\) is the predicted probability distribution, and \\(\\theta_{\\lambda}\\) is the merged model with coefficients \\(\\lambda\\).</p> <p>Task-wise AdaMerging learns a single coefficient per task and is formulated as:</p> \\[ \\theta = \\theta_0 + \\sum_{i=1}^{n} \\lambda_i \\tau_i \\] <p>where \\(\\lambda_i\\) represents the merging coefficient for the \\(i\\)-th task, and \\(\\tau_i\\) denotes the task vector for the \\(i\\)-th task.</p> <p>Layer-wise AdaMerging learns coefficients for each layer of each task and is articulated as:</p> \\[\\theta^l = \\theta_0^l + \\sum_{i=1}^{n} \\lambda^{l}_{i} \\tau^{l}_{i}\\] <p>where the merging coefficient \\(\\lambda^{l}_{i}\\) and task vector \\(\\tau^{l}_{i}\\) are specific to each layer \\(l\\) of the model.</p> <p>By leveraging this adaptive learning approach, AdaMerging significantly enhances the model's ability to generalize across tasks and layers, resulting in a more robust and finely-tuned performance profile. The method's reliance on entropy minimization ensures that the merging process continually seeks the most informative and stable configuration, adapting to the specific needs of the dataset and tasks at hand.</p>"},{"location":"algorithms/adamerging/#adamerging-analysis","title":"AdaMerging Analysis","text":"<p>Task-wise Coefficients.  The below Figure shows the changes during the iteration process of merging coefficient optimization of each task vector in Task-wise AdaMerging and AdaMerging++, which is shown every ten steps. We consistently observe that the merging coefficients of each task vector are inconsistent. When the number of tasks is relatively large, it is obviously undesirable to grid search the coefficients of each task, but our AdaMerging avoids this manual search process.</p>  Model merging coefficients \\(\\{\u03bb_k\\}_{k=1}^K\\) change with respect to training steps on ViT-B/32: (a) Task-wise AdaMerging; (b) Task-wise AdaMerging++. Each line represents the change process of the coefficient \\(\u03bb_k\\) of a task vector \\(T_k (k \\in \\{1, 2, . . . , K\\})\\).  <p>Layer-wise Coefficients. The following Figure shows the merging coefficients learned by Layer-wise AdaMerging and AdaMerging++ on ViT-B/32 respectively. We observed that:  </p> <ol> <li>The coefficients learned by each layer of each task vector are different, which shows that the importance of each layer in the model merging process is different. </li> <li>The coefficients learned by shallow layers are generally smaller than those of deep layers, which indicates that shallow layers rely more on the weights of the pre-trained model rather than the weights provided by task vectors, while the deep layers rely more on the weights provided by the task vectors. This may be since the shallow layer learns general features, which are cross-task, while the deep layer learns task-specific features <sup>2</sup>. This finding is also consistent with routing analysis in <sup>3</sup>.</li> </ol>  Learned model merging coefficients \\(\\{\u03bb_l^k\\}^{K,L}_{k=1,l=1}\\) of Layer-wise AdaMerging (Above) and AdaMerging++ (Below) on ViT-B/32.  The \\(k\\)-th row represents the \\(k\\)-th task vector, the \\(l\\)-th column represents the \\(l\\)-th layer, and the intersection point represents the coefficient \\(\u03bb^l_k\\)."},{"location":"algorithms/adamerging/#examples","title":"Examples","text":""},{"location":"algorithms/adamerging/#cli-usage","title":"CLI Usage","text":"<p>Configuration template for AdaMerging (CLIP):</p> config/method/adamerging/clip.yaml<pre><code># this option can be \"clip_task_wise_adamerging\"\nname: clip_layer_wise_adamerging\n# this weights can be a list of float, or a string that points to a *.np, *.pt file containing the weights\n# if weights is specified, skip the test-time adaptation training\nweights: null\n# learning rate\noptimizer: adam\nlr: 1e-3\ninit_values: 0.3\n# if `clamp_weights` is true, the weights will be clamped to [0, 1]\nclamp_weights: false\n# arguments of `functional_call`\ntie_weights: true\nstrict: false\n# this is overrided by `fabric.devices` if launched from the `fusion_bench` CLI.\ndevices: 1\nbatch_size: 16\nnum_workers: 8\nmax_steps: 1000\nfast_dev_run: ${fast_dev_run}\n# the path for saving the merging weights\nsave_merging_weights: 'merging_weights.pt'\ncache_dir: outputs\n</code></pre>"},{"location":"algorithms/adamerging/#task-wise-adamerging","title":"Task-wise AdaMerging","text":"<p>Merge CLIP-ViT-B/32 models from eight downstream image classification tasks using task-wise AdaMerging:</p> <pre><code>fusion_bench \\\n    path.log_dir=outputs/ViT-B-32/task_wise_adamerging \\\n    method=adamerging/clip \\\n        method.name=clip_task_wise_adamerging \\\n        method.save_merging_weights=merging_weights.pt \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre>"},{"location":"algorithms/adamerging/#layer-wise-adamerging","title":"Layer-wise AdaMerging","text":"<p>Merge CLIP-ViT-B/32 models from eight downstream image classification tasks using layer-wise AdaMerging:</p> <pre><code>fusion_bench \\\n    path.log_dir=outputs/ViT-B-32/layer_wise_adamerging \\\n    method=adamerging/clip \\\n        method.name=clip_layer_wise_adamerging \\\n        method.save_merging_weights=merging_weights.pt \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>Part of the output:</p> <pre><code>Profiler Report\n\n----------------------------------------------------------------------------------------------------------------------------------\n|  Action                       |  Mean duration (s)    |  Num calls            |  Total time (s)       |  Percentage %         |\n----------------------------------------------------------------------------------------------------------------------------------\n|  Total                        |  -                    |  26001                |  724.65               |  100 %                |\n----------------------------------------------------------------------------------------------------------------------------------\n|  backward pass                |  0.060172             |  8000                 |  481.38               |  66.429               |\n|  forward pass                 |  0.016124             |  8000                 |  128.99               |  17.801               |\n|  data loading                 |  0.0063443            |  8000                 |  50.754               |  7.004                |\n|  merging weights              |  0.050735             |  1000                 |  50.735               |  7.0013               |\n|  construct the wrapped model  |  7.2558               |  1                    |  7.2558               |  1.0013               |\n|  optimizer step               |  0.00098186           |  1000                 |  0.98186              |  0.13549              |\n----------------------------------------------------------------------------------------------------------------------------------\n</code></pre>"},{"location":"algorithms/adamerging/#api-usage","title":"API Usage","text":"<p>To use AdaMerging programmatically, you can use the specific algorithm classes:</p>"},{"location":"algorithms/adamerging/#task-wise-adamerging_1","title":"Task-wise AdaMerging","text":"<pre><code>from fusion_bench.method.adamerging import CLIPTaskWiseAdaMergingAlgorithm\nfrom omegaconf import DictConfig\n\n# Configuration for task-wise AdaMerging\nconfig = DictConfig({\n    'name': 'clip_task_wise_adamerging',\n    'lr': 1e-3,\n    'init_values': 0.3,\n    'max_steps': 1000,\n    'batch_size': 16,\n    'clamp_weights': False,\n    'save_merging_weights': 'merging_weights.pt'\n})\n\n# Initialize the algorithm\nalgorithm = CLIPTaskWiseAdaMergingAlgorithm(config)\n\n# Run the algorithm with a model pool and task pool\nmerged_model = algorithm.run(modelpool)\n</code></pre>"},{"location":"algorithms/adamerging/#layer-wise-adamerging_1","title":"Layer-wise AdaMerging","text":"<pre><code>from fusion_bench.method.adamerging import CLIPLayerWiseAdaMergingAlgorithm\nfrom omegaconf import DictConfig\n\n# Configuration for layer-wise AdaMerging\nconfig = DictConfig({\n    'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 1e-3},\n    'init_values': 0.3,\n    'max_steps': 1000,\n    'batch_size': 16,\n    'clamp_weights': False,\n    'merging_weights_save_path': 'layer_wise_weights.pt'\n})\n\n# Initialize the algorithm\nalgorithm = CLIPLayerWiseAdaMergingAlgorithm(config)\n\n# Run the algorithm\nmerged_model = algorithm.run(modelpool)\n</code></pre>"},{"location":"algorithms/adamerging/#implementation-details","title":"Implementation Details","text":"<ul> <li>CLIPTaskWiseAdaMergingAlgorithm</li> <li>CLIPLayerWiseAdaMergingAlgorithm</li> <li>GPT2LayerWiseAdaMergingAlgorithm</li> <li>FlanT5LayerWiseAdaMergingAlgorithm</li> </ul> <ol> <li> <p>(ICLR 2024) AdaMerging: Adaptive Model Merging for Multi-Task Learning. https://openreview.net/pdf?id=nZP6NgD3QY\u00a0\u21a9</p> </li> <li> <p>Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? Advances in neural information processing systems, 27, 2014.\u00a0\u21a9</p> </li> <li> <p>A. Tang, L. Shen, Y. Luo, N. Yin, L. Zhang, and D. Tao, \u201cMerging Multi-Task Models via Weight-Ensembling Mixture of Experts,\u201d ICML 2024. doi: 10.48550/arXiv.2402.00433.\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/bitdelta/","title":"BitDelta","text":"BitDelta applies 1-bit quantization to the weight delta between fine-tuned and base models. For each weight matrix, we quantize its delta as its sign bits and a trainable high-precision scale factor. The scale factor is initialized to achieve the best approximation error in L2 norm and further refined with a few distillation steps. BitDelta shows minimal degradation in model performance and reduces memory consumption in multi-tenancy serving by representing multiple fine-tuned models with a single high-precision base model and multiple 1-bit deltas."},{"location":"algorithms/bitdelta/#default-configurations","title":"Default Configurations","text":"config/method/bitdelta/bitdelta.yaml<pre><code># =============================================================================\n# FusionBench Method Configuration: BitDelta\n# =============================================================================\n_target_: fusion_bench.method.bitdelta.BitDeltaAlgorithm\nsave_dir: null\nsave_full_model: false\n# training arguments\nlr: 1e-4\nbatch_size: 4\nnum_steps: 100\n# dataset arguments\ndataset_name: c4\nsubset: en\nsplit: train\nmax_length: 128\n</code></pre>"},{"location":"algorithms/bitdelta/#example-usage","title":"Example Usage","text":"<pre><code>fusion_bench method=bitdelta/bitdelta modelpool=CausalLMPool/vicuna-7b-v1.5\n</code></pre>"},{"location":"algorithms/bitdelta/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.BitDeltaAlgorithm</li> </ul>"},{"location":"algorithms/concrete_subspace/","title":"Concrete Subspace Learning","text":"(a) Framework overview. Our proposed framework comprises two main steps: first, establishing a common subspace for task vectors across various tasks using a shared mask, and second, merging the models within this shared subspace. (b) Mask sampling. Here we illustrate the procedure for sampling discrete binary masks and our differentiable Concrete mask. It's important to note that while a Concrete mask can also be binarized, this binarization process is non-differentiable."},{"location":"algorithms/concrete_subspace/#contrete-masking","title":"Contrete Masking","text":""},{"location":"algorithms/concrete_subspace/#the-gumbel-max-trick","title":"The Gumbel-Max Trick","text":"<p>Consider a discrete categorical distribution parameterized by logits \\(\\mathbf{x} = (x_1, \\dots, x_n) \\in \\mathbb{R}^{n}\\), where \\(x_i\\) is the logit of the \\(i\\)-th category. The Gumbel-Max trick <sup>1</sup><sup>2</sup><sup>3</sup> states a reparameterization trick to sample from the categorical distribution by sampling from the standard Gumbel distribution \\(\\text{Gumbel}(\\mu=0,\\beta=1)\\) and taking the argmax of the sum of the Gumbel random variables and the logits.</p> <p>This trick proceeds as follows: sample \\(n\\) Gumbel random variables \\(g_1, \\dots, g_n\\) independently from the standard Gumbel distribution \\(\\text{Gumbel}(\\mu=0,\\beta=1)\\) (We can draw a random sample \\(u\\) from a unifrom distribution on the interval \\((0,1)\\) and then transform it into a Gumbel-distributed variable \\(g\\) using the formula \\(g=-\\log(-\\log u)\\).), find the index \\(i\\) of that maximizes \\(x_i + g_i\\), then we have</p> \\[   {\\arg\\max}_{i\\in[n]} (x_i + g_i) \\sim \\text{Categorical}(\\text{softmax}(\\mathbf{x})). \\] <p>If we represent the categorical distribution as a one-hot vector \\(\\mathbf{y} = (y_1, \\dots, y_n) \\in \\{0,1\\}^n\\), where \\(y_i=1\\) indicates that the \\(i\\)-th category is sampled and for all \\(j\\neq i\\), \\(y_j=0\\), then we have</p> \\[   \\mathbb{P}(y_k=1) = \\mathbb{P}\\left({\\arg\\max}_{i\\in[n]} (x_i + g_i) = k\\right) = \\frac{\\exp(x_k)}{\\sum_{i=1}^n \\exp(x_i)}. \\]"},{"location":"algorithms/concrete_subspace/#continuous-relaxation-of-the-discrete-categorical-distribution","title":"Continuous Relaxation of the discrete Categorical Distribution","text":"<p>Since the derivative of the \\({\\arg\\max}\\) function is not defined, we cannot backpropagate the gradients through it. To address this issue,  (Maddison et al., 2017)<sup>4</sup> proposed to use a continuous relaxation of the discrete categorical distribution. A CONCRETE random variable (CONtinuous relaxation of disCRETE random variable) relax the condition that the one-hot vector \\(\\mathbf{y}\\) must be located at the vertices of the \\((n-1)\\)-dimensional simplex \\(\\Delta^{n-1}\\), and instead, it allows \\(\\mathbf{y}\\) to be located anywhere inside the simplex \\(\\Delta^{n-1}\\), i.e. \\(\\{ y\\in \\mathbb{R}^n | y_i \\in [0,1], \\sum_{i=1}^n y_i =1 \\}\\).</p> <p>To sample a Concrete random variable \\(\\mathbf{y}\\) from a distribution that is parameterized by a temperature hyperparameter \\(\\lambda &gt; 0\\) and a vector of logits \\(\\mathbf{x} = (x_1, \\dots, x_n) \\in \\mathbb{R}^{n}\\), we have</p> \\[   \\mathbf{y} = \\text{softmax}\\left(\\frac{\\mathbf{x} + \\mathbf{g}}{\\lambda}\\right), \\quad   y_i = \\frac{\\exp\\left((x_i + g_i)/{\\lambda}\\right)}{\\sum_{j=1}^n \\exp\\left(({x_j + g_j})/{\\lambda}\\right)} \\quad \\text{for} \\,\\, i\\in[n]. \\] <p>where \\(\\mathbf{g} = (g_1, \\dots, g_n)\\) is a vector of Gumbel random variables that are independently sampled from the standard Gumbel distribution \\(\\text{Gumbel}(\\mu=0,\\beta=1)\\).</p>"},{"location":"algorithms/concrete_subspace/#concrete-masking","title":"Concrete Masking","text":"<p>A subspace mask \\(\\mathbf{m}\\) is a binary vector that identifies a subspace of the parameter space. For a neural network parametrized by \\(\\theta\\), we can use a subspace mask \\(\\mathbf{m}\\) to identify a subspace of the parameter space \\(\\mathbf{\\theta}\\) by setting the parameters that are not in the subspace to zero, i.e. \\(\\mathbf{\\theta} \\circ \\mathbf{m}\\), where \\(\\circ\\) denotes the element-wise product. We can draw a random sample \\(\\mathbf{m}\\) from a Bernoulli distribution \\(\\text{Bernoulli}(\\mathbf{p}=\\sigma(\\mathbf{x}))\\), where \\(\\mathbf{p}\\) is the probability (\\(\\mathbf{x}\\) denotes the logits) of each parameter being activated. However, the discrete Bernoulli distribution is not differentiable, so we cannot backpropagate the gradients through it to optimize the parameters \\(\\mathbf{p}\\) or \\(\\mathbf{x}\\).</p> <p>To address this issue, we introduce the Concrete mask which can be drawn from a continuous relaxation of Bernoulli distribution. Before we introduce the Concrete mask, we first review the Gumbel-Max trick in the two-class case.</p> <p>Let \\(p_0\\) and \\(p_1\\) denote the unnormalized probabilities of a Bernoulli random variable being 0 and 1, respectively, with \\(x\\) representing the logits. Then, the probability of the event \\(m=1\\) is given by</p> \\[   \\mathbb{P}(m=1) = \\frac{p_1}{p_0 + p_1} = \\sigma(x), \\] <p>where \\(\\sigma\\) denotes the sigmoid function. In the context of the Gumbel-Max trick, the occurrence of the event \\(m=1\\) is determined by the condition \\(g_1 + \\log p_1 &gt; g_0 + \\log p_0\\), where \\(g_0\\) and \\(g_1\\) are two independent standard Gumbel random variables. Thus we have</p> \\[   \\mathbb{P}(m=1) = \\mathbb{P}(g_1 + \\log p_1 &gt; g_0 + \\log p_0)   = \\mathbb{P}\\left((g_1 - g_0) + (\\log p_1 - \\log p_0)&gt; 0\\right). \\] <p>Because the difference of two standard Gumbel random variables is a Logistic random variable, we can replace \\(g_1 - g_0\\) by \\(\\log u - \\log(1-u)\\) where \\(u\\) is a random variable sampled from a uniform distribution on the interval \\((0,1)\\). Substitute this into Eq.(\\ref{eq:appendix_P_m_1}) and express the probability in terms of the logits \\(x\\) to simplify the expression, we have</p> \\[   \\mathbb{P}(m=1) = \\mathbb{P}\\left(\\log \\frac{u}{1-u} + \\log \\frac{\\sigma(x)}{1-\\sigma(x)} &gt; 0\\right), \\quad u \\sim \\text{Uniform}(0,1). \\] <p>The binary Concrete distribution offers a continuous relaxation of the discrete Bernoulli random variables, which is beneficial for gradient-based optimization as it allows for the backpropagation of gradients even through the sampling process. Instead of making a hard decision as the above equation, we use a temperature parameter \\(\\lambda\\) to control the steepness of the sigmoid function, and hence control how close our 'soft' decisions are to being 'hard' decisions. The continuous version of the Bernoulli random variable is then given by</p> \\[   \\hat{m} = \\sigma\\left(\\left(\\log \\frac{u}{1 - u} + \\log \\frac{\\sigma(x)}{1 - \\sigma(x)}\\right) / \\lambda\\right). \\] <p>As the temperature \\(\\lambda\\) approaches zero, the sigmoid function becomes a step function, and the Concrete random variable \\(\\hat{m}\\) becomes a Bernoulli random variable, as shown in the following Figure. In the limit when \\(\\lambda \\to 0\\), this results in sampling \\(m=1\\) if \\(\\log \\frac{\\sigma(x)}{1 - \\sigma(x)} &gt; -\\log \\frac{u}{1 - u}\\), consistent with the original Gumbel-Max trick. The binary Concrete distribution thus provides a differentiable approximation to Bernoulli random variables. We can further binarize the Concrete mask by setting the entries with values greater than 0.5 to 1 and the rest to 0.</p>       The sigmoid function \\(\\sigma(\\cdot/\\lambda)\\) with different temperatures \\(\\lambda\\)."},{"location":"algorithms/concrete_subspace/#method-analysis","title":"Method Analysis","text":""},{"location":"algorithms/concrete_subspace/#concrete-adamerging","title":"Concrete AdaMerging","text":"Performance comparison between AdaMerging and Concrete AdaMerging. Here we show the whole process of applying AdaMerging and Concrete AdaMerging to CLIP-ViT-B/32, the y-axes are shared by these two subfigures: (a) shows the performance of the merged model during the meta-learning phase of the Concrete AdaMerging; (b) illustrates the comparison between AdaMerging with and without the Concrete mask."},{"location":"algorithms/concrete_subspace/#examples","title":"Examples","text":""},{"location":"algorithms/concrete_subspace/#cli-usage","title":"CLI Usage","text":"<p>Merging CLIP models on eight image classification tasks, using the concrete task arithmetic algorithm</p> <pre><code># tensorboard logs and learned checkpoints of the shared mask can be found at https://huggingface.co/tanganke/clip-vit-base-patch32_concrete-task-arithmetic_tblogs\nfusion_bench \\\n    path.log_dir=outputs/ViT-B-32/concrete_task_arithmetic \\\n    method=concrete_subspace/clip_concrete_task_arithmetic \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>results</p> <pre><code>{\n    \"svhn\": {\n        \"accuracy\": 0.903003990650177,\n        \"loss\": 0.37700024247169495\n    },\n    \"stanford_cars\": {\n        \"accuracy\": 0.6326327323913574,\n        \"loss\": 1.2553859949111938\n    },\n    \"resisc45\": {\n        \"accuracy\": 0.7558730244636536,\n        \"loss\": 1.017554759979248\n    },\n    \"eurosat\": {\n        \"accuracy\": 0.9407407641410828,\n        \"loss\": 0.20871955156326294\n    },\n    \"gtsrb\": {\n        \"accuracy\": 0.8285035490989685,\n        \"loss\": 0.5861473679542542\n    },\n    \"mnist\": {\n        \"accuracy\": 0.9800000190734863,\n        \"loss\": 0.08148527890443802\n    },\n    \"dtd\": {\n        \"accuracy\": 0.5249999761581421,\n        \"loss\": 2.2731478214263916\n    },\n    \"sun397\": {\n        \"accuracy\": 0.6421158909797668,\n        \"loss\": 1.4108904600143433\n    }\n}\n</code></pre> <p>Concrete AdaMerging (Layer-wise)</p> <pre><code># tensorboard logs and learned checkpoints of the shared mask can be found at https://huggingface.co/tanganke/clip-vit-base-patch32_concrete-layer-wise_adamerging_tblogs\nfusion_bench \\\n    path.log_dir=outputs/ViT-B-32/clip_concrete_layer_wise_adamerging \\\n    method=concrete_subspace/clip_concrete_layer_wise_adamerging \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre>"},{"location":"algorithms/concrete_subspace/#further-reading","title":"Further Reading","text":"<ul> <li> <p>       X. Yi, S. Zheng, L. Wang, X. Wang, and L. He, \u201cA safety realignment framework via subspace-oriented model fusion for large language models.\u201d arXiv, May 14, 2024. doi: 10.48550/arXiv.2405.09055.</p> <p>The paper introduces a safety realignment framework for large language models via subspace-oriented model fusion (SOMF, the authors learn a shared mask on the weight space of large language model), which combines safeguard capabilities of initially aligned models with fine-tuned models to ensure safety without compromising performance on downstream tasks.</p> </li> </ul> <ol> <li> <p>E. J. Gumbel. Statistical Theory of Extreme Values and Some Practical Applications. A Series of Lectures. Technical Report PB175818, National Bureau of Standards, Washington, D. C. Applied Mathematics Div., 1954. URL https://ntrl.ntis.gov/NTRL/dashboard/searchResults/titleDetail/PB175818.xhtml.\u00a0\u21a9</p> </li> <li> <p>R. Duncan Luce. Individual Choice Behavior. Individual Choice Behavior. John Wiley, Oxford, England, 1959\u00a0\u21a9</p> </li> <li> <p>Chris J Maddison, Daniel Tarlow, and Tom Minka. A* sampling. Advances in neural information processing systems, 27, 2014.\u00a0\u21a9</p> </li> <li> <p>Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables, March 2017. URL http://arxiv.org/abs/1611.00712.\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/depth_upscaling/","title":"Depth Upscaling","text":"<p>Depth Upscaling is a model scaling technique that increases the depth (number of layers) of neural networks by duplicating and concatenating existing layers<sup>1</sup>. This approach has been shown to be effective for scaling large language models, as demonstrated in the SOLAR 10.7B model.</p>"},{"location":"algorithms/depth_upscaling/#usage","title":"Usage","text":"<p>The <code>DepthUpscalingAlgorithm</code> is used to upscale the depth of PyTorch models. Here's a basic guide on how to use it:</p> <p>First, import the necessary modules:</p> <pre><code>from omegaconf import DictConfig\nfrom torch import nn\nfrom fusion_bench.method.depth_upscaling import DepthUpscalingAlgorithm\n</code></pre> <p>Create an instance of <code>DepthUpscalingAlgorithm</code> by passing the layer indices directly to the constructor.  The layer indices determine the upscaling pattern.</p> <pre><code>algorithm = DepthUpscalingAlgorithm(layer_indices=[0, 1, 1, 0])\n</code></pre> <p>Assume we have a list of PyTorch models (<code>nn.ModuleList</code> instances) that we want to upscale. Here, we're creating a list of linear models as an example:</p> <pre><code>layers = nn.ModuleList([nn.Linear(10, 10) for _ in range(2)])\n</code></pre> <p>Then, we can pass the layers to the <code>run</code> method of our algorithm:</p> <pre><code>upscaled_model = algorithm.run(layers)\n</code></pre> <p>The <code>run</code> method will return an upscaled model. The type of the returned model will be the same as the input models (in this case, <code>nn.ModuleList</code>), and its length will be determined by the layer indices specified in the configuration.</p>"},{"location":"algorithms/depth_upscaling/#layer-index-patterns","title":"Layer Index Patterns","text":"<p>The <code>layer_indices</code> parameter supports flexible specifications:</p> <ul> <li>Integer indices: Direct layer references (0-indexed)</li> <li>String expressions: Python expressions that evaluate to lists of integers</li> <li>Mixed patterns: Combination of integers and strings</li> </ul> <pre><code># Example patterns:\n# [0, 1, 1, 0] - Use layers 0, 1, 1, 0 (4 layers total)\n# [\"range(0,12)\", \"range(6,12)\"] - First 12 layers + layers 6-11 (18 layers total)\n# [0, 2, 4, \"range(6,12)\"] - Layers 0, 2, 4, then layers 6-11 (9 layers total)\n</code></pre>"},{"location":"algorithms/depth_upscaling/#examples","title":"Examples","text":""},{"location":"algorithms/depth_upscaling/#basic-example","title":"Basic Example","text":"<p>Here's a simple example of depth upscaling with basic layers:</p> <pre><code>from torch import nn\nfrom fusion_bench.method.depth_upscaling import DepthUpscalingAlgorithm\n\n# Create a simple model with 4 layers\nmodel = nn.ModuleList([\n    nn.Linear(100, 100),\n    nn.ReLU(),\n    nn.Linear(100, 50),\n    nn.Linear(50, 10)\n])\n\n# Create upscaling algorithm with specific pattern\n# This will create: layer0, layer1, layer2, layer1, layer3\nalgorithm = DepthUpscalingAlgorithm(layer_indices=[0, 1, 2, 1, 3])\n\n# Apply depth upscaling\nupscaled_model = algorithm.run(model)\nprint(f\"Original model layers: {len(model)}\")\nprint(f\"Upscaled model layers: {len(upscaled_model)}\")\n# Outputs:\n# Original model layers: 4\n# Upscaled model layers: 5\n</code></pre>"},{"location":"algorithms/depth_upscaling/#solar-style-mistral-model-upscaling","title":"SOLAR-style Mistral Model Upscaling","text":"<p>Here we provide an example of how to use the <code>DepthUpscalingAlgorithm</code> to upscale the depth of a Mistral model <sup>1</sup>.</p>  Credit to \"SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling\" <pre><code>from omegaconf import DictConfig\nfrom torch import nn\nfrom transformers import AutoModelForCausalLM, MistralConfig, MistralForCausalLM\nfrom fusion_bench.method.depth_upscaling import DepthUpscalingAlgorithm\n\n# create a Mistral model\n# here we randomly initialize the model for demonstration purposes\n# in practice, you would load a pretrained model\nmodel_config = MistralConfig(\n    # https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json\n    **{\n        \"architectures\": [\"MistralForCausalLM\"],\n        \"bos_token_id\": 1,\n        \"eos_token_id\": 2,\n        \"hidden_act\": \"silu\",\n        \"hidden_size\": 4096,\n        \"initializer_range\": 0.02,\n        \"intermediate_size\": 14336,\n        \"max_position_embeddings\": 32768,\n        \"model_type\": \"mistral\",\n        \"num_attention_heads\": 32,\n        \"num_hidden_layers\": 32,\n        \"num_key_value_heads\": 8,\n        \"rms_norm_eps\": 1e-05,\n        \"rope_theta\": 10000.0,\n        \"sliding_window\": 4096,\n        \"tie_word_embeddings\": False,\n        \"torch_dtype\": \"bfloat16\",\n        \"transformers_version\": \"4.34.0.dev0\",\n        \"use_cache\": True,\n        \"vocab_size\": 32000,\n    }\n)\nprint('creating model')\nmodel: MistralForCausalLM = AutoModelForCausalLM.from_config(model_config)\n\n# Initialize the algorithm with layer indices\nalgorithm = DepthUpscalingAlgorithm(layer_indices=[\"range(0,24)\", \"range(8,32)\"])\nprint('upscaling model')\nupscaled_model = algorithm.run(model.model.layers)\n\n# substitute the model with the upscaled model\nmodel.model.layers = upscaled_model\n</code></pre>"},{"location":"algorithms/depth_upscaling/#cli-usage","title":"CLI Usage","text":"<p>The <code>DepthUpscalingAlgorithm</code> is integrated into the <code>fusion_bench</code> package. You can use it by specifying <code>\"depth_upscaling\"</code> as the method name in the command line or configuration file.</p> config/method/depth_upscaling.yaml<pre><code># =============================================================================\n# FusionBench Method Configuration: Depth Upscaling\n# =============================================================================\n# Constructs a deeper model by stacking/selecting layers from existing models.\n#\n# - layer_indices: list[int | str] specifying which layers to use. Strings are Python\n#   expressions evaluated to lists, e.g., \"range(6,12)\".\n# - Example: [0, 2, 4, \"range(6,12)\"] selects 1st, 3rd, 5th, and 7th-12th layers.\n# =============================================================================\n_target_: DepthUpscalingAlgorithm\n# this should be a list of integers or string, indicating the sequence of layers. \n# If the entry is an integer, it will use the n-th layer of the model. \n# If the entry is a string, it will use the layers specified by the string. \n# The string should be a valid python expression that evaluates to a list of integers.\n# for example, [\"range(0,12)\", \"range(6,12)\"] will use the first 12 layers and the last 6 layers of the model to construct the new model\n# [0, 2, 4, \"range(6,12)\"] will use the 1st, 3rd, 5th, and the 7th to 12th layers of the model to construct the new model\nlayer_indices: null\n</code></pre> <p>You can then run the <code>fusion_bench</code> command with the specified configuration file:</p> <pre><code>fusion_bench method=depth_upscaling ...\n</code></pre>"},{"location":"algorithms/depth_upscaling/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.depth_upscaling.DepthUpscalingAlgorithm</li> </ul> <ol> <li> <p>SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling \u21a9\u21a9</p> </li> </ol>"},{"location":"algorithms/dummy/","title":"Dummy Algorithm","text":"<p>The Dummy Algorithm is a simple algorithm that does not perform any fusion operation. Instead, it returns a pretrained model if one is available in the model pool. If no pretrained model is available, it returns the first model in the model pool. This algorithm is useful for testing and debugging purposes, as it allows you to quickly check if the model pool is set up correctly and the fusion process is working as expected.</p>"},{"location":"algorithms/dummy/#usage","title":"Usage","text":"<p>To use the Dummy Algorithm, you need to specify <code>\"dummy\"</code> as the algorithm name.</p> <pre><code>fusion_bench method=dummy ...\n</code></pre>"},{"location":"algorithms/dummy/#implementation-details","title":"Implementation Details","text":"<p>The implementation of the Dummy Algorithm is straightforward. Here is the main method of the <code>DummyAlgorithm</code> class:</p> <ul> <li>fusion_bench.method.dummy.DummyAlgorithm</li> </ul>"},{"location":"algorithms/fisher_merging/","title":"(Diagonal) Fisher Merging","text":"<p>Fisher merging <sup>1</sup> is a parameter-weighted averaging method that assigns weights to model parameters based on the Fisher information matrix computed on labeled data. This approach allows for more informed model combination by considering the importance of each parameter as indicated by the Fisher information.</p>"},{"location":"algorithms/fisher_merging/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>The Fisher information matrix \\(F_\\theta\\) of a model with parameters \\(\\theta\\) is defined as:</p> \\[ F_\\theta = \\mathbb{E}_{x \\sim p(x)} \\left[ \\nabla_\\theta \\log p(y|x, \\theta) \\nabla_\\theta \\log p(y|x, \\theta)^T \\right] \\] <p>where:</p> <ul> <li>\\(p(x)\\) is the data distribution</li> <li>\\(p(y|x, \\theta)\\) is the model's output distribution (e.g., softmax output for classification)</li> <li>\\(\\nabla_\\theta\\) is the gradient with respect to the model's parameters \\(\\theta\\)</li> </ul> <p>The Fisher information matrix quantifies the importance of each parameter and can estimate task similarity, making it valuable for auxiliary-task learning and multi-task learning scenarios <sup>2</sup>.</p>"},{"location":"algorithms/fisher_merging/#diagonal-fisher-approximation","title":"Diagonal Fisher Approximation","text":"<p>Since the full Fisher information matrix is computationally expensive and memory-intensive, we use the diagonal Fisher information matrix approximation:</p> \\[ \\hat{F}_\\theta = \\mathbb{E}_{x \\sim p(x)} \\left[ \\left(\\nabla_\\theta \\log p(y|x, \\theta)\\right)^2 \\right] \\] <p>Given \\(n\\) models with parameters \\(\\theta_i\\) and diagonal Fisher information matrices \\(\\hat{F}_{\\theta_i}\\), the Fisher merging algorithm computes the merged model's parameters as:</p> \\[ \\theta^{(j)} = \\frac{\\sum_{i=1}^{n} \\hat{F}_{\\theta_i}^{(j)} \\theta_i^{(j)}}{\\sum_{i=1}^{n} \\hat{F}_{\\theta_i}^{(j)}} \\] <p>where \\(j\\) indexes individual parameters. This creates a per-parameter weighted average where weights are determined by the Fisher information of each parameter.</p>"},{"location":"algorithms/fisher_merging/#examples","title":"Examples","text":""},{"location":"algorithms/fisher_merging/#cli-usage","title":"CLI Usage","text":""},{"location":"algorithms/fisher_merging/#clip-vision-model-fisher-merging","title":"CLIP Vision Model Fisher Merging","text":"<p>Configuration template for CLIP Fisher merging:</p> config/method/fisher_merging/clip_fisher_merging.yaml<pre><code>_target_: fusion_bench.method.FisherMergingForCLIPVisionModel\n# this should be a list of strings, regular expressions that match the names of the parameters that should be excluded from the fisher merging\nexclude_param_names_regex: []\n# boolean, whether to normalize fisher weights (L2 norm) or not\nnormalize_fisher_weight: true\n# float, the minimal value in fisher weights, used for tackling the potential numerical issues\nminimal_fisher_weight: 1e-6\n# common choices: 256, 512, 1024, 2048\nnum_fisher_examples: 256\ndataloader_kwargs:\n  batch_size: 32\n  num_workers: 0\n</code></pre> <p>Example merging eight CLIP-ViT-B/32 models:</p> <pre><code>fusion_bench method=fisher_merging/clip_fisher_merging \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>Merge eight CLIP-ViT-L/14 models with custom batch settings:</p> <pre><code>fusion_bench \\\n  method=fisher_merging/clip_fisher_merging \\\n    method.dataloader_kwargs.batch_size=8 \\\n    method.dataloader_kwargs.num_workers=4 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    taskpool.clip_model=openai/clip-vit-large-patch14\n</code></pre>"},{"location":"algorithms/fisher_merging/#gpt-2-fisher-merging","title":"GPT-2 Fisher Merging","text":"<p>Configuration template for GPT-2 Fisher merging:</p> config/method/fisher_merging/gpt2_fisher_merging.yaml<pre><code>_target_: fusion_bench.method.FisherMergingAlgorithmForGPT2\n# this should be a list of strings, regular expressions that match the names of the parameters that should be excluded from the fisher merging\nexclude_param_names_regex: []\n# boolean, whether to normalize fisher weights (L2 norm) or not\nnormalize_fisher_weight: true\n# float, the minimal value in fisher weights, used for tackling the potential numerical issues\nminimal_fisher_weight: 1e-6\n# common choices: 256, 512, 1024, 2048\nnum_fisher_examples: 256\ncache_dir: outputs\nbatch_size: 32\nnum_workers: 0\n</code></pre> <p>Example merging GPT-2 models for text classification:</p> <pre><code>fusion_bench \\\n  method=fisher_merging/gpt2_fisher_merging \\\n    method.num_fisher_examples=512 \\\n    method.batch_size=8 \\\n    method.num_workers=2 \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\n</code></pre>"},{"location":"algorithms/fisher_merging/#api-usage","title":"API Usage","text":""},{"location":"algorithms/fisher_merging/#clip-fisher-merging","title":"CLIP Fisher Merging","text":"<pre><code>from fusion_bench.method.fisher_merging.clip_fisher_merging import FisherMergingForCLIPVisionModel\n\nalgorithm = FisherMergingForCLIPVisionModel(\n    exclude_param_names_regex=[],\n    normalize_fisher_weight=True,\n    minimal_fisher_weight=1e-6,\n    num_fisher_examples=256,\n    dataloader_kwargs={\n        \"batch_size\": 32,\n        \"num_workers\": 4\n    },\n)\n\nmerged_model = algorithm.run(modelpool)\n</code></pre>"},{"location":"algorithms/fisher_merging/#gpt-2-fisher-merging_1","title":"GPT-2 Fisher Merging","text":"<pre><code>from fusion_bench.method.fisher_merging.gpt2_fisher_merging import FisherMergingAlgorithmForGPT2\n\nalgorithm = FisherMergingAlgorithmForGPT2(\n    exclude_param_names_regex=[],\n    normalize_fisher_weight=True,\n    minimal_fisher_weight=1e-6,\n    num_fisher_examples=256,\n    cache_dir=\"outputs\",\n    batch_size=32,\n    num_workers=0\n)\n\nmerged_model = algorithm.run(modelpool)\n</code></pre>"},{"location":"algorithms/fisher_merging/#implementation-details","title":"Implementation Details","text":"<ul> <li><code>fusion_bench.method.fisher_merging.FisherMergingAlgorithm</code>: Base Fisher merging implementation</li> <li><code>fusion_bench.method.fisher_merging.clip_fisher_merging.FisherMergingForCLIPVisionModel</code>: CLIP vision model specialization</li> <li><code>fusion_bench.method.fisher_merging.gpt2_fisher_merging.FisherMergingAlgorithmForGPT2</code>: GPT-2 text classification specialization</li> </ul> <ol> <li> <p>M. Matena, C. Raffel. \"Merging Models with Fisher-Weighted Averaging\" http://arxiv.org/abs/2111.09832\u00a0\u21a9</p> </li> <li> <p>C. Wu, et al. \"Pi-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation\". https://github.com/TencentARC/pi-Tuning\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/isotropic_merging/","title":"Isotropic Merging","text":""},{"location":"algorithms/isotropic_merging/#examples","title":"Examples","text":""},{"location":"algorithms/isotropic_merging/#cli-usage","title":"CLI Usage","text":"<p>Merge CLIP-ViT-B/32 models on eight image classification tasks using ISO-C, with a scaling factor of 1.5:</p> <pre><code>fusion_bench \\\n    path.log_dir=outputs/ViT-B-32/iso_c \\\n    method=isotropic_merging/iso_c \\\n    method.scaling_factor=1.5 \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>Merge CLIP-ViT-B/32 models on eight image classification tasks using ISO-CTS, with a scaling factor of 1.5:</p> <pre><code>fusion_bench \\\n    path.log_dir=outputs/ViT-B-32/iso_cts \\\n    method=isotropic_merging/iso_cts \\\n    method.scaling_factor=1.5 \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>Merge CLIP-ViT-L/14 models on eight image classification tasks using ISO-C, with a scaling factor of 1.5:</p> <pre><code>fusion_bench \\\n    path.log_dir=outputs/ViT-L-14/iso_c \\\n    method=isotropic_merging/iso_c \\\n    method.scaling_factor=1.5 \\\n    modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14 \\\n    taskpool.base_model=openai/clip-vit-large-patch14\n</code></pre>"},{"location":"algorithms/isotropic_merging/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.ISO_C_Merge</li> <li>fusion_bench.method.ISO_CTS_Merge</li> </ul>"},{"location":"algorithms/layer_recombination/","title":"Layer Recombination","text":""},{"location":"algorithms/max-model_predictor/","title":"Max-Model Predictor","text":"<p>The max-model predictor algorithm is a type of ensemble method. Formally, a max-model predictor is defined as follows:</p> <p>Definition (Max-Model Predictor) <sup>1</sup> Given a set of predictors \\(H = \\{h_1, h_2, \\ldots, h_n\\}\\), with \\(h_i: \\mathcal{X} \\times \\mathcal{Y}_i \\mapsto \\mathbb{R}\\), the max-model predictor \\(h_H\\) is defined as:</p> \\[h_H(x,y) = \\max_{h_i\\in H} h_i(x,y).\\] <p>Take the flu detection problem as an example <sup>1</sup>.  Doctors want to build a learning model to detect what type of virus one patient is affected based on her symptoms, for appropriate treatment. However, the types of influenza diverse geographically (Rejmanek et al., 2015), which means the distribution of patient records collected by a hospital in California may be different from those in Florida. In an extreme case, some types are unknown to the other hospital. Assume there are 4 types of influenza in the United States. In California, 2 of 4 are commonly detected, while in Florida 3 of 4 types are often detected. We assume in the two states, doctors separately trained two models \\(h_{CA}\\) and \\(h_{FL}\\) which work locally well in California and Florida respectively. However, a direct ensemble of the two local models may not work well on all the patients. Let \\(h_{US}\\) denote the ideal global model trained on the combination of local datasets. When we input a patient record \\(x\\), each model outputs its prediction as shown in the following table:</p> <p>Table: Example of flu detection on a patient \\(x\\) affected with type 2 flu. Where \u201c\u2212\u201d means this model is not able to predict the corresponding class.  Taking the maximal score as prediction, \\(h_{FL}\\) is consistent with \\(h_{US}\\), but the combination of two local models \\(h_{CA,FL}\\) is not since \\(3/4 &gt; 4/7\\).</p> Type 1 2 3 4 \\(h_{US}(x)\\) 2/10 4/10 1/10 3/10 \\(h_{CA}(x)\\) - - 1/4 3/4 \\(h_{FL}(x)\\) 2/7 4/7 1/7 - \\(h_{\\{CA,FL\\}}(x)\\) 2/7 4/7 1/4 3/4 The illustration of running our method on the flu example."},{"location":"algorithms/max-model_predictor/#example","title":"Example","text":""},{"location":"algorithms/max-model_predictor/#cli-usage","title":"CLI Usage","text":"<p>Configuration template for the Max Predictor Algorithm:</p> config/method/ensemble/max_model_predictor.yaml<pre><code># =============================================================================\n# FusionBench Method Configuration: Max Model Predictor\n# =============================================================================\n# Selects the model with maximum confidence or performance per example/task.\n# No additional hyperparameters are required.\n# =============================================================================\n_target_: fusion_bench.method.MaxModelPredictorAlgorithm\n</code></pre> <p>To create a max predictor ensemble of models for a specific task, you can use the following command:</p> <pre><code>fusion_bench method=ensemble/max_model_predictor \\\n  modelpool=&lt;modelpool_name&gt; \\\n  taskpool=&lt;taskpool_name&gt;\n</code></pre>"},{"location":"algorithms/max-model_predictor/#api-usage","title":"API Usage","text":"<p>Here is an example of how to use the Max-Model Predictor Algorithm:</p> <pre><code>from fusion_bench.method import MaxModelPredictorAlgorithm\nfrom fusion_bench.modelpool import BaseModelPool\n\n# Instantiate the MaxPredictorAlgorithm\nalgorithm = MaxModelPredictorAlgorithm()\n\n# Assume we have a ModelPool instance that contains the models we want to ensemble.\nmodelpool = BaseModelPool(...) # or a list of nn.Module\n\n# Run the algorithm on the model pool.\nmax_model_predictor = algorithm.run(modelpool)\n</code></pre>"},{"location":"algorithms/max-model_predictor/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.MaxModelPredictorAlgorithm</li> </ul> <ol> <li> <p>Zhu et.al. ICML 2019. Heterogeneous model reuse via optimizing multiparty multiclass margin\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"algorithms/model_recombination/","title":"Model Recombination","text":"Credit to FedMR <p>Model recombination is a technique that shuffles the layers of different models to create new combinations.  This method randomly redistributes the parameters across models at the layer level, which can help discover new model configurations and potentially improve performance.</p>"},{"location":"algorithms/model_recombination/#examples","title":"Examples","text":""},{"location":"algorithms/model_recombination/#api-usage","title":"API Usage","text":"<p>Here's an example demonstrating how to recombine models:</p> <pre><code>import torch\nfrom torch import nn\nfrom fusion_bench.method import ModelRecombinationAlgorithm\nfrom fusion_bench.modelpool import BaseModelPool\n\n# Create sample models\nmodels = {\n    \"model_1\": nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 1)),\n    \"model_2\": nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 1)), \n    \"model_3\": nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 1)),\n}\n\n# Create model pool\nmodelpool = BaseModelPool(models)\n\n# Initialize the recombination algorithm\nrecombination_algorithm = ModelRecombinationAlgorithm(\n    return_modelpool=True  # Return the entire recombined model pool\n)\n\n# Run recombination\nrecombined_pool = recombination_algorithm.run(modelpool)\n\nprint(f\"Original pool size: {len(modelpool)}\")\nprint(f\"Recombined pool size: {len(recombined_pool)}\")\n</code></pre>"},{"location":"algorithms/model_recombination/#cli-usage","title":"CLI Usage","text":"<p>This section provides a guide on how to use the <code>fusion_bench</code> command-line interface for model recombination.</p>"},{"location":"algorithms/model_recombination/#configuration-files","title":"Configuration Files","text":"<p>Configuration template for the model recombination method:</p> config/method/model_recombination.yaml<pre><code># =============================================================================\n# FusionBench Method Configuration: Model Recombination\n# =============================================================================\n# Recombines submodules/layers from multiple models to form a new model.\n#\n# - return_modelpool: override run() argument to return model pool instead of merged model.\n#   Set to null to respect runtime argument; set to true/false to force behavior.\n# =============================================================================\n_target_: fusion_bench.method.ModelRecombinationAlgorithm\n# if `return_model_pool` is not null, the argument `return_modelpool` passed to the `run` method will be ignored.\nreturn_modelpool: null\n</code></pre> <p>Run the fusion_bench command with model recombination:</p> <pre><code>fusion_bench \\\n    method=model_recombination \\\n    method.return_modelpool=false \\\n    modelpool=your_modelpool_config \\\n    taskpool=dummy  # evaluates parameter counts of recombined models\n</code></pre> <p>You can also override the return_modelpool parameter via command line:</p> <pre><code>fusion_bench \\\n    method=model_recombination \\\n    method.return_modelpool=true \\\n    modelpool=your_modelpool_config \\\n    taskpool=your_taskpool_config\n</code></pre>"},{"location":"algorithms/model_recombination/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.ModelRecombinationAlgorithm</li> <li>fusion_bench.method.model_recombination.recombine_modellist</li> <li>fusion_bench.method.model_recombination.recombine_modeldict</li> <li>fusion_bench.method.model_recombination.recombine_state_dict</li> </ul>"},{"location":"algorithms/moe_based_merging/","title":"MoE-based Model Merging","text":"<p>MoE-based model merging is a technique that combines multiple fine-tuned dense models into a single Mixture-of-Experts (MoE) model. This approach leverages the specialization of different expert models by treating each fine-tuned model as an expert in the resulting MoE architecture. The method involves upscaling the architecture to MoE format and substituting the experts with the weights from different specialized models.</p>"},{"location":"algorithms/moe_based_merging/#examples","title":"Examples","text":""},{"location":"algorithms/moe_based_merging/#api-usage","title":"API Usage","text":""},{"location":"algorithms/moe_based_merging/#basic-example","title":"Basic Example","text":"<p>Here's an example demonstrating how to merge multiple fine-tuned models into a Mixtral MoE model:</p> <pre><code>from fusion_bench.method import (\n    MixtralForCausalLMMergingAlgorithm,\n    MixtralMoEMergingAlgorithm,\n)\nfrom fusion_bench.modelpool import CausalLMPool\nfrom fusion_bench.utils import print_parameters\n\n# Create a model pool with your fine-tuned expert models\nmodel_pool = CausalLMPool(\n    models={\n        \"_pretrained_\": \"path_to_base_model\",\n        \"expert_1\": \"path_to_finetuned_model_1\",\n        \"expert_2\": \"path_to_finetuned_model_2\",\n        \"expert_3\": \"path_to_finetuned_model_3\",\n        \"expert_4\": \"path_to_finetuned_model_4\",\n    },\n    tokenizer=\"path_to_base_model\",\n    model_kwargs={\"torch_dtype\": \"bfloat16\"},\n)\n\n\n# Initialize the merging algorithm with direct parameters\nmerging_algorithm = MixtralForCausalLMMergingAlgorithm(\n    experts_per_token=2,  # Number of experts to activate per token\n    save_checkpoint=None  # Optional: path to save the merged model\n)\n\n# Run the merging process to get a MoE model\nmoe_model = merging_algorithm.run(model_pool)\n\nprint(\"Merged MoE model:\")\nprint_parameters(moe_model)\n\n# Save the merged MoE model\nmoe_model.save_pretrained(\"path_to_save_moe_model\")\n</code></pre>"},{"location":"algorithms/moe_based_merging/#cli-usage","title":"CLI Usage","text":"<p>This section provides a guide on how to use the <code>fusion_bench</code> command-line interface to merge models using MoE-based merging.</p>"},{"location":"algorithms/moe_based_merging/#configuration-files","title":"Configuration Files","text":"<p>Configuration template for the MoE merging method:</p> config/method/mixtral_moe_merging.yaml<pre><code>name: mixtral_moe_upscaling # or \"mixtral_for_causal_lm_moe_upscaling\"\nexperts_per_token: 2\n# path to save the upscaled model\nsave_checkpoint: null\n</code></pre> <p>Configuration template for the model pool:</p> config/modelpool/CausalLMPool/mixtral_moe_merging.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: path_to_your_pretrained_model\n  expert_1: path_to_your_expert_model_1\n  expert_2: path_to_your_expert_model_2\n  expert_3: path_to_your_expert_model_3\n  expert_4: path_to_your_expert_model_4\ntokenizer: ${.models._pretrained_}\nmodel_kwargs:\n  torch_dtype: bfloat16\n</code></pre>"},{"location":"algorithms/moe_based_merging/#running-moe-merging","title":"Running MoE Merging","text":"<p>Run the fusion_bench command with MoE merging configuration:</p> <pre><code>fusion_bench \\\n    method=mixtral_moe_merging \\\n    modelpool=CausalLMPool/mixtral_moe_merging \\\n    taskpool=dummy # this evaluates parameter counts of the merged model\n</code></pre>"},{"location":"algorithms/moe_based_merging/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.MixtralMoEMergingAlgorithm</li> <li>fusion_bench.method.MixtralForCausalLMMergingAlgorithm</li> </ul>"},{"location":"algorithms/moe_based_upscaling/","title":"MoE-based Model Upscaling (Sparse Upcycling)","text":"<p>Sparse upcycling is a technique used to initialize a sparsely activated Mixture-of-Experts (MoE) model from a dense checkpoint. This approach leverages previously incurred training costs to improve the performance of large models while reducing the computational expense. In the process, dense Transformer blocks are partially replaced with MoE blocks, where the MLPs in a Transformer block are replaced by multiple experts. The experts are chosen based on routing probabilities determined by a router. The initialized MoE model is then further trained to recover the performance. This method results in improved performance for both language and vision models while using only a fraction of the original dense pretraining cost <sup>1</sup>.</p>"},{"location":"algorithms/moe_based_upscaling/#examples","title":"Examples","text":""},{"location":"algorithms/moe_based_upscaling/#basic-example","title":"Basic Example","text":"<p>Here's an example demonstrating how to upscale a pre-trained Mistral model to a Mixtral model:</p> <pre><code>import os\nfrom transformers import MistralForCausalLM\nfrom fusion_bench.method import (\n    MixtralForCausalLMUpscalingAlgorithm,\n)\nfrom fusion_bench.utils import print_parameters\n\n# Load a pre-trained Mistral model\npretrained_model = MistralForCausalLM.from_pretrained(\n    \"path_to_mistral_model\"  # Replace with actual model path\n)\nprint(\"Pretrained model:\")\nprint_parameters(pretrained_model)\n# Output:\n# Pretrained model:\n# trainable params: 7.24B || all params: 7.24B || trainable%: 100.0000\n\n# Initialize the upscaling algorithm with direct parameters\nupscaling_algorithm = MixtralForCausalLMUpscalingAlgorithm(\n    num_experts=4,  # Number of expert channels\n    experts_per_token=2,  # Experts to choose per token\n    save_checkpoint=None  # Optional: path to save the model\n)\n\n# Run the upscaling process to get a Mixtral model\nmixtral_model = upscaling_algorithm.run(pretrained_model)\n\nprint(\"Mixtral model:\")\nprint_parameters(mixtral_model)\n# Output:\n# Mixtral model:\n# trainable params: 24.15B || all params: 24.15B || trainable%: 100.0000\n\n# Save the upscaled Mixtral model\nmixtral_model.save_pretrained(\"path_to_save_mixtral_model\")\n</code></pre>"},{"location":"algorithms/moe_based_upscaling/#api-usage","title":"API Usage","text":""},{"location":"algorithms/moe_based_upscaling/#direct-model-upscaling","title":"Direct Model Upscaling","text":"<pre><code>from transformers import MistralForCausalLM\nfrom fusion_bench.method.mixture_of_experts.mixtral_upcycling import (\n    MixtralForCausalLMUpscalingAlgorithm,\n    MixtralUpscalingAlgorithm,\n)\n\n# Load source model\nmodel = MistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n\n# For CausalLM models (includes lm_head)\ncausal_lm_algorithm = MixtralForCausalLMUpscalingAlgorithm(\n    num_experts=8,\n    experts_per_token=2,\n    save_checkpoint=\"./mixtral-8x7b\"\n)\nmixtral_causal_lm = causal_lm_algorithm.run(model)\n</code></pre>"},{"location":"algorithms/moe_based_upscaling/#using-modelpool","title":"Using ModelPool","text":"<pre><code>from fusion_bench import BaseModelPool\n\n# Create a model pool\nmodel_dict = {\"_pretrained_\": model}\nmodelpool = BaseModelPool(model_dict)\n\n# Run upscaling with modelpool\nmixtral_model = upscaling_algorithm.run(modelpool)\n</code></pre> <p>A Jupyter notebook example is also available at our repo.</p>"},{"location":"algorithms/moe_based_upscaling/#cli-usage","title":"CLI Usage","text":"<p>This section provides a guide on how to use the <code>fusion_bench</code> command-line interface to upscale a Mistral model to a Mixtral model.</p>"},{"location":"algorithms/moe_based_upscaling/#configuration-files","title":"Configuration Files","text":"<p>Configuration template for the MoE upscaling method:</p> config/method/mixtral_moe_upscaling.yaml<pre><code># or fusion_bench.method.MixtralUpscalingAlgorithm\n_target_: fusion_bench.method.MixtralForCausalLMUpscalingAlgorithm\nnum_experts: 4\nexperts_per_token: 2\n# path to save the upscaled model\nsave_checkpoint: null\n</code></pre> <p>Configuration template for the model pool:</p> config/modelpool/CausalLMPool/mistral-7b.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: mistralai/Mistral-7B-v0.1\ntokenizer: ${.models._pretrained_}\nmodel_kwargs:\n  torch_dtype: bfloat16\n</code></pre>"},{"location":"algorithms/moe_based_upscaling/#cli-commands","title":"CLI Commands","text":"<pre><code>fusion_bench \\\n    method=mixtral_moe_upscaling \\\n    modelpool=CausalLMPool/mistral-7b \\\n        modelpool.models._pretrained_=path_to_your_pretrained_model \\\n    taskpool=dummy # this is a dummy taskpool that does nothing but print the parameter counts of the upscaled model\n</code></pre>"},{"location":"algorithms/moe_based_upscaling/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.MixtralUpscalingAlgorithm</li> <li>fusion_bench.method.MixtralForCausalLMUpscalingAlgorithm</li> </ul> <ol> <li> <p>Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints. http://arxiv.org/abs/2212.05055\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/pwe_moe/","title":"PWEMoE: Pareto-Driven Weight-Ensembling Mixture of Experts","text":"Overview of PWE MoE     (a) An illustration of Pareto front learning in MOOP. Where \\(P_1\\) and \\(P_2\\) are performance metrics for two tasks, colored lines represent different Pareto optimal solutions, and the solid black line represents the Pareto front.     (b) An overview of the model up-scaling process.     We upcycle the MLP modules to MoE modules and merge the remaining parts using task arithmetic.     (c) The MoE module, comprising a routing network and a parameter decoder network.     The routing network accepts a user preference vector and generates routing weights for weight-ensembling. <p>Abstract</p> <p>Solving multi-objective optimization problems for large deep neural networks is a challenging task due to the complexity of the loss landscape and the expensive computational cost of training and evaluating models. Efficient Pareto front approximation of large models enables multi-objective optimization for various tasks such as multi-task learning and trade-off analysis. Existing algorithms for learning Pareto set, including (1) evolutionary, hypernetworks, and hypervolume-maximization methods, are computationally expensive and have restricted scalability to large models; (2) Scalarization algorithms, where a separate model is trained for each objective ray, which is inefficient for learning the entire Pareto set and fails to capture the objective trade-offs effectively. Inspired by the recent success of model merging, we propose a practical and scalable approach to Pareto set learning problem via mixture of experts (MoE) based model fusion. By ensembling the weights of specialized single-task models, the MoE module can effectively capture the trade-offs between multiple objectives and closely approximate the entire Pareto set of large neural networks. Once the routers are learned and a preference vector is set, the MoE module can be unloaded, thus no additional computational cost is introduced during inference. We conduct extensive experiments on vision and language tasks using large-scale models such as CLIP-ViT and GPT-2. The experimental results demonstrate that our method efficiently approximates the entire Pareto front of large models. Using only hundreds of trainable parameters of the MoE routers, our method even has lower memory usage compared to linear scalarization and algorithms that learn a single Pareto optimal solution, and are scalable to both the number of objectives and the size of the model. Our method significantly reduces the computational burden of learning the Pareto set, for example, in the two-task case, it can be achieved in just a few minutes. Code is available at: GitHub .</p>"},{"location":"algorithms/pwe_moe/#examples","title":"Examples","text":"<p>Not tested yet</p> <p>The examples provided below have not been tested yet.</p> <p>For a thoroughly tested and verified implementation of the algorithm, please refer to the original repository: tanganke/pareto_set_learning .  Additionally, the experimental results and further insights into the algorithm can be found in the original research paper: arXiv:2406.09770 .</p> <p>PWEMoE-LS on eight image classification tasks using CLIP-ViT-B/32 models, and the results are logged to <code>outputs/ViT-B-32/PWEMoE-LS-8tasks</code>.</p> <pre><code>fusion_bench \\\n    path.log_dir=outputs/ViT-B-32/PWEMoE-LS-8tasks \\\n    method=pwe_moe/pwe_moe_ls_for_clip \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre>"},{"location":"algorithms/pwe_moe/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoEAlgorithmForCLIP</li> <li>fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoELinearScalarizationForCLIP</li> <li>fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoExactParetoOptimalForCLIP</li> </ul>"},{"location":"algorithms/regmean/","title":"RegMean","text":"<p>RegMean (Regression Mean) is a dataless knowledge fusion approach that formulates model merging as a linear regression problem<sup>1</sup>. The algorithm aims to find optimal weights for each linear layer in the merged model by minimizing the discrepancy in predictions between the merge and candidate models.</p>"},{"location":"algorithms/regmean/#algorithm-overview","title":"Algorithm Overview","text":"<p>For a transformer layer \\(l\\), to obtain the merge weights for a linear layer \\(W^{(l)}_{M}\\), RegMean provides a precise closed-form solution for merging those from \\(K\\) candidate models:</p> \\[W^{(l)}_{M} = \\left[\\sum_{i=1}^{K}  (X^{(l)}_i)^{\\top} X^{(l)}_i\\right]^{-1} \\sum_{i=1}^{K} (X^{(l)}_i)^{\\top} X^{(l)}_i W^{(l)}_i\\] <p>where:</p> <ul> <li>\\(W^{(l)}_i\\) is the weight matrix of the \\(i\\)-th candidate model at layer \\(l\\)</li> <li>\\(X^{(l)}_i\\) represents the input activations to layer \\(l\\) for model \\(i\\)</li> <li>The formula computes a weighted combination that minimizes prediction discrepancy</li> </ul>"},{"location":"algorithms/regmean/#examples","title":"Examples","text":""},{"location":"algorithms/regmean/#cli-usage","title":"CLI Usage","text":"<p>Configuration templates for RegMean:</p> config/method/regmean/clip_regmean.yaml<pre><code>_target_: fusion_bench.method.RegMeanAlgorithmForCLIP\n# list, regular expression of names of parameters that need to be excluded\nexclude_param_names_regex: []\n# numbers of examples to compute regmean weights\nnum_regmean_examples: 256\nweight_transpose: true\n# float, reduce non-diagonal elements in regmean weights by multiplying this scalar\nreduce_non_diagonal_ratio: 0.95\ndataloader_kwargs:\n  batch_size: 32\n  num_workers: 0\n</code></pre> config/method/regmean/gpt2_regmean.yaml<pre><code>_target_: fusion_bench.method.RegMeanAlgorithmForGPT2\n# list, regular expression of names of parameters that need to be excluded\nexclude_param_names_regex: []\n# numbers of examples to compute regmean weights\nnum_regmean_examples: 256\n# float, reduce non-diagonal elements in regmean weights by multiplying this scalar\nreduce_non_diagonal_ratio: 0.6\nweight_transpose: false\ncache_dir: outputs\nbatch_size: 32\nnum_workers: 0\n</code></pre>"},{"location":"algorithms/regmean/#clip-models","title":"CLIP Models","text":"<p>Merge CLIP-ViT-B/32 models on eight image classification tasks:</p> <pre><code>fusion_bench method=regmean/clip_regmean \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>Merge CLIP-ViT-L/14 models on eight image classification tasks:</p> <pre><code>fusion_bench \\\n  method=regmean/clip_regmean \\\n    method.dataloader_kwargs.batch_size=8 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    taskpool.base_model=openai/clip-vit-large-patch14\n</code></pre>"},{"location":"algorithms/regmean/#language-models","title":"Language Models","text":"<p>Merge GPT-2 models for text classification tasks:</p> <pre><code>fusion_bench \\\n  method=regmean/gpt2_regmean \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\n</code></pre>"},{"location":"algorithms/regmean/#api-usage","title":"API Usage","text":"<p>To use RegMean programmatically:</p>"},{"location":"algorithms/regmean/#clip-models_1","title":"CLIP Models","text":"<pre><code>from fusion_bench.method.regmean import RegMeanAlgorithmForCLIP\nfrom omegaconf import DictConfig\n\n# Configuration for CLIP RegMean\nconfig = DictConfig({\n    'exclude_param_names_regex': [],\n    'num_regmean_examples': 256,\n    'weight_transpose': True,\n    'reduce_non_diagonal_ratio': 0.95,\n    'dataloader_kwargs': {\n        'batch_size': 32,\n        'num_workers': 0\n    }\n})\n\n# Initialize the algorithm\nalgorithm = RegMeanAlgorithmForCLIP(**config)\n\n# Run the algorithm with a model pool\nmerged_model = algorithm.run(modelpool)\n</code></pre>"},{"location":"algorithms/regmean/#gpt-2-models","title":"GPT-2 Models","text":"<pre><code>from fusion_bench.method.regmean import RegMeanAlgorithmForGPT2\nfrom omegaconf import DictConfig\n\n# Configuration for GPT-2 RegMean\nconfig = DictConfig({\n    'exclude_param_names_regex': [],\n    'num_regmean_examples': 256,\n    'reduce_non_diagonal_ratio': 0.6,\n    'weight_transpose': False,\n    'cache_dir': 'outputs',\n    'batch_size': 32,\n    'num_workers': 0\n})\n\n# Initialize the algorithm\nalgorithm = RegMeanAlgorithmForGPT2(**config)\n\n# Run the algorithm\nmerged_model = algorithm.run(modelpool)\n</code></pre>"},{"location":"algorithms/regmean/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.RegMeanAlgorithmForCLIP</li> <li>fusion_bench.method.RegMeanAlgorithmForGPT2</li> </ul> <ol> <li> <p>Xisen Jin, et al. \"Dataless Knowledge Fusion by Merging Weights of Language Models.\" http://arxiv.org/abs/2212.09849\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/regmean_plusplus/","title":"RegMean++","text":""},{"location":"algorithms/regmean_plusplus/#revisiting-the-regmean-algorithm","title":"Revisiting the RegMean Algorithm","text":"<p>Regression Mean (RegMean)<sup>1</sup>, an approach that formulates model merging as a linear regression problem, aims to find the optimal weights for each linear layer in the merge model by minimizing the discrepancy in predictions between the merge and candidate models. At a transformer layer \\(l\\), to obtain the merge weights for a linear layer \\(W^{(l)}_{M}\\) , RegMean provides a precise closed-form solution for merging those from \\(K\\) candidate models as follows:</p> \\[W^{(l)}_{M} = \\left[\\sum_{i=1}^{K}  (X^{(l)}_i)^{\\top} X^{(l)}_i\\right]^{-1} \\sum_{i=1}^{K} (X^{(l)}_i)^{\\top} X^{(l)}_i W^{(l)}_i.\\]"},{"location":"algorithms/regmean_plusplus/#problem-of-regmean-and-how-regmean-addresses-it","title":"Problem of RegMean and How RegMean++ Addresses It","text":"<p>RegMean merges each linear layer independently, overlooking how the features and information in the earlier layers propagate through the layers and influence the final prediction in the merge model. To address this, RegMean++<sup>2</sup> is proposed to explicitly incorporate both intra- and cross-layer dependencies between merge models' layers into RegMean's objective.</p> Comparison between RegMean and RegMean++ for model merging. RegMean++ leverages representations from the merge model for merging, enabling accurate alignment with its behavior. <p>The key difference between RegMean++ and RegMean lies in how input feature \\(X^{(l,j)}_i\\) for the \\(j\\)-th linear layer is obtained: For input features that are activations (cushion representations between transformer layers), RegMean++ computes \\(X^{(l,j)}_i\\) based on the activations produced by the previous merge layer \\(f_{M}^{(l-1)}\\) in the merge model, that is, \\(X^{(l)}_i = f_{M}^{(l-1)}(X^{(l-1)}_{i})\\) while RegMean relies on the activations produced by the previous candidate layer \\(f_{i}^{(l-1)}\\) in the candidate model, that is, \\(X^{(l)}_i = f_{i}^{(l-1)}(X^{(l-1)}_{i})\\).</p>"},{"location":"algorithms/regmean_plusplus/#examples","title":"Examples","text":""},{"location":"algorithms/regmean_plusplus/#cli-usage","title":"CLI Usage","text":"<p>The following command lines can be used to run and evaluate the RegMean++ algorithm on eight image classification tasks:</p> <ul> <li> <p>For CLIP-ViT-B/32 models: <pre><code>fusion_bench \\\n    method=regmean_plusplus/clip_regmean_plusplus \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n        taskpool.base_model=openai/clip-vit-base-patch32\n</code></pre></p> </li> <li> <p>For CLIP-ViT-B/16 models: <pre><code>fusion_bench \\\n    method=regmean_plusplus/clip_regmean_plusplus \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch16_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n        taskpool.base_model=openai/clip-vit-base-patch16\n</code></pre></p> </li> <li> <p>For CLIP-ViT-L/14 models: <pre><code>fusion_bench \\\n    method=regmean_plusplus/clip_regmean_plusplus \\\n    modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n        taskpool.base_model=openai/clip-vit-large-patch14\n</code></pre></p> </li> </ul>"},{"location":"algorithms/regmean_plusplus/#citation","title":"Citation","text":"<pre><code>@article{nguyen2025regmean++,\n  title={RegMean++: Enhancing Effectiveness and Generalization of Regression Mean for Model Merging},\n  author={Nguyen, The-Hai and Huu-Tien, Dang and Suzuki, Takeshi and Nguyen, Le-Minh},\n  journal={arXiv preprint arXiv:2508.03121},\n  year={2025}\n}\n</code></pre>"},{"location":"algorithms/regmean_plusplus/#implementation-details","title":"Implementation Details","text":"<ul> <li>RegMeanAlgorithmPlusPlus</li> <li>RegMeanAlgorithmForCLIPPlusPlus</li> </ul> <ol> <li> <p>Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. \"Dataless Knowledge Fusion by Merging Weights of Language Models.\" The Eleventh International Conference on Learning Representations.\u00a0\u21a9</p> </li> <li> <p>The-Hai Nguyen, Huu-Tien Dang, Takeshi Suzuki, and Le-Minh Nguyen. \"RegMean++: Enhancing Effectiveness and Generalization of Regression Mean for Model Merging\". arXiv preprint arXiv:2508.03121 (2025).\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/simple_averaging/","title":"Simple Averaging","text":"<p>Simple averaging, also known as isotropic merging or ModelSoups, aims to yield a more robust and generalizable model by combining multiple models of the same architecture.</p> <p>Simple averaging is a technique frequently employed when there are multiple models that have been fine-tuned or independently trained from scratch. Specifically, if we possess \\(n\\) models that share a common architecture but different weights denoted as \\(\\theta_i\\), the weights of the merged model, represented as \\(\\theta\\), are computed as follows:</p> \\[ \\theta = \\frac{1}{n} \\sum_{i=1}^{n} \\theta_i \\] <p>This equation simply states that each weight of the final model is the average of the corresponding weights in the individual models. For example, if we have three models and the weight of the first neuron in the first layer is 0.1, 0.2, and 0.3 in each model respectively, the weight of that neuron in the final model will be (0.1 + 0.2 + 0.3) / 3 = 0.2.</p> <p>Simple averaging is a straightforward and scalable method for model fusion. It does not require any additional training or fine-tuning, making it a good choice when computational resources are limited, where maintaining an ensemble of models is not feasible.</p> <p>This method often assumes that all models are equally good.  If some models are significantly better than others, it might be beneficial to assign more weight to the better models when averaging.  This can be done by using weighted averaging, where each model's contribution to the final model is weighted by its performance on a validation set or some other metric. See Weighed Averaging for more details. Otherwise, the poor model may have a negative impact on the merged model.</p>"},{"location":"algorithms/simple_averaging/#examples","title":"Examples","text":""},{"location":"algorithms/simple_averaging/#cli-usage","title":"CLI Usage","text":"<p>Configuration template for the standard Simple Averaging algorithm:</p> config/method/simple_average.yaml<pre><code># =============================================================================\n# FusionBench Method Configuration: Simple Average\n# =============================================================================\n# Equally averages parameters of all models in the model pool.\n#\n# Usage notes\n# - No hyperparameters required; behavior is deterministic given model order.\n# - Ensure models are architecture-compatible (same shapes) before merging.\n# =============================================================================\n_target_: fusion_bench.method.SimpleAverageAlgorithm\n</code></pre> <p>Use the following command to run the standard Simple Averaging algorithm:</p> <pre><code>fusion_bench method=simple_average ...\n</code></pre>"},{"location":"algorithms/simple_averaging/#api-usage","title":"API Usage","text":""},{"location":"algorithms/simple_averaging/#algorithm-class","title":"Algorithm Class","text":"<p>In this example, we demonstrate how to use the <code>SimpleAverageAlgorithm</code> class:</p> <pre><code>from fusion_bench.method.simple_average import SimpleAverageAlgorithm\n\n# Instantiate the SimpleAverageAlgorithm\nalgorithm = SimpleAverageAlgorithm()\n\n# Assume we have a model pool with multiple models of the same architecture\nmodelpool = ...  # BaseModelPool instance\n\n# Run the algorithm on the model pool\n# Returns a new model with averaged parameters\nmerged_model = algorithm.run(modelpool)\n</code></pre>"},{"location":"algorithms/simple_averaging/#low-level-function-usage","title":"Low-level Function Usage","text":"<p>You can also use the low-level <code>simple_average</code> function directly:</p> <pre><code>from fusion_bench.method.simple_average import simple_average\n\n# For a list of models\nmodels = [model1, model2, model3]\naveraged_model = simple_average(models)\n\n# For a list of state dictionaries\nstate_dicts = [model1.state_dict(), model2.state_dict(), model3.state_dict()]\naveraged_state_dict = simple_average(state_dicts)\n</code></pre>"},{"location":"algorithms/simple_averaging/#variants","title":"Variants","text":""},{"location":"algorithms/simple_averaging/#standard-simple-averaging","title":"Standard Simple Averaging","text":"<p>The basic implementation (<code>SimpleAverageAlgorithm</code>) directly averages model parameters without any modifications.</p>"},{"location":"algorithms/simple_averaging/#dare-simple-averaging","title":"DARE Simple Averaging","text":"<p>A variant that incorporates DARE (Drop And REscale) techniques for improved performance.</p> <ul> <li>Sparsity-aware merging: Applies random dropping to parameters before averaging</li> <li>Rescaling: Optionally rescales remaining parameters after dropping to maintain magnitude</li> </ul> <p>The DARE variant is particularly useful when dealing with fine-tuned models that may have redundant or conflicting parameters.</p>"},{"location":"algorithms/simple_averaging/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.simple_average.SimpleAverageAlgorithm</li> <li>fusion_bench.method.simple_average.simple_average</li> </ul>"},{"location":"algorithms/simple_ensemble/","title":"Simple Ensemble","text":"<p>Ensemble methods are simple and effective ways to improve the performance of machine learning models.  These methods combine the outputs of multiple models to create a stronger model.  A simple ensemble takes the average of the predictions from multiple models without any weighting.</p> <p>Formally, given a set of \\(n\\) models, each model \\(f_i\\) produces a prediction \\(f_i(x)\\) for an input \\(x\\). The final prediction \\(F(x)\\) of the simple ensemble is the unweighted average of the individual model predictions:</p> \\[ F(x) = \\frac{1}{n} \\sum_{i=1}^n f_i(x) \\] <p>This approach assumes that all models contribute equally to the final prediction and is particularly effective when the individual models have similar performance levels. </p>"},{"location":"algorithms/simple_ensemble/#examples","title":"Examples","text":""},{"location":"algorithms/simple_ensemble/#cli-usage","title":"CLI Usage","text":"<p>Configuration template for the ensemble algorithm:</p> config/method/ensemble/simple_ensemble.yaml<pre><code># =============================================================================\n# FusionBench Method Configuration: Simple Ensemble\n# =============================================================================\n# Averages model predictions uniformly.\n#\n# device_map: leave null for single device or provide a mapping for multi-device setups.\n# =============================================================================\n_target_: fusion_bench.method.SimpleEnsembleAlgorithm\ndevice_map: null # Set to null for single device, or specify mapping\n</code></pre> <p>create a simple ensemble of CLIP-ViT models for image classification tasks.</p> <pre><code>fusion_bench \\\n  method=ensemble/simple_ensemble \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \n</code></pre>"},{"location":"algorithms/simple_ensemble/#api-usage","title":"API Usage","text":"<p>The following Python code snippet demonstrates how to use the <code>SimpleEnsembleAlgorithm</code> class from the <code>fusion_bench.method</code> module to create a simple ensemble of PyTorch models.</p> <pre><code>from fusion_bench.method import SimpleEnsembleAlgorithm\n\n# Instantiate the SimpleEnsembleAlgorithm\nalgorithm = SimpleEnsembleAlgorithm()\n\n# Assume we have a list of PyTorch models (nn.Module instances) or a modelpool that we want to ensemble.\nmodels = [...]\n\n# Run the algorithm on the modelpool or models.\nensemble_model = algorithm.run(modelpool)  # or algorithm.run(models)\n</code></pre> <p>Here's a step-by-step explanation:</p> <ol> <li> <p>Instantiate the <code>SimpleEnsembleAlgorithm</code>: </p> <ul> <li>The algorithm requires no parameters for initialization since it uses equal weights for all models.</li> </ul> </li> <li> <p>Prepare your models: </p> <ul> <li>You can either use a <code>BaseModelPool</code> instance that contains your models, or directly provide a list of PyTorch <code>nn.Module</code> instances.</li> <li>The algorithm will load models from the modelpool using <code>modelpool.load_model()</code> for each model name.</li> </ul> </li> <li> <p>Run the algorithm: </p> <ul> <li>The <code>run</code> method processes the modelpool and returns an <code>EnsembleModule</code> that represents the simple ensemble of the input models.</li> <li>The resulting ensemble computes the average of all model predictions.</li> </ul> </li> </ol>"},{"location":"algorithms/simple_ensemble/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.SimpleEnsembleAlgorithm</li> </ul>"},{"location":"algorithms/slerp/","title":"Spherical Linear Interpolation (SLERP)","text":"<p>SLERP stands for Spherical LinEar inteRPolation<sup>1</sup>.</p>"},{"location":"algorithms/slerp/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.SlerpMergeAlgorithm: Architecture-agnostic implementation.</li> <li>fusion_bench.method.SlerpForCausalLM: SLERP for large language models</li> </ul> <ol> <li> <p>SLERP For Model Merging \u2013 A Primer https://www.coinfeeds.ai/ai-blog/slerp-model-merging-primer\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/smile_upscaling/","title":"SMILE Upscaling","text":"The architecture of the Sparse MIxture of Low-rank Experts (SMILE) module.<sup>2</sup>"},{"location":"algorithms/smile_upscaling/#taxonomy-for-smile-upscaling","title":"Taxonomy for SMILE Upscaling","text":"<p>Here we present the taxonomy for the SMILE upscaling method following \"A Survey on Model MoErging\" by Yadav et al. (2024) <sup>1</sup>.</p> Expert Training Standard Expert Data Private Routing Dataset None Input Granularity Step Depth Granularity Module Expert Selection Sparse Expert Aggregation Output Generalization In-Distribution User Dataset Zero-Shot"},{"location":"algorithms/smile_upscaling/#configurations","title":"Configurations","text":"<p>The SMILE upscaling method offers several configuration options, which are located in the <code>config/method/</code> directory.</p> <ol> <li>General <code>nn.Module</code> Upscaling:      This configuration is designed for upscaling any neural network module (<code>nn.Module</code>).</li> <li>Mistral Model Upscaling:      This specific configuration is for Mistral models.</li> </ol> <p>Each configuration file contains detailed parameters and options that can be adjusted to meet the specific needs of your model and application.</p> config/method/smile_upscaling/smile_upscaling.yaml<pre><code># =============================================================================\n# FusionBench Method Configuration: SMILE Upscaling\n# =============================================================================\n_target_: fusion_bench.method.SmileUpscalingAlgorithm\n# merge device on cuda can accelerate the SVD computation\ndevice: cpu\n# device to compute svd\nupscaling_accelerator: cuda\nfull_matrices: true # set to false if you are sure k &lt; rank\ngate_k: 1\nk: 128\ntop_k: 1\nrouting_use_diff: true\n# average the remaining part, if this is set the False, the remaining part will kept as base model (the pretrained model)\naverage_experts: false\n# path to save/load the model\nmodel_path: null\n</code></pre> config/method/smile_upscaling/smile_mistral_upscaling.yaml<pre><code>_target_: fusion_bench.method.smile_upscaling.smile_mistral_upscaling.SmileMistralUpscalingAlgorithm\n# device to put the models on\ndevice: cpu\n# device to perform SVD on\naccelerator: cuda\n# path to save/load the model\nmodel_path: null\nmodel_dtype: null\n# SmileMoE parameters\nnum_experts_per_tok: 1\nrank_of_router: 8\n# if rank_of_expert &lt; 0, dense expert is used.\nrank_of_expert: 512\n</code></pre>"},{"location":"algorithms/smile_upscaling/#examples","title":"Examples","text":""},{"location":"algorithms/smile_upscaling/#clip-vit-b32-on-eight-tasks","title":"CLIP-ViT-B/32 on eight tasks","text":"<p>Evaluate single fine-tuned models and save the results to <code>outputs/ViT-B-32/single-task/</code> and <code>outputs/ViT-L-14/single-task/</code> for CLIP-ViT-B/32 and CLIP-ViT-L/14 models, respectively.</p> <pre><code># evaluate singlue fine-tuned models\nfor task in sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd\ndo\n    fusion_bench method=dummy \\\n        modelpool=clip-vit-base-patch32_individual \\\n            modelpool.models.0.path=tanganke/clip-vit-base-patch32_${task} \\\n        taskpool=clip-vit-classification_TA8 \\\n        report_save_path=\"outputs/ViT-B-32/single-task/clip-vit-base-patch32_${task}.json\"\ndone\n\n# if you have multiple GPUs, you can run the following code to evaluate the CLIP-ViT-L/14 models in parallel\n# evaluate singlue fine-tuned models clip-vit-large\ntasks=(sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd)\nCUDA_DEVICES=(0 1 2 3 4 5 6 7)  # List of CUDA devices to use\n\nfor i in \"${!CUDA_DEVICES[@]}\"; do\n    task=${tasks[$i]}\n    CUDA_VISIBLE_DEVICES=${CUDA_DEVICES[$i]} fusion_bench method=dummy \\\n        modelpool=CLIPVisionModelPool/clip-vit-large-patch14_individual \\\n            modelpool.models._pretrained_=tanganke/clip-vit-large-patch14_${task} \\\n        taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n            taskpool.clip_model=openai/clip-vit-large-patch14 \\\n        report_save_path=\"outputs/ViT-L-14/single-task/clip-vit-large-patch14_${task}.json\" &amp;\ndone\n</code></pre> <p>Upscale eight CLIP-ViT-B/32 models with SMILE, each CLIP-ViT-B/32 model is trained on a downstream task.</p> <pre><code>gate_k=16\nk=32\nfusion_bench \\\n    method=smile_upscaling/smile_upscaling \\\n        method.device=cuda \\\n        method.gate_k=$gate_k method.k=$k \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    report_save_path=\"outputs/ViT-B-32/eight_tasks/gate_k\\=${gate_k}_k\\=${k}.json\"\n</code></pre> <p>Hyperparameter search for SMILE upscaling. Pre-run results can be found in <code>examples/smile_upscaling/clip-vit-base-patch32.ipynb</code>.</p> <pre><code>for gate_k in 1 2 4 8 16 32 64 128 256 512 768; do\n    for k in 4 8 16 32 64 128 -1; do\n        fusion_bench \\\n            method=smile_upscaling/smile_upscaling \\\n                method.device=cuda \\\n                method.gate_k=$gate_k method.k=$k \\\n            modelpool=Seq2SeqLMPool/clip-vit-base-patch32_TA8 \\\n            taskpool=clip-vit-classification_TA8 \\\n            report_save_path=\"outputs/ViT-B-32/eight_tasks/gate_k\\=${gate_k}_k\\=${k}.json\"\n    done\ndone\n</code></pre> <p>Ablations on number of experts per token (Top-K). Pre-run results can be found in <code>examples/smile_upscaling/clip-vit-base-patch32-ablations-topk.ipynb</code>.</p> <pre><code>gate_k=16\nk=32\nfor top_k in 1 2 4\ndo\nfusion_bench \\\n    method=smile_upscaling/smile_upscaling \\\n        method.device=cuda \\\n        method.gate_k=$gate_k method.k=$k \\\n    modelpool=Seq2SeqLMPool/clip-vit-base-patch32_TA8 \\\n    taskpool=clip-vit-classification_TA8 \\\n    report_save_path=\"outputs/ViT-B-32/ablation/gate_k\\=${gate_k}_k\\=${k}.json\"\ndone\n</code></pre>"},{"location":"algorithms/smile_upscaling/#clip-vit-l14-on-eight-tasks","title":"CLIP-ViT-L/14 on eight tasks","text":"<p>hyperparameter search for SMILE upscaling. Pre-run results can be found in <code>examples/smile_upscaling/clip-vit-large-patch14.ipynb</code>.</p> <pre><code>for gate_k in 1 2 4 8 16 32 64 128; do\n    for k in 4 8 16 32 64 128 -1; do\n        fusion_bench \\\n            method=smile_upscaling/smile_upscaling \\\n                method.gate_k=$gate_k method.k=$k \\\n            modelpool=Seq2SeqLMPool/clip-vit-large-patch14_TA8 \\\n            taskpool=clip-vit-classification_TA8 \\\n                taskpool.clip_model=openai/clip-vit-large-patch14 \\\n            report_save_path=\"outputs/ViT-B-32/eight_tasks/gate_k\\=${gate_k}_k\\=${k}.json\"\n    done\ndone\n</code></pre>"},{"location":"algorithms/smile_upscaling/#flan-t5-models-on-eight-tasks-from-glue-benchmark","title":"Flan-T5 models on eight tasks from GLUE benchmark","text":"<p>Hyperparameter search for full fine-tuned and lora fine-tuned Flan-T5 models. Pre-run results can be found in <code>examples/smile_upscaling/flan-t5-base.ipynb</code> and <code>examples/smile_upscaling/flan-t5-base-lora16.ipynb</code>.</p> <pre><code># hyperparameter search for full fine-tuned flan-t5-base\nfor gate_k in 4 8 16 32; do\n    for k in 16 32 64 128; do\n        fusion_bench \\\n            method=smile_upscaling/smile_upscaling \\\n                method.device=cpu \\\n                method.gate_k=$gate_k method.k=$k \\\n            modelpool=Seq2SeqLMPool/flan-t5-base_glue \\\n            taskpool=flan-t5_glue_text_generation \\\n            report_save_path=\"outputs/flan-t5-base/glue_text_generation/gate_k\\=${gate_k}_k\\=${k}.json\"\n    done\ndone\n\n# hyperparameter search for lora fine-tuned flan-t5-base\nfor gate_k in 2 4 8; do\n    for k in 4 8 16; do\n        fusion_bench \\\n            method=smile_upscaling/smile_upscaling \\\n                method.device=cuda \\\n                method.gate_k=$gate_k method.k=$k \\\n            modelpool=Seq2SeqLMPool/flan-t5-base_glue_lora16 \\\n            taskpool=flan-t5_glue_text_generation \\\n            report_save_path=\"outputs/flan-t5-base_lora16/glue_text_generation/gate_k\\=${gate_k}_k\\=${k}.json\"\n    done\ndone\n</code></pre>"},{"location":"algorithms/smile_upscaling/#upscale-mistral-7b-models","title":"Upscale Mistral-7B models","text":"<p>Here we upscale several Mistral-7B models using SMILE. The models are trained on different tasks and are used as experts in the SMILE upscaling.</p> <p>We first provide an example of the upscaled model, where we upscale the linear layers of the original Mistral model into a SMILE linear layer.</p> <pre><code>import torch\nfrom accelerate import init_empty_weights\nfrom transformers import AutoConfig\n\nfrom fusion_bench.models.modeling_smile_mistral import (\n    SmileMistralConfig,\n    SmileMistralForCausalLM,\n)\n\n\nconfig = AutoConfig.from_pretrained(\n    \"mistralai/Mistral-7B-v0.1\"\n)\nconfig = SmileMistralConfig(\n    num_experts_per_tok=1,\n    rank_of_router=8,\n    rank_of_expert=8,\n    num_local_experts=3,\n    **config.to_dict()\n)\nwith init_empty_weights():\n    model = SmileMistralForCausalLM(config)\nmodel.to(dtype=torch.float16).to_empty(device=\"cuda\")\n</code></pre> <p>The model architecture is as follows:</p> <pre><code>SmileMistralForCausalLM(\n  (model): SmileMistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x SmileMistralDecoderLayer(\n        (self_attn): SmileMistralAttention(\n          (q_proj): SingularMoELinear(in_features=4096, out_features=4096, num_local_experts=3, num_experts_per_tok=1, rank_of_router=8, rank_of_expert=8)\n          (k_proj): SingularMoELinear(in_features=4096, out_features=1024, num_local_experts=3, num_experts_per_tok=1, rank_of_router=8, rank_of_expert=8)\n          (v_proj): SingularMoELinear(in_features=4096, out_features=1024, num_local_experts=3, num_experts_per_tok=1, rank_of_router=8, rank_of_expert=8)\n          (o_proj): SingularMoELinear(in_features=4096, out_features=4096, num_local_experts=3, num_experts_per_tok=1, rank_of_router=8, rank_of_expert=8)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): SmileMistralMLP(\n          (gate_proj): SingularMoELinear(in_features=4096, out_features=14336, num_local_experts=3, num_experts_per_tok=1, rank_of_router=8, rank_of_expert=8)\n          (up_proj): SingularMoELinear(in_features=4096, out_features=14336, num_local_experts=3, num_experts_per_tok=1, rank_of_router=8, rank_of_expert=8)\n          (down_proj): SingularMoELinear(in_features=14336, out_features=4096, num_local_experts=3, num_experts_per_tok=1, rank_of_router=8, rank_of_expert=8)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)\n</code></pre> <p>Knowing the model architecture, we can upscale the Mistral-7B models using the following steps:</p> <ol> <li> <p>Prepare the following 4 configuration files in <code>configs/modelpool</code>:</p> config/modelpool/smile_mistral_exp_v1.yaml<pre><code>type: AutoModelForCausalLMPool\nmodels:\n- name: _pretrained_\n    path: mistralai/Mistral-7B-v0.1\n- name: expert_1\n    path: meta-math/MetaMath-Mistral-7B\n\ndtype: float16\n</code></pre> config/modelpool/smile_mistral_exp_v2.yaml<pre><code>type: AutoModelForCausalLMPool\nmodels:\n- name: _pretrained_\n    path: mistralai/Mistral-7B-v0.1\n- name: expert_1\n    path: cognitivecomputations/dolphin-2.1-mistral-7b\n\ndtype: float16\n</code></pre> config/modelpool/smile_mistral_exp_v3.yaml<pre><code>type: AutoModelForCausalLMPool\nmodels:\n- name: _pretrained_\n    path: mistralai/Mistral-7B-v0.1\n- name: expert_1\n    path: uukuguy/speechless-code-mistral-7b-v1.0\n\ndtype: float16\n</code></pre> config/modelpool/smile_mistral_exp_v4.yaml<pre><code>type: AutoModelForCausalLMPool\nmodels:\n- name: _pretrained_\n    path: mistralai/Mistral-7B-v0.1\n- name: expert_1\n    path: meta-math/MetaMath-Mistral-7B\n- name: expert_2\n    path: cognitivecomputations/dolphin-2.1-mistral-7b\n- name: expert_3\n    path: uukuguy/speechless-code-mistral-7b-v1.0\n\ndtype: float16\n</code></pre> </li> <li> <p>Upscale Mistral-7B models. The upscaled models are saved in <code>outputs/mistral/gate_k-${gate_k}_k-${k}/version_${version}</code>.</p> <pre><code>function model_fusion() {\n    output_dir=outputs/mistral/gate_k-${gate_k}_k-${k}/version_${version}\n    fusion_bench \\\n        method=smile_upscaling/smile_mistral_upscaling \\\n            method.rank_of_router=$gate_k method.rank_of_expert=$k \\\n            method.model_path=${output_dir} \\\n        modelpool=smile_mistral_exp_v${version} \\\n            modelpool.dtype=float32 \\\n        taskpool=dummy \\\n        report_save_path=\"${output_dir}/model_info.json\"\n}\n\ngate_k=8\nfor k in 8 16 32 64 128 256 384 512; do\n    for version in 1 2 3 4; do\n        model_fusion\n    done\ndone\n</code></pre> </li> <li> <p>Use lm-evaluation-harness to evaluate the models. We use the default configurations for each task.</p> <pre><code># For some GPUs, the following environment variables need to be set\n# export NCCL_P2P_DISABLE=\"1\"\n# export NCCL_IB_DISABLE=\"1\"\n\nfunction model_eval() {\n    output_dir=outputs/mistral/gate_k-${gate_k}_k-${k}/version_${version}\n\n    # Check if ${output_dir}/${task}.json exists as a directory and return if it does\n    if [ -d \"${output_dir}/${task}.json\" ]; then\n        echo \"Directory ${output_dir}/${task}.json already exists. Skipping evaluation.\"\n        return\n    fi\n\n    lm_eval --model hf \\\n        --model_args pretrained=${output_dir},dtype=\"float16\",parallelize=True \\\n        --tasks ${task} \\\n        --output_path ${output_dir}/${task}.json \\\n        --batch_size 6\n}\n</code></pre> <p>The above function can be used to evaluate the models on specified task. Pre-run results can be found in <code>examples/smile_upscaling/mistral_gsm8k.ipynb</code>.</p> <pre><code># Evaluate all the models on GSM8K task\ngate_k=8\ntask=gsm8k\nfor k in 8 16 32 64 128 256 384 512; do\n    for version in 1 2 3 4; do\n        model_eval\n    done\ndone\n\n# Evaluate all M0;123 models on truthfulqa gsm8k arc_challenge mmlu\nk=8\nversion=4\nfor task in truthfulqa gsm8k arc_challenge mmlu; do\n    model_eval\ndone\n</code></pre> <p>The reported metrics are:</p> <ul> <li>mmlu (general): acc</li> <li>truthfulqa (truthful): mc2</li> <li>gsm8k (math): flexible exact match</li> <li>arc_challenge (reasoning): acc_norm</li> </ul> </li> </ol>"},{"location":"algorithms/smile_upscaling/#scope","title":"Scope","text":""},{"location":"algorithms/smile_upscaling/#projection-merge-experiments","title":"Projection Merge Experiments","text":"<p>Pre-run results can be found in <code>examples/smile_upscaling/clip-vit-base-patch32_single-task_projection-merging.ipynb</code>.</p> <pre><code># project into different subspaces\nfor task in sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd\ndo\n    # Space I\n    CUDA_VISIBLE_DEVICES=0 fusion_bench \\\n        method=smile_upscaling/singular_projection_merging \\\n            method.device=cuda method.rank=low method.k=-1 method.full_matrices=false \\\n        modelpool=CLIPVisionModelPool/clip-vit-base-patch32_single_finetuned \\\n            modelpool.models.finetuned=tanganke/clip-vit-base-patch32_${task} \\\n        taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n        report_save_path=\"outputs/ViT-B-32/single-task/projection_merging_zone1_${task}.json\" &amp;\n\n    # Space II\n    CUDA_VISIBLE_DEVICES=1 fusion_bench \\\n        method=smile_upscaling/singular_projection_merging \\\n            method.device=cuda method.rank=high method.k=-1 method.full_matrices=false \\\n        modelpool=CLIPVisionModelPool/clip-vit-base-patch32_single_finetuned \\\n            modelpool.models.finetuned=tanganke/clip-vit-base-patch32_${task} \\\n        taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n        report_save_path=\"outputs/ViT-B-32/single-task/projection_merging_zone2_${task}.json\" &amp;\n\n    # Space II+III\n    CUDA_VISIBLE_DEVICES=2 fusion_bench \\\n        method=smile_upscaling/singular_projection_merging \\\n            method.device=cuda method.rank=high method.k=-1 method.full_matrices=true \\\n        modelpool=CLIPVisionModelPool/clip-vit-base-patch32_single_finetuned \\\n            modelpool.models.finetuned=tanganke/clip-vit-base-patch32_${task} \\\n        taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n        report_save_path=\"outputs/ViT-B-32/single-task/projection_merging_zone23_${task}.json\" &amp;\n    wait\ndone\n</code></pre>"},{"location":"algorithms/smile_upscaling/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.SmileUpscalingAlgorithm</li> </ul> <ol> <li> <p>Yadav, Prateek, et al. \"A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning.\" arXiv preprint arXiv:2408.07057 (2024).\u00a0\u21a9</p> </li> <li> <p>A. Tang et. al. SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models. Aug, 2024. https://arxiv.org/abs/2408.10174 \u21a9</p> </li> </ol>"},{"location":"algorithms/task_arithmetic/","title":"Task Arithmetic","text":"<p>In the rapidly advancing field of machine learning, multi-task learning has emerged as a powerful paradigm, allowing models to leverage information from multiple tasks to improve performance and generalization. One intriguing method in this domain is Task Arithmetic, which involves the combination of task-specific vectors derived from model parameters. </p> Task Arithmetic. This figure is credited to <sup>2</sup> <p>Task Vector. A task vector is used to encapsulate the adjustments needed by a model to specialize in a specific task.  It is derived from the differences between a pre-trained model's parameters and those fine-tuned for a particular task.  Formally, if \\(\\theta_i\\) represents the model parameters fine-tuned for the i-th task and \\(\\theta_0\\) denotes the parameters of the pre-trained model, the task vector for the i-th task is defined as:</p> \\[\\tau_i = \\theta_i - \\theta_0\\] <p>This representation is crucial for methods like Task Arithmetic, where multiple task vectors are aggregated and scaled to form a comprehensive multi-task model.</p> <p>Task Arithmetic<sup>1</sup> begins by computing a task vector \\(\\tau_i\\) for each individual task, using the set of model parameters \\(\\theta_0 \\cup \\{\\theta_i\\}_i\\) where \\(\\theta_0\\) is the pre-trained model and \\(\\theta_i\\) are the fine-tuned parameters for i-th task. These task vectors are then aggregated to form a multi-task vector. Subsequently, the multi-task vector is combined with the pre-trained model parameters to obtain the final multi-task model. This process involves scaling the combined vector element-wise by a scaling coefficient (denoted as \\(\\lambda\\)), before adding it to the initial pre-trained model parameters.  The resulting formulation for obtaining a multi-task model is expressed as </p> \\[ \\theta = \\theta_0 + \\lambda \\sum_{i} \\tau_i. \\] <p>The choice of the scaling coefficient \\(\\lambda\\) plays a crucial role in the final model performance. Typically, \\(\\lambda\\) is chosen based on validation set performance. </p> <p>Recent work has also explored task arithmetic in the tangent space, which can provide improved editing of pre-trained models<sup>2</sup>. </p>"},{"location":"algorithms/task_arithmetic/#examples","title":"Examples","text":""},{"location":"algorithms/task_arithmetic/#cli-usage","title":"CLI Usage","text":"<p>Configuration template for the Task Arithmetic algorithm:</p> config/method/task_arithmetic.yaml<pre><code># =============================================================================\n# FusionBench Method Configuration: Task Arithmetic\n# =============================================================================\n# Performs task vector arithmetic: base + lambda * \\sum_i (task_i - base).\n#\n# Notes\n# - scaling_factor controls the contribution of the task delta.\n# - Model compatibility is required (matching parameter shapes).\n# =============================================================================\n_target_: fusion_bench.method.TaskArithmeticAlgorithm\nscaling_factor: 0.3\n</code></pre> <p>Use the following command to run the Task Arithmetic algorithm:</p> <pre><code>fusion_bench method=task_arithmetic ...\n</code></pre> <p>For example, to run the Task Arithmetic algorithm on two models with scaling factor 0.5:</p> <pre><code>fusion_bench method=task_arithmetic \\\n    method.scaling_factor=0.5 \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_svhn_and_mnist \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-base-patch32_svhn_and_mnist\n</code></pre> <p>where the configuration for the model pool is:</p> config/modelpool/CLIPVisionModelPool/clip-vit-base-patch32_svhn_and_mnist.yaml<pre><code>_target_: fusion_bench.modelpool.CLIPVisionModelPool\n_recursive_: False\nprocessor: openai/clip-vit-base-patch32\nmodels:\n  _pretrained_: openai/clip-vit-base-patch32\n  svhn: tanganke/clip-vit-base-patch32_svhn\n  mnist: tanganke/clip-vit-base-patch32_mnist\nplatform: hf\n</code></pre> <p>and the configuration for the task pool:</p> config/taskpool/CLIPVisionModelTaskPool/clip-vit-base-patch32_svhn_and_mnist.yaml<pre><code>defaults:\n  - /dataset/image_classification/test@test_datasets:\n      - svhn\n      - mnist\n_target_: fusion_bench.taskpool.CLIPVisionModelTaskPool\n_recursive_: false\ntest_datasets: ??? # The datasets to evaluate the model on\nbase_model: openai/clip-vit-base-patch32\nclip_model: ${.base_model} # The base model to use\nprocessor: ${.base_model} # The base model to use\ndata_processor: ${.processor}\ndataloader_kwargs:\n  batch_size: 128 # The batch size for the data loader\n  num_workers: 8 # The number of worker processes for data loading\n  pin_memory: True # Whether to pin memory in data loader\n  drop_last: False # Whether to drop the last incomplete batch\n  shuffle: False # Whether to shuffle the data\n# === layer-wise feature saving ===\n# The path to save the features to, if none then the features are not saved\n# This is the path to a directory, the features of task `task_name` will be saved in `feature_save_path/task_name.csv`\nlayer_wise_feature_save_path: null\nlayer_wise_feature_first_token_only: true # Whether to save only the first token of the features\n# The maximum number of samples to save the features for\nlayer_wise_feature_max_num: 1000\n</code></pre> <p>Use Task Arithmetic to merge 8 CLIP-ViT-B-32 models from different image classification tasks and evaluate the performance of the merged model.</p> <pre><code>fusion_bench \\\n  method=task_arithmetic \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8_model_only \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre>"},{"location":"algorithms/task_arithmetic/#api-usage","title":"API Usage","text":"<p>To use the Task Arithmetic algorithm, you can use the <code>TaskArithmeticAlgorithm</code> class from the <code>fusion_bench.method</code> module.</p> <pre><code>from torch import nn\nfrom fusion_bench.method.task_arithmetic import TaskArithmeticAlgorithm\n\n# Instantiate the TaskArithmeticAlgorithm\nalgorithm = TaskArithmeticAlgorithm(scaling_factor=0.5)\n\n# Assume we have a dict of PyTorch models (nn.Module instances) that we want to merge.\n# The models should all have the same architecture.\n# the dict must contain the pre-trained model with the key '_pretrained_', and arbitrary number of fine-tuned models.\nmodels = {'_pretrained_': nn.Linear(10,10), 'model_1': nn.Linear(10,10), 'model_2': nn.Linear(10,10)}\n\n# Run the algorithm on the models.\n# This will return a new model that is the result of task arithmetic on the input models.\nmerged_model = algorithm.run(models)\n</code></pre>"},{"location":"algorithms/task_arithmetic/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.task_arithmetic.TaskArithmeticAlgorithm</li> </ul> <ol> <li> <p>(ICLR 2023) Editing Models with Task Arithmetic. http://arxiv.org/abs/2212.04089\u00a0\u21a9</p> </li> <li> <p>(NIPS 2023 Oral) Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard, \u201cTask Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models,\u201d doi: 10.48550/arXiv.2305.12827.\u00a0\u21a9</p> </li> <li> <p>(ICLR 2024) AdaMerging: Adaptive Model Merging for Multi-Task Learning. http://arxiv.org/abs/2310.02575\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/task_singular_vector/","title":"Task Singular Vector","text":"<p>Task Singular Vector Merging (TSVM) is a model merging technique that uses Singular Value Decomposition (SVD) to combine multiple task-specific fine-tuned models into a single merged model. This method is particularly effective for merging models that have been fine-tuned on different tasks from a common pre-trained base model.</p>"},{"location":"algorithms/task_singular_vector/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"algorithms/task_singular_vector/#problem-setup","title":"Problem Setup","text":"<p>Let \\(\\theta_0\\) be the parameters of a pre-trained base model, and \\(\\{\\theta_1, \\theta_2, \\ldots, \\theta_n\\}\\) be the parameters of \\(n\\) models fine-tuned on different tasks from the same base model \\(\\theta_0\\).</p>"},{"location":"algorithms/task_singular_vector/#task-vector-computation","title":"Task Vector Computation","text":"<p>For each fine-tuned model \\(i\\), we first compute the task vector \\(\\tau_i\\), which represents the parameter changes from the base model:</p> \\[\\tau_i = \\theta_i - \\theta_0\\]"},{"location":"algorithms/task_singular_vector/#svd-based-merging-algorithm","title":"SVD-Based Merging Algorithm","text":"<p>The core innovation of TSVM lies in applying SVD to the task vectors and then merging them in the singular vector space.</p>"},{"location":"algorithms/task_singular_vector/#step-1-svd-decomposition","title":"Step 1: SVD Decomposition","text":"<p>For each parameter matrix \\(W^{(i)}_k\\) in task vector \\(\\tau_i\\) (where \\(k\\) indexes the layer/parameter group), if the matrix is 2-dimensional, we compute its SVD:</p> \\[W^{(i)}_k = U^{(i)}_k S^{(i)}_k (V^{(i)}_k)^T\\] <p>where:</p> <ul> <li>\\(U^{(i)}_k \\in \\mathbb{R}^{m \\times r}\\) contains the left singular vectors</li> <li>\\(S^{(i)}_k \\in \\mathbb{R}^{r \\times r}\\) is a diagonal matrix of singular values  </li> <li>\\(V^{(i)}_k \\in \\mathbb{R}^{n \\times r}\\) contains the right singular vectors</li> <li>\\(r = \\min(m, n)\\) is the rank</li> </ul>"},{"location":"algorithms/task_singular_vector/#step-2-dimension-reduction-and-concatenation","title":"Step 2: Dimension Reduction and Concatenation","text":"<p>To reduce memory usage and computational complexity, we apply a reduction factor:</p> \\[\\text{reduction_factor} = \\frac{1}{T}\\] <p>where \\(T\\) is the number of tasks.</p> <p>For each task \\(i\\), we select only the top \\(\\lfloor r \\cdot \\text{reduction_factor} \\rfloor\\) singular components and place them in task-specific positions within larger matrices:</p> \\[U = [U^{(1)}_k[:, :d], U^{(2)}_k[:, :d], \\ldots, U^{(T)}_k[:, :d]]\\] \\[S = \\text{diag}(S^{(1)}_k[:d], S^{(2)}_k[:d], \\ldots, S^{(T)}_k[:d])\\] \\[V = [V^{(1)}_k[:, :d], V^{(2)}_k[:, :d], \\ldots, V^{(T)}_k[:, :d]]\\] <p>where \\(d = \\lfloor r \\cdot \\text{reduction_factor} \\rfloor\\).</p>"},{"location":"algorithms/task_singular_vector/#step-3-second-level-svd","title":"Step 3: Second-Level SVD","text":"<p>We then compute the SVD of the concatenated matrices:</p> \\[U = \\hat{U} \\hat{S} (\\hat{V})^T\\] \\[V = \\hat{U} \\hat{S} (\\hat{V})^T\\]"},{"location":"algorithms/task_singular_vector/#step-4-final-reconstruction","title":"Step 4: Final Reconstruction","text":"<p>The merged task vector for parameter \\(k\\) is reconstructed as:</p> \\[\\tau_{\\text{TSVM}} = \\hat{U} \\cdot (\\hat{V})^T \\cdot \\text{diag}(S) \\cdot \\hat{U} \\cdot (\\hat{V})^T\\]"},{"location":"algorithms/task_singular_vector/#handling-non-2d-parameters","title":"Handling Non-2D Parameters","text":"<p>For parameters that are not 2-dimensional (e.g., bias vectors, normalization parameters), TSVM simply computes the arithmetic mean:</p> \\[\\tau_{\\text{TSVM,non-2D}} = \\frac{1}{T} \\sum_{i=1}^{T} \\tau^{(i)}_k\\]"},{"location":"algorithms/task_singular_vector/#final-model-construction","title":"Final Model Construction","text":"<p>The final merged model parameters are obtained by adding the merged task vector to the base model:</p> \\[\\theta_{\\text{TSVM}} = \\theta_0 + \\alpha \\tau_{\\text{TSVM}}\\] <p>where \\(\\alpha\\) is an optional global scaling factor.</p>"},{"location":"algorithms/task_singular_vector/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.TaskSingularVectorMerging</li> </ul>"},{"location":"algorithms/ties_merging/","title":"Ties Merging","text":"Ties-Merging. Credit to <sup>1</sup> <p>Ties-Merging<sup>1</sup> represents a novel and structured approach to consolidating multiple task-specific models into a single, efficient multi-task model. This method employs a sequence of deliberate steps to systematically merge task vectors, ensuring that the final model effectively integrates the strengths of each individual task-specific model and resolves potential conflicts between them.</p> <p>Task Vectors. Similar to Task Arithmetic, TIES merging begins by computing task vectors for each fine-tuned model:</p> \\[\\tau_t = \\theta_t - \\theta_0\\] <p>where \\(\\theta_t\\) represents the parameters of the model fine-tuned for task \\(t\\), and \\(\\theta_0\\) denotes the parameters of the pre-trained model.</p> <p>The TIES-Merging algorithm operates through three primary steps:</p> <ol> <li>Trim: This initial step involves refining the task-specific models by trimming unnecessary parameters, focusing the model on essential elements for each task.</li> <li>Elect Sign of Parameters: In this step, the algorithm selects the appropriate signs for the parameters, ensuring that the integrated model parameters are optimally oriented for multi-task learning.</li> <li>Disjoint Merge: Finally, the method performs a disjoint merge to combine the task-specific parameters into a single cohesive task vector, denoted as \\(\\tau\\).</li> </ol> <p>Given the final merged task vector \\(\\tau_m\\), the ultimate model is determined as:</p> \\[\\theta = \\theta_0 + \\lambda \\tau_m\\] <p>where \\(\\lambda\\) is a hyperparameter (scaling factor) chosen based on validation set performance to ensure optimal model performance.</p> <p>By following these structured steps, Ties-Merging effectively integrates multiple task-specific models into a unified multi-task model, balancing the contributions of each task to enhance overall performance. The process ensures that the final model retains the benefits of the pre-trained model while optimally incorporating the diverse knowledge contained within the individual task-specific models.</p>"},{"location":"algorithms/ties_merging/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"Task Arithmetic and Ties-Merging. Here we illustrate the average performance of models merged using Task Arithmetic and Ties-Merging methods, with varying scaling coefficients.  The subfigures represent different models: CLIP-ViT-B/32, CLIP-ViT-L/14, Flan-T5-base (LoRA fine-tuned), and Flan-T5-large (LoRA fine-tuned).  <p>In the above figure, we show the average performance of Task Arithmetic and Ties-Merging merged models as the scaling coefficient varies. Subfigure (a), (b), (c), and (d) show the results of CLIP-ViT-B/32, CLIP-ViT-L/14, Flan-T5-base (LoRA fine-tuned), and Flan-T5-large (LoRA fine-tuned), respectively. It is evident that the merged multi-task model hits a peak in average performance across various tasks when the scaling coefficient is set around 0.3. This value was empirically selected as the scaling coefficient in our experiments. As we increase the scaling coefficient beyond this point, the average performance of the model begins to decline, eventually even falling below the level of the pre-trained model\u2019s original performance. This suggests that too high of a scaling coefficient can have a negative impact on the knowledge that the pre-trained model initially possessed, emphasizing the importance of calibrating the scaling coefficient parameter \\(\\lambda\\) to avoid diminishing the model\u2019s existing strengths.</p>"},{"location":"algorithms/ties_merging/#examples","title":"Examples","text":""},{"location":"algorithms/ties_merging/#cli-usage","title":"CLI Usage","text":"<p>Configuration template for the Ties-Merging algorithm:</p> config/method/ties_merging.yaml<pre><code># =============================================================================\n# FusionBench Method Configuration: Ties Merging\n# =============================================================================\n_target_: fusion_bench.method.TiesMergingAlgorithm\n# Scaling factor $\\lambda$\nscaling_factor: 0.3\nthreshold: 20\n# List of keys to remove from the state dict, default is empty\nremove_keys: []\n# Function to merge the models, default is sum. Options are 'sum', 'mean', and 'max'\nmerge_func: sum\n</code></pre> <p>Use the following command to run the Ties-Merging algorithm:</p> <pre><code>fusion_bench method=ties_merging ...\n</code></pre> <p>For example, to run the Ties-Merging algorithm with custom parameters:</p> <pre><code>fusion_bench method=ties_merging \\\n    method.scaling_factor=0.3 \\\n    method.threshold=20 \\\n    method.merge_func=sum \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_svhn_and_mnist \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-base-patch32_svhn_and_mnist\n</code></pre> <p>Use Ties-Merging to merge 8 CLIP-ViT-B-32 models from different image classification tasks and evaluate the performance of the merged model.</p> <pre><code>fusion_bench method=ties_merging \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8_model_only \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre>"},{"location":"algorithms/ties_merging/#saving-merged-model","title":"Saving Merged Model","text":"<p>If you want to persist the merged model, use the program-level save option <code>merged_model_save_path</code>.  The actual save behavior is delegated to the selected ModelPool via its <code>save_model</code> implementation.</p> <p>Example: save the merged model into a directory under the log folder</p> <pre><code>fusion_bench \\\n  path.log_dir=outputs/clip-vit-base-patch32/ties_merging \\\n  merged_model_save_path=$\\{path.log_dir\\}/merged_model \\\n  method=ties_merging \\\n    method.scaling_factor=0.3 \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8_model_only \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre>"},{"location":"algorithms/ties_merging/#api-usage","title":"API Usage","text":"<p>To use the Ties-Merging algorithm programmatically, you can use the <code>TiesMergingAlgorithm</code> class from the <code>fusion_bench.method</code> module.</p> <pre><code>from torch import nn\nfrom fusion_bench.method.ties_merging import TiesMergingAlgorithm\n\n# Instantiate the TiesMergingAlgorithm\nalgorithm = TiesMergingAlgorithm(\n    scaling_factor=0.3,\n    threshold=20,\n    remove_keys=[],\n    merge_func=\"sum\"\n)\n\n# Assume we have a dict of PyTorch models (nn.Module instances) that we want to merge.\n# The models should all have the same architecture.\n# The dict must contain the pre-trained model with the key '_pretrained_', and arbitrary number of fine-tuned models.\nmodels = {\n    '_pretrained_': nn.Linear(10, 10), \n    'model_1': nn.Linear(10, 10), \n    'model_2': nn.Linear(10, 10)\n}\n\n# Run the algorithm on the models.\n# This will return a new model that is the result of TIES merging on the input models.\nmerged_model = algorithm.run(models)\n</code></pre>"},{"location":"algorithms/ties_merging/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.ties_merging.TiesMergingAlgorithm</li> </ul> <ol> <li> <p>(NIPS 2023) Resolving Interference When Merging Models. http://arxiv.org/abs/2306.01708\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/weight_ensembling_moe/","title":"Weight-Ensembling Mixture of Experts (Data-Adaptive Model Merging)","text":"(a) Framework overview. This figure shows the overall framework of our proposed method to merge the pre-trained model and fine-tuned task-specific models. We merge weights in the Transformer Layers except for the MLPs. For the MLPs, we upcycle them into weight-assembling MoE modules. (b) Wieght-Ensembling Mixture of Experts (MoE) Module. Here we outline the detailed structure of the Weight-Ensembling MoE module, composed of the router, pre-trained MLP weights, and a collection of task vectors. Collaboration between shared weights and task vectors is employed to create input-conditioned weights dynamically. In this way, we separate shared information and task-specific knowledge, which are then combined based on input in time.      <p>This method is designed to handle a wide range of tasks by segregating shared information and task-specific knowledge.  It dynamically combines these elements based on the input samples.</p> <p>The Weight-Ensembling MoE module consists of three main components: the router, the pre-trained MLP weights, and a collection of task vectors.  The router, which is an MLP, processes the input data and generates routing weights. These weights determine how the knowledge from different tasks is combined. The pre-trained MLP weights are crucial as they have been trained to recognize a wide range of data patterns.  The task vectors represent the differences between the MLPs that have been fine-tuned for specific tasks and the pre-trained ones, capturing the unique adjustments made to optimize them for specific tasks. The routing weights are averaged across the input tokens, and these weights are used to select task vectors from a dictionary matrix. These task vectors are then added to the pre-trained MLP weights to create input-conditioned weights.</p> <p>Algorithm Requirements:</p> Method Access to labeled tasks data Access to validation data (labeled) Test time adaptation Fisher Merging Yes (Estimate Fisher information matrix) No No RegMean Yes (compute Gram Matrix) No No Task Arithmetic No Yes (select sacling factor) No Ties-Merging No Yes (select sacling factor) No AdaMerging No No Yes Ours No No Yes"},{"location":"algorithms/weight_ensembling_moe/#wemoe-v2-e-wemoe","title":"WEMoE V2: E-WEMoE","text":"<p>L. Shen, A. Tang, E. Yang et al. Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging. Oct, 2024.<sup>1</sup></p> <p> </p>    (a) Overview of the Efficient Weight-Ensembling Mixture of Experts (E-WEMoE) Framework. It merges all non-MLP modules through task arithmetic and upgrades the MLP modules into an efficient E-WEMoE module. (b) E-WEMoE Module. The module includes a router shared across all Transformer blocks, the pre-trained MLP module, and a set of sparse task-specific vectors w.r.t. MLP modules.         Comparison of (a) trainable parameters and (b) total parameters between WEMoE and E-WEMoE-90%.         Comparison of the relationship between parameter count and performance across various model merging methods."},{"location":"algorithms/weight_ensembling_moe/#parameters-comparison","title":"Parameters Comparison","text":"<p>Tip for reducing the parameter count</p> <p>Here we present the parameter count for the method outlined in the original paper<sup>2</sup>.  An effective strategy to minimize the number of parameters involves employing Singular Value Decomposition (SVD) to compress the task vectors.  This approach significantly cuts down on the number of parameters while only marginally impacting performance.  For additional information, please refer to the Twin-Merging paper<sup>3</sup>.  Which not only reduces the number of parameters but also conducts extensive experiments to demonstrate the effectiveness of data-adaptive merging on language domain.</p> <p>Here is the number of parameters compared to a single pre-trained model (OpenCLIP CLIP-ViT-B/32):</p> Method Trainable Parameters Total Parameters Paremeters Reduced by Merging Single Pre-trained 113.45M (100%) 113.45M - WEMoE (2-layer, 1 task) 7.10M (4.00%) 177.21M - WEMoE (2-layer, 2 tasks) 7.11M (3.04%) 233.89M 2*113.45-233.89=-6.99M WEMoE (2-layer, 3 tasks) 7.11M (2.45%) 290.57M 3*113.45-290.57=49.78M WEMoE (2-layer, 4 tasks) 7.12M (2.02%) 347.25M 4*113.45-347.25=106.55M WEMoE (2-layer, 5 tasks) 7.13M (1.77%) 403.93M 5*113.45-403.93=163.32M WEMoE (2-layer, 6 tasks) 7.14M (1.55%) 460.61M 6*113.45-460.61=220.09M WEMoE (2-layer, 7 tasks) 7.15M (1.38%) 517.28M 7*113.45-517.28=276.87M WEMoE (2-layer, 8 tasks) 7.16M (1.25%) 573.96M 8*113.45-573.96=333.64M <p>The number of parameter count of HuggingFace CLIP vision models (of type <code>transformers.models.clip.modeling_clip.CLIPVisionModel</code>) are different from the OpenCLIP models downloaded from the task arithmetic repo, because the OpenCLIP models (of type <code>src.modeling.ImageEncoder</code>) include the embedding layer for text tokens, while the HuggingFace CLIP vision models do not. Therefore, the relative parameter count of the upscaled model using Transformer CLIP vision models will be larger than the OpenCLIP models.</p> OpenCLIP models of type <code>src.modeling.ImageEncoder</code>Transfomers CLIP vision model of type <code>transformers.models.clip.modeling_clip.CLIPVisionModel</code> <pre><code>ImageEncoder( # (1)\n  (model): CLIP(\n    (visual): VisualTransformer( # (2)\n      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (transformer): Transformer(\n        (resblocks): ModuleList(\n          (0-11): 12 x ResidualAttentionBlock(\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_attn): Identity()\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (ln): Identity()\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n          )\n        )\n      )\n      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (token_embedding): Embedding(49408, 512) # (3)\n    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n)\n</code></pre> <ol> <li>trainable params: 113.45M || all params: 113.45M || trainable%: 100.0000</li> <li>trainable params: 87.85M || all params: 87.85M || trainable%: 100.0000</li> <li>trainable params: 25.30M || all params: 25.30M || trainable%: 100.0000</li> </ol> <pre><code>CLIPVisionModel( # (1)\n  (vision_model): CLIPVisionTransformer(\n    (embeddings): CLIPVisionEmbeddings(\n      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n      (position_embedding): Embedding(50, 768)\n    )\n    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (encoder): CLIPEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n)\n</code></pre> <ol> <li>trainable params: 87.85M || all params: 87.85M || trainable%: 100.0000</li> </ol>"},{"location":"algorithms/weight_ensembling_moe/#loss-landscape-visualization","title":"Loss Landscape Visualization","text":"Visualization of the joint loss \\(\\mathcal{L}_1 + \\mathcal{L}_2\\) and five task pairs for CLIP-ViT-B/32 in the loss landscape.     We perform interpolations between pre-trained weights and two fine-tuned weights in the weight space on a 2D plane using the formula \\(\\theta=\\theta_0 + \\lambda_1 \\tau_1 + \\lambda_2 \\tau_2\\), where \\(\\theta_0\\) represents pre-trained weights, \\(\\tau_i=\\theta_i -\\theta_0\\)  are two task vectors with \\(\\lambda_i\\) in the range [-1, 1]."},{"location":"algorithms/weight_ensembling_moe/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>In the below figure, we show the performance of the merged models with varying numbers of steps. Figure (b) shows the performance of the merged WEMoE models with varying number of steps. In Figure (a), we merge CLIP-ViT-B/32 models with different learning rate configurations. We observe that the performance of the merged model shows an upward trend with an increase in the number of training steps, and it converges rapidly, reaching a high accuracy level in just 200 steps. Furthermore, the influence of different learning rates is not significant, suggesting that our method is insensitive to the learning rate parameter. This is a desirable property as it reduces the need for hyperparameter tuning.</p>  The performance of the merged models with a varying number of steps. (a) CLIP-ViT-B/32 model with different learning rates. (b) Comparison of CLIP-ViT-B/32 and CLIP-ViT-L/14."},{"location":"algorithms/weight_ensembling_moe/#ablations-of-router-depth","title":"Ablations of Router Depth","text":"<p>Table: Parameter comparison of WEMoE (1-layer) and WEMoE (2-layer) on CLIP-ViT-B/32 models (OpenCLIP).</p> Method Number of Trainable Parameters AdaMerging (layer-wise) 1.3K WEMoE (1-layer) 73.8K (0.01%) WEMoE (2-layer) 7.16M (1.25%) <p>Table: Ablation study of the router depth on the performance of the up-scaled CLIP-ViT-B/32 models (OpenCLIP).</p> Method SUN397 CARS RESISC45 EuroSAT SVHN GRSRB MNIST DTD Avg. AdaMerging (layer-wise) 66.6 68.3 82.4 92.5 86.5 93.7 97.7 61.1 80.9 WEMoE (1-layer) 73.2 76.7 93.8 98.6 95.7 98.6 99.5 74.5 88.3 WEMoE (2-layer) 74.1 77.4 93.7 99.1 96.2 98.9 99.6 76.4 89.4 <p>To explore the influence of router depth on the performance of the scaled-up model, we perform an ablation study where the router depth is varied. In WEMoE modules, the router is implemented as a multi-layer perceptron (MLP).</p> <ul> <li>WEMoE (0-layer) functions as a bias-only model, representing a special case of an MLP with no hidden layers. It generates a constant routing weight for all inputs, captured by the formula as \\(r(h) = b_0\\), indicating that it does not adjust based on the input.   When we only up-scale the MLP modules of the vision Transformers to MoE modules, WEMoE (0-layer) can be considered as a partial implementation of AdaMerging. Add when we up-scale the vision Transformers layer-wisely, WEMoE (0-layer) can be considered equivalent to AdaMerging.   For WEMoE (0-layer), the MoE modules can be unloaded, thus no additional parameters and inference cost are introduced.</li> <li>For WEMoE (1-layer), each router is a one-layer MLP that takes the input sample \\(h\\) and outputs the routing weight \\(r(h)\\), which is adaptive to the input. The routing weight is calculated as \\(r(h) = W_1 h + b_1\\).</li> <li>For WEMoE (2-layer), each router is a two-layer MLP and the routing weight is calculated as \\(r(h) = W_2 ReLU(W_1 h + b_1) + b_2\\).</li> </ul> <p>In the above two Tables, we present additional findings to support our argument. We compare the number of trainable parameters and performance between WEMoE (1-layer) and WEMoE (2-layer). The data reveal that WEMoE (1-layer) possesses 73.8K trainable parameters, which constitute only 0.01% of the total parameters in the merged model. Notably, the performance of WEMoE (1-layer) is significantly better than AdaMerging and nearly matches that of WEMoE (2-layer) across all tasks. This evidence underscores our claim that the MoE design is crucial for performance enhancement.</p>"},{"location":"algorithms/weight_ensembling_moe/#examples","title":"Examples","text":""},{"location":"algorithms/weight_ensembling_moe/#cli-usage","title":"CLI Usage","text":"<p>multi-task model fusion experiment on eight image classification tasks.</p> <pre><code># merge eight CLIP-ViT-B/32 models using WE MoE\nfusion_bench \\\n  method=wemoe/weight_ensembling_moe \\\n    method.name=clip_weight_ensembling_moe \\\n    method.use_grad_accumulate=false \\\n    method.save_checkpoint=outputs/clip-vit-base-patch32_TA8_weight_ensembling_moe_checkpoint.ckpt \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>merge eight CLIP-ViT-L/14 models:</p> <pre><code># merge eight CLIP-ViT-L/14 models using WE MoE, fine-tune the routers\nfusion_bench print_config=false \\\n  method=wemoe/weight_ensembling_moe \\\n    method.name=clip_weight_ensembling_moe \\\n    method.use_grad_accumulate=true \\\n    method.save_checkpoint=outputs/clip-vit-large-patch14_TA8_weight_ensembling_moe_checkpoint.ckpt \\\n    method.batch_size=4 method.devices=4 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=dummy &amp;&amp;\n\n# load the checkpoint and evaluate the model\nfusion_bench \\\n  method=wemoe/weight_ensembling_moe \\\n    method.name=clip_weight_ensembling_moe \\\n    method.checkpoint=outputs/clip-vit-large-patch14_TA8_weight_ensembling_moe_checkpoint.ckpt \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    taskpool.clip_model=openai/clip-vit-large-patch14\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.CLIPWeightEnsemblingMoEAlgorithm</li> </ul> <ol> <li> <p>L. Shen, A. Tang, E. Yang et al. Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging. Oct, 2024.\u00a0\u21a9</p> </li> <li> <p>Anke Tang et.al. ICML 2024. Merging Multi-Task Models via Weight-Ensembling Mixture of Experts. http://arxiv.org/abs/2402.00433 ICML 2024.\u00a0\u21a9</p> </li> <li> <p>Z. Lu, C. Fan, W. Wei, X. Qu, D. Chen, and Y. Cheng, \u201cTwin-Merging: Dynamic Integration of Modular Expertise in Model Merging,\u201d doi: 10.48550/arXiv.2406.15479. NeurIPS 2024.\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/weighted_averaging/","title":"Weighted Averaging","text":"<p>Weighted averaging, also known as weight-ensembling, combines multiple models by averaging their parameters according to specified weights. This approach allows for non-uniform combination of models, where better-performing or more reliable models can be given higher weights in the final merged model.</p> <p>In the context of model fusion, if we have \\(n\\) models with their respective parameters \\(\\theta_i\\) and model-wise weights \\(w_i\\), the parameters of the final merged model \\(\\theta\\) are computed as:</p> \\[ \\theta = \\sum_{i=1}^{n} w_i \\theta_i \\] <p>where the weights \\(w_i\\) can optionally be normalized to sum to 1.</p>"},{"location":"algorithms/weighted_averaging/#examples","title":"Examples","text":""},{"location":"algorithms/weighted_averaging/#cli-usage","title":"CLI Usage","text":""},{"location":"algorithms/weighted_averaging/#general-pytorch-models","title":"General Pytorch Models","text":"<p>The <code>WeightedAverageAlgorithm</code> works with general PyTorch models and performs weighted averaging of all model parameters.</p> <p>Configuration template for the standard Weighted Averaging algorithm:</p> config/method/linear/weighted_average.yaml<pre><code># =============================================================================\n# FusionBench Method Configuration: Weighted Average (Linear)\n# =============================================================================\n_target_: fusion_bench.method.WeightedAverageAlgorithm\nnormalize: true # if true, the weights will be normalized before merging\nweights: # List of weights for each model\n  - 0.5\n  - 0.5\n</code></pre> <p>Use the following command to run the Weighted Averaging algorithm:</p> <pre><code>fusion_bench method=linear/weighted_average ...\n</code></pre> <p>The following command merges eight CLIP-ViT models using a weighted average approach:</p> <pre><code># Note: Since `method.normalize=true`, the weights are normalized to sum to 1, making this example equivalent to simple averaging.\nfusion_bench \\\n    method=linear/weighted_average \\\n    method.normalize=true \\\n    method.weights=[0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3] \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8_model_only \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre>"},{"location":"algorithms/weighted_averaging/#large-language-models","title":"Large Language Models","text":"<p>The <code>WeightedAverageForLLama</code> is specialized for large language models with additional features:</p> <ul> <li>Backbone-only merging option</li> <li>Model saving capabilities</li> <li>Hub integration support</li> </ul> <p>Configuration template for LLaMA/Mistral model weighted averaging:</p> config/method/linear/weighted_average_for_llama.yaml<pre><code>_target_: fusion_bench.method.WeightedAverageForLLama\nnormalize: true # if true, the weights will be normalized before merging\nweights: # List of weights for each model\n  - 0.5\n  - 0.5\n# if true, only the backbone of the model will be merged and the head will be keeped as the pre-trained model (if the pre-trained model is provided, otherwise the head of the first model will be used)\n# if false, the whole model will be merged\nbackbone_only: true\nmerged_model_save_path: null\nsave_tokenizer: true\npush_to_hub: false\n</code></pre> <p>Use the following command:</p> <pre><code>fusion_bench method=linear/weighted_average_for_llama ...\n</code></pre> <p>Example of merging LLaMA models with different weights:</p> <pre><code>fusion_bench \\\n    method=linear/weighted_average_for_llama \\\n    method.weights=[0.3,0.7] \\\n    method.normalize=true \\\n    method.backbone_only=true \\\n    method.merged_model_save_path=outputs/merged_llama_model \\\n    modelpool=CausalLMPool/simle_mixtral_exp_v4.yaml \\\n    taskpool=dummy\n</code></pre>"},{"location":"algorithms/weighted_averaging/#api-usage","title":"API Usage","text":""},{"location":"algorithms/weighted_averaging/#general-pytorch-models_1","title":"General Pytorch Models","text":"<pre><code>from fusion_bench.method.weighted_average import WeightedAverageAlgorithm\n\n# Create the algorithm with custom weights\nalgorithm = WeightedAverageAlgorithm(\n    normalize=True,  # Normalize weights to sum to 1\n    weights=[0.3, 0.5, 0.2],  # Custom weights for 3 models\n    verbose=True\n)\n\n# Run on a model pool\nmerged_model = algorithm.run(modelpool)\n</code></pre>"},{"location":"algorithms/weighted_averaging/#large-language-models_1","title":"Large Language Models","text":"<pre><code>from fusion_bench.method import WeightedAverageForLLama\n\n# Create the algorithm for LLaMA models\nalgorithm = WeightedAverageForLLama(\n    normalize=True,\n    weights=[0.4, 0.6],\n    backbone_only=True,  # Only merge backbone, keep heads\n    merged_model_save_path=\"./merged_model\",\n    save_tokenizer=True,\n    push_to_hub=False\n)\n\n# Run on a CausalLMPool\nmerged_model = algorithm.run(causal_lm_pool)\n</code></pre>"},{"location":"algorithms/weighted_averaging/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.weighted_average.weighted_average.WeightedAverageAlgorithm</li> <li>fusion_bench.method.weighted_average.llama.WeightedAverageForLLama</li> </ul>"},{"location":"algorithms/weighted_ensemble/","title":"Weighted Ensemble","text":"<p>A weighted ensemble is a machine learning technique that combines the predictions of multiple models to produce a final prediction. The idea is to leverage the strengths of each individual model to improve overall performance and robustness.</p> <p>Formally, a weighted ensemble can be defined as follows:</p> <p>Given a set of \\(n\\) models, each model \\(f_i\\) produces a prediction \\(f_i(x)\\) for an input \\(x\\). Each model \\(i\\) also has an associated weight \\(w_i\\). The final prediction \\(F(x)\\) of the weighted ensemble is a weighted sum of the individual model predictions:</p> \\[ F(x) = w_1 f_1(x) + w_2 f_2(x) + ... + w_n f_n(x) \\] <p>The weights \\(w_i\\) are typically non-negative and sum to 1 (i.e., \\(\\sum_{i=1}^n w_i = 1\\)), which ensures that the final prediction is a convex combination of the individual model predictions. The weights can be determined in various ways. They could be set based on the performance of the models on a validation set, or they could be learned as part of the training process. In some cases, all models might be given equal weight. The goal of a weighted ensemble is to produce a final prediction that is more accurate or robust than any individual model. This is particularly useful when the individual models have complementary strengths and weaknesses.</p>"},{"location":"algorithms/weighted_ensemble/#examples","title":"Examples","text":""},{"location":"algorithms/weighted_ensemble/#cli-usage","title":"CLI Usage","text":"<p>Configuration template for the weighted ensemble algorithm:</p> config/method/ensemble/weighted_ensemble.yaml<pre><code># =============================================================================\n# FusionBench Method Configuration: Weighted Ensemble\n# =============================================================================\n# Ensembles model predictions using specified per-model weights.\n#\n# - Set normalize=true to rescale weights to sum to 1.\n# - weights: one float per model in the pool (order-sensitive). If null, uses equal weights.\n# =============================================================================\n_target_: fusion_bench.method.WeightedEnsembleAlgorithm\nnormalize: true\n# this should be a list of floats, one for each model in the ensemble\n# If weights is null, the ensemble will use the default weights, which are equal weights for all models.\nweights: null\n</code></pre> <p>Construct a weighted ensemble using our CLI tool <code>fusion_bench</code>:</p> <pre><code># With specific weights, override via method.weights\nfusion_bench method=ensemble/weighted_ensemble \\\n    method.weights=[0.3, 0.7] \\\n  modelpool=... \\\n  taskpool=...\n\n# With equal weights (default)\nfusion_bench method=ensemble/weighted_ensemble \\\n  modelpool=... \\\n  taskpool=...\n</code></pre>"},{"location":"algorithms/weighted_ensemble/#api-usage","title":"API Usage","text":"<p>The following Python code snippet demonstrates how to use the <code>WeightedEnsembleAlgorithm</code> class from the <code>fusion_bench.method</code> module to create a weighted ensemble of PyTorch models.</p> <pre><code>from fusion_bench.method import WeightedEnsembleAlgorithm\n\n# Instantiate the algorithm\nalgorithm = WeightedEnsembleAlgorithm(weights=[0.3, 0.7], normalize=True)\n\n# Assume we have a modelpool or a list of PyTorch models (nn.Module instances) that we want to ensemble.\nmodels = [...]  # List of nn.Module instances\n\n# Run the algorithm on the modelpool or models.\nensemble_model = algorithm.run(modelpool)  # or algorithm.run(models)\n</code></pre> <p>Here's a step-by-step explanation:</p> <ol> <li> <p>Instantiate the <code>WeightedEnsembleAlgorithm</code>: </p> <ul> <li>The algorithm is instantiated with two parameters: <code>weights</code> (a list of floats representing the weights for each model) and <code>normalize</code> (whether to normalize the weights).</li> <li>If <code>weights</code> is set to <code>None</code>, the algorithm will automatically assign equal weights to all models.</li> </ul> </li> <li> <p>Prepare your models: </p> <ul> <li>You can either use a <code>BaseModelPool</code> instance that contains your models, or directly provide a list of PyTorch <code>nn.Module</code> instances.</li> <li>If you provide a list of models directly, the algorithm will automatically wrap them in a <code>BaseModelPool</code>.</li> </ul> </li> <li> <p>Run the algorithm: </p> <ul> <li>The <code>run</code> method processes the modelpool and returns a <code>WeightedEnsembleModule</code> that represents the weighted ensemble of the input models.</li> </ul> </li> </ol> <p>Here we list the options for the weighted ensemble algorithm:</p> Option Default Description <code>weights</code> <code>null</code> A list of floats representing the weights for each model in the ensemble. If <code>null</code>, equal weights are automatically assigned to all models. <code>normalize</code> <code>True</code> Whether to normalize the weights so that they sum to 1. Default is <code>True</code>. <p>If <code>normalize</code> is set to <code>True</code>, the weights will be normalized so that they sum to 1. Mathematically, this means that the weights \\(w_i\\) will be divided by the sum of all weights, so that</p> \\[ F(x) = \\frac{w_1}{\\sum_{i=1}^n w_i} f_1(x) + \\frac{w_2}{\\sum_{i=1}^n w_i} f_2(x) + ... + \\frac{w_n}{\\sum_{i=1}^n w_i} f_n(x) \\] <p>When <code>weights</code> is set to <code>null</code> (or <code>None</code> in Python), the algorithm automatically assigns equal weights to all models: \\(w_i = \\frac{1}{n}\\) where \\(n\\) is the number of models.</p>"},{"location":"algorithms/weighted_ensemble/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.WeightedEnsembleAlgorithm</li> </ul>"},{"location":"algorithms/wudi_merging/","title":"WUDI-Merging","text":"<p>WUDI-Merging (Whoever started the interference shoUld enD It) is a novel data-free model merging method that minimizes interference between task vectors without requiring additional data or rescaling coefficients. The method is based on the theoretical insight that task vectors of linear layers constitute an approximate linear subspace for their corresponding inputs.</p>"},{"location":"algorithms/wudi_merging/#examples","title":"Examples","text":""},{"location":"algorithms/wudi_merging/#cli-usage","title":"CLI Usage","text":"<p>Merging eight CLIP models trained on different classification tasks:</p> <pre><code>fusion_bench \\\n    method=wudi/wudi \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre>"},{"location":"algorithms/wudi_merging/#api-usage","title":"API Usage","text":"<pre><code>from fusion_bench.method import WUDIMerging\nfrom fusion_bench.modelpool import CLIPVisionModelPool\n\n# Initialize the method\nmethod = WUDIMerging(iter_num=400, exclude_keys=None)\n\n# Load model pool\nmodelpool = CLIPVisionModelPool(\n    models={\n        \"model1\": \"path/to/model1\",\n        \"model2\": \"path/to/model2\",\n        # ... more models\n    }\n)\n\n# Run merging\nmerged_model = method.run(modelpool)\n</code></pre>"},{"location":"algorithms/wudi_merging/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.WUDIMerging</li> <li>Implementation: Added in pull request #144</li> </ul>"},{"location":"algorithms/pruning/magnitude_pruning/","title":"Magnitude Pruning","text":""},{"location":"algorithms/pruning/magnitude_pruning/#examples","title":"Examples","text":""},{"location":"algorithms/pruning/magnitude_pruning/#pruning-a-llama-model","title":"Pruning a Llama Model","text":""},{"location":"algorithms/pruning/magnitude_pruning/#unstructured-magnitude-pruning","title":"Unstructured Magnitude Pruning","text":"<p>The following command prunes a Llama model with a sparsity ratio of 0.7 (70% of the weights are pruned) using unstructured magnitude pruning. The pruned model is saved to <code>outputs/llama/magnitude_pruning/unstructured/0.7</code>.</p> <pre><code>fusion_bench \\\n    --config-name llama_magnitude_pruning \\\n    method.prune_type=unstructured \\\n    method.sparsity_ratio=0.7 \\\n    modelpool.models.0.path=decapoda-research/llama-7b-hf \\\n    merged_model_save_path=outputs/llama/magnitude_pruning/unstructured/0.7\n</code></pre>"},{"location":"algorithms/pruning/magnitude_pruning/#semi-structured-magnitude-pruning","title":"Semi-Structured Magnitude Pruning","text":"<p>The following command prunes a Llama model with a 2:4 semi-structured pruning ratio using magnitude pruning. The pruned model is saved to <code>outputs/llama/magnitude_pruning/semistructure/2_4</code>.</p> <pre><code>fusion_bench \\\n    --config-name llama_magnitude_pruning \\\n    method.prune_type=semistructured \\\n    method.n=2 method.m=4 \\\n    modelpool.models.0.path=decapoda-research/llama-7b-hf \\\n    merged_model_save_path=outputs/llama/magnitude_pruning/semistructure/2_4\n</code></pre> <p>Below is an example of how to visualize the pruned weights of the first layer of the pruned model.</p> <pre><code>from transformers import AutoModelForCausalLM\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\n\n# Load the pruned model\nmodel = AutoModelForCausalLM.from_pretrained(\"outputs/llama/magnitude_pruning/semistructure/2_4\")\n\n# Extract the tensor data\ntensor_data = model.model.layers[0].self_attn.q_proj.weight[:32, :32]\n\n# Convert to NumPy array\ntensor_data_np = tensor_data.detach().cpu().numpy()\n\n# Plot heatmap\nplt.figure(figsize=(10, 8))\nax = sns.heatmap(tensor_data_np, center=0, cmap=\"coolwarm\", annot=False)\n\n# Add grid lines for 4x4 cells\nfor i in range(0, tensor_data_np.shape[0], 4):\n    ax.axhline(i, color=\"black\", linewidth=0.5)\n    ax.axvline(i, color=\"black\", linewidth=0.5)\n\nplt.title(\"Heatmap of q_proj.weight[:32, :32]\")\nplt.show()\n</code></pre> <p>The following image shows the pruned weights of the first layer of the pruned model.</p> <p></p>"},{"location":"algorithms/pruning/magnitude_pruning/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.method.pruning.llama_magnitude_prune.MagnitudePruningForLlama</li> </ul>"},{"location":"api/","title":"API Reference","text":"Breaking Changes in v0.2 <p>Recent upgrade to version &gt;= v0.2.0 may cause some breaking changes. Make some documented instructions may be outdated. You can install a specific version by <code>pip install fusion-bench==0.1.6</code> or checkout to a specific version by <code>git checkout v0.1.6</code>. If you encounter any issues, please feel free to raise an issue. We are working on the documentation and will update it as soon as possible. </p> <p>Use version &gt;=0.2.0 is recommended.</p> <p>Here we provides an overview of the API reference for FusionBench.</p>"},{"location":"api/#entry-points","title":"Entry Points","text":"<ul> <li>fusion_bench.scripts.cli.main</li> </ul>"},{"location":"api/#fusion_bench.scripts.cli.main","title":"<code>main(cfg)</code>","text":"<p>Main entry point for the FusionBench command-line interface.</p> <p>This function serves as the primary entry point for the <code>fusion_bench</code> CLI command. It is decorated with Hydra's main decorator to handle configuration management, command-line argument parsing, and configuration file loading.</p> <p>The function performs the following operations: 1. Resolves any interpolations in the configuration using OmegaConf 2. Instantiates the appropriate program class based on the configuration 3. Executes the program's run method to perform the fusion task</p> <p>Parameters:</p> <ul> <li> <code>cfg</code>               (<code>DictConfig</code>)           \u2013            <p>The Hydra configuration object containing all settings for the fusion task. This includes method configuration, model pool configuration, task pool configuration, and other runtime parameters. The configuration is automatically loaded by Hydra from the specified config files and command-line overrides.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            <p>This function doesn't return a value but executes the fusion program which may save results, log outputs, or perform other side effects as configured.</p> </li> </ul> Example <p>This function is typically called automatically when running: <pre><code>fusion_bench method=... modelpool=... taskpool=...\n</code></pre></p> <p>The Hydra decorator handles parsing these command-line arguments and loading the corresponding configuration files to populate the cfg parameter.</p> Source code in <code>fusion_bench/scripts/cli.py</code> <pre><code>@hydra.main(\n    config_path=_get_default_config_path(),\n    config_name=\"fabric_model_fusion\",\n    version_base=None,\n)\ndef main(cfg: DictConfig) -&gt; None:\n    \"\"\"\n    Main entry point for the FusionBench command-line interface.\n\n    This function serves as the primary entry point for the `fusion_bench` CLI command.\n    It is decorated with Hydra's main decorator to handle configuration management,\n    command-line argument parsing, and configuration file loading.\n\n    The function performs the following operations:\n    1. Resolves any interpolations in the configuration using OmegaConf\n    2. Instantiates the appropriate program class based on the configuration\n    3. Executes the program's run method to perform the fusion task\n\n    Args:\n        cfg (DictConfig): The Hydra configuration object containing all settings\n            for the fusion task. This includes method configuration, model pool\n            configuration, task pool configuration, and other runtime parameters.\n            The configuration is automatically loaded by Hydra from the specified\n            config files and command-line overrides.\n\n    Returns:\n        None: This function doesn't return a value but executes the fusion\n            program which may save results, log outputs, or perform other\n            side effects as configured.\n\n    Example:\n        This function is typically called automatically when running:\n        ```bash\n        fusion_bench method=... modelpool=... taskpool=...\n        ```\n\n        The Hydra decorator handles parsing these command-line arguments and\n        loading the corresponding configuration files to populate the cfg parameter.\n    \"\"\"\n    OmegaConf.resolve(cfg)\n    program: BaseHydraProgram = instantiate(cfg)\n    program.run()\n</code></pre>"},{"location":"api/#class-definitions","title":"Class Definitions","text":"<p>Base class for all FusionBench components.</p> <ul> <li>fusion_bench.BaseAlgorithm: Base class for all algorithms.</li> <li>fusion_bench.BaseModelPool: Base class for all model pools.</li> <li>fusion_bench.BaseTaskPool: Base class for all task pools.</li> </ul>"},{"location":"api/#modules","title":"Modules","text":"<ul> <li>fusion_bench.mixins: Mixins.</li> <li>fusion_bench.program: Program definitions.</li> <li>fusion_bench.method: Implementation of methods.</li> <li>fusion_bench.modelpool: Model pools.</li> <li>fusion_bench.taskpool: Task pools.</li> <li>fusion_bench.utils: Utility functions.</li> <li>fusion_bench.models: Model definitions and utilities.</li> <li>fusion_bench.dataset: Dataset definitions and utilities.</li> <li>fusion_bench.tasks: Task definitions and utilities.</li> <li>fusion_bench.metrics: Metric definitions and utilities.</li> <li>fusion_bench.optim: Implementation of optimizers and learning rate schedulers.</li> <li>fusion_bench.constants: Constant definitions.</li> <li>fusion_bench.compat (deprecated): Compatibility layer for v0.1.x, this is deprecated and will be removed in future versions.</li> </ul>"},{"location":"api/fusion_bench.compat/","title":"fusion_bench.compat","text":""},{"location":"api/fusion_bench.compat/#method","title":"Method","text":""},{"location":"api/fusion_bench.compat/#fusion_bench.compat.method.ModelFusionAlgorithm","title":"<code>ModelFusionAlgorithm</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for model fusion algorithms (for v0.1.x versions, deprecated). For implementing new method, use <code>fusion_bench.method.BaseModelFusionAlgorithm</code> instead.</p> <p>This class provides a template for implementing model fusion algorithms. Subclasses must implement the <code>run</code> method to define the fusion logic.</p> <p>Attributes:</p> <ul> <li> <code>config</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration for the algorithm.</p> </li> </ul> Source code in <code>fusion_bench/compat/method/base_algorithm.py</code> <pre><code>class ModelFusionAlgorithm(ABC):\n    \"\"\"\n    Abstract base class for model fusion algorithms (for v0.1.x versions, deprecated).\n    For implementing new method, use `fusion_bench.method.BaseModelFusionAlgorithm` instead.\n\n    This class provides a template for implementing model fusion algorithms.\n    Subclasses must implement the `run` method to define the fusion logic.\n\n    Attributes:\n        config (DictConfig): Configuration for the algorithm.\n    \"\"\"\n\n    _program: \"BaseHydraProgram\" = None\n    \"\"\"A reference to the program that is running the algorithm.\"\"\"\n\n    def __init__(self, algorithm_config: Optional[DictConfig] = None):\n        \"\"\"\n        Initialize the model fusion algorithm with the given configuration.\n\n        Args:\n            algorithm_config (Optional[DictConfig]): Configuration for the algorithm. Defaults to an empty configuration if not provided.\n                Get access to the configuration using `self.config`.\n        \"\"\"\n        if algorithm_config is None:\n            algorithm_config = DictConfig({})\n        self.config = algorithm_config\n\n    def on_run_start(self):\n        \"\"\"\n        Hook method called at the start of the run.\n        Can be overridden by subclasses to perform initialization tasks.\n        \"\"\"\n        pass\n\n    def on_run_end(self):\n        \"\"\"\n        Hook method called at the end of the run.\n        Can be overridden by subclasses to perform cleanup tasks.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def run(self, modelpool: \"BaseModelPool\") -&gt; Any:\n        \"\"\"\n        Fuse the models in the given model pool.\n\n        This method must be implemented by subclasses to define the fusion logic.\n        `modelpool` is an object responsible for managing the models to be fused and optional datasets to be used for fusion.\n\n        Args:\n            modelpool: The pool of models to fuse.\n\n        Returns:\n            The fused model.\n\n        Examples:\n            &gt;&gt;&gt; algorithm = SimpleAverageAlgorithm()\n            &gt;&gt;&gt; modelpool = ModelPool()\n            &gt;&gt;&gt; merged_model = algorithm.fuse(modelpool)\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.method.ModelFusionAlgorithm.__init__","title":"<code>__init__(algorithm_config=None)</code>","text":"<p>Initialize the model fusion algorithm with the given configuration.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_config</code>               (<code>Optional[DictConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration for the algorithm. Defaults to an empty configuration if not provided. Get access to the configuration using <code>self.config</code>.</p> </li> </ul> Source code in <code>fusion_bench/compat/method/base_algorithm.py</code> <pre><code>def __init__(self, algorithm_config: Optional[DictConfig] = None):\n    \"\"\"\n    Initialize the model fusion algorithm with the given configuration.\n\n    Args:\n        algorithm_config (Optional[DictConfig]): Configuration for the algorithm. Defaults to an empty configuration if not provided.\n            Get access to the configuration using `self.config`.\n    \"\"\"\n    if algorithm_config is None:\n        algorithm_config = DictConfig({})\n    self.config = algorithm_config\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.method.ModelFusionAlgorithm.on_run_end","title":"<code>on_run_end()</code>","text":"<p>Hook method called at the end of the run. Can be overridden by subclasses to perform cleanup tasks.</p> Source code in <code>fusion_bench/compat/method/base_algorithm.py</code> <pre><code>def on_run_end(self):\n    \"\"\"\n    Hook method called at the end of the run.\n    Can be overridden by subclasses to perform cleanup tasks.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.method.ModelFusionAlgorithm.on_run_start","title":"<code>on_run_start()</code>","text":"<p>Hook method called at the start of the run. Can be overridden by subclasses to perform initialization tasks.</p> Source code in <code>fusion_bench/compat/method/base_algorithm.py</code> <pre><code>def on_run_start(self):\n    \"\"\"\n    Hook method called at the start of the run.\n    Can be overridden by subclasses to perform initialization tasks.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.method.ModelFusionAlgorithm.run","title":"<code>run(modelpool)</code>  <code>abstractmethod</code>","text":"<p>Fuse the models in the given model pool.</p> <p>This method must be implemented by subclasses to define the fusion logic. <code>modelpool</code> is an object responsible for managing the models to be fused and optional datasets to be used for fusion.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool</code>)           \u2013            <p>The pool of models to fuse.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The fused model.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; algorithm = SimpleAverageAlgorithm()\n&gt;&gt;&gt; modelpool = ModelPool()\n&gt;&gt;&gt; merged_model = algorithm.fuse(modelpool)\n</code></pre> Source code in <code>fusion_bench/compat/method/base_algorithm.py</code> <pre><code>@abstractmethod\ndef run(self, modelpool: \"BaseModelPool\") -&gt; Any:\n    \"\"\"\n    Fuse the models in the given model pool.\n\n    This method must be implemented by subclasses to define the fusion logic.\n    `modelpool` is an object responsible for managing the models to be fused and optional datasets to be used for fusion.\n\n    Args:\n        modelpool: The pool of models to fuse.\n\n    Returns:\n        The fused model.\n\n    Examples:\n        &gt;&gt;&gt; algorithm = SimpleAverageAlgorithm()\n        &gt;&gt;&gt; modelpool = ModelPool()\n        &gt;&gt;&gt; merged_model = algorithm.fuse(modelpool)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.method.AlgorithmFactory","title":"<code>AlgorithmFactory</code>","text":"<p>Factory class to create and manage different model fusion algorithms.</p> <p>This class provides methods to create algorithms based on a given configuration, register new algorithms, and list available algorithms.</p> Source code in <code>fusion_bench/compat/method/__init__.py</code> <pre><code>class AlgorithmFactory:\n    \"\"\"\n    Factory class to create and manage different model fusion algorithms.\n\n    This class provides methods to create algorithms based on a given configuration,\n    register new algorithms, and list available algorithms.\n    \"\"\"\n\n    _aglorithms = {\n        # single task learning (fine-tuning)\n        \"clip_finetune\": \".classification.clip_finetune.ImageClassificationFineTuningForCLIP\",\n        # analysis\n        # model merging methods\n        \"clip_task_wise_adamerging\": \".adamerging.clip_task_wise_adamerging.CLIPTaskWiseAdaMergingAlgorithm\",\n        \"clip_layer_wise_adamerging\": \".adamerging.clip_layer_wise_adamerging.CLIPLayerWiseAdaMergingAlgorithm\",\n        \"clip_layer_wise_adamerging_doge_ta\": \".doge_ta.clip_layer_wise_adamerging.CLIPLayerWiseAdaMergingAlgorithm\",\n        \"singular_projection_merging\": \"fusion_bench.method.smile_upscaling.singular_projection_merging.SingularProjectionMergingAlgorithm\",\n        \"clip_layer_wise_adamerging_surgery\": \".surgery.clip_layer_wise_adamerging_surgery.CLIPLayerWiseAdaMergingSurgeryAlgorithm\",\n        \"clip_task_wise_gossip\": \".gossip.clip_task_wise_gossip.CLIPTaskWiseGossipAlgorithm\",\n        \"clip_layer_wise_gossip\": \".gossip.clip_layer_wise_gossip.CLIPLayerWiseGossipAlgorithm\",\n        # plug-and-play model merging methods\n        \"clip_concrete_task_arithmetic\": \".concrete_subspace.clip_concrete_task_arithmetic.ConcreteTaskArithmeticAlgorithmForCLIP\",\n        \"clip_concrete_task_wise_adamerging\": \".concrete_subspace.clip_concrete_adamerging.ConcreteTaskWiseAdaMergingForCLIP\",\n        \"clip_concrete_layer_wise_adamerging\": \".concrete_subspace.clip_concrete_adamerging.ConcreteLayerWiseAdaMergingForCLIP\",\n        \"clip_post_defense_AWM\": \".concrete_subspace.clip_post_defense.PostDefenseAWMAlgorithmForCLIP\",\n        \"clip_post_defense_SAU\": \".concrete_subspace.clip_post_defense.PostDefenseSAUAlgorithmForCLIP\",\n        \"clip_safe_concrete_layer_wise_adamerging\": \".concrete_subspace.clip_safe_concrete_adamerging.ConcreteSafeLayerWiseAdaMergingForCLIP\",\n        \"clip_safe_concrete_task_wise_adamerging\": \".concrete_subspace.clip_safe_concrete_adamerging.ConcreteSafeTaskWiseAdaMergingForCLIP\",\n        # model mixing methods\n        \"clip_weight_ensembling_moe\": \".we_moe.clip_we_moe.CLIPWeightEnsemblingMoEAlgorithm\",\n        \"sparse_clip_weight_ensembling_moe\": \"fusion_bench.method.SparseCLIPWeightEnsemblingMoEAlgorithm\",\n        \"smile_mistral_upscaling\": \".smile_upscaling.smile_mistral_upscaling.SmileMistralUpscalingAlgorithm\",\n        \"rankone_moe\": \".rankone_moe.clip_rankone_moe.CLIPRankOneMoEAlgorithm\",\n    }\n\n    @staticmethod\n    def create_algorithm(method_config: DictConfig) -&gt; ModelFusionAlgorithm:\n        \"\"\"\n        Create an instance of a model fusion algorithm based on the provided configuration.\n\n        Args:\n            method_config (DictConfig): The configuration for the algorithm. Must contain a 'name' attribute that specifies the type of the algorithm.\n\n        Returns:\n            ModelFusionAlgorithm: An instance of the specified algorithm.\n\n        Raises:\n            ValueError: If 'name' attribute is not found in the configuration or does not match any known algorithm names.\n        \"\"\"\n        warnings.warn(\n            \"AlgorithmFactory.create_algorithm() is deprecated and will be removed in future versions. \"\n            \"Please implement new model fusion algorithm using `fusion_bench.method.BaseModelFusionAlgorithm` instead.\",\n            DeprecationWarning,\n        )\n\n        from fusion_bench.utils import import_object\n\n        algorithm_name = method_config.name\n        if algorithm_name not in AlgorithmFactory._aglorithms:\n            raise ValueError(\n                f\"Unknown algorithm: {algorithm_name}, available algorithms: {AlgorithmFactory._aglorithms.keys()}.\"\n                \"You can register a new algorithm using `AlgorithmFactory.register_algorithm()` method.\"\n            )\n        algorithm_cls = AlgorithmFactory._aglorithms[algorithm_name]\n        if isinstance(algorithm_cls, str):\n            if algorithm_cls.startswith(\".\"):\n                algorithm_cls = f\"fusion_bench.method.{algorithm_cls[1:]}\"\n            algorithm_cls = import_object(algorithm_cls)\n        return algorithm_cls(method_config)\n\n    @staticmethod\n    def register_algorithm(\n        name: str, algorithm_cls: Type[ModelFusionAlgorithm]\n    ) -&gt; None:\n        \"\"\"\n        Register a new algorithm with the factory.\n\n        Args:\n            name (str): The name of the algorithm.\n            algorithm_cls: The class of the algorithm to register.\n        \"\"\"\n        AlgorithmFactory._aglorithms[name] = algorithm_cls\n\n    @classmethod\n    def available_algorithms(cls) -&gt; List[str]:\n        \"\"\"\n        Get a list of available algorithms.\n\n        Returns:\n            list: A list of available algorithm names.\n        \"\"\"\n        return list(cls._aglorithms.keys())\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.method.AlgorithmFactory.available_algorithms","title":"<code>available_algorithms()</code>  <code>classmethod</code>","text":"<p>Get a list of available algorithms.</p> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>List[str]</code> )          \u2013            <p>A list of available algorithm names.</p> </li> </ul> Source code in <code>fusion_bench/compat/method/__init__.py</code> <pre><code>@classmethod\ndef available_algorithms(cls) -&gt; List[str]:\n    \"\"\"\n    Get a list of available algorithms.\n\n    Returns:\n        list: A list of available algorithm names.\n    \"\"\"\n    return list(cls._aglorithms.keys())\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.method.AlgorithmFactory.create_algorithm","title":"<code>create_algorithm(method_config)</code>  <code>staticmethod</code>","text":"<p>Create an instance of a model fusion algorithm based on the provided configuration.</p> <p>Parameters:</p> <ul> <li> <code>method_config</code>               (<code>DictConfig</code>)           \u2013            <p>The configuration for the algorithm. Must contain a 'name' attribute that specifies the type of the algorithm.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ModelFusionAlgorithm</code> (              <code>ModelFusionAlgorithm</code> )          \u2013            <p>An instance of the specified algorithm.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If 'name' attribute is not found in the configuration or does not match any known algorithm names.</p> </li> </ul> Source code in <code>fusion_bench/compat/method/__init__.py</code> <pre><code>@staticmethod\ndef create_algorithm(method_config: DictConfig) -&gt; ModelFusionAlgorithm:\n    \"\"\"\n    Create an instance of a model fusion algorithm based on the provided configuration.\n\n    Args:\n        method_config (DictConfig): The configuration for the algorithm. Must contain a 'name' attribute that specifies the type of the algorithm.\n\n    Returns:\n        ModelFusionAlgorithm: An instance of the specified algorithm.\n\n    Raises:\n        ValueError: If 'name' attribute is not found in the configuration or does not match any known algorithm names.\n    \"\"\"\n    warnings.warn(\n        \"AlgorithmFactory.create_algorithm() is deprecated and will be removed in future versions. \"\n        \"Please implement new model fusion algorithm using `fusion_bench.method.BaseModelFusionAlgorithm` instead.\",\n        DeprecationWarning,\n    )\n\n    from fusion_bench.utils import import_object\n\n    algorithm_name = method_config.name\n    if algorithm_name not in AlgorithmFactory._aglorithms:\n        raise ValueError(\n            f\"Unknown algorithm: {algorithm_name}, available algorithms: {AlgorithmFactory._aglorithms.keys()}.\"\n            \"You can register a new algorithm using `AlgorithmFactory.register_algorithm()` method.\"\n        )\n    algorithm_cls = AlgorithmFactory._aglorithms[algorithm_name]\n    if isinstance(algorithm_cls, str):\n        if algorithm_cls.startswith(\".\"):\n            algorithm_cls = f\"fusion_bench.method.{algorithm_cls[1:]}\"\n        algorithm_cls = import_object(algorithm_cls)\n    return algorithm_cls(method_config)\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.method.AlgorithmFactory.register_algorithm","title":"<code>register_algorithm(name, algorithm_cls)</code>  <code>staticmethod</code>","text":"<p>Register a new algorithm with the factory.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm.</p> </li> <li> <code>algorithm_cls</code>               (<code>Type[ModelFusionAlgorithm]</code>)           \u2013            <p>The class of the algorithm to register.</p> </li> </ul> Source code in <code>fusion_bench/compat/method/__init__.py</code> <pre><code>@staticmethod\ndef register_algorithm(\n    name: str, algorithm_cls: Type[ModelFusionAlgorithm]\n) -&gt; None:\n    \"\"\"\n    Register a new algorithm with the factory.\n\n    Args:\n        name (str): The name of the algorithm.\n        algorithm_cls: The class of the algorithm to register.\n    \"\"\"\n    AlgorithmFactory._aglorithms[name] = algorithm_cls\n</code></pre>"},{"location":"api/fusion_bench.compat/#model-pool","title":"Model Pool","text":""},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool","title":"<code>ModelPool</code>","text":"<p>               Bases: <code>ABC</code></p> <p>This is the base class for all modelpools. For verison v0.1.x, deprecated. Please implemente new algorithms use <code>fusion_bench.modelpool.BaseModelPool</code>.</p> Source code in <code>fusion_bench/compat/modelpool/base_pool.py</code> <pre><code>class ModelPool(ABC):\n    \"\"\"\n    This is the base class for all modelpools.\n    For verison v0.1.x, deprecated.\n    Please implemente new algorithms use `fusion_bench.modelpool.BaseModelPool`.\n    \"\"\"\n\n    _model_names = None\n\n    def __init__(self, modelpool_config: Optional[DictConfig] = None):\n        \"\"\"\n        Initialize the ModelPool with the given configuration.\n\n        Args:\n            modelpool_config (Optional[DictConfig]): The configuration for the model pool.\n        \"\"\"\n        super().__init__()\n        self.config = modelpool_config\n\n        # check for duplicate model names\n        if self.config is not None and self.config.get(\"models\", None) is not None:\n            model_names = [model[\"name\"] for model in self.config[\"models\"]]\n            assert len(model_names) == len(\n                set(model_names)\n            ), \"Duplicate model names found in model pool\"\n            self._model_names = model_names\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Return the number of models in the model pool, exclude special models such as `_pretrained_`.\n\n        Returns:\n            int: The number of models in the model pool.\n        \"\"\"\n        return len(self.model_names)\n\n    @property\n    def model_names(self) -&gt; List[str]:\n        \"\"\"\n        This property returns a list of model names from the configuration, excluding any names that start or end with an underscore.\n        To obtain all model names, including those starting or ending with an underscore, use the `_model_names` attribute.\n\n        Returns:\n            list: A list of model names.\n        \"\"\"\n        names = [\n            name for name in self._model_names if name[0] != \"_\" and name[-1] != \"_\"\n        ]\n        return names\n\n    @property\n    def has_pretrained(self) -&gt; bool:\n        \"\"\"\n        Check if the pretrained model is available in the model pool.\n\n        Returns:\n            bool: True if the pretrained model is available, False otherwise.\n        \"\"\"\n        for model_config in self.config[\"models\"]:\n            if model_config.get(\"name\", None) == \"_pretrained_\":\n                return True\n        return False\n\n    def get_model_config(self, model_name: str) -&gt; Dict:\n        \"\"\"\n        Retrieves the configuration for a specific model from the model pool.\n\n        Args:\n            model_name (str): The name of the model for which to retrieve the configuration.\n\n        Returns:\n            dict: The configuration dictionary for the specified model.\n\n        Raises:\n            ValueError: If the specified model is not found in the model pool.\n        \"\"\"\n        for model in self.config[\"models\"]:\n            if model[\"name\"] == model_name:\n                return model\n        raise ValueError(f\"Model {model_name} not found in model pool\")\n\n    def load_model(self, model_config: Union[str, DictConfig]) -&gt; nn.Module:\n        \"\"\"\n        The models are load lazily, so this method should be implemented to load the model from the model pool.\n\n        Load the model from the model pool.\n\n        Args:\n            model_config (Union[str, DictConfig]): The configuration dictionary for the model to load.\n\n        Returns:\n            Any: The loaded model.\n        \"\"\"\n        raise NotImplementedError\n\n    def load_pretrained_or_first_model(self, *args, **kwargs):\n        \"\"\"\n        Load the pretrained model if available, otherwise load the first model in the list.\n\n        This method checks if a pretrained model is available. If it is, it loads the pretrained model.\n        If not, it loads the first model from the list of model names.\n\n        Returns:\n            nn.Module: The loaded model.\n        \"\"\"\n        if self.has_pretrained:\n            model = self.load_model(\"_pretrained_\", *args, **kwargs)\n        else:\n            model = self.load_model(self.model_names[0], *args, **kwargs)\n        return model\n\n    def save_model(self, model: nn.Module, path: str):\n        \"\"\"\n        Save the state dictionary of the model to the specified path.\n\n        Args:\n            model (nn.Module): The model whose state dictionary is to be saved.\n            path (str): The path where the state dictionary will be saved.\n        \"\"\"\n        with timeit_context(f\"Saving the state dict of model to {path}\"):\n            torch.save(model.state_dict(), path)\n\n    def models(self):\n        \"\"\"\n        Generator that yields models from the model pool.\n\n        Yields:\n            nn.Module: The next model in the model pool.\n        \"\"\"\n        for model_name in self.model_names:\n            yield self.load_model(model_name)\n\n    def named_models(self):\n        \"\"\"\n        Generator that yields model names and models from the model pool.\n\n        Yields:\n            tuple: A tuple containing the model name and the model.\n        \"\"\"\n        for model_name in self.model_names:\n            yield model_name, self.load_model(model_name)\n\n    def get_train_dataset(self, model_name: str):\n        \"\"\"\n        Get the training dataset for the model.\n\n        Args:\n            model_name (str): The name of the model for which to get the training dataset.\n\n        Returns:\n            Any: The training dataset for the model.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_test_dataset(self, model_name: str):\n        \"\"\"\n        Get the testing dataset for the model.\n\n        Args:\n            model_name (str): The name of the model for which to get the testing dataset.\n\n        Returns:\n            Any: The testing dataset for the model.\n        \"\"\"\n        raise NotImplementedError\n\n    def setup_taskpool(self, taskpool):\n        \"\"\"\n        Setup the taskpool before evaluation.\n        Such as setting the fabric, processor, tokenizer, etc.\n\n        Args:\n            taskpool (Any): The taskpool to setup.\n        \"\"\"\n        pass\n\n    def to_modellist(self) -&gt; List[nn.Module]:\n        \"\"\"\n        Convert the model pool to a list of models.\n\n        Returns:\n            list: A list of models.\n        \"\"\"\n        return [self.load_model(m) for m in self.model_names]\n\n    def to_modeldict(self) -&gt; Dict[str, nn.Module]:\n        \"\"\"\n        Convert the model pool to a dictionary of models.\n\n        Returns:\n            dict: A dictionary of models.\n        \"\"\"\n        return {m: self.load_model(m) for m in self.model_names}\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.has_pretrained","title":"<code>has_pretrained</code>  <code>property</code>","text":"<p>Check if the pretrained model is available in the model pool.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the pretrained model is available, False otherwise.</p> </li> </ul>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.model_names","title":"<code>model_names</code>  <code>property</code>","text":"<p>This property returns a list of model names from the configuration, excluding any names that start or end with an underscore. To obtain all model names, including those starting or ending with an underscore, use the <code>_model_names</code> attribute.</p> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>List[str]</code> )          \u2013            <p>A list of model names.</p> </li> </ul>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.__init__","title":"<code>__init__(modelpool_config=None)</code>","text":"<p>Initialize the ModelPool with the given configuration.</p> <p>Parameters:</p> <ul> <li> <code>modelpool_config</code>               (<code>Optional[DictConfig]</code>, default:                   <code>None</code> )           \u2013            <p>The configuration for the model pool.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/base_pool.py</code> <pre><code>def __init__(self, modelpool_config: Optional[DictConfig] = None):\n    \"\"\"\n    Initialize the ModelPool with the given configuration.\n\n    Args:\n        modelpool_config (Optional[DictConfig]): The configuration for the model pool.\n    \"\"\"\n    super().__init__()\n    self.config = modelpool_config\n\n    # check for duplicate model names\n    if self.config is not None and self.config.get(\"models\", None) is not None:\n        model_names = [model[\"name\"] for model in self.config[\"models\"]]\n        assert len(model_names) == len(\n            set(model_names)\n        ), \"Duplicate model names found in model pool\"\n        self._model_names = model_names\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of models in the model pool, exclude special models such as <code>_pretrained_</code>.</p> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>The number of models in the model pool.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/base_pool.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the number of models in the model pool, exclude special models such as `_pretrained_`.\n\n    Returns:\n        int: The number of models in the model pool.\n    \"\"\"\n    return len(self.model_names)\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.get_model_config","title":"<code>get_model_config(model_name)</code>","text":"<p>Retrieves the configuration for a specific model from the model pool.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model for which to retrieve the configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>Dict</code> )          \u2013            <p>The configuration dictionary for the specified model.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the specified model is not found in the model pool.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/base_pool.py</code> <pre><code>def get_model_config(self, model_name: str) -&gt; Dict:\n    \"\"\"\n    Retrieves the configuration for a specific model from the model pool.\n\n    Args:\n        model_name (str): The name of the model for which to retrieve the configuration.\n\n    Returns:\n        dict: The configuration dictionary for the specified model.\n\n    Raises:\n        ValueError: If the specified model is not found in the model pool.\n    \"\"\"\n    for model in self.config[\"models\"]:\n        if model[\"name\"] == model_name:\n            return model\n    raise ValueError(f\"Model {model_name} not found in model pool\")\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.get_test_dataset","title":"<code>get_test_dataset(model_name)</code>","text":"<p>Get the testing dataset for the model.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model for which to get the testing dataset.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>          \u2013            <p>The testing dataset for the model.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/base_pool.py</code> <pre><code>def get_test_dataset(self, model_name: str):\n    \"\"\"\n    Get the testing dataset for the model.\n\n    Args:\n        model_name (str): The name of the model for which to get the testing dataset.\n\n    Returns:\n        Any: The testing dataset for the model.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.get_train_dataset","title":"<code>get_train_dataset(model_name)</code>","text":"<p>Get the training dataset for the model.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model for which to get the training dataset.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>          \u2013            <p>The training dataset for the model.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/base_pool.py</code> <pre><code>def get_train_dataset(self, model_name: str):\n    \"\"\"\n    Get the training dataset for the model.\n\n    Args:\n        model_name (str): The name of the model for which to get the training dataset.\n\n    Returns:\n        Any: The training dataset for the model.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.load_model","title":"<code>load_model(model_config)</code>","text":"<p>The models are load lazily, so this method should be implemented to load the model from the model pool.</p> <p>Load the model from the model pool.</p> <p>Parameters:</p> <ul> <li> <code>model_config</code>               (<code>Union[str, DictConfig]</code>)           \u2013            <p>The configuration dictionary for the model to load.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code> (              <code>Module</code> )          \u2013            <p>The loaded model.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/base_pool.py</code> <pre><code>def load_model(self, model_config: Union[str, DictConfig]) -&gt; nn.Module:\n    \"\"\"\n    The models are load lazily, so this method should be implemented to load the model from the model pool.\n\n    Load the model from the model pool.\n\n    Args:\n        model_config (Union[str, DictConfig]): The configuration dictionary for the model to load.\n\n    Returns:\n        Any: The loaded model.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.load_pretrained_or_first_model","title":"<code>load_pretrained_or_first_model(*args, **kwargs)</code>","text":"<p>Load the pretrained model if available, otherwise load the first model in the list.</p> <p>This method checks if a pretrained model is available. If it is, it loads the pretrained model. If not, it loads the first model from the list of model names.</p> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The loaded model.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/base_pool.py</code> <pre><code>def load_pretrained_or_first_model(self, *args, **kwargs):\n    \"\"\"\n    Load the pretrained model if available, otherwise load the first model in the list.\n\n    This method checks if a pretrained model is available. If it is, it loads the pretrained model.\n    If not, it loads the first model from the list of model names.\n\n    Returns:\n        nn.Module: The loaded model.\n    \"\"\"\n    if self.has_pretrained:\n        model = self.load_model(\"_pretrained_\", *args, **kwargs)\n    else:\n        model = self.load_model(self.model_names[0], *args, **kwargs)\n    return model\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.models","title":"<code>models()</code>","text":"<p>Generator that yields models from the model pool.</p> <p>Yields:</p> <ul> <li>           \u2013            <p>nn.Module: The next model in the model pool.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/base_pool.py</code> <pre><code>def models(self):\n    \"\"\"\n    Generator that yields models from the model pool.\n\n    Yields:\n        nn.Module: The next model in the model pool.\n    \"\"\"\n    for model_name in self.model_names:\n        yield self.load_model(model_name)\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.named_models","title":"<code>named_models()</code>","text":"<p>Generator that yields model names and models from the model pool.</p> <p>Yields:</p> <ul> <li> <code>tuple</code>          \u2013            <p>A tuple containing the model name and the model.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/base_pool.py</code> <pre><code>def named_models(self):\n    \"\"\"\n    Generator that yields model names and models from the model pool.\n\n    Yields:\n        tuple: A tuple containing the model name and the model.\n    \"\"\"\n    for model_name in self.model_names:\n        yield model_name, self.load_model(model_name)\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.save_model","title":"<code>save_model(model, path)</code>","text":"<p>Save the state dictionary of the model to the specified path.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model whose state dictionary is to be saved.</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path where the state dictionary will be saved.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/base_pool.py</code> <pre><code>def save_model(self, model: nn.Module, path: str):\n    \"\"\"\n    Save the state dictionary of the model to the specified path.\n\n    Args:\n        model (nn.Module): The model whose state dictionary is to be saved.\n        path (str): The path where the state dictionary will be saved.\n    \"\"\"\n    with timeit_context(f\"Saving the state dict of model to {path}\"):\n        torch.save(model.state_dict(), path)\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.setup_taskpool","title":"<code>setup_taskpool(taskpool)</code>","text":"<p>Setup the taskpool before evaluation. Such as setting the fabric, processor, tokenizer, etc.</p> <p>Parameters:</p> <ul> <li> <code>taskpool</code>               (<code>Any</code>)           \u2013            <p>The taskpool to setup.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/base_pool.py</code> <pre><code>def setup_taskpool(self, taskpool):\n    \"\"\"\n    Setup the taskpool before evaluation.\n    Such as setting the fabric, processor, tokenizer, etc.\n\n    Args:\n        taskpool (Any): The taskpool to setup.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.to_modeldict","title":"<code>to_modeldict()</code>","text":"<p>Convert the model pool to a dictionary of models.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>Dict[str, Module]</code> )          \u2013            <p>A dictionary of models.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/base_pool.py</code> <pre><code>def to_modeldict(self) -&gt; Dict[str, nn.Module]:\n    \"\"\"\n    Convert the model pool to a dictionary of models.\n\n    Returns:\n        dict: A dictionary of models.\n    \"\"\"\n    return {m: self.load_model(m) for m in self.model_names}\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPool.to_modellist","title":"<code>to_modellist()</code>","text":"<p>Convert the model pool to a list of models.</p> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>List[Module]</code> )          \u2013            <p>A list of models.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/base_pool.py</code> <pre><code>def to_modellist(self) -&gt; List[nn.Module]:\n    \"\"\"\n    Convert the model pool to a list of models.\n\n    Returns:\n        list: A list of models.\n    \"\"\"\n    return [self.load_model(m) for m in self.model_names]\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPoolFactory","title":"<code>ModelPoolFactory</code>","text":"<p>Factory class to create and manage different model pools.</p> <p>This class provides methods to create model pools based on a given configuration, register new model pools, and list available model pools.</p> Source code in <code>fusion_bench/compat/modelpool/__init__.py</code> <pre><code>class ModelPoolFactory:\n    \"\"\"\n    Factory class to create and manage different model pools.\n\n    This class provides methods to create model pools based on a given configuration,\n    register new model pools, and list available model pools.\n    \"\"\"\n\n    _modelpool = {\n        \"NYUv2ModelPool\": \"fusion_bench.modelpool.nyuv2_modelpool.NYUv2ModelPool\",\n        \"huggingface_clip_vision\": HuggingFaceClipVisionPool,\n        \"HF_GPT2ForSequenceClassification\": GPT2ForSequenceClassificationPool,\n        \"AutoModelPool\": \".huggingface_automodel.AutoModelPool\",\n        # CausualLM\n        \"AutoModelForCausalLMPool\": \".huggingface_llm.AutoModelForCausalLMPool\",\n        \"LLamaForCausalLMPool\": \".huggingface_llm.LLamaForCausalLMPool\",\n        \"MistralForCausalLMPool\": \".huggingface_llm.MistralForCausalLMPool\",\n        # Seq2SeqLM\n        \"AutoModelForSeq2SeqLMPool\": AutoModelForSeq2SeqLMPool,\n        \"PeftModelForSeq2SeqLMPool\": PeftModelForSeq2SeqLMPool,\n    }\n\n    @staticmethod\n    def create_modelpool(modelpool_config: DictConfig) -&gt; ModelPool:\n        \"\"\"\n        Create an instance of a model pool based on the provided configuration.\n        This is for v0.1.x versions, deprecated.\n        For implementing new model pool, use `fusion_bench.modelpool.BaseModelPool` instead.\n\n        Args:\n            modelpool_config (DictConfig): The configuration for the model pool.\n            Must contain a 'type' attribute that specifies the type of the model pool.\n\n        Returns:\n            ModelPool: An instance of the specified model pool.\n\n        Raises:\n            ValueError: If 'type' attribute is not found in the configuration or does not match any known model pool types.\n        \"\"\"\n        warnings.warn(\n            \"ModelPoolFactory.create_modelpool() is deprecated and will be removed in future versions. \"\n            \"Please implement new model pool using `fusion_bench.modelpool.BaseModelPool` instead.\",\n            DeprecationWarning,\n        )\n\n        from fusion_bench.utils import import_object\n\n        modelpool_type = modelpool_config.get(\"type\")\n        if modelpool_type is None:\n            raise ValueError(\"Model pool type not specified\")\n\n        if modelpool_type not in ModelPoolFactory._modelpool:\n            raise ValueError(\n                f\"Unknown model pool: {modelpool_type}, available model pools: {ModelPoolFactory._modelpool.keys()}. You can register a new model pool using `ModelPoolFactory.register_modelpool()` method.\"\n            )\n        modelpool_cls = ModelPoolFactory._modelpool[modelpool_type]\n        if isinstance(modelpool_cls, str):\n            if modelpool_cls.startswith(\".\"):\n                modelpool_cls = f\"fusion_bench.compat.modelpool.{modelpool_cls[1:]}\"\n            modelpool_cls = import_object(modelpool_cls)\n        return modelpool_cls(modelpool_config)\n\n    @staticmethod\n    def register_modelpool(name: str, modelpool_cls):\n        \"\"\"\n        Register a new model pool with the factory.\n\n        Args:\n            name (str): The name of the model pool.\n            modelpool_cls: The class of the model pool to register.\n        \"\"\"\n        ModelPoolFactory._modelpool[name] = modelpool_cls\n\n    @classmethod\n    def available_modelpools(cls):\n        \"\"\"\n        Get a list of available model pools.\n\n        Returns:\n            list: A list of available model pool names.\n        \"\"\"\n        return list(cls._modelpool.keys())\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPoolFactory.available_modelpools","title":"<code>available_modelpools()</code>  <code>classmethod</code>","text":"<p>Get a list of available model pools.</p> <p>Returns:</p> <ul> <li> <code>list</code>          \u2013            <p>A list of available model pool names.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/__init__.py</code> <pre><code>@classmethod\ndef available_modelpools(cls):\n    \"\"\"\n    Get a list of available model pools.\n\n    Returns:\n        list: A list of available model pool names.\n    \"\"\"\n    return list(cls._modelpool.keys())\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPoolFactory.create_modelpool","title":"<code>create_modelpool(modelpool_config)</code>  <code>staticmethod</code>","text":"<p>Create an instance of a model pool based on the provided configuration. This is for v0.1.x versions, deprecated. For implementing new model pool, use <code>fusion_bench.modelpool.BaseModelPool</code> instead.</p> <p>Parameters:</p> <ul> <li> <code>modelpool_config</code>               (<code>DictConfig</code>)           \u2013            <p>The configuration for the model pool.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ModelPool</code> (              <code>ModelPool</code> )          \u2013            <p>An instance of the specified model pool.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If 'type' attribute is not found in the configuration or does not match any known model pool types.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/__init__.py</code> <pre><code>@staticmethod\ndef create_modelpool(modelpool_config: DictConfig) -&gt; ModelPool:\n    \"\"\"\n    Create an instance of a model pool based on the provided configuration.\n    This is for v0.1.x versions, deprecated.\n    For implementing new model pool, use `fusion_bench.modelpool.BaseModelPool` instead.\n\n    Args:\n        modelpool_config (DictConfig): The configuration for the model pool.\n        Must contain a 'type' attribute that specifies the type of the model pool.\n\n    Returns:\n        ModelPool: An instance of the specified model pool.\n\n    Raises:\n        ValueError: If 'type' attribute is not found in the configuration or does not match any known model pool types.\n    \"\"\"\n    warnings.warn(\n        \"ModelPoolFactory.create_modelpool() is deprecated and will be removed in future versions. \"\n        \"Please implement new model pool using `fusion_bench.modelpool.BaseModelPool` instead.\",\n        DeprecationWarning,\n    )\n\n    from fusion_bench.utils import import_object\n\n    modelpool_type = modelpool_config.get(\"type\")\n    if modelpool_type is None:\n        raise ValueError(\"Model pool type not specified\")\n\n    if modelpool_type not in ModelPoolFactory._modelpool:\n        raise ValueError(\n            f\"Unknown model pool: {modelpool_type}, available model pools: {ModelPoolFactory._modelpool.keys()}. You can register a new model pool using `ModelPoolFactory.register_modelpool()` method.\"\n        )\n    modelpool_cls = ModelPoolFactory._modelpool[modelpool_type]\n    if isinstance(modelpool_cls, str):\n        if modelpool_cls.startswith(\".\"):\n            modelpool_cls = f\"fusion_bench.compat.modelpool.{modelpool_cls[1:]}\"\n        modelpool_cls = import_object(modelpool_cls)\n    return modelpool_cls(modelpool_config)\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.modelpool.ModelPoolFactory.register_modelpool","title":"<code>register_modelpool(name, modelpool_cls)</code>  <code>staticmethod</code>","text":"<p>Register a new model pool with the factory.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the model pool.</p> </li> <li> <code>modelpool_cls</code>           \u2013            <p>The class of the model pool to register.</p> </li> </ul> Source code in <code>fusion_bench/compat/modelpool/__init__.py</code> <pre><code>@staticmethod\ndef register_modelpool(name: str, modelpool_cls):\n    \"\"\"\n    Register a new model pool with the factory.\n\n    Args:\n        name (str): The name of the model pool.\n        modelpool_cls: The class of the model pool to register.\n    \"\"\"\n    ModelPoolFactory._modelpool[name] = modelpool_cls\n</code></pre>"},{"location":"api/fusion_bench.compat/#task-pool","title":"Task Pool","text":""},{"location":"api/fusion_bench.compat/#fusion_bench.compat.taskpool.TaskPool","title":"<code>TaskPool</code>","text":"<p>A class to manage a pool of tasks for evaluation. This is the base class for version 0.1.x, deprecated. Use <code>fusion_bench.taskpool.BaseTaskPool</code> instead.</p> <p>Attributes:</p> <ul> <li> <code>config</code>               (<code>DictConfig</code>)           \u2013            <p>The configuration for the task pool.</p> </li> <li> <code>_all_task_names</code>               (<code>List[str]</code>)           \u2013            <p>A list of all task names in the task pool.</p> </li> </ul> Source code in <code>fusion_bench/compat/taskpool/base_pool.py</code> <pre><code>class TaskPool:\n    \"\"\"\n    A class to manage a pool of tasks for evaluation.\n    This is the base class for version 0.1.x, deprecated.\n    Use `fusion_bench.taskpool.BaseTaskPool` instead.\n\n    Attributes:\n        config (DictConfig): The configuration for the task pool.\n        _all_task_names (List[str]): A list of all task names in the task pool.\n    \"\"\"\n\n    _program = None\n\n    def __init__(self, taskpool_config: DictConfig):\n        \"\"\"\n        Initialize the TaskPool with the given configuration.\n\n        Args:\n            taskpool_config (DictConfig): The configuration for the task pool.\n        \"\"\"\n        super().__init__()\n        self.config = taskpool_config\n\n        # Check for duplicate task names\n        if self.config.get(\"tasks\", None) is not None:\n            task_names = [task[\"name\"] for task in self.config[\"tasks\"]]\n            assert len(task_names) == len(\n                set(task_names)\n            ), \"Duplicate task names found in the task pool\"\n            self._all_task_names = task_names\n\n    def evaluate(self, model):\n        \"\"\"\n        Evaluate the model on all tasks in the task pool, and return a report.\n\n        Take image classification as an example, the report will look like:\n\n        ```python\n        {\n            \"mnist\": {\n                \"accuracy\": 0.8,\n                \"loss\": 0.2,\n            },\n            &lt;task_name&gt;: {\n                &lt;metric_name&gt;: &lt;metric_value&gt;,\n                ...\n            },\n        }\n        ```\n\n        Args:\n            model: The model to evaluate.\n\n        Returns:\n            report (dict): A dictionary containing the results of the evaluation for each task.\n        \"\"\"\n        report = {}\n        for task_name in tqdm(self.task_names, desc=\"Evaluating tasks\"):\n            task = self.load_task(task_name)\n            result = task.evaluate(model)\n            report[task_name] = result\n        return report\n\n    @property\n    def task_names(self):\n        \"\"\"\n        Return a list of all task names in the task pool.\n\n        Returns:\n            List[str]: A list of all task names.\n        \"\"\"\n        return self._all_task_names\n\n    def get_task_config(self, task_name: str):\n        \"\"\"\n        Retrieve the configuration for a specific task from the task pool.\n\n        Args:\n            task_name (str): The name of the task for which to retrieve the configuration.\n\n        Returns:\n            DictConfig: The configuration dictionary for the specified task.\n\n        Raises:\n            ValueError: If the specified task is not found in the task pool.\n        \"\"\"\n        for task in self.config[\"tasks\"]:\n            if task[\"name\"] == task_name:\n                return task\n        raise ValueError(f\"Task {task_name} not found in the task pool\")\n\n    def load_task(self, task_name_or_config: Union[str, DictConfig]):\n        \"\"\"\n        Load a task from the task pool.\n\n        Args:\n            task_name_or_config (Union[str, DictConfig]): The name or configuration of the task to load.\n\n        Returns:\n            Any: The loaded task.\n\n        Raises:\n            NotImplementedError: If the method is not implemented in the subclass.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.taskpool.TaskPool.task_names","title":"<code>task_names</code>  <code>property</code>","text":"<p>Return a list of all task names in the task pool.</p> <p>Returns:</p> <ul> <li>           \u2013            <p>List[str]: A list of all task names.</p> </li> </ul>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.taskpool.TaskPool.__init__","title":"<code>__init__(taskpool_config)</code>","text":"<p>Initialize the TaskPool with the given configuration.</p> <p>Parameters:</p> <ul> <li> <code>taskpool_config</code>               (<code>DictConfig</code>)           \u2013            <p>The configuration for the task pool.</p> </li> </ul> Source code in <code>fusion_bench/compat/taskpool/base_pool.py</code> <pre><code>def __init__(self, taskpool_config: DictConfig):\n    \"\"\"\n    Initialize the TaskPool with the given configuration.\n\n    Args:\n        taskpool_config (DictConfig): The configuration for the task pool.\n    \"\"\"\n    super().__init__()\n    self.config = taskpool_config\n\n    # Check for duplicate task names\n    if self.config.get(\"tasks\", None) is not None:\n        task_names = [task[\"name\"] for task in self.config[\"tasks\"]]\n        assert len(task_names) == len(\n            set(task_names)\n        ), \"Duplicate task names found in the task pool\"\n        self._all_task_names = task_names\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.taskpool.TaskPool.evaluate","title":"<code>evaluate(model)</code>","text":"<p>Evaluate the model on all tasks in the task pool, and return a report.</p> <p>Take image classification as an example, the report will look like:</p> <pre><code>{\n    \"mnist\": {\n        \"accuracy\": 0.8,\n        \"loss\": 0.2,\n    },\n    &lt;task_name&gt;: {\n        &lt;metric_name&gt;: &lt;metric_value&gt;,\n        ...\n    },\n}\n</code></pre> <p>Parameters:</p> <ul> <li> <code>model</code>           \u2013            <p>The model to evaluate.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>report</code> (              <code>dict</code> )          \u2013            <p>A dictionary containing the results of the evaluation for each task.</p> </li> </ul> Source code in <code>fusion_bench/compat/taskpool/base_pool.py</code> <pre><code>def evaluate(self, model):\n    \"\"\"\n    Evaluate the model on all tasks in the task pool, and return a report.\n\n    Take image classification as an example, the report will look like:\n\n    ```python\n    {\n        \"mnist\": {\n            \"accuracy\": 0.8,\n            \"loss\": 0.2,\n        },\n        &lt;task_name&gt;: {\n            &lt;metric_name&gt;: &lt;metric_value&gt;,\n            ...\n        },\n    }\n    ```\n\n    Args:\n        model: The model to evaluate.\n\n    Returns:\n        report (dict): A dictionary containing the results of the evaluation for each task.\n    \"\"\"\n    report = {}\n    for task_name in tqdm(self.task_names, desc=\"Evaluating tasks\"):\n        task = self.load_task(task_name)\n        result = task.evaluate(model)\n        report[task_name] = result\n    return report\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.taskpool.TaskPool.get_task_config","title":"<code>get_task_config(task_name)</code>","text":"<p>Retrieve the configuration for a specific task from the task pool.</p> <p>Parameters:</p> <ul> <li> <code>task_name</code>               (<code>str</code>)           \u2013            <p>The name of the task for which to retrieve the configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DictConfig</code>          \u2013            <p>The configuration dictionary for the specified task.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the specified task is not found in the task pool.</p> </li> </ul> Source code in <code>fusion_bench/compat/taskpool/base_pool.py</code> <pre><code>def get_task_config(self, task_name: str):\n    \"\"\"\n    Retrieve the configuration for a specific task from the task pool.\n\n    Args:\n        task_name (str): The name of the task for which to retrieve the configuration.\n\n    Returns:\n        DictConfig: The configuration dictionary for the specified task.\n\n    Raises:\n        ValueError: If the specified task is not found in the task pool.\n    \"\"\"\n    for task in self.config[\"tasks\"]:\n        if task[\"name\"] == task_name:\n            return task\n    raise ValueError(f\"Task {task_name} not found in the task pool\")\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.taskpool.TaskPool.load_task","title":"<code>load_task(task_name_or_config)</code>","text":"<p>Load a task from the task pool.</p> <p>Parameters:</p> <ul> <li> <code>task_name_or_config</code>               (<code>Union[str, DictConfig]</code>)           \u2013            <p>The name or configuration of the task to load.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>          \u2013            <p>The loaded task.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If the method is not implemented in the subclass.</p> </li> </ul> Source code in <code>fusion_bench/compat/taskpool/base_pool.py</code> <pre><code>def load_task(self, task_name_or_config: Union[str, DictConfig]):\n    \"\"\"\n    Load a task from the task pool.\n\n    Args:\n        task_name_or_config (Union[str, DictConfig]): The name or configuration of the task to load.\n\n    Returns:\n        Any: The loaded task.\n\n    Raises:\n        NotImplementedError: If the method is not implemented in the subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.taskpool.TaskPoolFactory","title":"<code>TaskPoolFactory</code>","text":"<p>Factory class to create and manage different task pools. This is for v0.1.x versions, deprecated. For implementing new task pool, use <code>fusion_bench.taskpool.BaseTaskPool</code> instead.</p> <p>This class provides methods to create task pools based on a given configuration, register new task pools, and list available task pools.</p> Source code in <code>fusion_bench/compat/taskpool/__init__.py</code> <pre><code>class TaskPoolFactory:\n    \"\"\"\n    Factory class to create and manage different task pools.\n    This is for v0.1.x versions, deprecated.\n    For implementing new task pool, use `fusion_bench.taskpool.BaseTaskPool` instead.\n\n    This class provides methods to create task pools based on a given configuration,\n    register new task pools, and list available task pools.\n    \"\"\"\n\n    _taskpool_types = {\n        \"dummy\": DummyTaskPool,\n        \"clip_vit_classification\": \".clip_image_classification.CLIPImageClassificationTaskPool\",\n        \"FlanT5GLUETextGenerationTaskPool\": \".flan_t5_glue_text_generation.FlanT5GLUETextGenerationTaskPool\",\n        \"NYUv2TaskPool\": \"fusion_bench.taskpool.nyuv2_taskpool.NYUv2TaskPool\",\n    }\n\n    @staticmethod\n    def create_taskpool(taskpool_config: DictConfig):\n        \"\"\"\n        Create an instance of a task pool based on the provided configuration.\n\n        Args:\n            taskpool_config (DictConfig): The configuration for the task pool. Must contain a 'type' attribute that specifies the type of the task pool.\n\n        Returns:\n            TaskPool: An instance of the specified task pool.\n\n        Raises:\n            ValueError: If 'type' attribute is not found in the configuration or does not match any known task pool types.\n        \"\"\"\n        from fusion_bench.utils import import_object\n\n        taskpool_type = taskpool_config.get(\"type\")\n        if taskpool_type is None:\n            raise ValueError(\"Task pool type not specified\")\n\n        if taskpool_type not in TaskPoolFactory._taskpool_types:\n            raise ValueError(\n                f\"Unknown task pool: {taskpool_type}, available task pools: {TaskPoolFactory._taskpool_types.keys()}. You can register a new task pool using `TaskPoolFactory.register_taskpool()` method.\"\n            )\n        taskpool_cls = TaskPoolFactory._taskpool_types[taskpool_type]\n        if isinstance(taskpool_cls, str):\n            if taskpool_cls.startswith(\".\"):\n                taskpool_cls = f\"fusion_bench.compat.taskpool.{taskpool_cls[1:]}\"\n            taskpool_cls = import_object(taskpool_cls)\n        return taskpool_cls(taskpool_config)\n\n    @staticmethod\n    def register_taskpool(name: str, taskpool_cls):\n        \"\"\"\n        Register a new task pool with the factory.\n\n        Args:\n            name (str): The name of the task pool.\n            taskpool_cls: The class of the task pool to register.\n        \"\"\"\n        TaskPoolFactory._taskpool_types[name] = taskpool_cls\n\n    @classmethod\n    def available_taskpools(cls):\n        \"\"\"\n        Get a list of available task pools.\n\n        Returns:\n            list: A list of available task pool names.\n        \"\"\"\n        return list(cls._taskpool_types.keys())\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.taskpool.TaskPoolFactory.available_taskpools","title":"<code>available_taskpools()</code>  <code>classmethod</code>","text":"<p>Get a list of available task pools.</p> <p>Returns:</p> <ul> <li> <code>list</code>          \u2013            <p>A list of available task pool names.</p> </li> </ul> Source code in <code>fusion_bench/compat/taskpool/__init__.py</code> <pre><code>@classmethod\ndef available_taskpools(cls):\n    \"\"\"\n    Get a list of available task pools.\n\n    Returns:\n        list: A list of available task pool names.\n    \"\"\"\n    return list(cls._taskpool_types.keys())\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.taskpool.TaskPoolFactory.create_taskpool","title":"<code>create_taskpool(taskpool_config)</code>  <code>staticmethod</code>","text":"<p>Create an instance of a task pool based on the provided configuration.</p> <p>Parameters:</p> <ul> <li> <code>taskpool_config</code>               (<code>DictConfig</code>)           \u2013            <p>The configuration for the task pool. Must contain a 'type' attribute that specifies the type of the task pool.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TaskPool</code>          \u2013            <p>An instance of the specified task pool.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If 'type' attribute is not found in the configuration or does not match any known task pool types.</p> </li> </ul> Source code in <code>fusion_bench/compat/taskpool/__init__.py</code> <pre><code>@staticmethod\ndef create_taskpool(taskpool_config: DictConfig):\n    \"\"\"\n    Create an instance of a task pool based on the provided configuration.\n\n    Args:\n        taskpool_config (DictConfig): The configuration for the task pool. Must contain a 'type' attribute that specifies the type of the task pool.\n\n    Returns:\n        TaskPool: An instance of the specified task pool.\n\n    Raises:\n        ValueError: If 'type' attribute is not found in the configuration or does not match any known task pool types.\n    \"\"\"\n    from fusion_bench.utils import import_object\n\n    taskpool_type = taskpool_config.get(\"type\")\n    if taskpool_type is None:\n        raise ValueError(\"Task pool type not specified\")\n\n    if taskpool_type not in TaskPoolFactory._taskpool_types:\n        raise ValueError(\n            f\"Unknown task pool: {taskpool_type}, available task pools: {TaskPoolFactory._taskpool_types.keys()}. You can register a new task pool using `TaskPoolFactory.register_taskpool()` method.\"\n        )\n    taskpool_cls = TaskPoolFactory._taskpool_types[taskpool_type]\n    if isinstance(taskpool_cls, str):\n        if taskpool_cls.startswith(\".\"):\n            taskpool_cls = f\"fusion_bench.compat.taskpool.{taskpool_cls[1:]}\"\n        taskpool_cls = import_object(taskpool_cls)\n    return taskpool_cls(taskpool_config)\n</code></pre>"},{"location":"api/fusion_bench.compat/#fusion_bench.compat.taskpool.TaskPoolFactory.register_taskpool","title":"<code>register_taskpool(name, taskpool_cls)</code>  <code>staticmethod</code>","text":"<p>Register a new task pool with the factory.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the task pool.</p> </li> <li> <code>taskpool_cls</code>           \u2013            <p>The class of the task pool to register.</p> </li> </ul> Source code in <code>fusion_bench/compat/taskpool/__init__.py</code> <pre><code>@staticmethod\ndef register_taskpool(name: str, taskpool_cls):\n    \"\"\"\n    Register a new task pool with the factory.\n\n    Args:\n        name (str): The name of the task pool.\n        taskpool_cls: The class of the task pool to register.\n    \"\"\"\n    TaskPoolFactory._taskpool_types[name] = taskpool_cls\n</code></pre>"},{"location":"api/fusion_bench.constants/","title":"fusion_bench.constants","text":""},{"location":"api/fusion_bench.constants/#paths","title":"Paths","text":""},{"location":"api/fusion_bench.constants/#fusion_bench.constants.paths","title":"<code>fusion_bench.constants.paths</code>","text":""},{"location":"api/fusion_bench.constants/#fusion_bench.constants.paths.PROJECT_ROOT_PATH","title":"<code>PROJECT_ROOT_PATH = LIBRARY_PATH.parent</code>  <code>module-attribute</code>","text":"<p>Path to the project root directory.</p>"},{"location":"api/fusion_bench.constants/#fusion_bench.constants.paths.LIBRARY_PATH","title":"<code>LIBRARY_PATH = Path(importlib.import_module('fusion_bench').__path__[0])</code>  <code>module-attribute</code>","text":"<p>Path to the library directory.</p>"},{"location":"api/fusion_bench.constants/#fusion_bench.constants.paths.DEFAULT_CONFIG_PATH","title":"<code>DEFAULT_CONFIG_PATH = PROJECT_ROOT_PATH / 'config'</code>  <code>module-attribute</code>","text":"<p>Path to the default config directory.</p>"},{"location":"api/fusion_bench.constants/#clip-vision-tasks","title":"CLIP Vision Tasks","text":""},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision","title":"<code>fusion_bench.constants.clip_vision</code>","text":"<p>Constants for CLIP Vision Model Merging</p>"},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TA8","title":"<code>TASK_NAMES_TA8 = ['sun397', 'stanford-cars', 'resisc45', 'eurosat', 'svhn', 'gtsrb', 'mnist', 'dtd']</code>  <code>module-attribute</code>","text":"<p>The 8 tasks used in the Task Arithmetic paper.</p>"},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TALL8","title":"<code>TASK_NAMES_TALL8 = TASK_NAMES_TA8</code>  <code>module-attribute</code>","text":"<p>The 8 tasks used in the Tall Mask paper</p>"},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TALL10","title":"<code>TASK_NAMES_TALL10 = TASK_NAMES_TA8 + ['oxford_flowers102', 'pcam']</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TALL12","title":"<code>TASK_NAMES_TALL12 = TASK_NAMES_TALL10 + ['fer2013', 'oxford-iiit-pet']</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TALL14","title":"<code>TASK_NAMES_TALL14 = TASK_NAMES_TALL12 + ['stl10', 'cifar100']</code>  <code>module-attribute</code>","text":"<p>The 14 tasks used in the TALL mask paper</p>"},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TALL16","title":"<code>TASK_NAMES_TALL16 = TASK_NAMES_TALL14 + ['cifar10', 'food101']</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TALL18","title":"<code>TASK_NAMES_TALL18 = TASK_NAMES_TALL16 + ['fashion_mnist', 'emnist_letters']</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TALL20","title":"<code>TASK_NAMES_TALL20 = TASK_NAMES_TALL18 + ['kmnist', 'rendered-sst2']</code>  <code>module-attribute</code>","text":"<p>The 20 tasks used in the TALL mask paper</p>"},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TA8_CAP","title":"<code>TASK_NAMES_TA8_CAP = ['SUN397', 'Cars', 'RESISC45', 'EuroSAT', 'SVHN', 'GTSRB', 'MNIST', 'DTD']</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TALL8_CAP","title":"<code>TASK_NAMES_TALL8_CAP = TASK_NAMES_TA8_CAP</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TALL10_CAP","title":"<code>TASK_NAMES_TALL10_CAP = TASK_NAMES_TALL8_CAP + ['Flowers102', 'PCAM']</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TALL12_CAP","title":"<code>TASK_NAMES_TALL12_CAP = TASK_NAMES_TALL10_CAP + ['FER2013', 'OxfordIIITPet']</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TALL14_CAP","title":"<code>TASK_NAMES_TALL14_CAP = TASK_NAMES_TALL12_CAP + ['STL10', 'CIFAR100']</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TALL16_CAP","title":"<code>TASK_NAMES_TALL16_CAP = TASK_NAMES_TALL14_CAP + ['CIFAR10', 'Food101']</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TALL18_CAP","title":"<code>TASK_NAMES_TALL18_CAP = TASK_NAMES_TALL16_CAP + ['FashionMNIST', 'EMNIST']</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.constants/#fusion_bench.constants.clip_vision.TASK_NAMES_TALL20_CAP","title":"<code>TASK_NAMES_TALL20_CAP = TASK_NAMES_TALL18_CAP + ['KMNIST', 'RenderedSST2']</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.dataset/","title":"fusion_bench.dataset","text":""},{"location":"api/fusion_bench.dataset/#nyuv2-dataset","title":"NYUv2 Dataset","text":""},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.nyuv2.NYUv2","title":"<code>fusion_bench.dataset.nyuv2.NYUv2</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>NYUv2 dataset, 3 tasks + 1 generated useless task Included tasks:</p> <pre><code>1. Semantic Segmentation,\n2. Depth prediction,\n3. Surface Normal prediction,\n4. Noise prediction [to test auxiliary learning, purely conflict gradients]\n</code></pre> <p>Modified from https://github.com/lorenmt/auto-lambda/blob/main/create_dataset.py</p> <p>removed the <code>augmentation</code> arg and add <code>transform</code> args</p> Source code in <code>fusion_bench/dataset/nyuv2.py</code> <pre><code>class NYUv2(Dataset):\n    R\"\"\"\n    NYUv2 dataset, 3 tasks + 1 generated useless task\n    Included tasks:\n\n        1. Semantic Segmentation,\n        2. Depth prediction,\n        3. Surface Normal prediction,\n        4. Noise prediction [to test auxiliary learning, purely conflict gradients]\n\n    Modified from https://github.com/lorenmt/auto-lambda/blob/main/create_dataset.py\n\n    removed the `augmentation` arg and add `transform` args\n    \"\"\"\n\n    num_out_channels = {\n        \"segmentation\": 13,\n        \"depth\": 1,\n        \"normal\": 3,\n        \"noise\": 1,\n    }\n\n    def __init__(\n        self,\n        root: str,\n        train: bool = True,\n        transform: Optional[Callable] = None,\n        seg_transform: Optional[Callable] = None,\n        sn_transform: Optional[Callable] = None,\n        depth_transform: Optional[Callable] = None,\n    ):\n        \"\"\"\n        Initialize the NYUv2 dataset.\n\n        Args:\n            root (str): The root directory of the dataset.\n            train (bool, optional): If True, use training set. If False, use validation set. Defaults to True.\n            transform (Callable, optional): image transform. Defaults to None.\n            seg_transform (Callable, optional): segmentation transform. Defaults to None.\n            sn_transform (Callable, optional): surface normal transform. Defaults to None.\n            depth_transform (Callable, optional): depth transform. Defaults to None.\n        \"\"\"\n        self.root = os.path.expanduser(root)\n        self.train = train\n\n        self.transform = transform\n        self.seg_transform = seg_transform\n        self.sn_transform = sn_transform\n        self.depth_transform = depth_transform\n\n        if train:\n            self.data_path = self.root + \"/train\"\n        else:\n            self.data_path = self.root + \"/val\"\n\n        # calculate data length\n        self.data_len = len(\n            fnmatch.filter(os.listdir(self.data_path + \"/image\"), \"*.npy\")\n        )\n        self.noise = torch.rand(self.data_len, 1, 288, 384)\n\n    def __getitem__(self, index) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"\n        Retrieve an item from the dataset.\n\n        Args:\n            index (int): The index of the item to retrieve.\n\n        Returns:\n            tuple: A tuple containing the image and a dictionary of task-specific outputs.\n        \"\"\"\n        # load data from the pre-processed npy files\n        image = torch.from_numpy(\n            np.moveaxis(\n                np.load(self.data_path + \"/image/{:d}.npy\".format(index)), -1, 0\n            )\n        ).float()\n        semantic = torch.from_numpy(\n            np.load(self.data_path + \"/label/{:d}.npy\".format(index))\n        ).float()\n        depth = torch.from_numpy(\n            np.moveaxis(\n                np.load(self.data_path + \"/depth/{:d}.npy\".format(index)), -1, 0\n            )\n        ).float()\n        normal = torch.from_numpy(\n            np.moveaxis(\n                np.load(self.data_path + \"/normal/{:d}.npy\".format(index)), -1, 0\n            )\n        ).float()\n        noise = self.noise[index].float()\n\n        if self.transform is not None:\n            image = self.transform(image)\n        if self.seg_transform is not None:\n            semantic = self.seg_transform(semantic)\n        if self.sn_transform is not None:\n            normal = self.sn_transform(normal)\n        if self.depth_transform is not None:\n            depth = self.depth_transform(depth)\n\n        return image, {\n            \"segmentation\": semantic,\n            \"depth\": depth,\n            \"normal\": normal,\n            \"noise\": noise,\n        }\n\n    def __len__(self):\n        return self.data_len\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.nyuv2.NYUv2.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Retrieve an item from the dataset.</p> <p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>The index of the item to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code> (              <code>Tuple[Tensor, Dict[str, Tensor]]</code> )          \u2013            <p>A tuple containing the image and a dictionary of task-specific outputs.</p> </li> </ul> Source code in <code>fusion_bench/dataset/nyuv2.py</code> <pre><code>def __getitem__(self, index) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    \"\"\"\n    Retrieve an item from the dataset.\n\n    Args:\n        index (int): The index of the item to retrieve.\n\n    Returns:\n        tuple: A tuple containing the image and a dictionary of task-specific outputs.\n    \"\"\"\n    # load data from the pre-processed npy files\n    image = torch.from_numpy(\n        np.moveaxis(\n            np.load(self.data_path + \"/image/{:d}.npy\".format(index)), -1, 0\n        )\n    ).float()\n    semantic = torch.from_numpy(\n        np.load(self.data_path + \"/label/{:d}.npy\".format(index))\n    ).float()\n    depth = torch.from_numpy(\n        np.moveaxis(\n            np.load(self.data_path + \"/depth/{:d}.npy\".format(index)), -1, 0\n        )\n    ).float()\n    normal = torch.from_numpy(\n        np.moveaxis(\n            np.load(self.data_path + \"/normal/{:d}.npy\".format(index)), -1, 0\n        )\n    ).float()\n    noise = self.noise[index].float()\n\n    if self.transform is not None:\n        image = self.transform(image)\n    if self.seg_transform is not None:\n        semantic = self.seg_transform(semantic)\n    if self.sn_transform is not None:\n        normal = self.sn_transform(normal)\n    if self.depth_transform is not None:\n        depth = self.depth_transform(depth)\n\n    return image, {\n        \"segmentation\": semantic,\n        \"depth\": depth,\n        \"normal\": normal,\n        \"noise\": noise,\n    }\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.nyuv2.NYUv2.__init__","title":"<code>__init__(root, train=True, transform=None, seg_transform=None, sn_transform=None, depth_transform=None)</code>","text":"<p>Initialize the NYUv2 dataset.</p> <p>Parameters:</p> <ul> <li> <code>root</code>               (<code>str</code>)           \u2013            <p>The root directory of the dataset.</p> </li> <li> <code>train</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, use training set. If False, use validation set. Defaults to True.</p> </li> <li> <code>transform</code>               (<code>Callable</code>, default:                   <code>None</code> )           \u2013            <p>image transform. Defaults to None.</p> </li> <li> <code>seg_transform</code>               (<code>Callable</code>, default:                   <code>None</code> )           \u2013            <p>segmentation transform. Defaults to None.</p> </li> <li> <code>sn_transform</code>               (<code>Callable</code>, default:                   <code>None</code> )           \u2013            <p>surface normal transform. Defaults to None.</p> </li> <li> <code>depth_transform</code>               (<code>Callable</code>, default:                   <code>None</code> )           \u2013            <p>depth transform. Defaults to None.</p> </li> </ul> Source code in <code>fusion_bench/dataset/nyuv2.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    train: bool = True,\n    transform: Optional[Callable] = None,\n    seg_transform: Optional[Callable] = None,\n    sn_transform: Optional[Callable] = None,\n    depth_transform: Optional[Callable] = None,\n):\n    \"\"\"\n    Initialize the NYUv2 dataset.\n\n    Args:\n        root (str): The root directory of the dataset.\n        train (bool, optional): If True, use training set. If False, use validation set. Defaults to True.\n        transform (Callable, optional): image transform. Defaults to None.\n        seg_transform (Callable, optional): segmentation transform. Defaults to None.\n        sn_transform (Callable, optional): surface normal transform. Defaults to None.\n        depth_transform (Callable, optional): depth transform. Defaults to None.\n    \"\"\"\n    self.root = os.path.expanduser(root)\n    self.train = train\n\n    self.transform = transform\n    self.seg_transform = seg_transform\n    self.sn_transform = sn_transform\n    self.depth_transform = depth_transform\n\n    if train:\n        self.data_path = self.root + \"/train\"\n    else:\n        self.data_path = self.root + \"/val\"\n\n    # calculate data length\n    self.data_len = len(\n        fnmatch.filter(os.listdir(self.data_path + \"/image\"), \"*.npy\")\n    )\n    self.noise = torch.rand(self.data_len, 1, 288, 384)\n</code></pre>"},{"location":"api/fusion_bench.dataset/#image-classification-tasks","title":"Image Classification Tasks","text":""},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.clip_dataset.CLIPDataset","title":"<code>fusion_bench.dataset.clip_dataset.CLIPDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A dataset class for CLIP models that converts a dataset of dictionaries or tuples into a format suitable for CLIP processing.</p> <p>This class wraps an existing dataset and applies CLIP preprocessing to the images. It expects each item in the dataset to be either a dictionary with 'image' and 'label' keys, or a tuple/list of (image, label).</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The original dataset to wrap.</p> </li> <li> <code>processor</code>               (<code>CLIPProcessor</code>, default:                   <code>None</code> )           \u2013            <p>The CLIP processor for preparing inputs. If None, no preprocessing is applied and raw images are returned.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>dataset</code>           \u2013            <p>The wrapped dataset.</p> </li> <li> <code>processor</code>               (<code>CLIPProcessor</code>)           \u2013            <p>The CLIP processor used for image preprocessing.</p> </li> </ul> Source code in <code>fusion_bench/dataset/clip_dataset.py</code> <pre><code>class CLIPDataset(torch.utils.data.Dataset):\n    \"\"\"\n    A dataset class for CLIP models that converts a dataset of dictionaries or tuples\n    into a format suitable for CLIP processing.\n\n    This class wraps an existing dataset and applies CLIP preprocessing to the images.\n    It expects each item in the dataset to be either a dictionary with 'image' and 'label' keys,\n    or a tuple/list of (image, label).\n\n    Args:\n        dataset: The original dataset to wrap.\n        processor (CLIPProcessor): The CLIP processor for preparing inputs. If None, no preprocessing is applied and raw images are returned.\n\n    Attributes:\n        dataset: The wrapped dataset.\n        processor (CLIPProcessor): The CLIP processor used for image preprocessing.\n    \"\"\"\n\n    def __init__(self, dataset: Dataset, processor: Optional[CLIPProcessor] = None):\n        self.dataset = dataset\n        self.processor = processor\n\n    def __len__(self):\n        \"\"\"Returns the number of items in the dataset.\"\"\"\n        return len(self.dataset)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[torch.Tensor, int]:\n        \"\"\"\n        Retrieves and processes an item from the dataset.\n\n        Args:\n            idx (int): The index of the item to retrieve.\n\n        Returns:\n            tuple: A tuple containing the processed image tensor and the label.\n\n        Raises:\n            ValueError: If the item is neither a dictionary nor a tuple/list of length 2.\n        \"\"\"\n        item = self.dataset[idx]\n        if isinstance(item, dict):\n            item = item\n        elif isinstance(item, (tuple, list)):\n            assert len(item) == 2, \"Each item should be a tuple or list of length 2\"\n            item = {\"image\": item[0], \"label\": item[1]}\n        else:\n            raise ValueError(\"Each item should be a dictionary or a tuple of length 2\")\n        image = item[\"image\"]\n        if self.processor is not None:\n            if isinstance(self.processor, (ProcessorMixin, BaseImageProcessor)):\n                # Apply the processor to the image to get the input tensor\n                inputs = self.processor(images=[image], return_tensors=\"pt\")[\n                    \"pixel_values\"\n                ][0]\n            elif callable(self.processor):\n                inputs = self.processor(image)\n            else:\n                raise ValueError(\n                    \"The processor should be a CLIPProcessor or a callable function\"\n                )\n        else:\n            # if processor is None, return the raw image directly\n            inputs = image\n        # convert boolean label to int, this is for the case when the label is a binary classification task\n        if isinstance(item[\"label\"], bool):\n            item[\"label\"] = 1 if item[\"label\"] else 0\n        return inputs, item[\"label\"]\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.clip_dataset.CLIPDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves and processes an item from the dataset.</p> <p>Parameters:</p> <ul> <li> <code>idx</code>               (<code>int</code>)           \u2013            <p>The index of the item to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code> (              <code>Tuple[Tensor, int]</code> )          \u2013            <p>A tuple containing the processed image tensor and the label.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the item is neither a dictionary nor a tuple/list of length 2.</p> </li> </ul> Source code in <code>fusion_bench/dataset/clip_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Tuple[torch.Tensor, int]:\n    \"\"\"\n    Retrieves and processes an item from the dataset.\n\n    Args:\n        idx (int): The index of the item to retrieve.\n\n    Returns:\n        tuple: A tuple containing the processed image tensor and the label.\n\n    Raises:\n        ValueError: If the item is neither a dictionary nor a tuple/list of length 2.\n    \"\"\"\n    item = self.dataset[idx]\n    if isinstance(item, dict):\n        item = item\n    elif isinstance(item, (tuple, list)):\n        assert len(item) == 2, \"Each item should be a tuple or list of length 2\"\n        item = {\"image\": item[0], \"label\": item[1]}\n    else:\n        raise ValueError(\"Each item should be a dictionary or a tuple of length 2\")\n    image = item[\"image\"]\n    if self.processor is not None:\n        if isinstance(self.processor, (ProcessorMixin, BaseImageProcessor)):\n            # Apply the processor to the image to get the input tensor\n            inputs = self.processor(images=[image], return_tensors=\"pt\")[\n                \"pixel_values\"\n            ][0]\n        elif callable(self.processor):\n            inputs = self.processor(image)\n        else:\n            raise ValueError(\n                \"The processor should be a CLIPProcessor or a callable function\"\n            )\n    else:\n        # if processor is None, return the raw image directly\n        inputs = image\n    # convert boolean label to int, this is for the case when the label is a binary classification task\n    if isinstance(item[\"label\"], bool):\n        item[\"label\"] = 1 if item[\"label\"] else 0\n    return inputs, item[\"label\"]\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.clip_dataset.CLIPDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of items in the dataset.</p> Source code in <code>fusion_bench/dataset/clip_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Returns the number of items in the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.image_dataset.TransformedImageDataset","title":"<code>fusion_bench.dataset.image_dataset.TransformedImageDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A dataset class for image classification tasks that applies a transform to images.</p> <p>This class wraps an existing dataset and applies a specified transform to the images. It expects each item in the dataset to be either a dictionary with 'image' and 'label' keys, or a tuple/list of (image, label).</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The original dataset to wrap.</p> </li> <li> <code>transform</code>               (<code>Callable</code>)           \u2013            <p>A function/transform to apply on the image.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>dataset</code>           \u2013            <p>The wrapped dataset.</p> </li> <li> <code>transform</code>               (<code>Callable</code>)           \u2013            <p>The transform to be applied to the images.</p> </li> </ul> Source code in <code>fusion_bench/dataset/image_dataset.py</code> <pre><code>class TransformedImageDataset(Dataset):\n    \"\"\"\n    A dataset class for image classification tasks that applies a transform to images.\n\n    This class wraps an existing dataset and applies a specified transform to the images.\n    It expects each item in the dataset to be either a dictionary with 'image' and 'label' keys,\n    or a tuple/list of (image, label).\n\n    Args:\n        dataset: The original dataset to wrap.\n        transform (Callable): A function/transform to apply on the image.\n\n    Attributes:\n        dataset: The wrapped dataset.\n        transform (Callable): The transform to be applied to the images.\n    \"\"\"\n\n    def __init__(self, dataset: Dataset, transform: Callable):\n        super().__init__()\n        self.dataset = dataset\n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"Returns the number of items in the dataset.\"\"\"\n        return len(self.dataset)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[Any, Any]:\n        \"\"\"\n        Retrieves and processes an item from the dataset.\n\n        Args:\n            idx (int): The index of the item to retrieve.\n\n        Returns:\n            tuple: A tuple containing the processed image and the label.\n\n        Raises:\n            ValueError: If the item is neither a dictionary nor a tuple/list of length 2.\n        \"\"\"\n        item = self.dataset[idx]\n        if isinstance(item, dict):\n            item = item\n        elif isinstance(item, (tuple, list)):\n            assert len(item) == 2, \"Each item should be a tuple or list of length 2\"\n            item = {\"image\": item[0], \"label\": item[1]}\n        else:\n            raise ValueError(\"Each item should be a dictionary or a tuple of length 2\")\n        image = item[\"image\"]\n        inputs = self.transform(image)\n        return inputs, item[\"label\"]\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.image_dataset.TransformedImageDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves and processes an item from the dataset.</p> <p>Parameters:</p> <ul> <li> <code>idx</code>               (<code>int</code>)           \u2013            <p>The index of the item to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code> (              <code>Tuple[Any, Any]</code> )          \u2013            <p>A tuple containing the processed image and the label.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the item is neither a dictionary nor a tuple/list of length 2.</p> </li> </ul> Source code in <code>fusion_bench/dataset/image_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Retrieves and processes an item from the dataset.\n\n    Args:\n        idx (int): The index of the item to retrieve.\n\n    Returns:\n        tuple: A tuple containing the processed image and the label.\n\n    Raises:\n        ValueError: If the item is neither a dictionary nor a tuple/list of length 2.\n    \"\"\"\n    item = self.dataset[idx]\n    if isinstance(item, dict):\n        item = item\n    elif isinstance(item, (tuple, list)):\n        assert len(item) == 2, \"Each item should be a tuple or list of length 2\"\n        item = {\"image\": item[0], \"label\": item[1]}\n    else:\n        raise ValueError(\"Each item should be a dictionary or a tuple of length 2\")\n    image = item[\"image\"]\n    inputs = self.transform(image)\n    return inputs, item[\"label\"]\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.image_dataset.TransformedImageDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of items in the dataset.</p> Source code in <code>fusion_bench/dataset/image_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Returns the number of items in the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"api/fusion_bench.dataset/#gpt-2-on-glue-benchmark","title":"GPT-2 on GLUE Benchmark","text":""},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.gpt2_glue.TokenizedGLUE","title":"<code>fusion_bench.dataset.gpt2_glue.TokenizedGLUE</code>","text":"<p>A class to load and cache GLUE datasets for GPT-2 models.</p> <p>This class provides methods to load various GLUE datasets and tokenize them using a provided tokenizer. The datasets are cached to disk to avoid reloading and tokenizing them multiple times.</p> <p>Attributes:</p> <ul> <li> <code>tokenizer</code>               (<code>PreTrainedTokenizer</code>)           \u2013            <p>The tokenizer to use for tokenizing the datasets.</p> </li> </ul> Source code in <code>fusion_bench/dataset/gpt2_glue.py</code> <pre><code>class TokenizedGLUE:\n    \"\"\"\n    A class to load and cache GLUE datasets for GPT-2 models.\n\n    This class provides methods to load various GLUE datasets and tokenize them\n    using a provided tokenizer. The datasets are cached to disk to avoid\n    reloading and tokenizing them multiple times.\n\n    Attributes:\n        tokenizer (PreTrainedTokenizer): The tokenizer to use for tokenizing the datasets.\n    \"\"\"\n\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        \"\"\"\n        Initialize the TokenizedGLUE class with a tokenizer.\n\n        Args:\n            tokenizer (PreTrainedTokenizer): The tokenizer to use for tokenizing the datasets.\n        \"\"\"\n        super().__init__()\n        self.tokenizer = tokenizer\n\n    def load_dataset(\n        self, name: Literal[\"mrpc\", \"mnli\", \"cola\", \"sst2\", \"qnli\", \"qqp\", \"rte\"]\n    ) -&gt; Dataset:\n        \"\"\"\n        Load and tokenize a GLUE dataset.\n\n        This method loads a specified GLUE dataset, tokenizes it using the provided\n        tokenizer, and caches the tokenized dataset to disk.\n\n        Args:\n            name (Literal[\"mrpc\", \"mnli\", \"cola\", \"sst2\", \"qnli\", \"qqp\", \"rte\"]): The name of the GLUE dataset to load.\n\n        Returns:\n            Dataset: The tokenized GLUE dataset.\n        \"\"\"\n        glue_dataset_loaders = {\n            \"mrpc\": self.load_mrpc_dataset,\n            \"mnli\": self.load_mnli_dataset,\n            \"cola\": self.load_cola_dataset,\n            \"sst2\": self.load_sst2_dataset,\n            \"qnli\": self.load_qnli_dataset,\n            \"qqp\": self.load_qqp_dataset,\n            \"rte\": self.load_rte_dataset,\n            # \"wnli\": load_wnli_dataset,\n        }\n        return glue_dataset_loaders[name]()\n\n    @cache_dataset\n    def load_mrpc_dataset(self) -&gt; Dataset:\n        \"\"\"\n        Load and tokenize the MRPC dataset.\n\n        This method loads the MRPC dataset, tokenizes it using the provided\n        tokenizer, and caches the tokenized dataset to disk.\n\n        Returns:\n            Dataset: The tokenized MRPC dataset.\n        \"\"\"\n        dataset = load_dataset(\"glue\", \"mrpc\")\n        dataset = dataset.map(\n            partial(mrpc_tokenize_function, tokenizer=self.tokenizer),\n            batched=True,\n            remove_columns=[\"sentence1\", \"sentence2\"],\n        )\n        return dataset\n\n    @cache_dataset\n    def load_rte_dataset(self) -&gt; Dataset:\n        \"\"\"\n        Load and tokenize the RTE dataset.\n\n        This method loads the RTE dataset, tokenizes it using the provided\n        tokenizer, and caches the tokenized dataset to disk.\n\n        Returns:\n            Dataset: The tokenized RTE dataset.\n        \"\"\"\n        dataset = load_dataset(\"glue\", \"rte\")\n        dataset = dataset.map(\n            # RTE has the same format as MRPC\n            partial(mrpc_tokenize_function, tokenizer=self.tokenizer),\n            batched=True,\n            remove_columns=[\"sentence1\", \"sentence2\"],\n        )\n        return dataset\n\n    @cache_dataset\n    def load_wnli_dataset(self) -&gt; Dataset:\n        \"\"\"\n        Load and tokenize the WNLI dataset.\n\n        This method loads the WNLI dataset, tokenizes it using the provided\n        tokenizer, and caches the tokenized dataset to disk.\n\n        Returns:\n            Dataset: The tokenized WNLI dataset.\n        \"\"\"\n        dataset = load_dataset(\"glue\", \"wnli\")\n        dataset = dataset.map(\n            partial(mrpc_tokenize_function, tokenizer=self.tokenizer),\n            batched=True,\n            remove_columns=[\"sentence1\", \"sentence2\"],\n        )\n        return dataset\n\n    @cache_dataset\n    def load_qqp_dataset(self) -&gt; Dataset:\n        \"\"\"\n        Load and tokenize the QQP dataset.\n\n        This method loads the QQP dataset, tokenizes it using the provided\n        tokenizer, and caches the tokenized dataset to disk.\n\n        Returns:\n            Dataset: The tokenized QQP dataset.\n        \"\"\"\n        dataset = load_dataset(\"glue\", \"qqp\")\n        dataset = dataset.map(\n            partial(qqp_tokenize_function, tokenizer=self.tokenizer),\n            batched=True,\n            remove_columns=[\"question1\", \"question2\"],\n        )\n        return dataset\n\n    @cache_dataset\n    def load_mnli_dataset(self) -&gt; Dataset:\n        \"\"\"\n        Load and tokenize the MNLI dataset.\n\n        This method loads the MNLI dataset, tokenizes it using the provided\n        tokenizer, and caches the tokenized dataset to disk.\n\n        Returns:\n            Dataset: The tokenized MNLI dataset.\n        \"\"\"\n        dataset = load_dataset(\"glue\", \"mnli\")\n        dataset = dataset.map(\n            partial(mnli_tokenize_function, tokenizer=self.tokenizer),\n            batched=True,\n            remove_columns=[\"premise\", \"hypothesis\"],\n        )\n        return dataset\n\n    @cache_dataset\n    def load_cola_dataset(self) -&gt; Dataset:\n        \"\"\"\n        Load and tokenize the CoLA dataset.\n\n        This method loads the CoLA dataset, tokenizes it using the provided\n        tokenizer, and caches the tokenized dataset to disk.\n\n        Returns:\n            Dataset: The tokenized CoLA dataset.\n        \"\"\"\n        dataset = load_dataset(\"glue\", \"cola\")\n        dataset = dataset.map(\n            partial(cola_tokenize_function, tokenizer=self.tokenizer),\n            batched=True,\n            remove_columns=[\"sentence\"],\n        )\n        return dataset\n\n    @cache_dataset\n    def load_sst2_dataset(self) -&gt; Dataset:\n        \"\"\"\n        Load and tokenize the SST-2 dataset.\n\n        This method loads the SST-2 dataset, tokenizes it using the provided\n        tokenizer, and caches the tokenized dataset to disk.\n\n        Returns:\n            Dataset: The tokenized SST-2 dataset.\n        \"\"\"\n        dataset = load_dataset(\"glue\", \"sst2\")\n        dataset = dataset.map(\n            partial(cola_tokenize_function, tokenizer=self.tokenizer),\n            batched=True,\n            remove_columns=[\"sentence\"],\n        )\n        return dataset\n\n    @cache_dataset\n    def load_qnli_dataset(self) -&gt; Dataset:\n        \"\"\"\n        Load and tokenize the QNLI dataset.\n\n        This method loads the QNLI dataset, tokenizes it using the provided\n        tokenizer, and caches the tokenized dataset to disk.\n\n        Returns:\n            Dataset: The tokenized QNLI dataset.\n        \"\"\"\n        dataset = load_dataset(\"glue\", \"qnli\")\n        dataset = dataset.map(\n            partial(qnli_tokenize_function, tokenizer=self.tokenizer),\n            batched=True,\n            remove_columns=[\"question\", \"sentence\"],\n        )\n        return dataset\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.gpt2_glue.TokenizedGLUE.__init__","title":"<code>__init__(tokenizer)</code>","text":"<p>Initialize the TokenizedGLUE class with a tokenizer.</p> <p>Parameters:</p> <ul> <li> <code>tokenizer</code>               (<code>PreTrainedTokenizer</code>)           \u2013            <p>The tokenizer to use for tokenizing the datasets.</p> </li> </ul> Source code in <code>fusion_bench/dataset/gpt2_glue.py</code> <pre><code>def __init__(self, tokenizer: PreTrainedTokenizer):\n    \"\"\"\n    Initialize the TokenizedGLUE class with a tokenizer.\n\n    Args:\n        tokenizer (PreTrainedTokenizer): The tokenizer to use for tokenizing the datasets.\n    \"\"\"\n    super().__init__()\n    self.tokenizer = tokenizer\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.gpt2_glue.TokenizedGLUE.load_cola_dataset","title":"<code>load_cola_dataset()</code>","text":"<p>Load and tokenize the CoLA dataset.</p> <p>This method loads the CoLA dataset, tokenizes it using the provided tokenizer, and caches the tokenized dataset to disk.</p> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The tokenized CoLA dataset.</p> </li> </ul> Source code in <code>fusion_bench/dataset/gpt2_glue.py</code> <pre><code>@cache_dataset\ndef load_cola_dataset(self) -&gt; Dataset:\n    \"\"\"\n    Load and tokenize the CoLA dataset.\n\n    This method loads the CoLA dataset, tokenizes it using the provided\n    tokenizer, and caches the tokenized dataset to disk.\n\n    Returns:\n        Dataset: The tokenized CoLA dataset.\n    \"\"\"\n    dataset = load_dataset(\"glue\", \"cola\")\n    dataset = dataset.map(\n        partial(cola_tokenize_function, tokenizer=self.tokenizer),\n        batched=True,\n        remove_columns=[\"sentence\"],\n    )\n    return dataset\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.gpt2_glue.TokenizedGLUE.load_dataset","title":"<code>load_dataset(name)</code>","text":"<p>Load and tokenize a GLUE dataset.</p> <p>This method loads a specified GLUE dataset, tokenizes it using the provided tokenizer, and caches the tokenized dataset to disk.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>Literal['mrpc', 'mnli', 'cola', 'sst2', 'qnli', 'qqp', 'rte']</code>)           \u2013            <p>The name of the GLUE dataset to load.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The tokenized GLUE dataset.</p> </li> </ul> Source code in <code>fusion_bench/dataset/gpt2_glue.py</code> <pre><code>def load_dataset(\n    self, name: Literal[\"mrpc\", \"mnli\", \"cola\", \"sst2\", \"qnli\", \"qqp\", \"rte\"]\n) -&gt; Dataset:\n    \"\"\"\n    Load and tokenize a GLUE dataset.\n\n    This method loads a specified GLUE dataset, tokenizes it using the provided\n    tokenizer, and caches the tokenized dataset to disk.\n\n    Args:\n        name (Literal[\"mrpc\", \"mnli\", \"cola\", \"sst2\", \"qnli\", \"qqp\", \"rte\"]): The name of the GLUE dataset to load.\n\n    Returns:\n        Dataset: The tokenized GLUE dataset.\n    \"\"\"\n    glue_dataset_loaders = {\n        \"mrpc\": self.load_mrpc_dataset,\n        \"mnli\": self.load_mnli_dataset,\n        \"cola\": self.load_cola_dataset,\n        \"sst2\": self.load_sst2_dataset,\n        \"qnli\": self.load_qnli_dataset,\n        \"qqp\": self.load_qqp_dataset,\n        \"rte\": self.load_rte_dataset,\n        # \"wnli\": load_wnli_dataset,\n    }\n    return glue_dataset_loaders[name]()\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.gpt2_glue.TokenizedGLUE.load_mnli_dataset","title":"<code>load_mnli_dataset()</code>","text":"<p>Load and tokenize the MNLI dataset.</p> <p>This method loads the MNLI dataset, tokenizes it using the provided tokenizer, and caches the tokenized dataset to disk.</p> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The tokenized MNLI dataset.</p> </li> </ul> Source code in <code>fusion_bench/dataset/gpt2_glue.py</code> <pre><code>@cache_dataset\ndef load_mnli_dataset(self) -&gt; Dataset:\n    \"\"\"\n    Load and tokenize the MNLI dataset.\n\n    This method loads the MNLI dataset, tokenizes it using the provided\n    tokenizer, and caches the tokenized dataset to disk.\n\n    Returns:\n        Dataset: The tokenized MNLI dataset.\n    \"\"\"\n    dataset = load_dataset(\"glue\", \"mnli\")\n    dataset = dataset.map(\n        partial(mnli_tokenize_function, tokenizer=self.tokenizer),\n        batched=True,\n        remove_columns=[\"premise\", \"hypothesis\"],\n    )\n    return dataset\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.gpt2_glue.TokenizedGLUE.load_mrpc_dataset","title":"<code>load_mrpc_dataset()</code>","text":"<p>Load and tokenize the MRPC dataset.</p> <p>This method loads the MRPC dataset, tokenizes it using the provided tokenizer, and caches the tokenized dataset to disk.</p> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The tokenized MRPC dataset.</p> </li> </ul> Source code in <code>fusion_bench/dataset/gpt2_glue.py</code> <pre><code>@cache_dataset\ndef load_mrpc_dataset(self) -&gt; Dataset:\n    \"\"\"\n    Load and tokenize the MRPC dataset.\n\n    This method loads the MRPC dataset, tokenizes it using the provided\n    tokenizer, and caches the tokenized dataset to disk.\n\n    Returns:\n        Dataset: The tokenized MRPC dataset.\n    \"\"\"\n    dataset = load_dataset(\"glue\", \"mrpc\")\n    dataset = dataset.map(\n        partial(mrpc_tokenize_function, tokenizer=self.tokenizer),\n        batched=True,\n        remove_columns=[\"sentence1\", \"sentence2\"],\n    )\n    return dataset\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.gpt2_glue.TokenizedGLUE.load_qnli_dataset","title":"<code>load_qnli_dataset()</code>","text":"<p>Load and tokenize the QNLI dataset.</p> <p>This method loads the QNLI dataset, tokenizes it using the provided tokenizer, and caches the tokenized dataset to disk.</p> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The tokenized QNLI dataset.</p> </li> </ul> Source code in <code>fusion_bench/dataset/gpt2_glue.py</code> <pre><code>@cache_dataset\ndef load_qnli_dataset(self) -&gt; Dataset:\n    \"\"\"\n    Load and tokenize the QNLI dataset.\n\n    This method loads the QNLI dataset, tokenizes it using the provided\n    tokenizer, and caches the tokenized dataset to disk.\n\n    Returns:\n        Dataset: The tokenized QNLI dataset.\n    \"\"\"\n    dataset = load_dataset(\"glue\", \"qnli\")\n    dataset = dataset.map(\n        partial(qnli_tokenize_function, tokenizer=self.tokenizer),\n        batched=True,\n        remove_columns=[\"question\", \"sentence\"],\n    )\n    return dataset\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.gpt2_glue.TokenizedGLUE.load_qqp_dataset","title":"<code>load_qqp_dataset()</code>","text":"<p>Load and tokenize the QQP dataset.</p> <p>This method loads the QQP dataset, tokenizes it using the provided tokenizer, and caches the tokenized dataset to disk.</p> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The tokenized QQP dataset.</p> </li> </ul> Source code in <code>fusion_bench/dataset/gpt2_glue.py</code> <pre><code>@cache_dataset\ndef load_qqp_dataset(self) -&gt; Dataset:\n    \"\"\"\n    Load and tokenize the QQP dataset.\n\n    This method loads the QQP dataset, tokenizes it using the provided\n    tokenizer, and caches the tokenized dataset to disk.\n\n    Returns:\n        Dataset: The tokenized QQP dataset.\n    \"\"\"\n    dataset = load_dataset(\"glue\", \"qqp\")\n    dataset = dataset.map(\n        partial(qqp_tokenize_function, tokenizer=self.tokenizer),\n        batched=True,\n        remove_columns=[\"question1\", \"question2\"],\n    )\n    return dataset\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.gpt2_glue.TokenizedGLUE.load_rte_dataset","title":"<code>load_rte_dataset()</code>","text":"<p>Load and tokenize the RTE dataset.</p> <p>This method loads the RTE dataset, tokenizes it using the provided tokenizer, and caches the tokenized dataset to disk.</p> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The tokenized RTE dataset.</p> </li> </ul> Source code in <code>fusion_bench/dataset/gpt2_glue.py</code> <pre><code>@cache_dataset\ndef load_rte_dataset(self) -&gt; Dataset:\n    \"\"\"\n    Load and tokenize the RTE dataset.\n\n    This method loads the RTE dataset, tokenizes it using the provided\n    tokenizer, and caches the tokenized dataset to disk.\n\n    Returns:\n        Dataset: The tokenized RTE dataset.\n    \"\"\"\n    dataset = load_dataset(\"glue\", \"rte\")\n    dataset = dataset.map(\n        # RTE has the same format as MRPC\n        partial(mrpc_tokenize_function, tokenizer=self.tokenizer),\n        batched=True,\n        remove_columns=[\"sentence1\", \"sentence2\"],\n    )\n    return dataset\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.gpt2_glue.TokenizedGLUE.load_sst2_dataset","title":"<code>load_sst2_dataset()</code>","text":"<p>Load and tokenize the SST-2 dataset.</p> <p>This method loads the SST-2 dataset, tokenizes it using the provided tokenizer, and caches the tokenized dataset to disk.</p> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The tokenized SST-2 dataset.</p> </li> </ul> Source code in <code>fusion_bench/dataset/gpt2_glue.py</code> <pre><code>@cache_dataset\ndef load_sst2_dataset(self) -&gt; Dataset:\n    \"\"\"\n    Load and tokenize the SST-2 dataset.\n\n    This method loads the SST-2 dataset, tokenizes it using the provided\n    tokenizer, and caches the tokenized dataset to disk.\n\n    Returns:\n        Dataset: The tokenized SST-2 dataset.\n    \"\"\"\n    dataset = load_dataset(\"glue\", \"sst2\")\n    dataset = dataset.map(\n        partial(cola_tokenize_function, tokenizer=self.tokenizer),\n        batched=True,\n        remove_columns=[\"sentence\"],\n    )\n    return dataset\n</code></pre>"},{"location":"api/fusion_bench.dataset/#fusion_bench.dataset.gpt2_glue.TokenizedGLUE.load_wnli_dataset","title":"<code>load_wnli_dataset()</code>","text":"<p>Load and tokenize the WNLI dataset.</p> <p>This method loads the WNLI dataset, tokenizes it using the provided tokenizer, and caches the tokenized dataset to disk.</p> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The tokenized WNLI dataset.</p> </li> </ul> Source code in <code>fusion_bench/dataset/gpt2_glue.py</code> <pre><code>@cache_dataset\ndef load_wnli_dataset(self) -&gt; Dataset:\n    \"\"\"\n    Load and tokenize the WNLI dataset.\n\n    This method loads the WNLI dataset, tokenizes it using the provided\n    tokenizer, and caches the tokenized dataset to disk.\n\n    Returns:\n        Dataset: The tokenized WNLI dataset.\n    \"\"\"\n    dataset = load_dataset(\"glue\", \"wnli\")\n    dataset = dataset.map(\n        partial(mrpc_tokenize_function, tokenizer=self.tokenizer),\n        batched=True,\n        remove_columns=[\"sentence1\", \"sentence2\"],\n    )\n    return dataset\n</code></pre>"},{"location":"api/fusion_bench.metrics/","title":"fusion_bench.metrics","text":""},{"location":"api/fusion_bench.metrics/#nyuv2-tasks","title":"NYUv2 Tasks","text":""},{"location":"api/fusion_bench.metrics/#fusion_bench.metrics.nyuv2","title":"<code>fusion_bench.metrics.nyuv2</code>","text":""},{"location":"api/fusion_bench.metrics/#fusion_bench.metrics.nyuv2.metric_classes","title":"<code>metric_classes = {'segmentation': SegmentationMetric, 'depth': DepthMetric, 'normal': NormalMetric, 'noise': NoiseMetric}</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.metrics/#fusion_bench.metrics.nyuv2.SegmentationMetric","title":"<code>SegmentationMetric</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>fusion_bench/metrics/nyuv2/segmentation.py</code> <pre><code>class SegmentationMetric(Metric):\n    metric_names = [\"mIoU\", \"pixAcc\"]\n\n    def __init__(self, num_classes=13):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.add_state(\n            \"record\",\n            default=torch.zeros(\n                (self.num_classes, self.num_classes), dtype=torch.int64\n            ),\n            dist_reduce_fx=\"sum\",\n        )\n\n    def reset(self):\n        self.record.zero_()\n\n    def update(self, preds: Tensor, target: Tensor):\n        preds = preds.softmax(1).argmax(1).flatten()\n        target = target.long().flatten()\n\n        k = (target &gt;= 0) &amp; (target &lt; self.num_classes)\n        inds = self.num_classes * target[k].to(torch.int64) + preds[k]\n        self.record += torch.bincount(inds, minlength=self.num_classes**2).reshape(\n            self.num_classes, self.num_classes\n        )\n\n    def compute(self):\n        \"\"\"\n        return mIoU and pixel accuracy\n        \"\"\"\n        h = cast(Tensor, self.record).float()\n        iu = torch.diag(h) / (h.sum(1) + h.sum(0) - torch.diag(h))\n        acc = torch.diag(h).sum() / h.sum()\n        return [torch.mean(iu), acc]\n</code></pre>"},{"location":"api/fusion_bench.metrics/#fusion_bench.metrics.nyuv2.SegmentationMetric.compute","title":"<code>compute()</code>","text":"<p>return mIoU and pixel accuracy</p> Source code in <code>fusion_bench/metrics/nyuv2/segmentation.py</code> <pre><code>def compute(self):\n    \"\"\"\n    return mIoU and pixel accuracy\n    \"\"\"\n    h = cast(Tensor, self.record).float()\n    iu = torch.diag(h) / (h.sum(1) + h.sum(0) - torch.diag(h))\n    acc = torch.diag(h).sum() / h.sum()\n    return [torch.mean(iu), acc]\n</code></pre>"},{"location":"api/fusion_bench.metrics/#fusion_bench.metrics.nyuv2.DepthMetric","title":"<code>DepthMetric</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>fusion_bench/metrics/nyuv2/depth.py</code> <pre><code>class DepthMetric(Metric):\n    metric_names = [\"abs_err\", \"rel_err\"]\n\n    def __init__(self):\n        super().__init__()\n\n        self.add_state(\"abs_record\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"rel_record\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"batch_size\", default=[], dist_reduce_fx=\"cat\")\n\n    def reset(self):\n        self.abs_record = []\n        self.rel_record = []\n        self.batch_size = []\n\n    def update(self, preds: Tensor, target: Tensor):\n        binary_mask = (torch.sum(target, dim=1) != 0).unsqueeze(1)\n        preds = preds.masked_select(binary_mask)\n        target = target.masked_select(binary_mask)\n        abs_err = torch.abs(preds - target)\n        rel_err = torch.abs(preds - target) / target\n        abs_err = torch.sum(abs_err) / torch.nonzero(binary_mask, as_tuple=False).size(\n            0\n        )\n        rel_err = torch.sum(rel_err) / torch.nonzero(binary_mask, as_tuple=False).size(\n            0\n        )\n        self.abs_record.append(abs_err)\n        self.rel_record.append(rel_err)\n        self.batch_size.append(torch.asarray(preds.size(0), device=preds.device))\n\n    def compute(self):\n        records = torch.stack(\n            [torch.stack(self.abs_record), torch.stack(self.rel_record)]\n        )\n        batch_size = torch.stack(self.batch_size)\n        return [(records[i] * batch_size).sum() / batch_size.sum() for i in range(2)]\n</code></pre>"},{"location":"api/fusion_bench.metrics/#fusion_bench.metrics.nyuv2.NormalMetric","title":"<code>NormalMetric</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>fusion_bench/metrics/nyuv2/normal.py</code> <pre><code>class NormalMetric(Metric):\n    metric_names = [\"mean\", \"median\", \"&lt;11.25\", \"&lt;22.5\", \"&lt;30\"]\n\n    def __init__(self):\n        super(NormalMetric, self).__init__()\n\n        self.add_state(\"record\", default=[], dist_reduce_fx=\"cat\")\n\n    def update(self, preds, target):\n        # gt has been normalized on the NYUv2 dataset\n        preds = preds / torch.norm(preds, p=2, dim=1, keepdim=True)\n        binary_mask = torch.sum(target, dim=1) != 0\n        error = (\n            torch.acos(\n                torch.clamp(\n                    torch.sum(preds * target, 1).masked_select(binary_mask), -1, 1\n                )\n            )\n            .detach()\n            .cpu()\n            .numpy()\n        )\n        error = np.degrees(error)\n        self.record.append(torch.from_numpy(error))\n\n    def compute(self):\n        \"\"\"\n        returns mean, median, and percentage of pixels with error less than 11.25, 22.5, and 30 degrees (\"mean\", \"median\", \"&lt;11.25\", \"&lt;22.5\", \"&lt;30\")\n        \"\"\"\n        if self.record is None:\n            return torch.asarray([0.0, 0.0, 0.0, 0.0, 0.0])\n\n        records = torch.concatenate(self.record)\n        return [\n            torch.mean(records),\n            torch.median(records),\n            torch.mean((records &lt; 11.25) * 1.0),\n            torch.mean((records &lt; 22.5) * 1.0),\n            torch.mean((records &lt; 30) * 1.0),\n        ]\n</code></pre>"},{"location":"api/fusion_bench.metrics/#fusion_bench.metrics.nyuv2.NormalMetric.compute","title":"<code>compute()</code>","text":"<p>returns mean, median, and percentage of pixels with error less than 11.25, 22.5, and 30 degrees (\"mean\", \"median\", \"&lt;11.25\", \"&lt;22.5\", \"&lt;30\")</p> Source code in <code>fusion_bench/metrics/nyuv2/normal.py</code> <pre><code>def compute(self):\n    \"\"\"\n    returns mean, median, and percentage of pixels with error less than 11.25, 22.5, and 30 degrees (\"mean\", \"median\", \"&lt;11.25\", \"&lt;22.5\", \"&lt;30\")\n    \"\"\"\n    if self.record is None:\n        return torch.asarray([0.0, 0.0, 0.0, 0.0, 0.0])\n\n    records = torch.concatenate(self.record)\n    return [\n        torch.mean(records),\n        torch.median(records),\n        torch.mean((records &lt; 11.25) * 1.0),\n        torch.mean((records &lt; 22.5) * 1.0),\n        torch.mean((records &lt; 30) * 1.0),\n    ]\n</code></pre>"},{"location":"api/fusion_bench.metrics/#fusion_bench.metrics.nyuv2.NoiseMetric","title":"<code>NoiseMetric</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>fusion_bench/metrics/nyuv2/noise.py</code> <pre><code>class NoiseMetric(Metric):\n    def __init__(self):\n        super().__init__()\n\n    def update(self, preds: Tensor, target: Tensor):\n        pass\n\n    def compute(self):\n        return [1]\n</code></pre>"},{"location":"api/fusion_bench.metrics/#continual-learning-metrics","title":"Continual Learning Metrics","text":""},{"location":"api/fusion_bench.metrics/#fusion_bench.metrics.continual_learning","title":"<code>fusion_bench.metrics.continual_learning</code>","text":""},{"location":"api/fusion_bench.metrics/#fusion_bench.metrics.continual_learning.compute_backward_transfer","title":"<code>compute_backward_transfer(acc_Ti, acc_ii)</code>","text":"<p>Compute the backward transfer (BWT) of a model on a set of tasks.</p> Equation <p>\\(BWT = \\frac{1}{n} \\sum_{k=1}^{n} (acc_{T,i}[k] - acc_{i,i}[k])\\)</p> <p>Returns:</p> <ul> <li> <code>float</code> (              <code>float</code> )          \u2013            <p>The backward transfer of the model.</p> </li> </ul> Source code in <code>fusion_bench/metrics/continual_learning/backward_transfer.py</code> <pre><code>def compute_backward_transfer(\n    acc_Ti: Dict[str, float], acc_ii: Dict[str, float]\n) -&gt; float:\n    R\"\"\"\n    Compute the backward transfer (BWT) of a model on a set of tasks.\n\n    Equation:\n        $BWT = \\frac{1}{n} \\sum_{k=1}^{n} (acc_{T,i}[k] - acc_{i,i}[k])$\n\n    Returns:\n        float: The backward transfer of the model.\n    \"\"\"\n    assert set(acc_ii.keys()) == set(acc_Ti.keys())\n    bwt = 0\n    for task_name in acc_ii:\n        bwt += acc_Ti[task_name] - acc_ii[task_name]\n    return bwt / len(acc_ii)\n</code></pre>"},{"location":"api/fusion_bench.mixins/","title":"fusion_bench.mixins","text":"<p>The mixins module provides reusable functionality through mixin classes that can be combined with other classes to add specific capabilities. These mixins follow the composition-over-inheritance principle and are designed to be modular, flexible, and easy to integrate.</p>"},{"location":"api/fusion_bench.mixins/#basic-mixin-composition","title":"Basic Mixin Composition","text":"<pre><code>from fusion_bench.mixins import (\n    LightningFabricMixin, \n    SimpleProfilerMixin,\n    auto_register_config\n)\n\n@auto_register_config\nclass MyAlgorithm(\n    LightningFabricMixin,\n    SimpleProfilerMixin,\n    BaseAlgorithm\n):\n    def __init__(self, learning_rate: float = 0.001, batch_size: int = 32, **kwargs):\n        super().__init__(**kwargs)\n\n    def run(self, modelpool):\n        # ... implement the algorithm here\n</code></pre>"},{"location":"api/fusion_bench.mixins/#class-definitions","title":"Class Definitions","text":""},{"location":"api/fusion_bench.mixins/#configuration-and-instantiation","title":"Configuration and Instantiation","text":"<ul> <li>fusion_bench.mixins.HydraConfigMixin: A mixin class that provides configuration-based instantiation capabilities.</li> <li>fusion_bench.mixins.auto_register_config: Decorator for automatically mapping constructor parameters to configuration keys.</li> </ul>"},{"location":"api/fusion_bench.mixins/#serialization-and-persistence","title":"Serialization and Persistence","text":"<ul> <li>fusion_bench.mixins.YAMLSerializationMixin: Provides methods for serializing and deserializing objects to and from YAML format.</li> <li>fusion_bench.mixins.BaseYAMLSerializable: Base class for objects that support YAML serialization.</li> </ul>"},{"location":"api/fusion_bench.mixins/#distributed-computing-and-training","title":"Distributed Computing and Training","text":"<ul> <li>fusion_bench.mixins.LightningFabricMixin: Integrates with Lightning Fabric for automatic distributed environment and accelerator management.</li> <li>fusion_bench.mixins.FabricTrainingMixin: Extends Lightning Fabric integration with training-specific utilities.</li> </ul>"},{"location":"api/fusion_bench.mixins/#performance-and-debugging","title":"Performance and Debugging","text":"<ul> <li>fusion_bench.mixins.SimpleProfilerMixin: Provides simple profiling capabilities for measuring execution time.</li> <li>fusion_bench.mixins.PyinstrumentProfilerMixin: Offers advanced statistical profiling using the pyinstrument library.</li> </ul>"},{"location":"api/fusion_bench.mixins/#computer-vision","title":"Computer Vision","text":"<ul> <li>fusion_bench.mixins.CLIPClassificationMixin: Supports CLIP-based image classification tasks.</li> </ul>"},{"location":"api/fusion_bench.mixins/#class-decorators","title":"Class Decorators","text":"<ul> <li>fusion_bench.mixins.auto_register_config</li> </ul>"},{"location":"api/fusion_bench.mixins/#references","title":"References","text":""},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.HydraConfigMixin","title":"<code>HydraConfigMixin</code>","text":"<p>A mixin class that provides configuration-based instantiation capabilities.</p> <p>This mixin enables classes to be instantiated directly from Hydra configuration files, supporting both direct instantiation and target-based instantiation patterns. It's particularly useful in FusionBench for creating model pools, task pools, and fusion algorithms from YAML configurations.</p> <p>The mixin handles: - Configuration loading and composition - Target class validation - Nested configuration group navigation - Object instantiation with proper error handling</p> <p>Example:</p> <pre><code>class MyAlgorithm(HydraConfigMixin):\n    def __init__(self, param1: str, param2: int = 10):\n        self.param1 = param1\n        self.param2 = param2\n\n# Instantiate from config\nalgorithm = MyAlgorithm.from_config(\"algorithms/my_algorithm\")\n</code></pre> Note <p>This mixin requires Hydra to be properly initialized before use. Typically, this is handled by the main FusionBench CLI application.</p> Source code in <code>fusion_bench/mixins/hydra_config.py</code> <pre><code>class HydraConfigMixin:\n    R\"\"\"\n    A mixin class that provides configuration-based instantiation capabilities.\n\n    This mixin enables classes to be instantiated directly from Hydra configuration\n    files, supporting both direct instantiation and target-based instantiation patterns.\n    It's particularly useful in FusionBench for creating model pools, task pools,\n    and fusion algorithms from YAML configurations.\n\n    The mixin handles:\n    - Configuration loading and composition\n    - Target class validation\n    - Nested configuration group navigation\n    - Object instantiation with proper error handling\n\n    Example:\n\n    ```python\n    class MyAlgorithm(HydraConfigMixin):\n        def __init__(self, param1: str, param2: int = 10):\n            self.param1 = param1\n            self.param2 = param2\n\n    # Instantiate from config\n    algorithm = MyAlgorithm.from_config(\"algorithms/my_algorithm\")\n    ```\n\n    Note:\n        This mixin requires Hydra to be properly initialized before use.\n        Typically, this is handled by the main FusionBench CLI application.\n    \"\"\"\n\n    @classmethod\n    def from_config(\n        cls,\n        config_name: Union[str, Path],\n        overrides: Optional[List[str]] = None,\n    ) -&gt; T:\n        \"\"\"\n        Create an instance of the class from a Hydra configuration.\n\n        This method loads a Hydra configuration file and instantiates the class\n        using the configuration parameters. It supports both direct parameter\n        passing and target-based instantiation patterns.\n\n        Args:\n            config_name: The name/path of the configuration file to load.\n                        Can be a string like \"algorithms/simple_average\" or\n                        a Path object. The .yaml extension is optional.\n            overrides: Optional list of configuration overrides in the format\n                      [\"key=value\", \"nested.key=value\"]. These allow runtime\n                      modification of configuration parameters.\n\n        Returns:\n            An instance of the class configured according to the loaded configuration.\n\n        Raises:\n            RuntimeError: If Hydra is not properly initialized.\n            ImportError: If a target class specified in the config cannot be imported.\n            ValueError: If required configuration parameters are missing.\n\n        Example:\n            ```python\n            # Load with basic config\n            obj = MyClass.from_config(\"my_config\")\n\n            # Load with overrides\n            obj = MyClass.from_config(\n                \"my_config\",\n                overrides=[\"param1=new_value\", \"param2=42\"]\n            )\n\n            # Load nested config\n            obj = MyClass.from_config(\"category/subcategory/my_config\")\n            ```\n\n        Note:\n            The method automatically handles nested configuration groups by\n            navigating through the configuration hierarchy based on the\n            config_name path structure.\n        \"\"\"\n        # Verify Hydra initialization\n        if not hydra.core.global_hydra.GlobalHydra.instance().is_initialized():\n            raise RuntimeError(\n                \"Hydra is not initialized. Please ensure Hydra is properly \"\n                \"initialized before calling from_config(). This is typically \"\n                \"handled by the FusionBench CLI application.\"\n            )\n        else:\n            # Compose the configuration with any provided overrides\n            cfg = compose(config_name=config_name, overrides=overrides)\n\n        # Navigate through nested configuration groups\n        # E.g., \"algorithms/simple_average\" -&gt; navigate to cfg.algorithms\n        config_groups = config_name.split(\"/\")[:-1]\n        for config_group in config_groups:\n            cfg = cfg[config_group]\n\n        # Handle target-based instantiation\n        if \"_target_\" in cfg:\n            # Validate that the target class matches the calling class\n            target_cls = import_object(cfg[\"_target_\"])\n            if target_cls != cls:\n                log.warning(\n                    f\"Configuration target mismatch: config specifies \"\n                    f\"'{cfg['_target_']}' but called on class '{cls.__name__}'. \"\n                    f\"This may indicate a configuration error.\"\n                )\n\n            # Instantiate using the target pattern with function call logging disabled\n            with set_print_function_call(False):\n                obj = instantiate(cfg)\n        else:\n            # Direct instantiation using configuration as keyword arguments\n            obj = cls(**cfg)\n\n        return obj\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.HydraConfigMixin.from_config","title":"<code>from_config(config_name, overrides=None)</code>  <code>classmethod</code>","text":"<p>Create an instance of the class from a Hydra configuration.</p> <p>This method loads a Hydra configuration file and instantiates the class using the configuration parameters. It supports both direct parameter passing and target-based instantiation patterns.</p> <p>Parameters:</p> <ul> <li> <code>config_name</code>               (<code>Union[str, Path]</code>)           \u2013            <p>The name/path of the configuration file to load.         Can be a string like \"algorithms/simple_average\" or         a Path object. The .yaml extension is optional.</p> </li> <li> <code>overrides</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of configuration overrides in the format       [\"key=value\", \"nested.key=value\"]. These allow runtime       modification of configuration parameters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>T</code>           \u2013            <p>An instance of the class configured according to the loaded configuration.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If Hydra is not properly initialized.</p> </li> <li> <code>ImportError</code>             \u2013            <p>If a target class specified in the config cannot be imported.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If required configuration parameters are missing.</p> </li> </ul> Example <pre><code># Load with basic config\nobj = MyClass.from_config(\"my_config\")\n\n# Load with overrides\nobj = MyClass.from_config(\n    \"my_config\",\n    overrides=[\"param1=new_value\", \"param2=42\"]\n)\n\n# Load nested config\nobj = MyClass.from_config(\"category/subcategory/my_config\")\n</code></pre> Note <p>The method automatically handles nested configuration groups by navigating through the configuration hierarchy based on the config_name path structure.</p> Source code in <code>fusion_bench/mixins/hydra_config.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config_name: Union[str, Path],\n    overrides: Optional[List[str]] = None,\n) -&gt; T:\n    \"\"\"\n    Create an instance of the class from a Hydra configuration.\n\n    This method loads a Hydra configuration file and instantiates the class\n    using the configuration parameters. It supports both direct parameter\n    passing and target-based instantiation patterns.\n\n    Args:\n        config_name: The name/path of the configuration file to load.\n                    Can be a string like \"algorithms/simple_average\" or\n                    a Path object. The .yaml extension is optional.\n        overrides: Optional list of configuration overrides in the format\n                  [\"key=value\", \"nested.key=value\"]. These allow runtime\n                  modification of configuration parameters.\n\n    Returns:\n        An instance of the class configured according to the loaded configuration.\n\n    Raises:\n        RuntimeError: If Hydra is not properly initialized.\n        ImportError: If a target class specified in the config cannot be imported.\n        ValueError: If required configuration parameters are missing.\n\n    Example:\n        ```python\n        # Load with basic config\n        obj = MyClass.from_config(\"my_config\")\n\n        # Load with overrides\n        obj = MyClass.from_config(\n            \"my_config\",\n            overrides=[\"param1=new_value\", \"param2=42\"]\n        )\n\n        # Load nested config\n        obj = MyClass.from_config(\"category/subcategory/my_config\")\n        ```\n\n    Note:\n        The method automatically handles nested configuration groups by\n        navigating through the configuration hierarchy based on the\n        config_name path structure.\n    \"\"\"\n    # Verify Hydra initialization\n    if not hydra.core.global_hydra.GlobalHydra.instance().is_initialized():\n        raise RuntimeError(\n            \"Hydra is not initialized. Please ensure Hydra is properly \"\n            \"initialized before calling from_config(). This is typically \"\n            \"handled by the FusionBench CLI application.\"\n        )\n    else:\n        # Compose the configuration with any provided overrides\n        cfg = compose(config_name=config_name, overrides=overrides)\n\n    # Navigate through nested configuration groups\n    # E.g., \"algorithms/simple_average\" -&gt; navigate to cfg.algorithms\n    config_groups = config_name.split(\"/\")[:-1]\n    for config_group in config_groups:\n        cfg = cfg[config_group]\n\n    # Handle target-based instantiation\n    if \"_target_\" in cfg:\n        # Validate that the target class matches the calling class\n        target_cls = import_object(cfg[\"_target_\"])\n        if target_cls != cls:\n            log.warning(\n                f\"Configuration target mismatch: config specifies \"\n                f\"'{cfg['_target_']}' but called on class '{cls.__name__}'. \"\n                f\"This may indicate a configuration error.\"\n            )\n\n        # Instantiate using the target pattern with function call logging disabled\n        with set_print_function_call(False):\n            obj = instantiate(cfg)\n    else:\n        # Direct instantiation using configuration as keyword arguments\n        obj = cls(**cfg)\n\n    return obj\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.YAMLSerializationMixin","title":"<code>YAMLSerializationMixin</code>","text":"Source code in <code>fusion_bench/mixins/serialization.py</code> <pre><code>class YAMLSerializationMixin:\n    _config_key: Optional[str] = None\n    _config_mapping: MutableBidict[str, str] = bidict()\n    R\"\"\"\n    `_config_mapping` is a dictionary mapping the attribute names of the class to the config option names. This is used to convert the class to a DictConfig.\n\n    &gt;&gt;&gt; algorithm.config\n        DictCOnfig({'_target_': 'SomeModelFusionAlgorithm', 'hyper_param_1': 1, 'hyper_param_2': 2})\n\n    By default, the `_target_` key is set to the class name as `type(self).__name__`.\n    \"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        for key, value in kwargs.items():\n            log.warning(f\"Unused argument: {key}={value}\")\n\n    @property\n    def config(self) -&gt; DictConfig:\n        R\"\"\"\n        Returns the configuration of the model pool as a DictConfig.\n\n        This property converts the model pool instance into a dictionary\n        configuration, which can be used for serialization or other purposes.\n\n        Returns:\n            DictConfig: The configuration of the model pool.\n        \"\"\"\n        config = {\"_target_\": f\"{type(self).__module__}.{type(self).__qualname__}\"}\n        for attr, key in self._config_mapping.items():\n            if hasattr(self, attr):\n                config[key] = getattr(self, attr)\n\n        try:\n            return OmegaConf.create(config)\n        except Exception as e:\n            return OmegaConf.create(config, flags={\"allow_objects\": True})\n\n    def to_yaml(self, path: Union[str, Path], resolve: bool = True):\n        \"\"\"\n        Save the model pool to a YAML file.\n\n        Args:\n            path (Union[str, Path]): The path to save the model pool to.\n        \"\"\"\n        OmegaConf.save(self.config, path, resolve=resolve)\n\n    @classmethod\n    def from_yaml(cls, path: Union[str, Path]):\n        \"\"\"\n        Load a model pool from a YAML file.\n\n        Args:\n            path (Union[str, Path]): The path to load the model pool from.\n\n        Returns:\n            BaseModelPool: The loaded model pool.\n        \"\"\"\n        config = OmegaConf.load(path)\n        if cls._config_key is not None and cls._config_key in config:\n            config = config[cls._config_key]\n        target_cls = import_object(config[\"_target_\"])\n        if target_cls != cls:\n            log.warning(\n                f\"The class {target_cls.__name__} is not the same as the class {cls.__name__}. \"\n                f\"Instantiating the class {target_cls.__name__} instead.\"\n            )\n        with set_print_function_call(False):\n            return instantiate(config)\n\n    def register_parameter_to_config(\n        self,\n        attr_name: str,\n        param_name: str,\n        value,\n    ):\n        \"\"\"\n        Set an attribute value and register its config mapping.\n\n        This method allows dynamic setting of object attributes while simultaneously\n        updating the configuration mapping that defines how the attribute should\n        be serialized in the configuration output.\n\n        Args:\n            attr_name (str): The name of the attribute to set on this object.\n            arg_name (str): The corresponding parameter name to use in the config\n                serialization. This is how the attribute will appear in YAML output.\n            value: The value to assign to the attribute.\n\n        \"\"\"\n        setattr(self, attr_name, value)\n        self._config_mapping[attr_name] = param_name\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.YAMLSerializationMixin.config","title":"<code>config</code>  <code>property</code>","text":"<p>Returns the configuration of the model pool as a DictConfig.</p> <p>This property converts the model pool instance into a dictionary configuration, which can be used for serialization or other purposes.</p> <p>Returns:</p> <ul> <li> <code>DictConfig</code> (              <code>DictConfig</code> )          \u2013            <p>The configuration of the model pool.</p> </li> </ul>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.YAMLSerializationMixin.from_yaml","title":"<code>from_yaml(path)</code>  <code>classmethod</code>","text":"<p>Load a model pool from a YAML file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>The path to load the model pool from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BaseModelPool</code>          \u2013            <p>The loaded model pool.</p> </li> </ul> Source code in <code>fusion_bench/mixins/serialization.py</code> <pre><code>@classmethod\ndef from_yaml(cls, path: Union[str, Path]):\n    \"\"\"\n    Load a model pool from a YAML file.\n\n    Args:\n        path (Union[str, Path]): The path to load the model pool from.\n\n    Returns:\n        BaseModelPool: The loaded model pool.\n    \"\"\"\n    config = OmegaConf.load(path)\n    if cls._config_key is not None and cls._config_key in config:\n        config = config[cls._config_key]\n    target_cls = import_object(config[\"_target_\"])\n    if target_cls != cls:\n        log.warning(\n            f\"The class {target_cls.__name__} is not the same as the class {cls.__name__}. \"\n            f\"Instantiating the class {target_cls.__name__} instead.\"\n        )\n    with set_print_function_call(False):\n        return instantiate(config)\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.YAMLSerializationMixin.register_parameter_to_config","title":"<code>register_parameter_to_config(attr_name, param_name, value)</code>","text":"<p>Set an attribute value and register its config mapping.</p> <p>This method allows dynamic setting of object attributes while simultaneously updating the configuration mapping that defines how the attribute should be serialized in the configuration output.</p> <p>Parameters:</p> <ul> <li> <code>attr_name</code>               (<code>str</code>)           \u2013            <p>The name of the attribute to set on this object.</p> </li> <li> <code>arg_name</code>               (<code>str</code>)           \u2013            <p>The corresponding parameter name to use in the config serialization. This is how the attribute will appear in YAML output.</p> </li> <li> <code>value</code>           \u2013            <p>The value to assign to the attribute.</p> </li> </ul> Source code in <code>fusion_bench/mixins/serialization.py</code> <pre><code>def register_parameter_to_config(\n    self,\n    attr_name: str,\n    param_name: str,\n    value,\n):\n    \"\"\"\n    Set an attribute value and register its config mapping.\n\n    This method allows dynamic setting of object attributes while simultaneously\n    updating the configuration mapping that defines how the attribute should\n    be serialized in the configuration output.\n\n    Args:\n        attr_name (str): The name of the attribute to set on this object.\n        arg_name (str): The corresponding parameter name to use in the config\n            serialization. This is how the attribute will appear in YAML output.\n        value: The value to assign to the attribute.\n\n    \"\"\"\n    setattr(self, attr_name, value)\n    self._config_mapping[attr_name] = param_name\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.YAMLSerializationMixin.to_yaml","title":"<code>to_yaml(path, resolve=True)</code>","text":"<p>Save the model pool to a YAML file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>The path to save the model pool to.</p> </li> </ul> Source code in <code>fusion_bench/mixins/serialization.py</code> <pre><code>def to_yaml(self, path: Union[str, Path], resolve: bool = True):\n    \"\"\"\n    Save the model pool to a YAML file.\n\n    Args:\n        path (Union[str, Path]): The path to save the model pool to.\n    \"\"\"\n    OmegaConf.save(self.config, path, resolve=resolve)\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.BaseYAMLSerializable","title":"<code>BaseYAMLSerializable</code>","text":"<p>               Bases: <code>YAMLSerializationMixin</code></p> <p>A base class for YAML-serializable classes with enhanced metadata support.</p> <p>This class extends <code>YAMLSerializationMixin</code> to provide additional metadata fields commonly used in FusionBench classes, including usage information and version tracking. It serves as a foundation for all serializable model components in the framework.</p> <p>The class automatically handles serialization of usage and version metadata alongside the standard configuration parameters, making it easier to track model provenance and intended usage patterns.</p> <p>Attributes:</p> <ul> <li> <code>_usage_</code>               (<code>Optional[str]</code>)           \u2013            <p>Description of the model's intended usage or purpose.</p> </li> <li> <code>_version_</code>               (<code>Optional[str]</code>)           \u2013            <p>Version information for the model or configuration.</p> </li> </ul> Example <pre><code>class MyAlgorithm(BaseYAMLSerializable):\n    _config_mapping = BaseYAMLSerializable._config_mapping | {\n        \"model_name\": \"model_name\",\n        \"num_layers\": \"num_layers\",\n    }\n\n    def __init__(self, _usage_: str = None, _version_: str = None):\n        super().__init__(_usage_=_usage_, _version_=_version_)\n\n# Usage with metadata\nmodel = MyAlgorithm(\n    _usage_=\"Text classification fine-tuning\",\n    _version_=\"1.0.0\"\n)\n\n# Serialization includes metadata\nconfig = model.config\n# DictConfig({\n#     '_target_': 'MyModel',\n#     '_usage_': 'Text classification fine-tuning',\n#     '_version_': '1.0.0'\n# })\n</code></pre> Note <p>The underscore prefix in <code>_usage_</code> and <code>_version_</code> follows the convention for metadata fields that are not core model parameters but provide important contextual information for model management and tracking.</p> Source code in <code>fusion_bench/mixins/serialization.py</code> <pre><code>@auto_register_config\nclass BaseYAMLSerializable(YAMLSerializationMixin):\n    \"\"\"\n    A base class for YAML-serializable classes with enhanced metadata support.\n\n    This class extends `YAMLSerializationMixin` to provide additional metadata\n    fields commonly used in FusionBench classes, including usage information\n    and version tracking. It serves as a foundation for all serializable\n    model components in the framework.\n\n    The class automatically handles serialization of usage and version metadata\n    alongside the standard configuration parameters, making it easier to track\n    model provenance and intended usage patterns.\n\n    Attributes:\n        _usage_ (Optional[str]): Description of the model's intended usage or purpose.\n        _version_ (Optional[str]): Version information for the model or configuration.\n\n    Example:\n        ```python\n        class MyAlgorithm(BaseYAMLSerializable):\n            _config_mapping = BaseYAMLSerializable._config_mapping | {\n                \"model_name\": \"model_name\",\n                \"num_layers\": \"num_layers\",\n            }\n\n            def __init__(self, _usage_: str = None, _version_: str = None):\n                super().__init__(_usage_=_usage_, _version_=_version_)\n\n        # Usage with metadata\n        model = MyAlgorithm(\n            _usage_=\"Text classification fine-tuning\",\n            _version_=\"1.0.0\"\n        )\n\n        # Serialization includes metadata\n        config = model.config\n        # DictConfig({\n        #     '_target_': 'MyModel',\n        #     '_usage_': 'Text classification fine-tuning',\n        #     '_version_': '1.0.0'\n        # })\n        ```\n\n    Note:\n        The underscore prefix in `_usage_` and `_version_` follows the convention\n        for metadata fields that are not core model parameters but provide\n        important contextual information for model management and tracking.\n    \"\"\"\n\n    def __init__(\n        self,\n        _recursive_: bool = False,\n        _usage_: Optional[str] = None,\n        _version_: Optional[str] = FUSION_BENCH_VERSION,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize a base YAML-serializable model with metadata support.\n\n        Args:\n            _usage_ (Optional[str], optional): Description of the model's intended\n                usage or purpose. This can include information about the training\n                domain, expected input types, or specific use cases. Defaults to None.\n            _version_ (Optional[str], optional): Version information for the model\n                or configuration. Can be used to track model iterations, dataset\n                versions, or compatibility information. Defaults to None.\n            **kwargs: Additional keyword arguments passed to the parent class.\n                Unused arguments will trigger warnings via the parent's initialization.\n\n        Example:\n            ```python\n            model = BaseYAMLSerializable(\n                _usage_=\"Image classification on CIFAR-10\",\n                _version_=\"2.1.0\"\n            )\n            ```\n        \"\"\"\n        super().__init__(**kwargs)\n        if _version_ != FUSION_BENCH_VERSION:\n            log.warning(\n                f\"Current fusion-bench version is {FUSION_BENCH_VERSION}, but the serialized version is {_version_}. \"\n                \"Attempting to use current version.\"\n            )\n            # override _version_ with current fusion-bench version\n            self._version_ = FUSION_BENCH_VERSION\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.BaseYAMLSerializable.__init__","title":"<code>__init__(_recursive_=False, _usage_=None, _version_=FUSION_BENCH_VERSION, **kwargs)</code>","text":"<p>Initialize a base YAML-serializable model with metadata support.</p> <p>Parameters:</p> <ul> <li> <code>_usage_</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Description of the model's intended usage or purpose. This can include information about the training domain, expected input types, or specific use cases. Defaults to None.</p> </li> <li> <code>_version_</code>               (<code>Optional[str]</code>, default:                   <code>FUSION_BENCH_VERSION</code> )           \u2013            <p>Version information for the model or configuration. Can be used to track model iterations, dataset versions, or compatibility information. Defaults to None.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments passed to the parent class. Unused arguments will trigger warnings via the parent's initialization.</p> </li> </ul> Example <pre><code>model = BaseYAMLSerializable(\n    _usage_=\"Image classification on CIFAR-10\",\n    _version_=\"2.1.0\"\n)\n</code></pre> Source code in <code>fusion_bench/mixins/serialization.py</code> <pre><code>def __init__(\n    self,\n    _recursive_: bool = False,\n    _usage_: Optional[str] = None,\n    _version_: Optional[str] = FUSION_BENCH_VERSION,\n    **kwargs,\n):\n    \"\"\"\n    Initialize a base YAML-serializable model with metadata support.\n\n    Args:\n        _usage_ (Optional[str], optional): Description of the model's intended\n            usage or purpose. This can include information about the training\n            domain, expected input types, or specific use cases. Defaults to None.\n        _version_ (Optional[str], optional): Version information for the model\n            or configuration. Can be used to track model iterations, dataset\n            versions, or compatibility information. Defaults to None.\n        **kwargs: Additional keyword arguments passed to the parent class.\n            Unused arguments will trigger warnings via the parent's initialization.\n\n    Example:\n        ```python\n        model = BaseYAMLSerializable(\n            _usage_=\"Image classification on CIFAR-10\",\n            _version_=\"2.1.0\"\n        )\n        ```\n    \"\"\"\n    super().__init__(**kwargs)\n    if _version_ != FUSION_BENCH_VERSION:\n        log.warning(\n            f\"Current fusion-bench version is {FUSION_BENCH_VERSION}, but the serialized version is {_version_}. \"\n            \"Attempting to use current version.\"\n        )\n        # override _version_ with current fusion-bench version\n        self._version_ = FUSION_BENCH_VERSION\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.LightningFabricMixin","title":"<code>LightningFabricMixin</code>","text":"<p>A mixin class for integrating Lightning Fabric into a project.</p> <p>This class provides methods to initialize and manage a Lightning Fabric instance for distributed computing, including setup with optional logging, device management for tensors and modules, and hyperparameter logging. It leverages the Lightning framework to facilitate distributed training and inference across multiple devices and nodes, with support for custom logging via TensorBoard.</p> <p>Attributes:</p> <ul> <li>_fabric (L.Fabric): The Lightning Fabric instance used for distributed computing.</li> </ul> <p>Note:</p> <p>This mixin is designed to be used with classes that require distributed computing capabilities and wish to leverage the Lightning Fabric for this purpose. It assumes the presence of a <code>config</code> attribute or parameter in the consuming class for configuration.</p> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>class LightningFabricMixin:\n    \"\"\"\n    A mixin class for integrating Lightning Fabric into a project.\n\n    This class provides methods to initialize and manage a Lightning Fabric instance for distributed computing,\n    including setup with optional logging, device management for tensors and modules, and hyperparameter logging.\n    It leverages the Lightning framework to facilitate distributed training and inference across multiple devices\n    and nodes, with support for custom logging via TensorBoard.\n\n    Attributes:\n\n    - _fabric (L.Fabric): The Lightning Fabric instance used for distributed computing.\n\n    Note:\n\n    This mixin is designed to be used with classes that require distributed computing capabilities and wish to\n    leverage the Lightning Fabric for this purpose. It assumes the presence of a `config` attribute or parameter\n    in the consuming class for configuration.\n    \"\"\"\n\n    _fabric_instance: L.Fabric = None\n\n    def setup_lightning_fabric(self, config: DictConfig):\n        \"\"\"\n        Initializes and launches the Lightning Fabric with optional logging.\n\n        This method sets up the Lightning Fabric for distributed computing based on the provided configuration. If a fabric\n        configuration is not found, it logs a warning and exits. Optionally, if a fabric logger configuration is provided,\n        it initializes a TensorBoardLogger with the specified settings.\n\n        Expected configuration keys:\n        - fabric: The configuration for the Lightning Fabric.\n        - fabric.loggers: The configuration for the TensorBoardLogger.\n        \"\"\"\n        if self._fabric_instance is None:\n            if config.get(\"fabric\", None) is None:\n                log.warning(\"No fabric configuration found. use default settings.\")\n                self._fabric_instance = L.Fabric()\n            else:\n                self._fabric_instance = instantiate(config.fabric)\n            if not _is_using_cli():  # if not using cli, launch the fabric\n                self._fabric_instance.launch()\n            # Set the log directory in config if it is not already set\n            if (\n                self.log_dir is not None\n                and hasattr(config, \"log_dir\")\n                and config.get(\"log_dir\", None) is None\n            ):\n                if self._fabric_instance.is_global_zero:\n                    log.info(f\"Setting log_dir to {self.log_dir}\")\n                config.log_dir = self.log_dir\n\n    @property\n    def fabric(self):\n        if self._fabric_instance is None:\n            self.setup_lightning_fabric(getattr(self, \"config\", DictConfig({})))\n        return self._fabric_instance\n\n    @fabric.setter\n    def fabric(self, instance: L.Fabric):\n        self._fabric_instance = instance\n\n    @property\n    def log_dir(self):\n        \"\"\"\n        Retrieves the log directory from the fabric's logger.\n        \"\"\"\n        if self.fabric is not None and len(self.fabric._loggers) &gt; 0:\n            log_dir = self.fabric.logger.log_dir\n\n            # Special handling for SwanLabLogger to get the correct log directory\n            if (\n                log_dir is None\n                and self.fabric.logger.__class__.__name__ == \"SwanLabLogger\"\n            ):\n                log_dir = self.fabric.logger.save_dir or self.fabric.logger._logdir\n\n            assert log_dir is not None, \"log_dir should not be None\"\n            if self.fabric.is_global_zero and not os.path.exists(log_dir):\n                os.makedirs(log_dir, exist_ok=True)\n            return log_dir\n        else:\n            return None\n\n    def to_device(self, obj: TensorOrModule) -&gt; TensorOrModule:\n        \"\"\"\n        Moves a tensor or module to the proper device.\n\n        Args:\n            obj (TensorOrModule): The tensor or module to move to the device.\n\n        Returns:\n            TensorOrModule: the same type of object as the input, moved to the device.\n        \"\"\"\n        return self.fabric.to_device(obj)\n\n    @rank_zero_only\n    def log_hyperparams(\n        self,\n        config: Optional[DictConfig] = None,\n        save_dir: Optional[str] = None,\n        filename: str = \"config.yaml\",\n    ):\n        R\"\"\"\n        Logs the hyperparameters and saves the configuration to a YAML file.\n        The YAML file is saved in the log directory by default with the name `config.yaml`, or in the specified save directory `save_dir`.\n\n        Args:\n            config (Optional[DictConfig]): The configuration to log and save. If not provided, the class's `config` attribute is used.\n            save_dir (Optional[str]): The directory in which to save the configuration file. If not provided, the log directory is used.\n            filename (str): The name of the configuration file. Default is `config.yaml`.\n        \"\"\"\n        if config is None:\n            config = self.config\n        if save_dir is None:\n            save_dir = self.log_dir\n        self.fabric.logger.log_hyperparams(\n            OmegaConf.to_container(config, resolve=True, enum_to_str=True)\n        )\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir, exist_ok=True)\n        OmegaConf.save(\n            config,\n            os.path.join(self.log_dir if save_dir is None else save_dir, filename),\n        )\n\n    @property\n    def tensorboard_summarywriter(\n        self,\n    ) -&gt; \"lightning.fabric.loggers.tensorboard.SummaryWriter\":\n        if isinstance(self.fabric.logger, TensorBoardLogger):\n            return self.fabric.logger.experiment\n        else:\n            raise AttributeError(\"the logger is not a TensorBoardLogger.\")\n\n    @property\n    def is_debug_mode(self):\n        if hasattr(self, \"config\") and self.config.get(\"fast_dev_run\", False):\n            return True\n        elif hasattr(self, \"_program\") and self._program.config.get(\n            \"fast_dev_run\", False\n        ):\n            return True\n        else:\n            return False\n\n    def log(self, name: str, value: Any, step: Optional[int] = None):\n        \"\"\"\n        Logs the metric to the fabric's logger.\n        \"\"\"\n        self.fabric.log(name, value, step=step)\n\n    def log_dict(self, metrics: dict, step: Optional[int] = None):\n        \"\"\"\n        Logs the metrics to the fabric's logger.\n        \"\"\"\n        self.fabric.log_dict(metrics, step=step)\n\n    def log_optimizer_lr(\n        self,\n        optimizer: torch.optim.Optimizer,\n        step: Optional[int] = None,\n        name_template: str = \"train/lr_group_{0}\",\n    ):\n        \"\"\"\n        Logs the learning rate of the optimizer to the fabric's logger.\n        \"\"\"\n        for i, param_group in enumerate(optimizer.param_groups):\n            self.fabric.log(name_template.format(i), param_group[\"lr\"], step=step)\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.LightningFabricMixin.log_dir","title":"<code>log_dir</code>  <code>property</code>","text":"<p>Retrieves the log directory from the fabric's logger.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.LightningFabricMixin.log","title":"<code>log(name, value, step=None)</code>","text":"<p>Logs the metric to the fabric's logger.</p> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>def log(self, name: str, value: Any, step: Optional[int] = None):\n    \"\"\"\n    Logs the metric to the fabric's logger.\n    \"\"\"\n    self.fabric.log(name, value, step=step)\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.LightningFabricMixin.log_dict","title":"<code>log_dict(metrics, step=None)</code>","text":"<p>Logs the metrics to the fabric's logger.</p> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>def log_dict(self, metrics: dict, step: Optional[int] = None):\n    \"\"\"\n    Logs the metrics to the fabric's logger.\n    \"\"\"\n    self.fabric.log_dict(metrics, step=step)\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.LightningFabricMixin.log_hyperparams","title":"<code>log_hyperparams(config=None, save_dir=None, filename='config.yaml')</code>","text":"<p>Logs the hyperparameters and saves the configuration to a YAML file. The YAML file is saved in the log directory by default with the name <code>config.yaml</code>, or in the specified save directory <code>save_dir</code>.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>Optional[DictConfig]</code>, default:                   <code>None</code> )           \u2013            <p>The configuration to log and save. If not provided, the class's <code>config</code> attribute is used.</p> </li> <li> <code>save_dir</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The directory in which to save the configuration file. If not provided, the log directory is used.</p> </li> <li> <code>filename</code>               (<code>str</code>, default:                   <code>'config.yaml'</code> )           \u2013            <p>The name of the configuration file. Default is <code>config.yaml</code>.</p> </li> </ul> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>@rank_zero_only\ndef log_hyperparams(\n    self,\n    config: Optional[DictConfig] = None,\n    save_dir: Optional[str] = None,\n    filename: str = \"config.yaml\",\n):\n    R\"\"\"\n    Logs the hyperparameters and saves the configuration to a YAML file.\n    The YAML file is saved in the log directory by default with the name `config.yaml`, or in the specified save directory `save_dir`.\n\n    Args:\n        config (Optional[DictConfig]): The configuration to log and save. If not provided, the class's `config` attribute is used.\n        save_dir (Optional[str]): The directory in which to save the configuration file. If not provided, the log directory is used.\n        filename (str): The name of the configuration file. Default is `config.yaml`.\n    \"\"\"\n    if config is None:\n        config = self.config\n    if save_dir is None:\n        save_dir = self.log_dir\n    self.fabric.logger.log_hyperparams(\n        OmegaConf.to_container(config, resolve=True, enum_to_str=True)\n    )\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir, exist_ok=True)\n    OmegaConf.save(\n        config,\n        os.path.join(self.log_dir if save_dir is None else save_dir, filename),\n    )\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.LightningFabricMixin.log_optimizer_lr","title":"<code>log_optimizer_lr(optimizer, step=None, name_template='train/lr_group_{0}')</code>","text":"<p>Logs the learning rate of the optimizer to the fabric's logger.</p> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>def log_optimizer_lr(\n    self,\n    optimizer: torch.optim.Optimizer,\n    step: Optional[int] = None,\n    name_template: str = \"train/lr_group_{0}\",\n):\n    \"\"\"\n    Logs the learning rate of the optimizer to the fabric's logger.\n    \"\"\"\n    for i, param_group in enumerate(optimizer.param_groups):\n        self.fabric.log(name_template.format(i), param_group[\"lr\"], step=step)\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.LightningFabricMixin.setup_lightning_fabric","title":"<code>setup_lightning_fabric(config)</code>","text":"<p>Initializes and launches the Lightning Fabric with optional logging.</p> <p>This method sets up the Lightning Fabric for distributed computing based on the provided configuration. If a fabric configuration is not found, it logs a warning and exits. Optionally, if a fabric logger configuration is provided, it initializes a TensorBoardLogger with the specified settings.</p> <p>Expected configuration keys: - fabric: The configuration for the Lightning Fabric. - fabric.loggers: The configuration for the TensorBoardLogger.</p> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>def setup_lightning_fabric(self, config: DictConfig):\n    \"\"\"\n    Initializes and launches the Lightning Fabric with optional logging.\n\n    This method sets up the Lightning Fabric for distributed computing based on the provided configuration. If a fabric\n    configuration is not found, it logs a warning and exits. Optionally, if a fabric logger configuration is provided,\n    it initializes a TensorBoardLogger with the specified settings.\n\n    Expected configuration keys:\n    - fabric: The configuration for the Lightning Fabric.\n    - fabric.loggers: The configuration for the TensorBoardLogger.\n    \"\"\"\n    if self._fabric_instance is None:\n        if config.get(\"fabric\", None) is None:\n            log.warning(\"No fabric configuration found. use default settings.\")\n            self._fabric_instance = L.Fabric()\n        else:\n            self._fabric_instance = instantiate(config.fabric)\n        if not _is_using_cli():  # if not using cli, launch the fabric\n            self._fabric_instance.launch()\n        # Set the log directory in config if it is not already set\n        if (\n            self.log_dir is not None\n            and hasattr(config, \"log_dir\")\n            and config.get(\"log_dir\", None) is None\n        ):\n            if self._fabric_instance.is_global_zero:\n                log.info(f\"Setting log_dir to {self.log_dir}\")\n            config.log_dir = self.log_dir\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.LightningFabricMixin.to_device","title":"<code>to_device(obj)</code>","text":"<p>Moves a tensor or module to the proper device.</p> <p>Parameters:</p> <ul> <li> <code>obj</code>               (<code>TensorOrModule</code>)           \u2013            <p>The tensor or module to move to the device.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TensorOrModule</code> (              <code>TensorOrModule</code> )          \u2013            <p>the same type of object as the input, moved to the device.</p> </li> </ul> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>def to_device(self, obj: TensorOrModule) -&gt; TensorOrModule:\n    \"\"\"\n    Moves a tensor or module to the proper device.\n\n    Args:\n        obj (TensorOrModule): The tensor or module to move to the device.\n\n    Returns:\n        TensorOrModule: the same type of object as the input, moved to the device.\n    \"\"\"\n    return self.fabric.to_device(obj)\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin","title":"<code>FabricTrainingMixin</code>","text":"<p>               Bases: <code>LightningFabricMixin</code></p> <p>This is a general purpose mixin for training a model with PyTorch Lightning.</p> Source code in <code>fusion_bench/mixins/fabric_training.py</code> <pre><code>class FabricTrainingMixin(LightningFabricMixin):\n    \"\"\"\n    This is a general purpose mixin for training a model with PyTorch Lightning.\n    \"\"\"\n\n    _latest_saved_checkpoint_global_step: int = -1\n    \"\"\"The global step index of the latest saved checkpoint.\"\"\"\n    _expected_total_steps: int = None\n    \"\"\"The expected total number of steps of the entire training.\"\"\"\n    is_training: bool\n    \"\"\"Whether the training is in progress. If set to False, the training will stop.\"\"\"\n    epoch_idx: int\n    \"\"\"The epoch index, which is the number of epochs completed.\"\"\"\n    global_step_idx: int\n    \"\"\"The global step index, which is the number of parameter update steps.\"\"\"\n    max_epochs: int\n    \"\"\"Max number of epochs of the entire training.\"\"\"\n    max_steps: int\n    \"\"\"Max number of parameter update steps of the entire training.\"\"\"\n    max_steps_per_epoch: int\n    \"\"\"Max number of parameter update steps per epoch.\"\"\"\n    gradient_clip_algorithm: Literal[\"value\", \"norm\"]\n    \"\"\"The algorithm to clip gradients. Available options: 'value', 'norm'.\"\"\"\n    gradient_clip_val: float\n    \"\"\"The value to clip gradients. If None, no clipping is applied.\"\"\"\n    accumulate_grad_batches: int\n    \"\"\"The number of gradient accumulation steps. The effective global batch size is `the batch size per device` x `the number of devices` x `the number of gradient accumulation steps`.\"\"\"\n    lr_scheduler_interval: Literal[\"step\", \"epoch\"]\n    \"\"\"The interval to run the learning rate scheduler. Available options: 'step', 'epoch'.\"\"\"\n    lr_scheduler_frequency: int\n    \"\"\"The frequency to run the learning rate scheduler.\"\"\"\n    checkpoint_save_interval: Literal[\"step\", \"epoch\"]\n    \"\"\"The interval to save the model checkpoint. Available options: 'step', 'epoch'.\"\"\"\n    checkpoint_save_frequency: int\n    \"\"\"The frequency to save the model checkpoint.\"\"\"\n\n    def clip_gradients_if_needed(self, model, optimizer):\n        \"\"\"\n        Clips gradients if the gradient clipping value is set.\n\n        Args:\n            model (nn.Module): The model whose gradients need to be clipped.\n            optimizer (torch.optim.Optimizer): The optimizer used for training.\n        \"\"\"\n        fabric = self.fabric\n\n        if self.gradient_clip_val is not None:\n            if self.gradient_clip_algorithm == \"value\":\n                fabric.clip_gradients(model, optimizer, clip_val=self.gradient_clip_val)\n            elif self.gradient_clip_algorithm == \"norm\":\n                fabric.clip_gradients(model, optimizer, max_norm=self.gradient_clip_val)\n            else:\n                raise ValueError(\n                    f\"Unknown gradient clip algorithm: {self.gradient_clip_algorithm}. Available options: 'value', 'norm'\"\n                )\n\n    def compute_expected_total_steps(\n        self, train_dataloader: torch.utils.data.DataLoader\n    ):\n        \"\"\"\n        Computes the expected total number of steps for the entire training.\n\n        Args:\n            train_dataloader (torch.utils.data.DataLoader): The dataloader for the training data.\n        \"\"\"\n        # compute expected total steps\n        self._expected_total_steps = []\n        if self.max_steps &gt; 0:\n            self._expected_total_steps.append(self.max_steps)\n        if self.max_steps_per_epoch &gt; 0 and self.max_epochs &gt; 0:\n            self._expected_total_steps.append(\n                self.max_steps_per_epoch * self.max_epochs\n            )\n        if self.max_epochs &gt; 0:\n            self._expected_total_steps.append(\n                len(train_dataloader) * self.max_epochs // self.accumulate_grad_batches\n            )\n        self._expected_total_steps = min(self._expected_total_steps)\n        log.info(f\"Expected total steps: {self._expected_total_steps}\")\n\n    @property\n    def expected_total_steps(self):\n        \"\"\"\n        The expected total number of steps of the entire training. You need to run `compute_expected_total_steps` method to compute this value before accessing it.\n\n        Raises:\n            ValueError: If the expected total steps have not been computed.\n        \"\"\"\n        if self._expected_total_steps is None:\n            raise ValueError(\n                \"The expected total steps have not been computed. Run `compute_expected_total_steps` method.\"\n            )\n        else:\n            return self._expected_total_steps\n\n    def conditional_checkpoint_save(\n        self,\n        stage: Literal[\"end_of_step\", \"end_of_epoch\", \"end_of_training\"],\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Conditionally saves a checkpoint based on the current training stage.\n\n        Args:\n            stage (Literal[\"end_of_step\", \"end_of_epoch\", \"end_of_training\"]): The current stage of training.\n        \"\"\"\n        if stage == \"end_of_step\":\n            if (\n                self.checkpoint_save_interval == \"step\"\n                and (self.global_step_idx + 1) % self.checkpoint_save_frequency == 0\n            ):\n                save_path = os.path.join(\n                    self.log_dir, \"checkpoints\", f\"step={self.global_step_idx}.ckpt\"\n                )\n                self.save_checkpoint(save_path, *args, **kwargs)\n        elif stage == \"end_of_epoch\":\n            if (\n                self.checkpoint_save_interval == \"epoch\"\n                and (self.epoch_idx + 1) % self.checkpoint_save_frequency == 0\n            ):\n                save_path = os.path.join(\n                    self.log_dir, \"checkpoints\", f\"epoch={self.epoch_idx}.ckpt\"\n                )\n                self.save_checkpoint(save_path, *args, **kwargs)\n        elif stage == \"end_of_training\":\n            # if the checkpoint has not been saved yet, save it\n            if self.global_step_idx &gt; self._latest_saved_checkpoint_global_step:\n                save_path = os.path.join(\n                    self.log_dir,\n                    \"checkpoints\",\n                    f\"epoch={self.epoch_idx}_step={self.global_step_idx}.ckpt\",\n                )\n                self.save_checkpoint(save_path, *args, **kwargs)\n                try:\n                    os.symlink(\n                        src=save_path,\n                        dst=os.path.join(\n                            self.log_dir, \"checkpoints\", \"latest_model.ckpt\"\n                        ),\n                        target_is_directory=os.path.isdir(save_path),\n                    )\n                except Exception as e:\n                    log.error(f\"Failed to create symlink: {e}\")\n        else:\n            raise ValueError(\n                f\"Unknown stage: {stage}. Available options: 'end_of_step', 'end_of_epoch', 'end_of_training'\"\n            )\n\n    @abstractmethod\n    def save_checkpoint(self, path, **kwargs):\n        \"\"\"\n        Saves a checkpoint of the model.\n\n        Args:\n            path (str): The path where the checkpoint will be saved.\n\n        Raises:\n            NotImplementedError: If the method is not implemented.\n        \"\"\"\n        raise NotImplementedError(\"save_checkpoint method is not implemented\")\n\n    def train(\n        self,\n        model: Union[nn.Module, \"_FabricModule\"],\n        optimizer: Union[torch.optim.Optimizer, \"_FabricOptimizer\"],\n        lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n    ):\n        \"\"\"\n        Trains the model.\n\n        The global batch size is `the batch size per device` x `the number of devices` x `the number of gradient accumulation steps`.\n\n        Args:\n            model (Union[nn.Module, \"_FabricModule\"]): The model to be trained.\n            optimizer (Union[torch.optim.Optimizer, \"_FabricOptimizer\"]): The optimizer used for training.\n            lr_scheduler (torch.optim.lr_scheduler.LRScheduler): The learning rate scheduler.\n        \"\"\"\n        fabric = self.fabric\n        self.is_training = True\n        # number of parameter update iterations, not the number of batches\n        self.global_step_idx = 0\n        model.train()\n        optimizer.zero_grad()\n        for epoch_idx in tqdm(\n            range(self.max_epochs) if self.max_epochs &gt; 0 else itertools.count(0),\n            \"Training Epoch\",\n            dynamic_ncols=True,\n            leave=False,\n            disable=not fabric.is_global_zero,\n        ):\n            self.epoch_idx = epoch_idx\n            self.train_epoch(model, optimizer, lr_scheduler)\n            # run lr_scheduler at the end of the epoch if interval is set to \"epoch\"\n            if (\n                self.lr_scheduler_interval == \"epoch\"\n                and (epoch_idx + 1) % self.lr_scheduler_frequency == 0\n            ):\n                lr_scheduler.step()\n\n            # save the model at the end of the epoch if interval is set to \"epoch\" and frequency is met\n            self.conditional_checkpoint_save(stage=\"end_of_epoch\")\n\n            if not self.is_training:\n                break\n\n        optimizer.zero_grad()\n        # save the model at the end of training\n        self.conditional_checkpoint_save(stage=\"end_of_training\")\n        return model\n\n    @abstractmethod\n    def train_epoch(\n        self,\n        model: Union[nn.Module, \"_FabricModule\"],\n        optimizer: Union[torch.optim.Optimizer, \"_FabricOptimizer\"],\n        lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n    ):\n        \"\"\"\n        Trains the model for one epoch.\n\n        Args:\n            model (Union[nn.Module, \"_FabricModule\"]): The model to be trained.\n            optimizer (Union[torch.optim.Optimizer, \"_FabricOptimizer\"]): The optimizer used for training.\n            lr_scheduler (torch.optim.lr_scheduler.LRScheduler): The learning rate scheduler.\n\n        Raises:\n            NotImplementedError: If the method is not implemented.\n        \"\"\"\n        raise NotImplementedError(\n            \"Copy this as a template and implement your own train_epoch method\"\n        )\n        fabric = self.fabric\n\n        accumulated_loss = 0\n        for step_idx, batch in enumerate(\n            pbar := tqdm(\n                self.train_dataloader,\n                desc=\"Training Batches\",\n                dynamic_ncols=True,\n                leave=False,\n                disable=not fabric.is_global_zero,\n            )\n        ):\n            is_accumulating = (step_idx + 1) % self.accumulate_grad_batches != 0\n\n            # disable gradient synchronization if accumulating gradients across steps for improved performance\n            with fabric.no_backward_sync(self.model, enabled=is_accumulating):\n                # use_cache=True is not compatible with gradient checkpointing, so we disable it here\n                output = self.compute_loss(batch)\n                loss = output[\"loss\"] / self.accumulate_grad_batches\n\n                fabric.backward(loss)\n                accumulated_loss += loss.item()\n\n            # 1. update the model parameters if not accumulating gradients\n            # 2. step the lr_scheduler if interval is set to \"step\" and frequency is met\n            # 3. save the model if interval is set to \"step\" and frequency is met\n            # 4. log metrics\n            # 5. increase the global step index and reset the accumulated metrics\n            if not is_accumulating:\n                self.clip_gradients_if_needed(model, optimizer)\n\n                # run lr_scheduler at the end of the step if interval is set to \"step\"\n                if (\n                    self.lr_scheduler_interval == \"step\"\n                    and (self.global_step_idx + 1) % self.lr_scheduler_frequency == 0\n                ):\n                    lr_scheduler.step()\n\n                # update the model parameters and zero the gradients\n                optimizer.step()\n                optimizer.zero_grad()\n\n                metrics = {\n                    \"train/loss\": accumulated_loss,\n                    \"train/lr\": optimizer.param_groups[0][\"lr\"],\n                }\n\n                fabric.log_dict(metrics, step=self.global_step_idx)\n                pbar.set_postfix(metrics)\n\n                # save the model at the end of the step if interval is set to \"step\" and frequency is met\n                self.conditional_checkpoint_save(stage=\"end_of_step\")\n\n                # break if max_steps_per_epoch is set, and exit epoch\n                if (\n                    self.max_steps_per_epoch &gt; 0\n                    and step_idx + 1 &gt;= self.max_steps_per_epoch\n                ):\n                    break\n                # break if max_steps is set, and exit training\n                if self.max_steps &gt; 0 and self.global_step_idx &gt;= self.max_steps - 1:\n                    self.is_training = False\n                    break\n\n                self.global_step_idx += 1\n                accumulated_loss = 0\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.accumulate_grad_batches","title":"<code>accumulate_grad_batches</code>  <code>instance-attribute</code>","text":"<p>The number of gradient accumulation steps. The effective global batch size is <code>the batch size per device</code> x <code>the number of devices</code> x <code>the number of gradient accumulation steps</code>.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.checkpoint_save_frequency","title":"<code>checkpoint_save_frequency</code>  <code>instance-attribute</code>","text":"<p>The frequency to save the model checkpoint.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.checkpoint_save_interval","title":"<code>checkpoint_save_interval</code>  <code>instance-attribute</code>","text":"<p>The interval to save the model checkpoint. Available options: 'step', 'epoch'.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.epoch_idx","title":"<code>epoch_idx</code>  <code>instance-attribute</code>","text":"<p>The epoch index, which is the number of epochs completed.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.expected_total_steps","title":"<code>expected_total_steps</code>  <code>property</code>","text":"<p>The expected total number of steps of the entire training. You need to run <code>compute_expected_total_steps</code> method to compute this value before accessing it.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the expected total steps have not been computed.</p> </li> </ul>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.global_step_idx","title":"<code>global_step_idx</code>  <code>instance-attribute</code>","text":"<p>The global step index, which is the number of parameter update steps.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.gradient_clip_algorithm","title":"<code>gradient_clip_algorithm</code>  <code>instance-attribute</code>","text":"<p>The algorithm to clip gradients. Available options: 'value', 'norm'.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.gradient_clip_val","title":"<code>gradient_clip_val</code>  <code>instance-attribute</code>","text":"<p>The value to clip gradients. If None, no clipping is applied.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.is_training","title":"<code>is_training</code>  <code>instance-attribute</code>","text":"<p>Whether the training is in progress. If set to False, the training will stop.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.lr_scheduler_frequency","title":"<code>lr_scheduler_frequency</code>  <code>instance-attribute</code>","text":"<p>The frequency to run the learning rate scheduler.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.lr_scheduler_interval","title":"<code>lr_scheduler_interval</code>  <code>instance-attribute</code>","text":"<p>The interval to run the learning rate scheduler. Available options: 'step', 'epoch'.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.max_epochs","title":"<code>max_epochs</code>  <code>instance-attribute</code>","text":"<p>Max number of epochs of the entire training.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.max_steps","title":"<code>max_steps</code>  <code>instance-attribute</code>","text":"<p>Max number of parameter update steps of the entire training.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.max_steps_per_epoch","title":"<code>max_steps_per_epoch</code>  <code>instance-attribute</code>","text":"<p>Max number of parameter update steps per epoch.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.clip_gradients_if_needed","title":"<code>clip_gradients_if_needed(model, optimizer)</code>","text":"<p>Clips gradients if the gradient clipping value is set.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model whose gradients need to be clipped.</p> </li> <li> <code>optimizer</code>               (<code>Optimizer</code>)           \u2013            <p>The optimizer used for training.</p> </li> </ul> Source code in <code>fusion_bench/mixins/fabric_training.py</code> <pre><code>def clip_gradients_if_needed(self, model, optimizer):\n    \"\"\"\n    Clips gradients if the gradient clipping value is set.\n\n    Args:\n        model (nn.Module): The model whose gradients need to be clipped.\n        optimizer (torch.optim.Optimizer): The optimizer used for training.\n    \"\"\"\n    fabric = self.fabric\n\n    if self.gradient_clip_val is not None:\n        if self.gradient_clip_algorithm == \"value\":\n            fabric.clip_gradients(model, optimizer, clip_val=self.gradient_clip_val)\n        elif self.gradient_clip_algorithm == \"norm\":\n            fabric.clip_gradients(model, optimizer, max_norm=self.gradient_clip_val)\n        else:\n            raise ValueError(\n                f\"Unknown gradient clip algorithm: {self.gradient_clip_algorithm}. Available options: 'value', 'norm'\"\n            )\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.compute_expected_total_steps","title":"<code>compute_expected_total_steps(train_dataloader)</code>","text":"<p>Computes the expected total number of steps for the entire training.</p> <p>Parameters:</p> <ul> <li> <code>train_dataloader</code>               (<code>DataLoader</code>)           \u2013            <p>The dataloader for the training data.</p> </li> </ul> Source code in <code>fusion_bench/mixins/fabric_training.py</code> <pre><code>def compute_expected_total_steps(\n    self, train_dataloader: torch.utils.data.DataLoader\n):\n    \"\"\"\n    Computes the expected total number of steps for the entire training.\n\n    Args:\n        train_dataloader (torch.utils.data.DataLoader): The dataloader for the training data.\n    \"\"\"\n    # compute expected total steps\n    self._expected_total_steps = []\n    if self.max_steps &gt; 0:\n        self._expected_total_steps.append(self.max_steps)\n    if self.max_steps_per_epoch &gt; 0 and self.max_epochs &gt; 0:\n        self._expected_total_steps.append(\n            self.max_steps_per_epoch * self.max_epochs\n        )\n    if self.max_epochs &gt; 0:\n        self._expected_total_steps.append(\n            len(train_dataloader) * self.max_epochs // self.accumulate_grad_batches\n        )\n    self._expected_total_steps = min(self._expected_total_steps)\n    log.info(f\"Expected total steps: {self._expected_total_steps}\")\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.conditional_checkpoint_save","title":"<code>conditional_checkpoint_save(stage, *args, **kwargs)</code>","text":"<p>Conditionally saves a checkpoint based on the current training stage.</p> <p>Parameters:</p> <ul> <li> <code>stage</code>               (<code>Literal['end_of_step', 'end_of_epoch', 'end_of_training']</code>)           \u2013            <p>The current stage of training.</p> </li> </ul> Source code in <code>fusion_bench/mixins/fabric_training.py</code> <pre><code>def conditional_checkpoint_save(\n    self,\n    stage: Literal[\"end_of_step\", \"end_of_epoch\", \"end_of_training\"],\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Conditionally saves a checkpoint based on the current training stage.\n\n    Args:\n        stage (Literal[\"end_of_step\", \"end_of_epoch\", \"end_of_training\"]): The current stage of training.\n    \"\"\"\n    if stage == \"end_of_step\":\n        if (\n            self.checkpoint_save_interval == \"step\"\n            and (self.global_step_idx + 1) % self.checkpoint_save_frequency == 0\n        ):\n            save_path = os.path.join(\n                self.log_dir, \"checkpoints\", f\"step={self.global_step_idx}.ckpt\"\n            )\n            self.save_checkpoint(save_path, *args, **kwargs)\n    elif stage == \"end_of_epoch\":\n        if (\n            self.checkpoint_save_interval == \"epoch\"\n            and (self.epoch_idx + 1) % self.checkpoint_save_frequency == 0\n        ):\n            save_path = os.path.join(\n                self.log_dir, \"checkpoints\", f\"epoch={self.epoch_idx}.ckpt\"\n            )\n            self.save_checkpoint(save_path, *args, **kwargs)\n    elif stage == \"end_of_training\":\n        # if the checkpoint has not been saved yet, save it\n        if self.global_step_idx &gt; self._latest_saved_checkpoint_global_step:\n            save_path = os.path.join(\n                self.log_dir,\n                \"checkpoints\",\n                f\"epoch={self.epoch_idx}_step={self.global_step_idx}.ckpt\",\n            )\n            self.save_checkpoint(save_path, *args, **kwargs)\n            try:\n                os.symlink(\n                    src=save_path,\n                    dst=os.path.join(\n                        self.log_dir, \"checkpoints\", \"latest_model.ckpt\"\n                    ),\n                    target_is_directory=os.path.isdir(save_path),\n                )\n            except Exception as e:\n                log.error(f\"Failed to create symlink: {e}\")\n    else:\n        raise ValueError(\n            f\"Unknown stage: {stage}. Available options: 'end_of_step', 'end_of_epoch', 'end_of_training'\"\n        )\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.save_checkpoint","title":"<code>save_checkpoint(path, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Saves a checkpoint of the model.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path where the checkpoint will be saved.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If the method is not implemented.</p> </li> </ul> Source code in <code>fusion_bench/mixins/fabric_training.py</code> <pre><code>@abstractmethod\ndef save_checkpoint(self, path, **kwargs):\n    \"\"\"\n    Saves a checkpoint of the model.\n\n    Args:\n        path (str): The path where the checkpoint will be saved.\n\n    Raises:\n        NotImplementedError: If the method is not implemented.\n    \"\"\"\n    raise NotImplementedError(\"save_checkpoint method is not implemented\")\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.train","title":"<code>train(model, optimizer, lr_scheduler)</code>","text":"<p>Trains the model.</p> <p>The global batch size is <code>the batch size per device</code> x <code>the number of devices</code> x <code>the number of gradient accumulation steps</code>.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Union[Module, _FabricModule]</code>)           \u2013            <p>The model to be trained.</p> </li> <li> <code>optimizer</code>               (<code>Union[Optimizer, _FabricOptimizer]</code>)           \u2013            <p>The optimizer used for training.</p> </li> <li> <code>lr_scheduler</code>               (<code>LRScheduler</code>)           \u2013            <p>The learning rate scheduler.</p> </li> </ul> Source code in <code>fusion_bench/mixins/fabric_training.py</code> <pre><code>def train(\n    self,\n    model: Union[nn.Module, \"_FabricModule\"],\n    optimizer: Union[torch.optim.Optimizer, \"_FabricOptimizer\"],\n    lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n):\n    \"\"\"\n    Trains the model.\n\n    The global batch size is `the batch size per device` x `the number of devices` x `the number of gradient accumulation steps`.\n\n    Args:\n        model (Union[nn.Module, \"_FabricModule\"]): The model to be trained.\n        optimizer (Union[torch.optim.Optimizer, \"_FabricOptimizer\"]): The optimizer used for training.\n        lr_scheduler (torch.optim.lr_scheduler.LRScheduler): The learning rate scheduler.\n    \"\"\"\n    fabric = self.fabric\n    self.is_training = True\n    # number of parameter update iterations, not the number of batches\n    self.global_step_idx = 0\n    model.train()\n    optimizer.zero_grad()\n    for epoch_idx in tqdm(\n        range(self.max_epochs) if self.max_epochs &gt; 0 else itertools.count(0),\n        \"Training Epoch\",\n        dynamic_ncols=True,\n        leave=False,\n        disable=not fabric.is_global_zero,\n    ):\n        self.epoch_idx = epoch_idx\n        self.train_epoch(model, optimizer, lr_scheduler)\n        # run lr_scheduler at the end of the epoch if interval is set to \"epoch\"\n        if (\n            self.lr_scheduler_interval == \"epoch\"\n            and (epoch_idx + 1) % self.lr_scheduler_frequency == 0\n        ):\n            lr_scheduler.step()\n\n        # save the model at the end of the epoch if interval is set to \"epoch\" and frequency is met\n        self.conditional_checkpoint_save(stage=\"end_of_epoch\")\n\n        if not self.is_training:\n            break\n\n    optimizer.zero_grad()\n    # save the model at the end of training\n    self.conditional_checkpoint_save(stage=\"end_of_training\")\n    return model\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.FabricTrainingMixin.train_epoch","title":"<code>train_epoch(model, optimizer, lr_scheduler)</code>  <code>abstractmethod</code>","text":"<p>Trains the model for one epoch.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Union[Module, _FabricModule]</code>)           \u2013            <p>The model to be trained.</p> </li> <li> <code>optimizer</code>               (<code>Union[Optimizer, _FabricOptimizer]</code>)           \u2013            <p>The optimizer used for training.</p> </li> <li> <code>lr_scheduler</code>               (<code>LRScheduler</code>)           \u2013            <p>The learning rate scheduler.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If the method is not implemented.</p> </li> </ul> Source code in <code>fusion_bench/mixins/fabric_training.py</code> <pre><code>@abstractmethod\ndef train_epoch(\n    self,\n    model: Union[nn.Module, \"_FabricModule\"],\n    optimizer: Union[torch.optim.Optimizer, \"_FabricOptimizer\"],\n    lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n):\n    \"\"\"\n    Trains the model for one epoch.\n\n    Args:\n        model (Union[nn.Module, \"_FabricModule\"]): The model to be trained.\n        optimizer (Union[torch.optim.Optimizer, \"_FabricOptimizer\"]): The optimizer used for training.\n        lr_scheduler (torch.optim.lr_scheduler.LRScheduler): The learning rate scheduler.\n\n    Raises:\n        NotImplementedError: If the method is not implemented.\n    \"\"\"\n    raise NotImplementedError(\n        \"Copy this as a template and implement your own train_epoch method\"\n    )\n    fabric = self.fabric\n\n    accumulated_loss = 0\n    for step_idx, batch in enumerate(\n        pbar := tqdm(\n            self.train_dataloader,\n            desc=\"Training Batches\",\n            dynamic_ncols=True,\n            leave=False,\n            disable=not fabric.is_global_zero,\n        )\n    ):\n        is_accumulating = (step_idx + 1) % self.accumulate_grad_batches != 0\n\n        # disable gradient synchronization if accumulating gradients across steps for improved performance\n        with fabric.no_backward_sync(self.model, enabled=is_accumulating):\n            # use_cache=True is not compatible with gradient checkpointing, so we disable it here\n            output = self.compute_loss(batch)\n            loss = output[\"loss\"] / self.accumulate_grad_batches\n\n            fabric.backward(loss)\n            accumulated_loss += loss.item()\n\n        # 1. update the model parameters if not accumulating gradients\n        # 2. step the lr_scheduler if interval is set to \"step\" and frequency is met\n        # 3. save the model if interval is set to \"step\" and frequency is met\n        # 4. log metrics\n        # 5. increase the global step index and reset the accumulated metrics\n        if not is_accumulating:\n            self.clip_gradients_if_needed(model, optimizer)\n\n            # run lr_scheduler at the end of the step if interval is set to \"step\"\n            if (\n                self.lr_scheduler_interval == \"step\"\n                and (self.global_step_idx + 1) % self.lr_scheduler_frequency == 0\n            ):\n                lr_scheduler.step()\n\n            # update the model parameters and zero the gradients\n            optimizer.step()\n            optimizer.zero_grad()\n\n            metrics = {\n                \"train/loss\": accumulated_loss,\n                \"train/lr\": optimizer.param_groups[0][\"lr\"],\n            }\n\n            fabric.log_dict(metrics, step=self.global_step_idx)\n            pbar.set_postfix(metrics)\n\n            # save the model at the end of the step if interval is set to \"step\" and frequency is met\n            self.conditional_checkpoint_save(stage=\"end_of_step\")\n\n            # break if max_steps_per_epoch is set, and exit epoch\n            if (\n                self.max_steps_per_epoch &gt; 0\n                and step_idx + 1 &gt;= self.max_steps_per_epoch\n            ):\n                break\n            # break if max_steps is set, and exit training\n            if self.max_steps &gt; 0 and self.global_step_idx &gt;= self.max_steps - 1:\n                self.is_training = False\n                break\n\n            self.global_step_idx += 1\n            accumulated_loss = 0\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.SimpleProfilerMixin","title":"<code>SimpleProfilerMixin</code>","text":"<p>A mixin class that provides simple profiling capabilities using Lightning's SimpleProfiler.</p> <p>This mixin allows for easy profiling of code blocks using a context manager or manual start/stop methods. It measures the execution time of named actions and provides a summary of the profiling results. Unlike statistical profilers, this provides precise timing measurements for specific code blocks.</p> Note <p>This mixin uses Lightning's SimpleProfiler which measures wall-clock time for named actions. It's suitable for timing discrete operations rather than detailed function-level profiling.</p> <p>Examples:</p> <pre><code>class MyClass(SimpleProfilerMixin):\n    def do_something(self):\n        with self.profile(\"data_loading\"):\n            # Load data here\n            data = load_data()\n\n        with self.profile(\"model_training\"):\n            # Train model here\n            model.train(data)\n\n        # Print the profiling summary\n        self.print_profile_summary(\"Training Profile\")\n</code></pre> <p>Attributes:</p> <ul> <li> <code>_profiler</code>               (<code>SimpleProfiler</code>)           \u2013            <p>An instance of the SimpleProfiler class used for profiling.</p> </li> </ul> Source code in <code>fusion_bench/mixins/simple_profiler.py</code> <pre><code>class SimpleProfilerMixin:\n    \"\"\"\n    A mixin class that provides simple profiling capabilities using Lightning's SimpleProfiler.\n\n    This mixin allows for easy profiling of code blocks using a context manager or manual\n    start/stop methods. It measures the execution time of named actions and provides\n    a summary of the profiling results. Unlike statistical profilers, this provides\n    precise timing measurements for specific code blocks.\n\n    Note:\n        This mixin uses Lightning's SimpleProfiler which measures wall-clock time\n        for named actions. It's suitable for timing discrete operations rather than\n        detailed function-level profiling.\n\n    Examples:\n        ```python\n        class MyClass(SimpleProfilerMixin):\n            def do_something(self):\n                with self.profile(\"data_loading\"):\n                    # Load data here\n                    data = load_data()\n\n                with self.profile(\"model_training\"):\n                    # Train model here\n                    model.train(data)\n\n                # Print the profiling summary\n                self.print_profile_summary(\"Training Profile\")\n        ```\n\n    Attributes:\n        _profiler (SimpleProfiler): An instance of the SimpleProfiler class used for profiling.\n    \"\"\"\n\n    _profiler: SimpleProfiler = None\n\n    @property\n    def profiler(self) -&gt; SimpleProfiler:\n        \"\"\"\n        Get the SimpleProfiler instance, creating it if necessary.\n\n        Returns:\n            SimpleProfiler: The profiler instance used for timing measurements.\n        \"\"\"\n        # Lazy initialization of the profiler instance\n        if self._profiler is None:\n            self._profiler = SimpleProfiler()\n        return self._profiler\n\n    @contextmanager\n    def profile(self, action_name: str) -&gt; Generator:\n        \"\"\"\n        Context manager for profiling a code block.\n\n        This context manager automatically starts profiling when entering the block\n        and stops profiling when exiting the block (even if an exception occurs).\n\n        Args:\n            action_name: A descriptive name for the action being profiled.\n                        This name will appear in the profiling summary.\n\n        Yields:\n            str: The action name that was provided.\n\n        Example:\n\n        ```python\n        with self.profile(\"data_processing\"):\n            # Process data here\n            result = process_large_dataset()\n        ```\n        \"\"\"\n        try:\n            self.start_profile(action_name)\n            yield action_name\n        finally:\n            self.stop_profile(action_name)\n\n    def start_profile(self, action_name: str):\n        \"\"\"\n        Start profiling for a named action.\n\n        This method begins timing for the specified action. You must call\n        stop_profile() with the same action name to complete the measurement.\n\n        Args:\n            action_name: A descriptive name for the action being profiled.\n                        This name will appear in the profiling summary.\n\n        Example:\n            ```python\n            self.start_profile(\"model_inference\")\n            result = model.predict(data)\n            self.stop_profile(\"model_inference\")\n            ```\n        \"\"\"\n        self.profiler.start(action_name)\n\n    def stop_profile(self, action_name: str):\n        \"\"\"\n        Stop profiling for a named action.\n\n        This method ends timing for the specified action that was previously\n        started with start_profile().\n\n        Args:\n            action_name: The name of the action to stop profiling.\n                        Must match the name used in start_profile().\n\n        Example:\n            ```python\n            self.start_profile(\"data_loading\")\n            data = load_data()\n            self.stop_profile(\"data_loading\")\n            ```\n        \"\"\"\n        self.profiler.stop(action_name)\n\n    @rank_zero_only\n    def print_profile_summary(self, title: Optional[str] = None):\n        \"\"\"\n        Print a summary of all profiled actions.\n\n        This method outputs a formatted summary showing the timing information\n        for all actions that have been profiled. The output includes action names\n        and their execution times.\n\n        Args:\n            title: Optional title to print before the profiling summary.\n                  If provided, this will be printed as a header.\n\n        Note:\n            This method is decorated with @rank_zero_only, meaning it will only\n            execute on the main process in distributed training scenarios.\n\n        Example:\n            ```python\n            # After profiling some actions\n            self.print_profile_summary(\"Training Performance Summary\")\n            ```\n        \"\"\"\n        if title is not None:\n            print(title)\n        print(self.profiler.summary())\n\n    def __del__(self):\n        \"\"\"\n        Cleanup when the object is destroyed.\n\n        Ensures that the profiler instance is properly cleaned up to prevent\n        memory leaks when the mixin instance is garbage collected.\n        \"\"\"\n        if self._profiler is not None:\n            del self._profiler\n            self._profiler = None\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.SimpleProfilerMixin.profiler","title":"<code>profiler</code>  <code>property</code>","text":"<p>Get the SimpleProfiler instance, creating it if necessary.</p> <p>Returns:</p> <ul> <li> <code>SimpleProfiler</code> (              <code>SimpleProfiler</code> )          \u2013            <p>The profiler instance used for timing measurements.</p> </li> </ul>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.SimpleProfilerMixin.__del__","title":"<code>__del__()</code>","text":"<p>Cleanup when the object is destroyed.</p> <p>Ensures that the profiler instance is properly cleaned up to prevent memory leaks when the mixin instance is garbage collected.</p> Source code in <code>fusion_bench/mixins/simple_profiler.py</code> <pre><code>def __del__(self):\n    \"\"\"\n    Cleanup when the object is destroyed.\n\n    Ensures that the profiler instance is properly cleaned up to prevent\n    memory leaks when the mixin instance is garbage collected.\n    \"\"\"\n    if self._profiler is not None:\n        del self._profiler\n        self._profiler = None\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.SimpleProfilerMixin.print_profile_summary","title":"<code>print_profile_summary(title=None)</code>","text":"<p>Print a summary of all profiled actions.</p> <p>This method outputs a formatted summary showing the timing information for all actions that have been profiled. The output includes action names and their execution times.</p> <p>Parameters:</p> <ul> <li> <code>title</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional title to print before the profiling summary.   If provided, this will be printed as a header.</p> </li> </ul> Note <p>This method is decorated with @rank_zero_only, meaning it will only execute on the main process in distributed training scenarios.</p> Example <pre><code># After profiling some actions\nself.print_profile_summary(\"Training Performance Summary\")\n</code></pre> Source code in <code>fusion_bench/mixins/simple_profiler.py</code> <pre><code>@rank_zero_only\ndef print_profile_summary(self, title: Optional[str] = None):\n    \"\"\"\n    Print a summary of all profiled actions.\n\n    This method outputs a formatted summary showing the timing information\n    for all actions that have been profiled. The output includes action names\n    and their execution times.\n\n    Args:\n        title: Optional title to print before the profiling summary.\n              If provided, this will be printed as a header.\n\n    Note:\n        This method is decorated with @rank_zero_only, meaning it will only\n        execute on the main process in distributed training scenarios.\n\n    Example:\n        ```python\n        # After profiling some actions\n        self.print_profile_summary(\"Training Performance Summary\")\n        ```\n    \"\"\"\n    if title is not None:\n        print(title)\n    print(self.profiler.summary())\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.SimpleProfilerMixin.profile","title":"<code>profile(action_name)</code>","text":"<p>Context manager for profiling a code block.</p> <p>This context manager automatically starts profiling when entering the block and stops profiling when exiting the block (even if an exception occurs).</p> <p>Parameters:</p> <ul> <li> <code>action_name</code>               (<code>str</code>)           \u2013            <p>A descriptive name for the action being profiled.         This name will appear in the profiling summary.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>Generator</code> )          \u2013            <p>The action name that was provided.</p> </li> </ul> <p>Example:</p> <pre><code>with self.profile(\"data_processing\"):\n    # Process data here\n    result = process_large_dataset()\n</code></pre> Source code in <code>fusion_bench/mixins/simple_profiler.py</code> <pre><code>@contextmanager\ndef profile(self, action_name: str) -&gt; Generator:\n    \"\"\"\n    Context manager for profiling a code block.\n\n    This context manager automatically starts profiling when entering the block\n    and stops profiling when exiting the block (even if an exception occurs).\n\n    Args:\n        action_name: A descriptive name for the action being profiled.\n                    This name will appear in the profiling summary.\n\n    Yields:\n        str: The action name that was provided.\n\n    Example:\n\n    ```python\n    with self.profile(\"data_processing\"):\n        # Process data here\n        result = process_large_dataset()\n    ```\n    \"\"\"\n    try:\n        self.start_profile(action_name)\n        yield action_name\n    finally:\n        self.stop_profile(action_name)\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.SimpleProfilerMixin.start_profile","title":"<code>start_profile(action_name)</code>","text":"<p>Start profiling for a named action.</p> <p>This method begins timing for the specified action. You must call stop_profile() with the same action name to complete the measurement.</p> <p>Parameters:</p> <ul> <li> <code>action_name</code>               (<code>str</code>)           \u2013            <p>A descriptive name for the action being profiled.         This name will appear in the profiling summary.</p> </li> </ul> Example <pre><code>self.start_profile(\"model_inference\")\nresult = model.predict(data)\nself.stop_profile(\"model_inference\")\n</code></pre> Source code in <code>fusion_bench/mixins/simple_profiler.py</code> <pre><code>def start_profile(self, action_name: str):\n    \"\"\"\n    Start profiling for a named action.\n\n    This method begins timing for the specified action. You must call\n    stop_profile() with the same action name to complete the measurement.\n\n    Args:\n        action_name: A descriptive name for the action being profiled.\n                    This name will appear in the profiling summary.\n\n    Example:\n        ```python\n        self.start_profile(\"model_inference\")\n        result = model.predict(data)\n        self.stop_profile(\"model_inference\")\n        ```\n    \"\"\"\n    self.profiler.start(action_name)\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.SimpleProfilerMixin.stop_profile","title":"<code>stop_profile(action_name)</code>","text":"<p>Stop profiling for a named action.</p> <p>This method ends timing for the specified action that was previously started with start_profile().</p> <p>Parameters:</p> <ul> <li> <code>action_name</code>               (<code>str</code>)           \u2013            <p>The name of the action to stop profiling.         Must match the name used in start_profile().</p> </li> </ul> Example <pre><code>self.start_profile(\"data_loading\")\ndata = load_data()\nself.stop_profile(\"data_loading\")\n</code></pre> Source code in <code>fusion_bench/mixins/simple_profiler.py</code> <pre><code>def stop_profile(self, action_name: str):\n    \"\"\"\n    Stop profiling for a named action.\n\n    This method ends timing for the specified action that was previously\n    started with start_profile().\n\n    Args:\n        action_name: The name of the action to stop profiling.\n                    Must match the name used in start_profile().\n\n    Example:\n        ```python\n        self.start_profile(\"data_loading\")\n        data = load_data()\n        self.stop_profile(\"data_loading\")\n        ```\n    \"\"\"\n    self.profiler.stop(action_name)\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.PyinstrumentProfilerMixin","title":"<code>PyinstrumentProfilerMixin</code>","text":"<p>A mixin class that provides statistical profiling capabilities using pyinstrument.</p> <p>This mixin allows for easy profiling of code blocks using a context manager. It provides methods to start and stop profiling actions, save profiling results to files, and print profiling summaries.</p> Note <p>This mixin requires the <code>pyinstrument</code> package to be installed. If not available, an ImportError will be raised when importing this module.</p> <p>Examples:</p> <pre><code>class MyClass(PyinstrumentProfilerMixin):\n    def do_something(self):\n        with self.profile(\"work\"):\n            # do some work here\n            ...\n\n        # save the profiling results\n        self.save_profile_report(\"profile_report.html\")\n\n        # or print the summary\n        self.print_profile_summary()\n</code></pre> <p>Attributes:</p> <ul> <li> <code>_profiler</code>               (<code>Profiler</code>)           \u2013            <p>An instance of the pyinstrument Profiler class.</p> </li> </ul> Source code in <code>fusion_bench/mixins/pyinstrument.py</code> <pre><code>class PyinstrumentProfilerMixin:\n    \"\"\"\n    A mixin class that provides statistical profiling capabilities using pyinstrument.\n\n    This mixin allows for easy profiling of code blocks using a context manager.\n    It provides methods to start and stop profiling actions, save profiling results\n    to files, and print profiling summaries.\n\n    Note:\n        This mixin requires the `pyinstrument` package to be installed.\n        If not available, an ImportError will be raised when importing this module.\n\n    Examples:\n\n    ```python\n    class MyClass(PyinstrumentProfilerMixin):\n        def do_something(self):\n            with self.profile(\"work\"):\n                # do some work here\n                ...\n\n            # save the profiling results\n            self.save_profile_report(\"profile_report.html\")\n\n            # or print the summary\n            self.print_profile_summary()\n    ```\n\n    Attributes:\n        _profiler (Profiler): An instance of the pyinstrument Profiler class.\n    \"\"\"\n\n    _profiler: Optional[Profiler] = None\n    _is_profiling: bool = False\n\n    @property\n    def profiler(self) -&gt; Optional[Profiler]:\n        \"\"\"Get the profiler instance, creating it if necessary.\"\"\"\n        if self._profiler is None:\n            self._profiler = Profiler()\n        return self._profiler\n\n    @contextmanager\n    def profile(self, action_name: Optional[str] = None) -&gt; Generator:\n        \"\"\"\n        Context manager for profiling a code block.\n\n        Args:\n            action_name: Optional name for the profiling action (for logging purposes).\n\n        Example:\n\n        ```python\n        with self.profile(\"expensive_operation\"):\n            # do some expensive work here\n            expensive_function()\n        ```\n        \"\"\"\n        try:\n            self.start_profile(action_name)\n            yield action_name\n        finally:\n            self.stop_profile(action_name)\n\n    def start_profile(self, action_name: Optional[str] = None):\n        \"\"\"\n        Start profiling.\n\n        Args:\n            action_name: Optional name for the profiling action.\n        \"\"\"\n        if self._is_profiling:\n            return\n\n        self.profiler.start()\n        self._is_profiling = True\n        if action_name:\n            print(f\"Started profiling: {action_name}\")\n\n    def stop_profile(self, action_name: Optional[str] = None):\n        \"\"\"\n        Stop profiling.\n\n        Args:\n            action_name: Optional name for the profiling action.\n        \"\"\"\n        if not self._is_profiling:\n            return\n\n        self.profiler.stop()\n        self._is_profiling = False\n        if action_name:\n            print(f\"Stopped profiling: {action_name}\")\n\n    @rank_zero_only\n    def print_profile_summary(\n        self, title: Optional[str] = None, unicode: bool = True, color: bool = True\n    ):\n        \"\"\"\n        Print a summary of the profiling results.\n\n        Args:\n            title: Optional title to print before the summary.\n            unicode: Whether to use unicode characters in the output.\n            color: Whether to use color in the output.\n        \"\"\"\n        if self.profiler is None:\n            print(\"No profiling data available.\")\n            return\n\n        if title is not None:\n            print(title)\n\n        print(self.profiler.output_text(unicode=unicode, color=color))\n\n    @rank_zero_only\n    def save_profile_report(\n        self,\n        output_path: Union[str, Path] = \"profile_report.html\",\n        format: str = \"html\",\n        title: Optional[str] = None,\n    ):\n        \"\"\"\n        Save the profiling results to a file.\n\n        Args:\n            output_path: Path where to save the profiling report.\n            format: Output format ('html', or 'text').\n            title: Optional title for the report.\n        \"\"\"\n        if self.profiler is None:\n            print(\"No profiling data available.\")\n            return\n\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        if format.lower() == \"html\":\n            content = self.profiler.output_html()\n        elif format.lower() == \"text\":\n            content = self.profiler.output_text(unicode=True, color=False)\n        else:\n            raise ValueError(f\"Unsupported format: {format}. Use 'html', or 'text'.\")\n\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(content)\n\n        print(f\"Profile report saved to: {output_path}\")\n\n    def reset_profile(self):\n        \"\"\"Reset the profiler to start fresh.\"\"\"\n        if self._is_profiling:\n            self.stop_profile()\n\n        self._profiler = None\n\n    def __del__(self):\n        \"\"\"Cleanup when the object is destroyed.\"\"\"\n        if self._is_profiling:\n            self.stop_profile()\n\n        if self._profiler is not None:\n            del self._profiler\n            self._profiler = None\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.PyinstrumentProfilerMixin.profiler","title":"<code>profiler</code>  <code>property</code>","text":"<p>Get the profiler instance, creating it if necessary.</p>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.PyinstrumentProfilerMixin.__del__","title":"<code>__del__()</code>","text":"<p>Cleanup when the object is destroyed.</p> Source code in <code>fusion_bench/mixins/pyinstrument.py</code> <pre><code>def __del__(self):\n    \"\"\"Cleanup when the object is destroyed.\"\"\"\n    if self._is_profiling:\n        self.stop_profile()\n\n    if self._profiler is not None:\n        del self._profiler\n        self._profiler = None\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.PyinstrumentProfilerMixin.print_profile_summary","title":"<code>print_profile_summary(title=None, unicode=True, color=True)</code>","text":"<p>Print a summary of the profiling results.</p> <p>Parameters:</p> <ul> <li> <code>title</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional title to print before the summary.</p> </li> <li> <code>unicode</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use unicode characters in the output.</p> </li> <li> <code>color</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use color in the output.</p> </li> </ul> Source code in <code>fusion_bench/mixins/pyinstrument.py</code> <pre><code>@rank_zero_only\ndef print_profile_summary(\n    self, title: Optional[str] = None, unicode: bool = True, color: bool = True\n):\n    \"\"\"\n    Print a summary of the profiling results.\n\n    Args:\n        title: Optional title to print before the summary.\n        unicode: Whether to use unicode characters in the output.\n        color: Whether to use color in the output.\n    \"\"\"\n    if self.profiler is None:\n        print(\"No profiling data available.\")\n        return\n\n    if title is not None:\n        print(title)\n\n    print(self.profiler.output_text(unicode=unicode, color=color))\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.PyinstrumentProfilerMixin.profile","title":"<code>profile(action_name=None)</code>","text":"<p>Context manager for profiling a code block.</p> <p>Parameters:</p> <ul> <li> <code>action_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name for the profiling action (for logging purposes).</p> </li> </ul> <p>Example:</p> <pre><code>with self.profile(\"expensive_operation\"):\n    # do some expensive work here\n    expensive_function()\n</code></pre> Source code in <code>fusion_bench/mixins/pyinstrument.py</code> <pre><code>@contextmanager\ndef profile(self, action_name: Optional[str] = None) -&gt; Generator:\n    \"\"\"\n    Context manager for profiling a code block.\n\n    Args:\n        action_name: Optional name for the profiling action (for logging purposes).\n\n    Example:\n\n    ```python\n    with self.profile(\"expensive_operation\"):\n        # do some expensive work here\n        expensive_function()\n    ```\n    \"\"\"\n    try:\n        self.start_profile(action_name)\n        yield action_name\n    finally:\n        self.stop_profile(action_name)\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.PyinstrumentProfilerMixin.reset_profile","title":"<code>reset_profile()</code>","text":"<p>Reset the profiler to start fresh.</p> Source code in <code>fusion_bench/mixins/pyinstrument.py</code> <pre><code>def reset_profile(self):\n    \"\"\"Reset the profiler to start fresh.\"\"\"\n    if self._is_profiling:\n        self.stop_profile()\n\n    self._profiler = None\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.PyinstrumentProfilerMixin.save_profile_report","title":"<code>save_profile_report(output_path='profile_report.html', format='html', title=None)</code>","text":"<p>Save the profiling results to a file.</p> <p>Parameters:</p> <ul> <li> <code>output_path</code>               (<code>Union[str, Path]</code>, default:                   <code>'profile_report.html'</code> )           \u2013            <p>Path where to save the profiling report.</p> </li> <li> <code>format</code>               (<code>str</code>, default:                   <code>'html'</code> )           \u2013            <p>Output format ('html', or 'text').</p> </li> <li> <code>title</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional title for the report.</p> </li> </ul> Source code in <code>fusion_bench/mixins/pyinstrument.py</code> <pre><code>@rank_zero_only\ndef save_profile_report(\n    self,\n    output_path: Union[str, Path] = \"profile_report.html\",\n    format: str = \"html\",\n    title: Optional[str] = None,\n):\n    \"\"\"\n    Save the profiling results to a file.\n\n    Args:\n        output_path: Path where to save the profiling report.\n        format: Output format ('html', or 'text').\n        title: Optional title for the report.\n    \"\"\"\n    if self.profiler is None:\n        print(\"No profiling data available.\")\n        return\n\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    if format.lower() == \"html\":\n        content = self.profiler.output_html()\n    elif format.lower() == \"text\":\n        content = self.profiler.output_text(unicode=True, color=False)\n    else:\n        raise ValueError(f\"Unsupported format: {format}. Use 'html', or 'text'.\")\n\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(content)\n\n    print(f\"Profile report saved to: {output_path}\")\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.PyinstrumentProfilerMixin.start_profile","title":"<code>start_profile(action_name=None)</code>","text":"<p>Start profiling.</p> <p>Parameters:</p> <ul> <li> <code>action_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name for the profiling action.</p> </li> </ul> Source code in <code>fusion_bench/mixins/pyinstrument.py</code> <pre><code>def start_profile(self, action_name: Optional[str] = None):\n    \"\"\"\n    Start profiling.\n\n    Args:\n        action_name: Optional name for the profiling action.\n    \"\"\"\n    if self._is_profiling:\n        return\n\n    self.profiler.start()\n    self._is_profiling = True\n    if action_name:\n        print(f\"Started profiling: {action_name}\")\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.PyinstrumentProfilerMixin.stop_profile","title":"<code>stop_profile(action_name=None)</code>","text":"<p>Stop profiling.</p> <p>Parameters:</p> <ul> <li> <code>action_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name for the profiling action.</p> </li> </ul> Source code in <code>fusion_bench/mixins/pyinstrument.py</code> <pre><code>def stop_profile(self, action_name: Optional[str] = None):\n    \"\"\"\n    Stop profiling.\n\n    Args:\n        action_name: Optional name for the profiling action.\n    \"\"\"\n    if not self._is_profiling:\n        return\n\n    self.profiler.stop()\n    self._is_profiling = False\n    if action_name:\n        print(f\"Stopped profiling: {action_name}\")\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.CLIPClassificationMixin","title":"<code>CLIPClassificationMixin</code>","text":"<p>               Bases: <code>LightningFabricMixin</code></p> <p>This mixin provides methods to classify images using the CLIP model.</p> <p>Attributes need to be set by the inheriting class:</p> <ul> <li><code>_dataloader_kwargs</code> (Dict[str, Any]): Keyword arguments for the dataloader.</li> <li><code>modelpool</code> (CLIPVisionModelPool): The model pool containing the CLIP models.</li> </ul> Source code in <code>fusion_bench/mixins/clip_classification.py</code> <pre><code>class CLIPClassificationMixin(LightningFabricMixin):\n    \"\"\"\n    This mixin provides methods to classify images using the CLIP model.\n\n    Attributes need to be set by the inheriting class:\n\n    - `_dataloader_kwargs` (Dict[str, Any]): Keyword arguments for the dataloader.\n    - `modelpool` (CLIPVisionModelPool): The model pool containing the CLIP models.\n    \"\"\"\n\n    dataloader_kwargs: Dict[str, Any] = {}\n    # the modelpool is set by inheriting class\n    modelpool: CLIPVisionModelPool = None\n    _clip_processor: CLIPProcessor = None\n    # a dict of zeroshot weights for each task, each key is the task name\n    zeroshot_weights: Dict[str, torch.Tensor] = {}\n    whether_setup_zero_shot_classification_head = False\n\n    @property\n    def clip_processor(self):\n        if self._clip_processor is None:\n            assert self.modelpool is not None, \"Model pool is not set\"\n            self._clip_processor = self.modelpool.load_processor()\n        return self._clip_processor\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(\n        self,\n        task: str,\n        batch_size: Optional[int] = None,\n        num_workers: Optional[int] = None,\n        **loader_kwargs,\n    ) -&gt; Iterator:\n        \"\"\"\n        Get an iterator for a shuffled test DataLoader.\n\n        This method creates a DataLoader for the test dataset of the specified task,\n        with shuffling enabled. It allows for optional customization of batch size,\n        number of workers, and other DataLoader keyword arguments.\n\n        Args:\n            task (str): The task identifier for which the test dataset is to be loaded.\n            batch_size (Optional[int]): The batch size to use for the DataLoader. If None, the default batch size is used.\n            num_workers (Optional[int]): The number of worker processes to use for data loading. If None, the default number of workers is used.\n            **loader_kwargs: Additional keyword arguments to pass to the DataLoader.\n\n        Returns:\n            Iterator: An iterator over the shuffled test DataLoader.\n        \"\"\"\n        # get dataloader kwargs\n        dataloader_kwargs = self.dataloader_kwargs.copy()\n        dataloader_kwargs[\"shuffle\"] = True\n        if batch_size is not None:\n            dataloader_kwargs[\"batch_size\"] = batch_size\n        if num_workers is not None:\n            dataloader_kwargs[\"num_workers\"] = num_workers\n        dataloader_kwargs.update(loader_kwargs)\n\n        # get the test dataset\n        clip_dataset = CLIPDataset(\n            self.modelpool.load_test_dataset(task), self.clip_processor\n        )\n        # create the dataloader\n        loader = DataLoader(clip_dataset, **dataloader_kwargs)\n        loader = self.fabric.setup_dataloaders(loader)\n        return iter(InfiniteDataLoader(loader))\n\n    @torch.no_grad()\n    def setup_zero_shot_classification_head(\n        self,\n        clip_processor: Optional[CLIPProcessor] = None,\n        clip_model: Optional[CLIPModel] = None,\n        task_names: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        Initializes a zero-shot classification head.\n\n        This method constructs a zero-shot classification head by generating text embeddings for each class name using a set of templates.\n        These embeddings function as the weights of the classification layer. The method also extracts the `visual_projection` and `logit_scale`\n        from the provided CLIP model, which are necessary for calculating the final logits.\n\n        Args:\n            clip_processor (Optional[CLIPProcessor]): The processor for the CLIP model. If not provided, it is loaded from the model pool.\n            clip_model (Optional[CLIPModel]): The CLIP model to use. If not provided, a pretrained model is loaded from the model pool.\n            task_names (Optional[List[str]]): A list of task names to set up the classification head for. If not provided, all models in the model pool will be used.\n        \"\"\"\n        self.whether_setup_zero_shot_classification_head = True\n        # load clip model if not provided\n        if clip_model is None:\n            if self.modelpool.has_pretrained:\n                clip_model = self.modelpool.load_clip_model(\"_pretrained_\")\n            else:\n                log.warning(\n                    f\"No pretrained CLIP model found, using the model from the model pool: {self.modelpool.model_names[0]}.\"\n                )\n                clip_model = self.modelpool.load_clip_model(\n                    self.modelpool.model_names[0]\n                )\n        if clip_processor is None:\n            clip_processor = self.clip_processor\n        clip_classifier = HFCLIPClassifier(clip_model, clip_processor)\n        self.visual_projection = deepcopy(clip_model.visual_projection)\n        self.visual_projection.requires_grad_(False)\n        self.logit_scale_exp = clip_model.logit_scale.data.clone().exp()\n        self.visual_projection = self.fabric.to_device(self.visual_projection)\n        self.logit_scale_exp = self.fabric.to_device(self.logit_scale_exp)\n\n        @cache_with_joblib()\n        def construct_classification_head(task: str):\n            nonlocal clip_classifier\n\n            classnames, templates = get_classnames_and_templates(task)\n            clip_classifier.set_classification_task(classnames, templates)\n            zeroshot_weights = clip_classifier.zeroshot_weights.detach().clone()\n\n            return zeroshot_weights\n\n        for task in tqdm(\n            self.modelpool.model_names if task_names is None else task_names,\n            \"Setting up zero-shot classification head\",\n            disable=not self.fabric.is_global_zero,\n        ):\n            zeroshot_weights = None\n            if self.fabric.is_global_zero:\n                zeroshot_weights = construct_classification_head(task)\n\n            self.fabric.barrier()\n            self.zeroshot_weights[task] = self.fabric.broadcast(zeroshot_weights, src=0)\n            self.zeroshot_weights[task] = self.to_device(self.zeroshot_weights[task])\n            self.fabric.barrier()\n\n        del clip_classifier\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    def compute_logits(\n        self,\n        module: Union[nn.Module, CLIPVisionModel, \"CLIPVisionTransformer\"],\n        images: torch.Tensor,\n        task: str,\n        image_embeds: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the classification logits for a batch of images for a specific task.\n\n        This method performs zero-shot classification by calculating the cosine similarity between image and text embeddings.\n        The image embeddings are obtained from the provided vision model, and the text embeddings (zero-shot weights) are pre-computed for the task.\n        The similarity scores are then scaled by the CLIP model's `logit_scale` to produce the final logits.\n\n        Args:\n            module (Union[nn.Module, CLIPVisionModel, \"CLIPVisionTransformer\"]): The vision encoder part of the CLIP model.\n            images (torch.Tensor): A batch of images to classify.\n            task (str): The name of the classification task.\n            image_embeds (Optional[torch.Tensor]): Pre-computed image embeddings. If provided, the method skips the image encoding step.\n\n        Returns:\n            torch.Tensor: A tensor of logits for each image, with shape (batch_size, num_classes).\n        \"\"\"\n        text_embeds = self.zeroshot_weights[task]\n\n        if image_embeds is None:\n            image_embeds = module(images)[1]\n        assert isinstance(\n            image_embeds, torch.Tensor\n        ), f\"`image_embeds` must be a tensor, but got {type(image_embeds)}\"\n        image_embeds = self.visual_projection(image_embeds)\n\n        # normalize embeddings\n        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n        # cosine similarity\n        logits_per_text = (\n            torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n        )\n        logits_per_image = logits_per_text.t()\n\n        return logits_per_image\n\n    def compute_features(\n        self,\n        module: Union[nn.Module, CLIPVisionModel, \"CLIPVisionTransformer\"],\n        images: torch.Tensor,\n        normalize: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Extracts image features using CLIP's vision encoder and visual projection.\n\n        Args:\n            module (Union[nn.Module, CLIPVisionModel, \"CLIPVisionTransformer\"]): The CLIP vision encoder module.\n            images (torch.Tensor): Input image batch to process.\n            normalize (bool): Whether to normalize the image embeddings.\n\n        Returns:\n            torch.Tensor: Normalized image embeddings with dimension matching CLIP's projection space (`projection_dim` in model config).\n        \"\"\"\n        image_embeds = module(images)[1]\n        image_embeds = self.visual_projection(image_embeds)\n\n        if normalize:\n            image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n        return image_embeds\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.CLIPClassificationMixin.compute_features","title":"<code>compute_features(module, images, normalize=True)</code>","text":"<p>Extracts image features using CLIP's vision encoder and visual projection.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Union[Module, CLIPVisionModel, CLIPVisionTransformer]</code>)           \u2013            <p>The CLIP vision encoder module.</p> </li> <li> <code>images</code>               (<code>Tensor</code>)           \u2013            <p>Input image batch to process.</p> </li> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to normalize the image embeddings.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>torch.Tensor: Normalized image embeddings with dimension matching CLIP's projection space (<code>projection_dim</code> in model config).</p> </li> </ul> Source code in <code>fusion_bench/mixins/clip_classification.py</code> <pre><code>def compute_features(\n    self,\n    module: Union[nn.Module, CLIPVisionModel, \"CLIPVisionTransformer\"],\n    images: torch.Tensor,\n    normalize: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Extracts image features using CLIP's vision encoder and visual projection.\n\n    Args:\n        module (Union[nn.Module, CLIPVisionModel, \"CLIPVisionTransformer\"]): The CLIP vision encoder module.\n        images (torch.Tensor): Input image batch to process.\n        normalize (bool): Whether to normalize the image embeddings.\n\n    Returns:\n        torch.Tensor: Normalized image embeddings with dimension matching CLIP's projection space (`projection_dim` in model config).\n    \"\"\"\n    image_embeds = module(images)[1]\n    image_embeds = self.visual_projection(image_embeds)\n\n    if normalize:\n        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n    return image_embeds\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.CLIPClassificationMixin.compute_logits","title":"<code>compute_logits(module, images, task, image_embeds=None)</code>","text":"<p>Computes the classification logits for a batch of images for a specific task.</p> <p>This method performs zero-shot classification by calculating the cosine similarity between image and text embeddings. The image embeddings are obtained from the provided vision model, and the text embeddings (zero-shot weights) are pre-computed for the task. The similarity scores are then scaled by the CLIP model's <code>logit_scale</code> to produce the final logits.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Union[Module, CLIPVisionModel, CLIPVisionTransformer]</code>)           \u2013            <p>The vision encoder part of the CLIP model.</p> </li> <li> <code>images</code>               (<code>Tensor</code>)           \u2013            <p>A batch of images to classify.</p> </li> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the classification task.</p> </li> <li> <code>image_embeds</code>               (<code>Optional[Tensor]</code>, default:                   <code>None</code> )           \u2013            <p>Pre-computed image embeddings. If provided, the method skips the image encoding step.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>torch.Tensor: A tensor of logits for each image, with shape (batch_size, num_classes).</p> </li> </ul> Source code in <code>fusion_bench/mixins/clip_classification.py</code> <pre><code>def compute_logits(\n    self,\n    module: Union[nn.Module, CLIPVisionModel, \"CLIPVisionTransformer\"],\n    images: torch.Tensor,\n    task: str,\n    image_embeds: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the classification logits for a batch of images for a specific task.\n\n    This method performs zero-shot classification by calculating the cosine similarity between image and text embeddings.\n    The image embeddings are obtained from the provided vision model, and the text embeddings (zero-shot weights) are pre-computed for the task.\n    The similarity scores are then scaled by the CLIP model's `logit_scale` to produce the final logits.\n\n    Args:\n        module (Union[nn.Module, CLIPVisionModel, \"CLIPVisionTransformer\"]): The vision encoder part of the CLIP model.\n        images (torch.Tensor): A batch of images to classify.\n        task (str): The name of the classification task.\n        image_embeds (Optional[torch.Tensor]): Pre-computed image embeddings. If provided, the method skips the image encoding step.\n\n    Returns:\n        torch.Tensor: A tensor of logits for each image, with shape (batch_size, num_classes).\n    \"\"\"\n    text_embeds = self.zeroshot_weights[task]\n\n    if image_embeds is None:\n        image_embeds = module(images)[1]\n    assert isinstance(\n        image_embeds, torch.Tensor\n    ), f\"`image_embeds` must be a tensor, but got {type(image_embeds)}\"\n    image_embeds = self.visual_projection(image_embeds)\n\n    # normalize embeddings\n    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n    # cosine similarity\n    logits_per_text = (\n        torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n    )\n    logits_per_image = logits_per_text.t()\n\n    return logits_per_image\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.CLIPClassificationMixin.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(task, batch_size=None, num_workers=None, **loader_kwargs)</code>  <code>cached</code>","text":"<p>Get an iterator for a shuffled test DataLoader.</p> <p>This method creates a DataLoader for the test dataset of the specified task, with shuffling enabled. It allows for optional customization of batch size, number of workers, and other DataLoader keyword arguments.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The task identifier for which the test dataset is to be loaded.</p> </li> <li> <code>batch_size</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The batch size to use for the DataLoader. If None, the default batch size is used.</p> </li> <li> <code>num_workers</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The number of worker processes to use for data loading. If None, the default number of workers is used.</p> </li> <li> <code>**loader_kwargs</code>           \u2013            <p>Additional keyword arguments to pass to the DataLoader.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator</code> (              <code>Iterator</code> )          \u2013            <p>An iterator over the shuffled test DataLoader.</p> </li> </ul> Source code in <code>fusion_bench/mixins/clip_classification.py</code> <pre><code>@functools.cache\ndef get_shuffled_test_loader_iter(\n    self,\n    task: str,\n    batch_size: Optional[int] = None,\n    num_workers: Optional[int] = None,\n    **loader_kwargs,\n) -&gt; Iterator:\n    \"\"\"\n    Get an iterator for a shuffled test DataLoader.\n\n    This method creates a DataLoader for the test dataset of the specified task,\n    with shuffling enabled. It allows for optional customization of batch size,\n    number of workers, and other DataLoader keyword arguments.\n\n    Args:\n        task (str): The task identifier for which the test dataset is to be loaded.\n        batch_size (Optional[int]): The batch size to use for the DataLoader. If None, the default batch size is used.\n        num_workers (Optional[int]): The number of worker processes to use for data loading. If None, the default number of workers is used.\n        **loader_kwargs: Additional keyword arguments to pass to the DataLoader.\n\n    Returns:\n        Iterator: An iterator over the shuffled test DataLoader.\n    \"\"\"\n    # get dataloader kwargs\n    dataloader_kwargs = self.dataloader_kwargs.copy()\n    dataloader_kwargs[\"shuffle\"] = True\n    if batch_size is not None:\n        dataloader_kwargs[\"batch_size\"] = batch_size\n    if num_workers is not None:\n        dataloader_kwargs[\"num_workers\"] = num_workers\n    dataloader_kwargs.update(loader_kwargs)\n\n    # get the test dataset\n    clip_dataset = CLIPDataset(\n        self.modelpool.load_test_dataset(task), self.clip_processor\n    )\n    # create the dataloader\n    loader = DataLoader(clip_dataset, **dataloader_kwargs)\n    loader = self.fabric.setup_dataloaders(loader)\n    return iter(InfiniteDataLoader(loader))\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.CLIPClassificationMixin.setup_zero_shot_classification_head","title":"<code>setup_zero_shot_classification_head(clip_processor=None, clip_model=None, task_names=None)</code>","text":"<p>Initializes a zero-shot classification head.</p> <p>This method constructs a zero-shot classification head by generating text embeddings for each class name using a set of templates. These embeddings function as the weights of the classification layer. The method also extracts the <code>visual_projection</code> and <code>logit_scale</code> from the provided CLIP model, which are necessary for calculating the final logits.</p> <p>Parameters:</p> <ul> <li> <code>clip_processor</code>               (<code>Optional[CLIPProcessor]</code>, default:                   <code>None</code> )           \u2013            <p>The processor for the CLIP model. If not provided, it is loaded from the model pool.</p> </li> <li> <code>clip_model</code>               (<code>Optional[CLIPModel]</code>, default:                   <code>None</code> )           \u2013            <p>The CLIP model to use. If not provided, a pretrained model is loaded from the model pool.</p> </li> <li> <code>task_names</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>A list of task names to set up the classification head for. If not provided, all models in the model pool will be used.</p> </li> </ul> Source code in <code>fusion_bench/mixins/clip_classification.py</code> <pre><code>@torch.no_grad()\ndef setup_zero_shot_classification_head(\n    self,\n    clip_processor: Optional[CLIPProcessor] = None,\n    clip_model: Optional[CLIPModel] = None,\n    task_names: Optional[List[str]] = None,\n):\n    \"\"\"\n    Initializes a zero-shot classification head.\n\n    This method constructs a zero-shot classification head by generating text embeddings for each class name using a set of templates.\n    These embeddings function as the weights of the classification layer. The method also extracts the `visual_projection` and `logit_scale`\n    from the provided CLIP model, which are necessary for calculating the final logits.\n\n    Args:\n        clip_processor (Optional[CLIPProcessor]): The processor for the CLIP model. If not provided, it is loaded from the model pool.\n        clip_model (Optional[CLIPModel]): The CLIP model to use. If not provided, a pretrained model is loaded from the model pool.\n        task_names (Optional[List[str]]): A list of task names to set up the classification head for. If not provided, all models in the model pool will be used.\n    \"\"\"\n    self.whether_setup_zero_shot_classification_head = True\n    # load clip model if not provided\n    if clip_model is None:\n        if self.modelpool.has_pretrained:\n            clip_model = self.modelpool.load_clip_model(\"_pretrained_\")\n        else:\n            log.warning(\n                f\"No pretrained CLIP model found, using the model from the model pool: {self.modelpool.model_names[0]}.\"\n            )\n            clip_model = self.modelpool.load_clip_model(\n                self.modelpool.model_names[0]\n            )\n    if clip_processor is None:\n        clip_processor = self.clip_processor\n    clip_classifier = HFCLIPClassifier(clip_model, clip_processor)\n    self.visual_projection = deepcopy(clip_model.visual_projection)\n    self.visual_projection.requires_grad_(False)\n    self.logit_scale_exp = clip_model.logit_scale.data.clone().exp()\n    self.visual_projection = self.fabric.to_device(self.visual_projection)\n    self.logit_scale_exp = self.fabric.to_device(self.logit_scale_exp)\n\n    @cache_with_joblib()\n    def construct_classification_head(task: str):\n        nonlocal clip_classifier\n\n        classnames, templates = get_classnames_and_templates(task)\n        clip_classifier.set_classification_task(classnames, templates)\n        zeroshot_weights = clip_classifier.zeroshot_weights.detach().clone()\n\n        return zeroshot_weights\n\n    for task in tqdm(\n        self.modelpool.model_names if task_names is None else task_names,\n        \"Setting up zero-shot classification head\",\n        disable=not self.fabric.is_global_zero,\n    ):\n        zeroshot_weights = None\n        if self.fabric.is_global_zero:\n            zeroshot_weights = construct_classification_head(task)\n\n        self.fabric.barrier()\n        self.zeroshot_weights[task] = self.fabric.broadcast(zeroshot_weights, src=0)\n        self.zeroshot_weights[task] = self.to_device(self.zeroshot_weights[task])\n        self.fabric.barrier()\n\n    del clip_classifier\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"api/fusion_bench.mixins/#fusion_bench.mixins.auto_register_config","title":"<code>auto_register_config(cls)</code>","text":"<p>Decorator to automatically register init parameters in _config_mapping.</p> <p>This decorator enhances classes that inherit from YAMLSerializationMixin by automatically mapping constructor parameters to configuration keys and dynamically setting instance attributes based on provided arguments.</p> <p>The decorator performs the following operations: 1. Inspects the class's init method signature 2. Automatically populates the _config_mapping dictionary with parameter names 3. Wraps the init method to handle both positional and keyword arguments 4. Sets instance attributes for all constructor parameters 5. Applies default values when parameters are not provided</p> <p>Parameters:</p> <ul> <li> <code>cls</code>               (<code>YAMLSerializationMixin</code>)           \u2013            <p>The class to be decorated. Must inherit from YAMLSerializationMixin to ensure proper serialization capabilities.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>YAMLSerializationMixin</code>          \u2013            <p>The decorated class with enhanced auto-registration functionality and modified init behavior.</p> </li> </ul> Behavior <ul> <li>Parameter Registration: All non-variadic parameters (excluding <code>*args</code>, <code>**kwargs</code>)   from the init method are automatically added to _config_mapping</li> <li>Positional Arguments: Handled in order and mapped to corresponding parameter names</li> <li>Keyword Arguments: Processed after positional arguments, overriding any conflicts</li> <li>Default Values: Applied when parameters are not provided via arguments</li> <li>Attribute Setting: All parameters become instance attributes accessible via dot notation</li> </ul> Note <ul> <li>The decorator wraps the original init method while preserving its signature for IDE support</li> <li>Parameters with <code>*args</code> or <code>**kwargs</code> signatures are ignored during registration</li> <li>The attributes are auto-registered, then the original init method is called,</li> <li>Type hints, method name, and other metadata are preserved using functools.wraps</li> <li>This decorator is designed to work seamlessly with the YAML serialization system</li> </ul> <p>Raises:</p> <ul> <li> <code>AttributeError</code>             \u2013            <p>If the class does not have the required _config_mapping attribute infrastructure (should inherit from YAMLSerializationMixin)</p> </li> </ul> Source code in <code>fusion_bench/mixins/serialization.py</code> <pre><code>def auto_register_config(cls):\n    \"\"\"\n    Decorator to automatically register __init__ parameters in _config_mapping.\n\n    This decorator enhances classes that inherit from YAMLSerializationMixin by\n    automatically mapping constructor parameters to configuration keys and\n    dynamically setting instance attributes based on provided arguments.\n\n    The decorator performs the following operations:\n    1. Inspects the class's __init__ method signature\n    2. Automatically populates the _config_mapping dictionary with parameter names\n    3. Wraps the __init__ method to handle both positional and keyword arguments\n    4. Sets instance attributes for all constructor parameters\n    5. Applies default values when parameters are not provided\n\n    Args:\n        cls (YAMLSerializationMixin): The class to be decorated. Must inherit from\n            YAMLSerializationMixin to ensure proper serialization capabilities.\n\n    Returns:\n        YAMLSerializationMixin: The decorated class with enhanced auto-registration\n            functionality and modified __init__ behavior.\n\n    Behavior:\n        - **Parameter Registration**: All non-variadic parameters (excluding ``*args``, ``**kwargs``)\n          from the __init__ method are automatically added to _config_mapping\n        - **Positional Arguments**: Handled in order and mapped to corresponding parameter names\n        - **Keyword Arguments**: Processed after positional arguments, overriding any conflicts\n        - **Default Values**: Applied when parameters are not provided via arguments\n        - **Attribute Setting**: All parameters become instance attributes accessible via dot notation\n\n    Note:\n        - The decorator wraps the original __init__ method while preserving its signature for IDE support\n        - Parameters with ``*args`` or ``**kwargs`` signatures are ignored during registration\n        - The attributes are auto-registered, then the original __init__ method is called,\n        - Type hints, method name, and other metadata are preserved using functools.wraps\n        - This decorator is designed to work seamlessly with the YAML serialization system\n\n    Raises:\n        AttributeError: If the class does not have the required _config_mapping attribute\n            infrastructure (should inherit from YAMLSerializationMixin)\n    \"\"\"\n    original_init = cls.__init__\n    sig = inspect.signature(original_init)\n\n    # Auto-register parameters in _config_mapping\n    if not \"_config_mapping\" in cls.__dict__:\n        cls._config_mapping = deepcopy(getattr(cls, \"_config_mapping\", bidict()))\n    if not isinstance(cls._config_mapping, bidict):\n        cls._config_mapping = bidict(cls._config_mapping)\n\n    registered_parameters = tuple(cls._config_mapping.values())\n\n    for param_name in list(sig.parameters.keys())[1:]:  # Skip 'self'\n        if (\n            sig.parameters[param_name].kind\n            not in [\n                _ParameterKind.VAR_POSITIONAL,\n                _ParameterKind.VAR_KEYWORD,\n            ]\n        ) and (param_name not in registered_parameters):\n            cls._config_mapping[param_name] = param_name\n\n    @wraps(original_init)\n    def __init__(self, *args, **kwargs):\n        log.debug(f\"set attributes for {self.__class__.__name__} in {cls.__name__}\")\n        # auto-register the attributes based on the signature\n        sig = inspect.signature(original_init)\n        param_names = list(sig.parameters.keys())[1:]  # Skip 'self'\n\n        # Handle positional arguments\n        for i, arg_value in enumerate(args):\n            if i &lt; len(param_names):\n                param_name = param_names[i]\n                if sig.parameters[param_name].kind not in [\n                    _ParameterKind.VAR_POSITIONAL,\n                    _ParameterKind.VAR_KEYWORD,\n                ]:\n                    _set_attr(self, param_name, arg_value)\n\n        # Handle keyword arguments and defaults\n        for param_name in param_names:\n            if sig.parameters[param_name].kind not in [\n                _ParameterKind.VAR_POSITIONAL,\n                _ParameterKind.VAR_KEYWORD,\n            ]:\n                # Skip if already set by positional argument\n                param_index = param_names.index(param_name)\n                if param_index &gt;= 0 and param_index &lt; len(args):\n                    continue\n\n                if param_name in kwargs:\n                    _set_attr(self, param_name, kwargs[param_name])\n                else:\n                    # Set default value if available and attribute doesn't exist\n                    default_value = sig.parameters[param_name].default\n                    if default_value is not Parameter.empty:\n                        _set_attr(self, param_name, default_value)\n\n        # Call the original __init__\n        result = original_init(self, *args, **kwargs)\n        return result\n\n    # Replace the original __init__ method while preserving its signature\n    cls.__init__ = __init__\n    return cls\n</code></pre>"},{"location":"api/fusion_bench.modelpool/","title":"fusion_bench.modelpool","text":""},{"location":"api/fusion_bench.modelpool/#base-class","title":"Base Class","text":""},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool","title":"<code>BaseModelPool</code>","text":"<p>               Bases: <code>HydraConfigMixin</code>, <code>BaseYAMLSerializable</code></p> <p>A class for managing and interacting with a pool of models along with their associated datasets or other specifications. For example, a model pool may contain multiple models, each with its own training, validation, and testing datasets. As for the specifications, a vision model pool may contain image preprocessor, and a language model pool may contain a tokenizer.</p> <p>Attributes:</p> <ul> <li> <code>_models</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration for all models in the pool.</p> </li> <li> <code>_train_datasets</code>               (<code>Optional[DictConfig]</code>)           \u2013            <p>Configuration for training datasets.</p> </li> <li> <code>_val_datasets</code>               (<code>Optional[DictConfig]</code>)           \u2013            <p>Configuration for validation datasets.</p> </li> <li> <code>_test_datasets</code>               (<code>Optional[DictConfig]</code>)           \u2013            <p>Configuration for testing datasets.</p> </li> <li> <code>_usage_</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional usage information.</p> </li> <li> <code>_version_</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional version information.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>class BaseModelPool(\n    HydraConfigMixin,\n    BaseYAMLSerializable,\n):\n    \"\"\"\n    A class for managing and interacting with a pool of models along with their associated datasets or other specifications. For example, a model pool may contain multiple models, each with its own training, validation, and testing datasets. As for the specifications, a vision model pool may contain image preprocessor, and a language model pool may contain a tokenizer.\n\n    Attributes:\n        _models (DictConfig): Configuration for all models in the pool.\n        _train_datasets (Optional[DictConfig]): Configuration for training datasets.\n        _val_datasets (Optional[DictConfig]): Configuration for validation datasets.\n        _test_datasets (Optional[DictConfig]): Configuration for testing datasets.\n        _usage_ (Optional[str]): Optional usage information.\n        _version_ (Optional[str]): Optional version information.\n    \"\"\"\n\n    _program = None\n    _config_key = \"modelpool\"\n    _models: Union[DictConfig, Dict[str, nn.Module]]\n    _config_mapping = BaseYAMLSerializable._config_mapping | {\n        \"_models\": \"models\",\n        \"_train_datasets\": \"train_datasets\",\n        \"_val_datasets\": \"val_datasets\",\n        \"_test_datasets\": \"test_datasets\",\n    }\n\n    def __init__(\n        self,\n        models: Union[DictConfig, Dict[str, nn.Module], List[nn.Module]],\n        *,\n        train_datasets: Optional[DictConfig] = None,\n        val_datasets: Optional[DictConfig] = None,\n        test_datasets: Optional[DictConfig] = None,\n        **kwargs,\n    ):\n        if isinstance(models, List):\n            models = {str(model_idx): model for model_idx, model in enumerate(models)}\n        self._models = models\n        self._train_datasets = train_datasets\n        self._val_datasets = val_datasets\n        self._test_datasets = test_datasets\n        super().__init__(**kwargs)\n\n    @property\n    def has_pretrained(self) -&gt; bool:\n        \"\"\"\n        Check if the model pool contains a pretrained model.\n\n        Returns:\n            bool: True if a pretrained model is available, False otherwise.\n        \"\"\"\n        return \"_pretrained_\" in self._models\n\n    @property\n    def all_model_names(self) -&gt; List[str]:\n        \"\"\"\n        Get the names of all models in the pool, including special models.\n\n        Returns:\n            List[str]: A list of all model names.\n        \"\"\"\n        return [name for name in self._models]\n\n    @property\n    def model_names(self) -&gt; List[str]:\n        \"\"\"\n        Get the names of regular models, excluding special models.\n\n        Returns:\n            List[str]: A list of regular model names.\n        \"\"\"\n        return [name for name in self._models if not self.is_special_model(name)]\n\n    @property\n    def train_dataset_names(self) -&gt; List[str]:\n        \"\"\"\n        Get the names of training datasets.\n\n        Returns:\n            List[str]: A list of training dataset names.\n        \"\"\"\n        return (\n            list(self._train_datasets.keys())\n            if self._train_datasets is not None\n            else []\n        )\n\n    @property\n    def val_dataset_names(self) -&gt; List[str]:\n        \"\"\"\n        Get the names of validation datasets.\n\n        Returns:\n            List[str]: A list of validation dataset names.\n        \"\"\"\n        return list(self._val_datasets.keys()) if self._val_datasets is not None else []\n\n    @property\n    def test_dataset_names(self) -&gt; List[str]:\n        \"\"\"\n        Get the names of testing datasets.\n\n        Returns:\n            List[str]: A list of testing dataset names.\n        \"\"\"\n        return (\n            list(self._test_datasets.keys()) if self._test_datasets is not None else []\n        )\n\n    def __len__(self):\n        return len(self.model_names)\n\n    @staticmethod\n    def is_special_model(model_name: str) -&gt; bool:\n        \"\"\"\n        Determine if a model is special based on its name.\n\n        Args:\n            model_name (str): The name of the model.\n\n        Returns:\n            bool: True if the model name indicates a special model, False otherwise.\n        \"\"\"\n        return model_name.startswith(\"_\") and model_name.endswith(\"_\")\n\n    def get_model_config(self, model_name: str, return_copy: bool = True) -&gt; DictConfig:\n        \"\"\"\n        Get the configuration for the specified model.\n\n        Args:\n            model_name (str): The name of the model.\n\n        Returns:\n            DictConfig: The configuration for the specified model.\n        \"\"\"\n        model_config = self._models[model_name]\n        if return_copy:\n            model_config = deepcopy(model_config)\n        return model_config\n\n    def get_model_path(self, model_name: str) -&gt; str:\n        \"\"\"\n        Get the path for the specified model.\n\n        Args:\n            model_name (str): The name of the model.\n\n        Returns:\n            str: The path for the specified model.\n        \"\"\"\n        if isinstance(self._models[model_name], str):\n            return self._models[model_name]\n        else:\n            raise ValueError(\n                \"Model path is not a string. Try to override this method in derived modelpool class.\"\n            )\n\n    def load_model(\n        self, model_name_or_config: Union[str, DictConfig], *args, **kwargs\n    ) -&gt; nn.Module:\n        \"\"\"\n        Load a model from the pool based on the provided configuration.\n\n        Args:\n            model_name_or_config (Union[str, DictConfig]): The model name or configuration.\n                - If str: should be a key in self._models\n                - If DictConfig: should be a configuration dict for instantiation\n            *args: Additional positional arguments passed to model instantiation.\n            **kwargs: Additional keyword arguments passed to model instantiation.\n\n        Returns:\n            nn.Module: The instantiated or retrieved model.\n        \"\"\"\n        log.debug(f\"Loading model: {model_name_or_config}\", stacklevel=2)\n\n        if isinstance(model_name_or_config, str):\n            model_name = model_name_or_config\n            # Handle string model names - lookup in the model pool\n            if model_name not in self._models:\n                raise KeyError(\n                    f\"Model '{model_name}' not found in model pool. \"\n                    f\"Available models: {list(self._models.keys())}\"\n                )\n            model_config = self._models[model_name]\n\n            # Handle different types of model configurations\n            match model_config:\n                case dict() | DictConfig() as config:\n                    # Configuration that needs instantiation\n                    log.debug(f\"Instantiating model '{model_name}' from configuration\")\n                    return instantiate(config, *args, **kwargs)\n\n                case nn.Module() as model:\n                    # Pre-instantiated model - return directly\n                    log.debug(\n                        f\"Returning pre-instantiated model '{model_name}' of type {type(model)}\"\n                    )\n                    return model\n\n                case _:\n                    # Unsupported model configuration type\n                    raise ValueError(\n                        f\"Unsupported model configuration type for '{model_name}': {type(model_config)}. \"\n                        f\"Expected nn.Module, dict, or DictConfig.\"\n                    )\n\n        elif isinstance(model_name_or_config, (dict, DictConfig)):\n            # Direct configuration - instantiate directly\n            log.debug(\"Instantiating model from direct DictConfig\")\n            model_config = model_name_or_config\n            return instantiate(model_config, *args, **kwargs)\n\n        else:\n            # Unsupported input type\n            raise TypeError(\n                f\"Unsupported input type: {type(model_name_or_config)}. \"\n                f\"Expected str or DictConfig.\"\n            )\n\n    def load_pretrained_model(self, *args, **kwargs):\n        assert (\n            self.has_pretrained\n        ), \"No pretrained model available. Check `_pretrained_` is in the `models` key.\"\n        model = self.load_model(\"_pretrained_\", *args, **kwargs)\n        return model\n\n    def load_pretrained_or_first_model(self, *args, **kwargs):\n        \"\"\"\n        Load the pretrained model if available, otherwise load the first available model.\n\n        Returns:\n            nn.Module: The loaded model.\n        \"\"\"\n        if self.has_pretrained:\n            model = self.load_model(\"_pretrained_\", *args, **kwargs)\n        else:\n            model = self.load_model(self.model_names[0], *args, **kwargs)\n        return model\n\n    def models(self) -&gt; Generator[nn.Module, None, None]:\n        for model_name in self.model_names:\n            yield self.load_model(model_name)\n\n    def named_models(self) -&gt; Generator[Tuple[str, nn.Module], None, None]:\n        for model_name in self.model_names:\n            yield model_name, self.load_model(model_name)\n\n    @property\n    def has_train_dataset(self) -&gt; bool:\n        \"\"\"\n        Check if the model pool contains training datasets.\n\n        Returns:\n            bool: True if training datasets are available, False otherwise.\n        \"\"\"\n        return self._train_datasets is not None and len(self._train_datasets) &gt; 0\n\n    @property\n    def has_val_dataset(self) -&gt; bool:\n        \"\"\"\n        Check if the model pool contains validation datasets.\n\n        Returns:\n            bool: True if validation datasets are available, False otherwise.\n        \"\"\"\n        return self._val_datasets is not None and len(self._val_datasets) &gt; 0\n\n    @property\n    def has_test_dataset(self) -&gt; bool:\n        \"\"\"\n        Check if the model pool contains testing datasets.\n\n        Returns:\n            bool: True if testing datasets are available, False otherwise.\n        \"\"\"\n        return self._test_datasets is not None and len(self._test_datasets) &gt; 0\n\n    def load_train_dataset(self, dataset_name: str, *args, **kwargs) -&gt; Dataset:\n        \"\"\"\n        Load the training dataset for the specified model.\n\n        Args:\n            dataset_name (str): The name of the model.\n\n        Returns:\n            Dataset: The instantiated training dataset.\n        \"\"\"\n        return instantiate(self._train_datasets[dataset_name], *args, **kwargs)\n\n    def train_datasets(self):\n        for dataset_name in self.train_dataset_names:\n            yield self.load_train_dataset(dataset_name)\n\n    def load_val_dataset(self, dataset_name: str, *args, **kwargs) -&gt; Dataset:\n        \"\"\"\n        Load the validation dataset for the specified model.\n\n        Args:\n            dataset_name (str): The name of the model.\n\n        Returns:\n            Dataset: The instantiated validation dataset.\n        \"\"\"\n        return instantiate(self._val_datasets[dataset_name], *args, **kwargs)\n\n    def val_datasets(self):\n        for dataset_name in self.val_dataset_names:\n            yield self.load_val_dataset(dataset_name)\n\n    def load_test_dataset(self, dataset_name: str, *args, **kwargs) -&gt; Dataset:\n        \"\"\"\n        Load the testing dataset for the specified model.\n\n        Args:\n            dataset_name (str): The name of the model.\n\n        Returns:\n            Dataset: The instantiated testing dataset.\n        \"\"\"\n        return instantiate(self._test_datasets[dataset_name], *args, **kwargs)\n\n    def test_datasets(self):\n        for dataset_name in self.test_dataset_names:\n            yield self.load_test_dataset(dataset_name)\n\n    def save_model(self, model: nn.Module, path: str, *args, **kwargs):\n        \"\"\"\n        Save the state dictionary of the model to the specified path.\n\n        Args:\n            model (nn.Module): The model whose state dictionary is to be saved.\n            path (str): The path where the state dictionary will be saved.\n        \"\"\"\n        with timeit_context(f\"Saving the state dict of model to {path}\"):\n            torch.save(model.state_dict(), path)\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.all_model_names","title":"<code>all_model_names</code>  <code>property</code>","text":"<p>Get the names of all models in the pool, including special models.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of all model names.</p> </li> </ul>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.has_pretrained","title":"<code>has_pretrained</code>  <code>property</code>","text":"<p>Check if the model pool contains a pretrained model.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if a pretrained model is available, False otherwise.</p> </li> </ul>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.has_test_dataset","title":"<code>has_test_dataset</code>  <code>property</code>","text":"<p>Check if the model pool contains testing datasets.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if testing datasets are available, False otherwise.</p> </li> </ul>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.has_train_dataset","title":"<code>has_train_dataset</code>  <code>property</code>","text":"<p>Check if the model pool contains training datasets.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if training datasets are available, False otherwise.</p> </li> </ul>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.has_val_dataset","title":"<code>has_val_dataset</code>  <code>property</code>","text":"<p>Check if the model pool contains validation datasets.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if validation datasets are available, False otherwise.</p> </li> </ul>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.model_names","title":"<code>model_names</code>  <code>property</code>","text":"<p>Get the names of regular models, excluding special models.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of regular model names.</p> </li> </ul>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.test_dataset_names","title":"<code>test_dataset_names</code>  <code>property</code>","text":"<p>Get the names of testing datasets.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of testing dataset names.</p> </li> </ul>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.train_dataset_names","title":"<code>train_dataset_names</code>  <code>property</code>","text":"<p>Get the names of training datasets.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of training dataset names.</p> </li> </ul>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.val_dataset_names","title":"<code>val_dataset_names</code>  <code>property</code>","text":"<p>Get the names of validation datasets.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of validation dataset names.</p> </li> </ul>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.get_model_config","title":"<code>get_model_config(model_name, return_copy=True)</code>","text":"<p>Get the configuration for the specified model.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DictConfig</code> (              <code>DictConfig</code> )          \u2013            <p>The configuration for the specified model.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def get_model_config(self, model_name: str, return_copy: bool = True) -&gt; DictConfig:\n    \"\"\"\n    Get the configuration for the specified model.\n\n    Args:\n        model_name (str): The name of the model.\n\n    Returns:\n        DictConfig: The configuration for the specified model.\n    \"\"\"\n    model_config = self._models[model_name]\n    if return_copy:\n        model_config = deepcopy(model_config)\n    return model_config\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.get_model_path","title":"<code>get_model_path(model_name)</code>","text":"<p>Get the path for the specified model.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The path for the specified model.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def get_model_path(self, model_name: str) -&gt; str:\n    \"\"\"\n    Get the path for the specified model.\n\n    Args:\n        model_name (str): The name of the model.\n\n    Returns:\n        str: The path for the specified model.\n    \"\"\"\n    if isinstance(self._models[model_name], str):\n        return self._models[model_name]\n    else:\n        raise ValueError(\n            \"Model path is not a string. Try to override this method in derived modelpool class.\"\n        )\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.is_special_model","title":"<code>is_special_model(model_name)</code>  <code>staticmethod</code>","text":"<p>Determine if a model is special based on its name.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the model name indicates a special model, False otherwise.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>@staticmethod\ndef is_special_model(model_name: str) -&gt; bool:\n    \"\"\"\n    Determine if a model is special based on its name.\n\n    Args:\n        model_name (str): The name of the model.\n\n    Returns:\n        bool: True if the model name indicates a special model, False otherwise.\n    \"\"\"\n    return model_name.startswith(\"_\") and model_name.endswith(\"_\")\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.load_model","title":"<code>load_model(model_name_or_config, *args, **kwargs)</code>","text":"<p>Load a model from the pool based on the provided configuration.</p> <p>Parameters:</p> <ul> <li> <code>model_name_or_config</code>               (<code>Union[str, DictConfig]</code>)           \u2013            <p>The model name or configuration. - If str: should be a key in self._models - If DictConfig: should be a configuration dict for instantiation</p> </li> <li> <code>*args</code>           \u2013            <p>Additional positional arguments passed to model instantiation.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments passed to model instantiation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: The instantiated or retrieved model.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def load_model(\n    self, model_name_or_config: Union[str, DictConfig], *args, **kwargs\n) -&gt; nn.Module:\n    \"\"\"\n    Load a model from the pool based on the provided configuration.\n\n    Args:\n        model_name_or_config (Union[str, DictConfig]): The model name or configuration.\n            - If str: should be a key in self._models\n            - If DictConfig: should be a configuration dict for instantiation\n        *args: Additional positional arguments passed to model instantiation.\n        **kwargs: Additional keyword arguments passed to model instantiation.\n\n    Returns:\n        nn.Module: The instantiated or retrieved model.\n    \"\"\"\n    log.debug(f\"Loading model: {model_name_or_config}\", stacklevel=2)\n\n    if isinstance(model_name_or_config, str):\n        model_name = model_name_or_config\n        # Handle string model names - lookup in the model pool\n        if model_name not in self._models:\n            raise KeyError(\n                f\"Model '{model_name}' not found in model pool. \"\n                f\"Available models: {list(self._models.keys())}\"\n            )\n        model_config = self._models[model_name]\n\n        # Handle different types of model configurations\n        match model_config:\n            case dict() | DictConfig() as config:\n                # Configuration that needs instantiation\n                log.debug(f\"Instantiating model '{model_name}' from configuration\")\n                return instantiate(config, *args, **kwargs)\n\n            case nn.Module() as model:\n                # Pre-instantiated model - return directly\n                log.debug(\n                    f\"Returning pre-instantiated model '{model_name}' of type {type(model)}\"\n                )\n                return model\n\n            case _:\n                # Unsupported model configuration type\n                raise ValueError(\n                    f\"Unsupported model configuration type for '{model_name}': {type(model_config)}. \"\n                    f\"Expected nn.Module, dict, or DictConfig.\"\n                )\n\n    elif isinstance(model_name_or_config, (dict, DictConfig)):\n        # Direct configuration - instantiate directly\n        log.debug(\"Instantiating model from direct DictConfig\")\n        model_config = model_name_or_config\n        return instantiate(model_config, *args, **kwargs)\n\n    else:\n        # Unsupported input type\n        raise TypeError(\n            f\"Unsupported input type: {type(model_name_or_config)}. \"\n            f\"Expected str or DictConfig.\"\n        )\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.load_pretrained_or_first_model","title":"<code>load_pretrained_or_first_model(*args, **kwargs)</code>","text":"<p>Load the pretrained model if available, otherwise load the first available model.</p> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The loaded model.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def load_pretrained_or_first_model(self, *args, **kwargs):\n    \"\"\"\n    Load the pretrained model if available, otherwise load the first available model.\n\n    Returns:\n        nn.Module: The loaded model.\n    \"\"\"\n    if self.has_pretrained:\n        model = self.load_model(\"_pretrained_\", *args, **kwargs)\n    else:\n        model = self.load_model(self.model_names[0], *args, **kwargs)\n    return model\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.load_test_dataset","title":"<code>load_test_dataset(dataset_name, *args, **kwargs)</code>","text":"<p>Load the testing dataset for the specified model.</p> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>The name of the model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The instantiated testing dataset.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def load_test_dataset(self, dataset_name: str, *args, **kwargs) -&gt; Dataset:\n    \"\"\"\n    Load the testing dataset for the specified model.\n\n    Args:\n        dataset_name (str): The name of the model.\n\n    Returns:\n        Dataset: The instantiated testing dataset.\n    \"\"\"\n    return instantiate(self._test_datasets[dataset_name], *args, **kwargs)\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.load_train_dataset","title":"<code>load_train_dataset(dataset_name, *args, **kwargs)</code>","text":"<p>Load the training dataset for the specified model.</p> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>The name of the model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The instantiated training dataset.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def load_train_dataset(self, dataset_name: str, *args, **kwargs) -&gt; Dataset:\n    \"\"\"\n    Load the training dataset for the specified model.\n\n    Args:\n        dataset_name (str): The name of the model.\n\n    Returns:\n        Dataset: The instantiated training dataset.\n    \"\"\"\n    return instantiate(self._train_datasets[dataset_name], *args, **kwargs)\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.load_val_dataset","title":"<code>load_val_dataset(dataset_name, *args, **kwargs)</code>","text":"<p>Load the validation dataset for the specified model.</p> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>The name of the model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The instantiated validation dataset.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def load_val_dataset(self, dataset_name: str, *args, **kwargs) -&gt; Dataset:\n    \"\"\"\n    Load the validation dataset for the specified model.\n\n    Args:\n        dataset_name (str): The name of the model.\n\n    Returns:\n        Dataset: The instantiated validation dataset.\n    \"\"\"\n    return instantiate(self._val_datasets[dataset_name], *args, **kwargs)\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.BaseModelPool.save_model","title":"<code>save_model(model, path, *args, **kwargs)</code>","text":"<p>Save the state dictionary of the model to the specified path.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model whose state dictionary is to be saved.</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path where the state dictionary will be saved.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def save_model(self, model: nn.Module, path: str, *args, **kwargs):\n    \"\"\"\n    Save the state dictionary of the model to the specified path.\n\n    Args:\n        model (nn.Module): The model whose state dictionary is to be saved.\n        path (str): The path where the state dictionary will be saved.\n    \"\"\"\n    with timeit_context(f\"Saving the state dict of model to {path}\"):\n        torch.save(model.state_dict(), path)\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#vision-model-pool","title":"Vision Model Pool","text":""},{"location":"api/fusion_bench.modelpool/#nyuv2-tasks-resnet","title":"NYUv2 Tasks (ResNet)","text":""},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.NYUv2ModelPool","title":"<code>NYUv2ModelPool</code>","text":"<p>               Bases: <code>ModelPool</code></p> Source code in <code>fusion_bench/modelpool/nyuv2_modelpool.py</code> <pre><code>class NYUv2ModelPool(ModelPool):\n    def load_model(\n        self, model_config: str | DictConfig, encoder_only: bool = True\n    ) -&gt; ResnetDilated | NYUv2Model:\n        if isinstance(model_config, str):\n            model_config = self.get_model_config(model_config)\n\n        encoder = resnet_dilated(model_config.encoder)\n        decoders = nn.ModuleDict(\n            {\n                task: DeepLabHead(2048, NYUv2.num_out_channels[task])\n                for task in model_config.decoders\n            }\n        )\n        model = NYUv2Model(encoder=encoder, decoders=decoders)\n        if model_config.get(\"ckpt_path\", None) is not None:\n            ckpt = torch.load(model_config.ckpt_path, map_location=\"cpu\")\n            if \"state_dict\" in ckpt:\n                ckpt = ckpt[\"state_dict\"]\n            model.load_state_dict(ckpt, strict=False)\n\n        if encoder_only:\n            return model.encoder\n        else:\n            return model\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#clip-vision-encoder","title":"CLIP Vision Encoder","text":""},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.CLIPVisionModelPool","title":"<code>CLIPVisionModelPool</code>","text":"<p>               Bases: <code>BaseModelPool</code></p> <p>A model pool for managing Hugging Face's CLIP Vision models.</p> <p>This class extends the base <code>ModelPool</code> class and overrides its methods to handle the specifics of the CLIP Vision models provided by the Hugging Face Transformers library.</p> Source code in <code>fusion_bench/modelpool/clip_vision/modelpool.py</code> <pre><code>class CLIPVisionModelPool(BaseModelPool):\n    \"\"\"\n    A model pool for managing Hugging Face's CLIP Vision models.\n\n    This class extends the base `ModelPool` class and overrides its methods to handle\n    the specifics of the CLIP Vision models provided by the Hugging Face Transformers library.\n    \"\"\"\n\n    _config_mapping = BaseModelPool._config_mapping | {\n        \"_processor\": \"processor\",\n        \"_platform\": \"hf\",\n    }\n\n    def __init__(\n        self,\n        models: DictConfig,\n        *,\n        processor: Optional[DictConfig] = None,\n        platform: Literal[\"hf\", \"huggingface\", \"modelscope\"] = \"hf\",\n        **kwargs,\n    ):\n        super().__init__(models, **kwargs)\n        self._processor = processor\n        self._platform = platform\n\n    def load_processor(self, *args, **kwargs) -&gt; CLIPProcessor:\n        assert self._processor is not None, \"Processor is not defined in the config\"\n        if isinstance(self._processor, str):\n            if rank_zero_only.rank == 0:\n                log.info(f\"Loading `transformers.CLIPProcessor`: {self._processor}\")\n            repo_path = resolve_repo_path(\n                repo_id=self._processor, repo_type=\"model\", platform=self._platform\n            )\n            processor = CLIPProcessor.from_pretrained(repo_path, *args, **kwargs)\n        else:\n            processor = instantiate(self._processor, *args, **kwargs)\n        return processor\n\n    def load_clip_model(self, model_name: str, *args, **kwargs) -&gt; CLIPModel:\n        model_config = self._models[model_name]\n\n        if isinstance(model_config, str):\n            if rank_zero_only.rank == 0:\n                log.info(f\"Loading `transformers.CLIPModel`: {model_config}\")\n            repo_path = resolve_repo_path(\n                repo_id=model_config, repo_type=\"model\", platform=self._platform\n            )\n            clip_model = CLIPModel.from_pretrained(repo_path, *args, **kwargs)\n            return clip_model\n        else:\n            assert isinstance(\n                model_config, DictConfig\n            ), \"Model config must be a DictConfig\"\n            model_config = deepcopy(model_config)\n            with open_dict(model_config):\n                model_config._target_ = \"transformers.CLIPModel.from_pretrained\"\n            clip_model = instantiate(model_config, *args, **kwargs)\n            return clip_model\n\n    @override\n    def save_model(self, model: CLIPVisionModel, path: str):\n        \"\"\"\n        Save a CLIP Vision model to the given path.\n\n        Args:\n            model (CLIPVisionModel): The model to save.\n            path (str): The path to save the model to.\n        \"\"\"\n        with timeit_context(f'Saving clip vision model to \"{path}\"'):\n            model.save_pretrained(path)\n\n    def load_model(\n        self, model_name_or_config: Union[str, DictConfig], *args, **kwargs\n    ) -&gt; CLIPVisionModel:\n        \"\"\"\n        Load a CLIPVisionModel from the model pool with support for various configuration formats.\n\n        This method provides flexible model loading capabilities, handling different types of model\n        configurations including string paths, pre-instantiated models, and complex configurations.\n\n        Supported configuration formats:\n        1. String model paths (e.g., Hugging Face model IDs)\n        2. Pre-instantiated nn.Module objects\n        3. DictConfig objects for complex configurations\n\n        Example configuration:\n        ```yaml\n        models:\n            # Simple string paths to Hugging Face models\n            cifar10: tanganke/clip-vit-base-patch32_cifar10\n            sun397: tanganke/clip-vit-base-patch32_sun397\n            stanford-cars: tanganke/clip-vit-base-patch32_stanford-cars\n\n            # Complex configuration with additional parameters\n            custom_model:\n                _target_: transformers.CLIPVisionModel.from_pretrained\n                pretrained_model_name_or_path: openai/clip-vit-base-patch32\n                torch_dtype: float16\n        ```\n\n        Args:\n            model_name_or_config (Union[str, DictConfig]): Either a model name from the pool\n                or a configuration dictionary for instantiating the model.\n            *args: Additional positional arguments passed to model loading/instantiation.\n            **kwargs: Additional keyword arguments passed to model loading/instantiation.\n\n        Returns:\n            CLIPVisionModel: The loaded CLIPVisionModel instance.\n        \"\"\"\n        # Check if we have a string model name that exists in our model pool\n        if (\n            isinstance(model_name_or_config, str)\n            and model_name_or_config in self._models\n        ):\n            model_name = model_name_or_config\n\n            # handle different model configuration types\n            match self._models[model_name_or_config]:\n                case str() as model_path:\n                    # Handle string model paths (e.g., Hugging Face model IDs)\n                    if rank_zero_only.rank == 0:\n                        log.info(\n                            f\"Loading model `{model_name}` of type `transformers.CLIPVisionModel` from {model_path}\"\n                        )\n                    # Resolve the repository path (supports both HuggingFace and ModelScope)\n                    repo_path = resolve_repo_path(\n                        model_path, repo_type=\"model\", platform=self._platform\n                    )\n                    # Load and return the CLIPVisionModel from the resolved path\n                    return CLIPVisionModel.from_pretrained(repo_path, *args, **kwargs)\n\n                case nn.Module() as model:\n                    # Handle pre-instantiated model objects\n                    if rank_zero_only.rank == 0:\n                        log.info(\n                            f\"Returning existing model `{model_name}` of type {type(model)}\"\n                        )\n                    return model\n\n                case _:\n                    # Handle other configuration types (e.g., DictConfig) via parent class\n                    # This fallback prevents returning None when the model config doesn't\n                    # match the expected string or nn.Module patterns\n                    return super().load_model(model_name_or_config, *args, **kwargs)\n\n        # If model_name_or_config is not a string in our pool, delegate to parent class\n        # This handles cases where model_name_or_config is a DictConfig directly\n        return super().load_model(model_name_or_config, *args, **kwargs)\n\n    def load_train_dataset(self, dataset_name: str, *args, **kwargs):\n        dataset_config = self._train_datasets[dataset_name]\n        if isinstance(dataset_config, str):\n            if rank_zero_only.rank == 0:\n                log.info(\n                    f\"Loading train dataset using `datasets.load_dataset`: {dataset_config}\"\n                )\n            dataset = self._load_dataset(dataset_config, split=\"train\")\n        else:\n            dataset = super().load_train_dataset(dataset_name, *args, **kwargs)\n        return dataset\n\n    def load_val_dataset(self, dataset_name: str, *args, **kwargs):\n        dataset_config = self._val_datasets[dataset_name]\n        if isinstance(dataset_config, str):\n            if rank_zero_only.rank == 0:\n                log.info(\n                    f\"Loading validation dataset using `datasets.load_dataset`: {dataset_config}\"\n                )\n            dataset = self._load_dataset(dataset_config, split=\"validation\")\n        else:\n            dataset = super().load_val_dataset(dataset_name, *args, **kwargs)\n        return dataset\n\n    def load_test_dataset(self, dataset_name: str, *args, **kwargs):\n        dataset_config = self._test_datasets[dataset_name]\n        if isinstance(dataset_config, str):\n            if rank_zero_only.rank == 0:\n                log.info(\n                    f\"Loading test dataset using `datasets.load_dataset`: {dataset_config}\"\n                )\n            dataset = self._load_dataset(dataset_config, split=\"test\")\n        else:\n            dataset = super().load_test_dataset(dataset_name, *args, **kwargs)\n        return dataset\n\n    def _load_dataset(self, name: str, split: str):\n        \"\"\"\n        Load a dataset by its name and split.\n\n        Args:\n            dataset_name (str): The name of the dataset.\n            split (str): The split of the dataset to load (e.g., \"train\", \"validation\", \"test\").\n\n        Returns:\n            Dataset: The loaded dataset.\n        \"\"\"\n        datset_dir = resolve_repo_path(\n            name, repo_type=\"dataset\", platform=self._platform\n        )\n        dataset = load_dataset(datset_dir, split=split)\n        return dataset\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.CLIPVisionModelPool.load_model","title":"<code>load_model(model_name_or_config, *args, **kwargs)</code>","text":"<p>Load a CLIPVisionModel from the model pool with support for various configuration formats.</p> <p>This method provides flexible model loading capabilities, handling different types of model configurations including string paths, pre-instantiated models, and complex configurations.</p> <p>Supported configuration formats: 1. String model paths (e.g., Hugging Face model IDs) 2. Pre-instantiated nn.Module objects 3. DictConfig objects for complex configurations</p> <p>Example configuration: <pre><code>models:\n    # Simple string paths to Hugging Face models\n    cifar10: tanganke/clip-vit-base-patch32_cifar10\n    sun397: tanganke/clip-vit-base-patch32_sun397\n    stanford-cars: tanganke/clip-vit-base-patch32_stanford-cars\n\n    # Complex configuration with additional parameters\n    custom_model:\n        _target_: transformers.CLIPVisionModel.from_pretrained\n        pretrained_model_name_or_path: openai/clip-vit-base-patch32\n        torch_dtype: float16\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>model_name_or_config</code>               (<code>Union[str, DictConfig]</code>)           \u2013            <p>Either a model name from the pool or a configuration dictionary for instantiating the model.</p> </li> <li> <code>*args</code>           \u2013            <p>Additional positional arguments passed to model loading/instantiation.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments passed to model loading/instantiation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>CLIPVisionModel</code> (              <code>CLIPVisionModel</code> )          \u2013            <p>The loaded CLIPVisionModel instance.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/clip_vision/modelpool.py</code> <pre><code>def load_model(\n    self, model_name_or_config: Union[str, DictConfig], *args, **kwargs\n) -&gt; CLIPVisionModel:\n    \"\"\"\n    Load a CLIPVisionModel from the model pool with support for various configuration formats.\n\n    This method provides flexible model loading capabilities, handling different types of model\n    configurations including string paths, pre-instantiated models, and complex configurations.\n\n    Supported configuration formats:\n    1. String model paths (e.g., Hugging Face model IDs)\n    2. Pre-instantiated nn.Module objects\n    3. DictConfig objects for complex configurations\n\n    Example configuration:\n    ```yaml\n    models:\n        # Simple string paths to Hugging Face models\n        cifar10: tanganke/clip-vit-base-patch32_cifar10\n        sun397: tanganke/clip-vit-base-patch32_sun397\n        stanford-cars: tanganke/clip-vit-base-patch32_stanford-cars\n\n        # Complex configuration with additional parameters\n        custom_model:\n            _target_: transformers.CLIPVisionModel.from_pretrained\n            pretrained_model_name_or_path: openai/clip-vit-base-patch32\n            torch_dtype: float16\n    ```\n\n    Args:\n        model_name_or_config (Union[str, DictConfig]): Either a model name from the pool\n            or a configuration dictionary for instantiating the model.\n        *args: Additional positional arguments passed to model loading/instantiation.\n        **kwargs: Additional keyword arguments passed to model loading/instantiation.\n\n    Returns:\n        CLIPVisionModel: The loaded CLIPVisionModel instance.\n    \"\"\"\n    # Check if we have a string model name that exists in our model pool\n    if (\n        isinstance(model_name_or_config, str)\n        and model_name_or_config in self._models\n    ):\n        model_name = model_name_or_config\n\n        # handle different model configuration types\n        match self._models[model_name_or_config]:\n            case str() as model_path:\n                # Handle string model paths (e.g., Hugging Face model IDs)\n                if rank_zero_only.rank == 0:\n                    log.info(\n                        f\"Loading model `{model_name}` of type `transformers.CLIPVisionModel` from {model_path}\"\n                    )\n                # Resolve the repository path (supports both HuggingFace and ModelScope)\n                repo_path = resolve_repo_path(\n                    model_path, repo_type=\"model\", platform=self._platform\n                )\n                # Load and return the CLIPVisionModel from the resolved path\n                return CLIPVisionModel.from_pretrained(repo_path, *args, **kwargs)\n\n            case nn.Module() as model:\n                # Handle pre-instantiated model objects\n                if rank_zero_only.rank == 0:\n                    log.info(\n                        f\"Returning existing model `{model_name}` of type {type(model)}\"\n                    )\n                return model\n\n            case _:\n                # Handle other configuration types (e.g., DictConfig) via parent class\n                # This fallback prevents returning None when the model config doesn't\n                # match the expected string or nn.Module patterns\n                return super().load_model(model_name_or_config, *args, **kwargs)\n\n    # If model_name_or_config is not a string in our pool, delegate to parent class\n    # This handles cases where model_name_or_config is a DictConfig directly\n    return super().load_model(model_name_or_config, *args, **kwargs)\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.CLIPVisionModelPool.save_model","title":"<code>save_model(model, path)</code>","text":"<p>Save a CLIP Vision model to the given path.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>CLIPVisionModel</code>)           \u2013            <p>The model to save.</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to save the model to.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/clip_vision/modelpool.py</code> <pre><code>@override\ndef save_model(self, model: CLIPVisionModel, path: str):\n    \"\"\"\n    Save a CLIP Vision model to the given path.\n\n    Args:\n        model (CLIPVisionModel): The model to save.\n        path (str): The path to save the model to.\n    \"\"\"\n    with timeit_context(f'Saving clip vision model to \"{path}\"'):\n        model.save_pretrained(path)\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#openclip-vision-encoder","title":"OpenCLIP Vision Encoder","text":""},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.OpenCLIPVisionModelPool","title":"<code>OpenCLIPVisionModelPool</code>","text":"<p>               Bases: <code>BaseModelPool</code></p> <p>A model pool for managing OpenCLIP Vision models (models from task vector paper).</p> Source code in <code>fusion_bench/modelpool/openclip_vision/modelpool.py</code> <pre><code>class OpenCLIPVisionModelPool(BaseModelPool):\n    \"\"\"\n    A model pool for managing OpenCLIP Vision models (models from task vector paper).\n    \"\"\"\n\n    _train_processor = None\n    _test_processor = None\n\n    def __init__(\n        self,\n        models: DictConfig,\n        classification_heads: Optional[DictConfig] = None,\n        **kwargs,\n    ):\n        super().__init__(models, **kwargs)\n        self._classification_heads = classification_heads\n\n    @property\n    def train_processor(self):\n        if self._train_processor is None:\n            encoder: ImageEncoder = self.load_pretrained_or_first_model()\n            self._train_processor = encoder.train_preprocess\n            if self._test_processor is None:\n                self._test_processor = encoder.val_preprocess\n        return self._train_processor\n\n    @property\n    def test_processor(self):\n        if self._test_processor is None:\n            encoder: ImageEncoder = self.load_pretrained_or_first_model()\n            if self._train_processor is None:\n                self._train_processor = encoder.train_preprocess\n            self._test_processor = encoder.val_preprocess\n        return self._test_processor\n\n    def load_model(\n        self, model_name_or_config: Union[str, DictConfig], *args, **kwargs\n    ) -&gt; ImageEncoder:\n        R\"\"\"\n        The model config can be:\n\n        - A string, which is the path to the model checkpoint in pickle format. Load directly using `torch.load`.\n        - {\"model_name\": str, \"pickle_path\": str}, load the model from the binary file (pickle format). This will first construct the model using `ImageEncoder(model_name)`, and then load the state dict from model located in the pickle file.\n        - {\"model_name\": str, \"state_dict_path\": str}, load the model from the state dict file. This will first construct the model using `ImageEncoder(model_name)`, and then load the state dict from the file.\n        - Default, load the model using `instantiate` from hydra.\n        \"\"\"\n        if (\n            isinstance(model_name_or_config, str)\n            and model_name_or_config in self._models\n        ):\n            model_config = self._models[model_name_or_config]\n        else:\n            model_config = model_name_or_config\n        if isinstance(model_config, DictConfig):\n            model_config = OmegaConf.to_container(model_config, resolve=True)\n\n        if isinstance(model_config, str):\n            # the model config is a string, which is the path to the model checkpoint in pickle format\n            # load the model using `torch.load`\n            # this is the original usage in the task arithmetic codebase\n            _check_and_redirect_open_clip_modeling()\n            log.info(f\"loading ImageEncoder from {model_config}\")\n            weights_only = kwargs[\"weights_only\"] if \"weights_only\" in kwargs else False\n            try:\n                encoder = torch.load(\n                    model_config, weights_only=weights_only, *args, **kwargs\n                )\n            except RuntimeError as e:\n                encoder = pickle.load(open(model_config, \"rb\"))\n        elif is_expr_match({\"model_name\": str, \"pickle_path\": str}, model_config):\n            # the model config is a dictionary with the following keys:\n            # - model_name: str, the name of the model\n            # - pickle_path: str, the path to the binary file (pickle format)\n            # load the model from the binary file (pickle format)\n            # this is useful when you use a newer version of torchvision\n            _check_and_redirect_open_clip_modeling()\n            log.info(\n                f\"loading ImageEncoder of {model_config['model_name']} from {model_config['pickle_path']}\"\n            )\n            weights_only = kwargs[\"weights_only\"] if \"weights_only\" in kwargs else False\n            try:\n                encoder = torch.load(\n                    model_config[\"pickle_path\"],\n                    weights_only=weights_only,\n                    *args,\n                    **kwargs,\n                )\n            except RuntimeError as e:\n                encoder = pickle.load(open(model_config[\"pickle_path\"], \"rb\"))\n            _encoder = ImageEncoder(model_config[\"model_name\"])\n            _encoder.load_state_dict(encoder.state_dict())\n            encoder = _encoder\n        elif is_expr_match({\"model_name\": str, \"state_dict_path\": str}, model_config):\n            # the model config is a dictionary with the following keys:\n            # - model_name: str, the name of the model\n            # - state_dict_path: str, the path to the state dict file\n            # load the model from the state dict file\n            log.info(\n                f\"loading ImageEncoder of {model_config['model_name']} from {model_config['state_dict_path']}\"\n            )\n            encoder = ImageEncoder(model_config[\"model_name\"])\n            encoder.load_state_dict(\n                torch.load(\n                    model_config[\"state_dict_path\"], weights_only=True, *args, **kwargs\n                )\n            )\n        elif isinstance(model_config, nn.Module):\n            # the model config is an existing model\n            log.info(f\"Returning existing model: {model_config}\")\n            encoder = model_config\n        else:\n            encoder = super().load_model(model_name_or_config, *args, **kwargs)\n        encoder = cast(ImageEncoder, encoder)\n\n        # setup the train and test processors\n        if self._train_processor is None and hasattr(encoder, \"train_preprocess\"):\n            self._train_processor = encoder.train_preprocess\n        if self._test_processor is None and hasattr(encoder, \"val_preprocess\"):\n            self._test_processor = encoder.val_preprocess\n\n        return encoder\n\n    def load_classification_head(\n        self, model_name_or_config: Union[str, DictConfig], *args, **kwargs\n    ) -&gt; ClassificationHead:\n        R\"\"\"\n        The model config can be:\n\n        - A string, which is the path to the model checkpoint in pickle format. Load directly using `torch.load`.\n        - Default, load the model using `instantiate` from hydra.\n        \"\"\"\n        if (\n            isinstance(model_name_or_config, str)\n            and model_name_or_config in self._classification_heads\n        ):\n            model_config = self._classification_heads[model_name_or_config]\n        else:\n            model_config = model_name_or_config\n\n        head = load_classifier_head(model_config, *args, **kwargs)\n        return head\n\n    def load_train_dataset(self, dataset_name: str, *args, **kwargs):\n        dataset_config = self._train_datasets[dataset_name]\n        if isinstance(dataset_config, str):\n            log.info(\n                f\"Loading train dataset using `datasets.load_dataset`: {dataset_config}\"\n            )\n            dataset = load_dataset(dataset_config, split=\"train\")\n        else:\n            dataset = super().load_train_dataset(dataset_name, *args, **kwargs)\n        return dataset\n\n    def load_val_dataset(self, dataset_name: str, *args, **kwargs):\n        dataset_config = self._val_datasets[dataset_name]\n        if isinstance(dataset_config, str):\n            log.info(\n                f\"Loading validation dataset using `datasets.load_dataset`: {dataset_config}\"\n            )\n            dataset = load_dataset(dataset_config, split=\"validation\")\n        else:\n            dataset = super().load_val_dataset(dataset_name, *args, **kwargs)\n        return dataset\n\n    def load_test_dataset(self, dataset_name: str, *args, **kwargs):\n        dataset_config = self._test_datasets[dataset_name]\n        if isinstance(dataset_config, str):\n            log.info(\n                f\"Loading test dataset using `datasets.load_dataset`: {dataset_config}\"\n            )\n            dataset = load_dataset(dataset_config, split=\"test\")\n        else:\n            dataset = super().load_test_dataset(dataset_name, *args, **kwargs)\n        return dataset\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.OpenCLIPVisionModelPool.load_classification_head","title":"<code>load_classification_head(model_name_or_config, *args, **kwargs)</code>","text":"<p>The model config can be:</p> <ul> <li>A string, which is the path to the model checkpoint in pickle format. Load directly using <code>torch.load</code>.</li> <li>Default, load the model using <code>instantiate</code> from hydra.</li> </ul> Source code in <code>fusion_bench/modelpool/openclip_vision/modelpool.py</code> <pre><code>def load_classification_head(\n    self, model_name_or_config: Union[str, DictConfig], *args, **kwargs\n) -&gt; ClassificationHead:\n    R\"\"\"\n    The model config can be:\n\n    - A string, which is the path to the model checkpoint in pickle format. Load directly using `torch.load`.\n    - Default, load the model using `instantiate` from hydra.\n    \"\"\"\n    if (\n        isinstance(model_name_or_config, str)\n        and model_name_or_config in self._classification_heads\n    ):\n        model_config = self._classification_heads[model_name_or_config]\n    else:\n        model_config = model_name_or_config\n\n    head = load_classifier_head(model_config, *args, **kwargs)\n    return head\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.OpenCLIPVisionModelPool.load_model","title":"<code>load_model(model_name_or_config, *args, **kwargs)</code>","text":"<p>The model config can be:</p> <ul> <li>A string, which is the path to the model checkpoint in pickle format. Load directly using <code>torch.load</code>.</li> <li>{\"model_name\": str, \"pickle_path\": str}, load the model from the binary file (pickle format). This will first construct the model using <code>ImageEncoder(model_name)</code>, and then load the state dict from model located in the pickle file.</li> <li>{\"model_name\": str, \"state_dict_path\": str}, load the model from the state dict file. This will first construct the model using <code>ImageEncoder(model_name)</code>, and then load the state dict from the file.</li> <li>Default, load the model using <code>instantiate</code> from hydra.</li> </ul> Source code in <code>fusion_bench/modelpool/openclip_vision/modelpool.py</code> <pre><code>def load_model(\n    self, model_name_or_config: Union[str, DictConfig], *args, **kwargs\n) -&gt; ImageEncoder:\n    R\"\"\"\n    The model config can be:\n\n    - A string, which is the path to the model checkpoint in pickle format. Load directly using `torch.load`.\n    - {\"model_name\": str, \"pickle_path\": str}, load the model from the binary file (pickle format). This will first construct the model using `ImageEncoder(model_name)`, and then load the state dict from model located in the pickle file.\n    - {\"model_name\": str, \"state_dict_path\": str}, load the model from the state dict file. This will first construct the model using `ImageEncoder(model_name)`, and then load the state dict from the file.\n    - Default, load the model using `instantiate` from hydra.\n    \"\"\"\n    if (\n        isinstance(model_name_or_config, str)\n        and model_name_or_config in self._models\n    ):\n        model_config = self._models[model_name_or_config]\n    else:\n        model_config = model_name_or_config\n    if isinstance(model_config, DictConfig):\n        model_config = OmegaConf.to_container(model_config, resolve=True)\n\n    if isinstance(model_config, str):\n        # the model config is a string, which is the path to the model checkpoint in pickle format\n        # load the model using `torch.load`\n        # this is the original usage in the task arithmetic codebase\n        _check_and_redirect_open_clip_modeling()\n        log.info(f\"loading ImageEncoder from {model_config}\")\n        weights_only = kwargs[\"weights_only\"] if \"weights_only\" in kwargs else False\n        try:\n            encoder = torch.load(\n                model_config, weights_only=weights_only, *args, **kwargs\n            )\n        except RuntimeError as e:\n            encoder = pickle.load(open(model_config, \"rb\"))\n    elif is_expr_match({\"model_name\": str, \"pickle_path\": str}, model_config):\n        # the model config is a dictionary with the following keys:\n        # - model_name: str, the name of the model\n        # - pickle_path: str, the path to the binary file (pickle format)\n        # load the model from the binary file (pickle format)\n        # this is useful when you use a newer version of torchvision\n        _check_and_redirect_open_clip_modeling()\n        log.info(\n            f\"loading ImageEncoder of {model_config['model_name']} from {model_config['pickle_path']}\"\n        )\n        weights_only = kwargs[\"weights_only\"] if \"weights_only\" in kwargs else False\n        try:\n            encoder = torch.load(\n                model_config[\"pickle_path\"],\n                weights_only=weights_only,\n                *args,\n                **kwargs,\n            )\n        except RuntimeError as e:\n            encoder = pickle.load(open(model_config[\"pickle_path\"], \"rb\"))\n        _encoder = ImageEncoder(model_config[\"model_name\"])\n        _encoder.load_state_dict(encoder.state_dict())\n        encoder = _encoder\n    elif is_expr_match({\"model_name\": str, \"state_dict_path\": str}, model_config):\n        # the model config is a dictionary with the following keys:\n        # - model_name: str, the name of the model\n        # - state_dict_path: str, the path to the state dict file\n        # load the model from the state dict file\n        log.info(\n            f\"loading ImageEncoder of {model_config['model_name']} from {model_config['state_dict_path']}\"\n        )\n        encoder = ImageEncoder(model_config[\"model_name\"])\n        encoder.load_state_dict(\n            torch.load(\n                model_config[\"state_dict_path\"], weights_only=True, *args, **kwargs\n            )\n        )\n    elif isinstance(model_config, nn.Module):\n        # the model config is an existing model\n        log.info(f\"Returning existing model: {model_config}\")\n        encoder = model_config\n    else:\n        encoder = super().load_model(model_name_or_config, *args, **kwargs)\n    encoder = cast(ImageEncoder, encoder)\n\n    # setup the train and test processors\n    if self._train_processor is None and hasattr(encoder, \"train_preprocess\"):\n        self._train_processor = encoder.train_preprocess\n    if self._test_processor is None and hasattr(encoder, \"val_preprocess\"):\n        self._test_processor = encoder.val_preprocess\n\n    return encoder\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#nlp-model-pool","title":"NLP Model Pool","text":""},{"location":"api/fusion_bench.modelpool/#gpt-2","title":"GPT-2","text":""},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.HuggingFaceGPT2ClassificationPool","title":"<code>HuggingFaceGPT2ClassificationPool = GPT2ForSequenceClassificationPool</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.GPT2ForSequenceClassificationPool","title":"<code>GPT2ForSequenceClassificationPool</code>","text":"<p>               Bases: <code>BaseModelPool</code></p> Source code in <code>fusion_bench/modelpool/huggingface_gpt2_classification.py</code> <pre><code>class GPT2ForSequenceClassificationPool(BaseModelPool):\n    _config_mapping = BaseModelPool._config_mapping | {\"_tokenizer\": \"tokenizer\"}\n\n    def __init__(self, tokenizer: DictConfig, **kwargs):\n        self._tokenizer = tokenizer\n        super().__init__(**kwargs)\n        self.setup()\n\n    def setup(self):\n        global tokenizer\n        self.tokenizer = tokenizer = instantiate(self._tokenizer)\n\n    def load_classifier(\n        self, model_config: str | DictConfig\n    ) -&gt; GPT2ForSequenceClassification:\n        if isinstance(model_config, str):\n            model_config = self.get_model_config(model_config, return_copy=True)\n        model_config._target_ = (\n            \"transformers.GPT2ForSequenceClassification.from_pretrained\"\n        )\n        model = instantiate(model_config)\n        return model\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#seq2seq-language-models-flan-t5","title":"Seq2Seq Language Models (Flan-T5)","text":""},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.Seq2SeqLMPool","title":"<code>Seq2SeqLMPool</code>","text":"<p>               Bases: <code>BaseModelPool</code></p> <p>A model pool specialized for sequence-to-sequence language models.</p> <p>This model pool provides management and loading capabilities for sequence-to-sequence (seq2seq) language models such as T5, BART, and mT5. It extends the base model pool functionality with seq2seq-specific features including tokenizer management and model configuration handling.</p> <p>Seq2seq models are particularly useful for tasks that require generating output sequences from input sequences, such as translation, summarization, question answering, and text generation. This pool streamlines the process of loading and configuring multiple seq2seq models for fusion and ensemble scenarios.</p> Key Features <ul> <li>Specialized loading for AutoModelForSeq2SeqLM models</li> <li>Integrated tokenizer management</li> <li>Support for model-specific keyword arguments</li> <li>Automatic dtype parsing and configuration</li> <li>Compatible with PEFT (Parameter-Efficient Fine-Tuning) adapters</li> </ul> <p>Attributes:</p> <ul> <li> <code>_tokenizer</code>           \u2013            <p>Configuration for the tokenizer associated with the models</p> </li> <li> <code>_model_kwargs</code>           \u2013            <p>Default keyword arguments applied to all model loading operations</p> </li> </ul> Example <pre><code>pool = Seq2SeqLMPool(\n    models={\n        \"t5_base\": \"t5-base\",\n        \"t5_large\": \"t5-large\",\n        \"custom_model\": \"/path/to/local/model\"\n    },\n    tokenizer={\"_target_\": \"transformers.T5Tokenizer\",\n              \"pretrained_model_name_or_path\": \"t5-base\"},\n    model_kwargs={\"torch_dtype\": \"float16\", \"device_map\": \"auto\"}\n)\nmodel = pool.load_model(\"t5_base\")\ntokenizer = pool.load_tokenizer()\n</code></pre> Source code in <code>fusion_bench/modelpool/seq2seq_lm/modelpool.py</code> <pre><code>class Seq2SeqLMPool(BaseModelPool):\n    \"\"\"A model pool specialized for sequence-to-sequence language models.\n\n    This model pool provides management and loading capabilities for sequence-to-sequence\n    (seq2seq) language models such as T5, BART, and mT5. It extends the base model pool\n    functionality with seq2seq-specific features including tokenizer management and\n    model configuration handling.\n\n    Seq2seq models are particularly useful for tasks that require generating output\n    sequences from input sequences, such as translation, summarization, question\n    answering, and text generation. This pool streamlines the process of loading\n    and configuring multiple seq2seq models for fusion and ensemble scenarios.\n\n    Key Features:\n        - Specialized loading for AutoModelForSeq2SeqLM models\n        - Integrated tokenizer management\n        - Support for model-specific keyword arguments\n        - Automatic dtype parsing and configuration\n        - Compatible with PEFT (Parameter-Efficient Fine-Tuning) adapters\n\n    Attributes:\n        _tokenizer: Configuration for the tokenizer associated with the models\n        _model_kwargs: Default keyword arguments applied to all model loading operations\n\n    Example:\n        ```python\n        pool = Seq2SeqLMPool(\n            models={\n                \"t5_base\": \"t5-base\",\n                \"t5_large\": \"t5-large\",\n                \"custom_model\": \"/path/to/local/model\"\n            },\n            tokenizer={\"_target_\": \"transformers.T5Tokenizer\",\n                      \"pretrained_model_name_or_path\": \"t5-base\"},\n            model_kwargs={\"torch_dtype\": \"float16\", \"device_map\": \"auto\"}\n        )\n        model = pool.load_model(\"t5_base\")\n        tokenizer = pool.load_tokenizer()\n        ```\n    \"\"\"\n\n    _config_mapping = BaseModelPool._config_mapping | {\n        \"_tokenizer\": \"tokenizer\",\n        \"_model_kwargs\": \"model_kwargs\",\n    }\n\n    def __init__(\n        self,\n        models: DictConfig,\n        *,\n        tokenizer: Optional[DictConfig],\n        model_kwargs: Optional[DictConfig] = None,\n        **kwargs,\n    ):\n        \"\"\"Initialize the sequence-to-sequence language model pool.\n\n        Sets up the model pool with configurations for models, tokenizer, and\n        default model loading parameters. Automatically processes model kwargs\n        to handle special configurations like torch_dtype parsing.\n\n        Args:\n            models: Configuration dictionary specifying the seq2seq models to manage.\n                Keys are model names, values can be model paths/names or detailed configs.\n            tokenizer: Configuration for the tokenizer to use with the models.\n                Can be a simple path/name or detailed configuration with _target_.\n            model_kwargs: Default keyword arguments applied to all model loading\n                operations. Common options include torch_dtype, device_map, etc.\n                The torch_dtype field is automatically parsed from string to dtype.\n            **kwargs: Additional arguments passed to the parent BaseModelPool.\n\n        Example:\n            ```python\n            pool = Seq2SeqLMPool(\n                models={\n                    \"base\": \"t5-base\",\n                    \"large\": {\"_target_\": \"transformers.AutoModelForSeq2SeqLM\",\n                             \"pretrained_model_name_or_path\": \"t5-large\"}\n                },\n                tokenizer=\"t5-base\",\n                model_kwargs={\"torch_dtype\": \"bfloat16\"}\n            )\n            ```\n        \"\"\"\n        super().__init__(models, **kwargs)\n        self._tokenizer = tokenizer\n        self._model_kwargs = model_kwargs\n        if self._model_kwargs is None:\n            self._model_kwargs = DictConfig({})\n        with flag_override(self._model_kwargs, \"allow_objects\", True):\n            if hasattr(self._model_kwargs, \"torch_dtype\"):\n                self._model_kwargs.torch_dtype = parse_dtype(\n                    self._model_kwargs.torch_dtype\n                )\n\n    def load_model(self, model_name_or_config: str | DictConfig, *args, **kwargs):\n        \"\"\"Load a sequence-to-sequence language model from the pool.\n\n        Loads a seq2seq model using the parent class loading mechanism while\n        automatically applying the pool's default model kwargs. The method\n        merges the pool's model_kwargs with any additional kwargs provided,\n        giving priority to the explicitly provided kwargs.\n\n        Args:\n            model_name_or_config: Either a string model name from the pool\n                configuration or a DictConfig containing model loading parameters.\n            *args: Additional positional arguments passed to the parent load_model method.\n            **kwargs: Additional keyword arguments that override the pool's default\n                model_kwargs. Common options include device, torch_dtype, etc.\n\n        Returns:\n            AutoModelForSeq2SeqLM: The loaded sequence-to-sequence language model.\n        \"\"\"\n        model_kwargs = deepcopy(self._model_kwargs)\n        model_kwargs.update(kwargs)\n        return super().load_model(model_name_or_config, *args, **model_kwargs)\n\n    def load_tokenizer(self, *args, **kwargs):\n        \"\"\"Load the tokenizer associated with the sequence-to-sequence models.\n\n        Loads a tokenizer based on the tokenizer configuration provided during\n        pool initialization. The tokenizer should be compatible with the seq2seq\n        models in the pool and is typically used for preprocessing input text\n        and postprocessing generated output.\n\n        Args:\n            *args: Additional positional arguments passed to the tokenizer constructor.\n            **kwargs: Additional keyword arguments passed to the tokenizer constructor.\n\n        Returns:\n            PreTrainedTokenizer: The loaded tokenizer instance compatible with\n                the seq2seq models in this pool.\n\n        Raises:\n            AssertionError: If no tokenizer configuration is provided.\n        \"\"\"\n        assert self._tokenizer is not None, \"Tokenizer is not defined in the config\"\n        tokenizer = isinstance(self._tokenizer, *args, **kwargs)\n        return tokenizer\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.Seq2SeqLMPool.__init__","title":"<code>__init__(models, *, tokenizer, model_kwargs=None, **kwargs)</code>","text":"<p>Initialize the sequence-to-sequence language model pool.</p> <p>Sets up the model pool with configurations for models, tokenizer, and default model loading parameters. Automatically processes model kwargs to handle special configurations like torch_dtype parsing.</p> <p>Parameters:</p> <ul> <li> <code>models</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration dictionary specifying the seq2seq models to manage. Keys are model names, values can be model paths/names or detailed configs.</p> </li> <li> <code>tokenizer</code>               (<code>Optional[DictConfig]</code>)           \u2013            <p>Configuration for the tokenizer to use with the models. Can be a simple path/name or detailed configuration with target.</p> </li> <li> <code>model_kwargs</code>               (<code>Optional[DictConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Default keyword arguments applied to all model loading operations. Common options include torch_dtype, device_map, etc. The torch_dtype field is automatically parsed from string to dtype.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to the parent BaseModelPool.</p> </li> </ul> Example <pre><code>pool = Seq2SeqLMPool(\n    models={\n        \"base\": \"t5-base\",\n        \"large\": {\"_target_\": \"transformers.AutoModelForSeq2SeqLM\",\n                 \"pretrained_model_name_or_path\": \"t5-large\"}\n    },\n    tokenizer=\"t5-base\",\n    model_kwargs={\"torch_dtype\": \"bfloat16\"}\n)\n</code></pre> Source code in <code>fusion_bench/modelpool/seq2seq_lm/modelpool.py</code> <pre><code>def __init__(\n    self,\n    models: DictConfig,\n    *,\n    tokenizer: Optional[DictConfig],\n    model_kwargs: Optional[DictConfig] = None,\n    **kwargs,\n):\n    \"\"\"Initialize the sequence-to-sequence language model pool.\n\n    Sets up the model pool with configurations for models, tokenizer, and\n    default model loading parameters. Automatically processes model kwargs\n    to handle special configurations like torch_dtype parsing.\n\n    Args:\n        models: Configuration dictionary specifying the seq2seq models to manage.\n            Keys are model names, values can be model paths/names or detailed configs.\n        tokenizer: Configuration for the tokenizer to use with the models.\n            Can be a simple path/name or detailed configuration with _target_.\n        model_kwargs: Default keyword arguments applied to all model loading\n            operations. Common options include torch_dtype, device_map, etc.\n            The torch_dtype field is automatically parsed from string to dtype.\n        **kwargs: Additional arguments passed to the parent BaseModelPool.\n\n    Example:\n        ```python\n        pool = Seq2SeqLMPool(\n            models={\n                \"base\": \"t5-base\",\n                \"large\": {\"_target_\": \"transformers.AutoModelForSeq2SeqLM\",\n                         \"pretrained_model_name_or_path\": \"t5-large\"}\n            },\n            tokenizer=\"t5-base\",\n            model_kwargs={\"torch_dtype\": \"bfloat16\"}\n        )\n        ```\n    \"\"\"\n    super().__init__(models, **kwargs)\n    self._tokenizer = tokenizer\n    self._model_kwargs = model_kwargs\n    if self._model_kwargs is None:\n        self._model_kwargs = DictConfig({})\n    with flag_override(self._model_kwargs, \"allow_objects\", True):\n        if hasattr(self._model_kwargs, \"torch_dtype\"):\n            self._model_kwargs.torch_dtype = parse_dtype(\n                self._model_kwargs.torch_dtype\n            )\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.Seq2SeqLMPool.load_model","title":"<code>load_model(model_name_or_config, *args, **kwargs)</code>","text":"<p>Load a sequence-to-sequence language model from the pool.</p> <p>Loads a seq2seq model using the parent class loading mechanism while automatically applying the pool's default model kwargs. The method merges the pool's model_kwargs with any additional kwargs provided, giving priority to the explicitly provided kwargs.</p> <p>Parameters:</p> <ul> <li> <code>model_name_or_config</code>               (<code>str | DictConfig</code>)           \u2013            <p>Either a string model name from the pool configuration or a DictConfig containing model loading parameters.</p> </li> <li> <code>*args</code>           \u2013            <p>Additional positional arguments passed to the parent load_model method.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments that override the pool's default model_kwargs. Common options include device, torch_dtype, etc.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AutoModelForSeq2SeqLM</code>          \u2013            <p>The loaded sequence-to-sequence language model.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/seq2seq_lm/modelpool.py</code> <pre><code>def load_model(self, model_name_or_config: str | DictConfig, *args, **kwargs):\n    \"\"\"Load a sequence-to-sequence language model from the pool.\n\n    Loads a seq2seq model using the parent class loading mechanism while\n    automatically applying the pool's default model kwargs. The method\n    merges the pool's model_kwargs with any additional kwargs provided,\n    giving priority to the explicitly provided kwargs.\n\n    Args:\n        model_name_or_config: Either a string model name from the pool\n            configuration or a DictConfig containing model loading parameters.\n        *args: Additional positional arguments passed to the parent load_model method.\n        **kwargs: Additional keyword arguments that override the pool's default\n            model_kwargs. Common options include device, torch_dtype, etc.\n\n    Returns:\n        AutoModelForSeq2SeqLM: The loaded sequence-to-sequence language model.\n    \"\"\"\n    model_kwargs = deepcopy(self._model_kwargs)\n    model_kwargs.update(kwargs)\n    return super().load_model(model_name_or_config, *args, **model_kwargs)\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.Seq2SeqLMPool.load_tokenizer","title":"<code>load_tokenizer(*args, **kwargs)</code>","text":"<p>Load the tokenizer associated with the sequence-to-sequence models.</p> <p>Loads a tokenizer based on the tokenizer configuration provided during pool initialization. The tokenizer should be compatible with the seq2seq models in the pool and is typically used for preprocessing input text and postprocessing generated output.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>           \u2013            <p>Additional positional arguments passed to the tokenizer constructor.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments passed to the tokenizer constructor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PreTrainedTokenizer</code>          \u2013            <p>The loaded tokenizer instance compatible with the seq2seq models in this pool.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AssertionError</code>             \u2013            <p>If no tokenizer configuration is provided.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/seq2seq_lm/modelpool.py</code> <pre><code>def load_tokenizer(self, *args, **kwargs):\n    \"\"\"Load the tokenizer associated with the sequence-to-sequence models.\n\n    Loads a tokenizer based on the tokenizer configuration provided during\n    pool initialization. The tokenizer should be compatible with the seq2seq\n    models in the pool and is typically used for preprocessing input text\n    and postprocessing generated output.\n\n    Args:\n        *args: Additional positional arguments passed to the tokenizer constructor.\n        **kwargs: Additional keyword arguments passed to the tokenizer constructor.\n\n    Returns:\n        PreTrainedTokenizer: The loaded tokenizer instance compatible with\n            the seq2seq models in this pool.\n\n    Raises:\n        AssertionError: If no tokenizer configuration is provided.\n    \"\"\"\n    assert self._tokenizer is not None, \"Tokenizer is not defined in the config\"\n    tokenizer = isinstance(self._tokenizer, *args, **kwargs)\n    return tokenizer\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.SequenceClassificationModelPool","title":"<code>SequenceClassificationModelPool</code>","text":"<p>               Bases: <code>BaseModelPool</code></p> Source code in <code>fusion_bench/modelpool/seq_classification_lm/seq_classification_lm.py</code> <pre><code>class SequenceClassificationModelPool(BaseModelPool):\n\n    def __init__(\n        self,\n        models,\n        *,\n        tokenizer: Optional[DictConfig],\n        model_kwargs: Optional[DictConfig] = None,\n        **kwargs,\n    ):\n        super().__init__(models, **kwargs)\n        # process `model_kwargs`\n        self._tokenizer = tokenizer\n        self._model_kwargs = model_kwargs\n        if self._model_kwargs is None:\n            self._model_kwargs = DictConfig({})\n        with flag_override(self._model_kwargs, \"allow_objects\", True):\n            if hasattr(self._model_kwargs, \"torch_dtype\"):\n                self._model_kwargs.torch_dtype = parse_dtype(\n                    self._model_kwargs.torch_dtype\n                )\n\n    @override\n    def load_model(\n        self,\n        model_name_or_config: str | DictConfig,\n        *args,\n        **kwargs,\n    ) -&gt; Union[PreTrainedModel, \"LlamaForSequenceClassification\"]:\n        model_kwargs = deepcopy(self._model_kwargs)\n        model_kwargs.update(kwargs)\n        if isinstance(model_name_or_config, str):\n            log.info(f\"Loading model: {model_name_or_config}\", stacklevel=2)\n        return super().load_model(model_name_or_config, *args, **model_kwargs)\n\n    def load_tokenizer(self, *args, **kwargs) -&gt; PreTrainedTokenizer:\n        assert self._tokenizer is not None, \"Tokenizer is not defined in the config\"\n        log.info(\"Loading tokenizer.\", stacklevel=2)\n        tokenizer = instantiate(self._tokenizer, *args, **kwargs)\n        return tokenizer\n\n    @override\n    def save_model(\n        self,\n        model: PreTrainedModel,\n        path: str,\n        push_to_hub: bool = False,\n        model_dtype: Optional[str] = None,\n        save_tokenizer: bool = False,\n        tokenizer_kwargs=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Save the model to the specified path.\n\n        Args:\n            model (PreTrainedModel): The model to be saved.\n            path (str): The path where the model will be saved.\n            push_to_hub (bool, optional): Whether to push the model to the Hugging Face Hub. Defaults to False.\n            save_tokenizer (bool, optional): Whether to save the tokenizer along with the model. Defaults to False.\n            **kwargs: Additional keyword arguments passed to the `save_pretrained` method.\n        \"\"\"\n        path = os.path.expanduser(path)\n        if save_tokenizer:\n            if tokenizer_kwargs is None:\n                tokenizer_kwargs = {}\n            # load the tokenizer\n            tokenizer = self.load_tokenizer(**tokenizer_kwargs)\n            tokenizer.save_pretrained(\n                path,\n                push_to_hub=push_to_hub,\n            )\n        if model_dtype is not None:\n            model.to(dtype=parse_dtype(model_dtype))\n        model.save_pretrained(\n            path,\n            push_to_hub=push_to_hub,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.SequenceClassificationModelPool.save_model","title":"<code>save_model(model, path, push_to_hub=False, model_dtype=None, save_tokenizer=False, tokenizer_kwargs=None, **kwargs)</code>","text":"<p>Save the model to the specified path.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>PreTrainedModel</code>)           \u2013            <p>The model to be saved.</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path where the model will be saved.</p> </li> <li> <code>push_to_hub</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to push the model to the Hugging Face Hub. Defaults to False.</p> </li> <li> <code>save_tokenizer</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the tokenizer along with the model. Defaults to False.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments passed to the <code>save_pretrained</code> method.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/seq_classification_lm/seq_classification_lm.py</code> <pre><code>@override\ndef save_model(\n    self,\n    model: PreTrainedModel,\n    path: str,\n    push_to_hub: bool = False,\n    model_dtype: Optional[str] = None,\n    save_tokenizer: bool = False,\n    tokenizer_kwargs=None,\n    **kwargs,\n):\n    \"\"\"\n    Save the model to the specified path.\n\n    Args:\n        model (PreTrainedModel): The model to be saved.\n        path (str): The path where the model will be saved.\n        push_to_hub (bool, optional): Whether to push the model to the Hugging Face Hub. Defaults to False.\n        save_tokenizer (bool, optional): Whether to save the tokenizer along with the model. Defaults to False.\n        **kwargs: Additional keyword arguments passed to the `save_pretrained` method.\n    \"\"\"\n    path = os.path.expanduser(path)\n    if save_tokenizer:\n        if tokenizer_kwargs is None:\n            tokenizer_kwargs = {}\n        # load the tokenizer\n        tokenizer = self.load_tokenizer(**tokenizer_kwargs)\n        tokenizer.save_pretrained(\n            path,\n            push_to_hub=push_to_hub,\n        )\n    if model_dtype is not None:\n        model.to(dtype=parse_dtype(model_dtype))\n    model.save_pretrained(\n        path,\n        push_to_hub=push_to_hub,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.PeftModelForSeq2SeqLMPool","title":"<code>PeftModelForSeq2SeqLMPool</code>","text":"<p>               Bases: <code>ModelPool</code></p> Source code in <code>fusion_bench/modelpool/PeftModelForSeq2SeqLM.py</code> <pre><code>class PeftModelForSeq2SeqLMPool(ModelPool):\n    def load_model(self, model_config: str | DictConfig):\n        \"\"\"\n        Load a model based on the provided configuration.\n\n        The configuration options of `model_config` are:\n\n        - name: The name of the model. If it is \"_pretrained_\", a pretrained Seq2Seq language model is returned.\n        - path: The path where the model is stored.\n        - is_trainable: A boolean indicating whether the model parameters should be trainable. Default is `True`.\n        - merge_and_unload: A boolean indicating whether to merge and unload the PEFT model after loading. Default is `True`.\n\n\n        Args:\n            model_config (str | DictConfig): The configuration for the model. This can be either a string (name of the model) or a DictConfig object containing the model configuration.\n\n\n        Returns:\n            model: The loaded model. If the model name is \"_pretrained_\", it returns a pretrained Seq2Seq language model. Otherwise, it returns a PEFT model.\n        \"\"\"\n        if isinstance(model_config, str):\n            model_config = self.get_model_config(model_config)\n        with timeit_context(f\"Loading model {model_config['name']}\"):\n            if model_config[\"name\"] == \"_pretrained_\":\n                model = AutoModelForSeq2SeqLM.from_pretrained(model_config[\"path\"])\n                return model\n            else:\n                model = self.load_model(\"_pretrained_\")\n                peft_model = PeftModel.from_pretrained(\n                    model,\n                    model_config[\"path\"],\n                    is_trainable=model_config.get(\"is_trainable\", True),\n                )\n                if model_config.get(\"merge_and_unload\", True):\n                    return peft_model.merge_and_unload()\n                else:\n                    return peft_model\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.PeftModelForSeq2SeqLMPool.load_model","title":"<code>load_model(model_config)</code>","text":"<p>Load a model based on the provided configuration.</p> <p>The configuration options of <code>model_config</code> are:</p> <ul> <li>name: The name of the model. If it is \"pretrained\", a pretrained Seq2Seq language model is returned.</li> <li>path: The path where the model is stored.</li> <li>is_trainable: A boolean indicating whether the model parameters should be trainable. Default is <code>True</code>.</li> <li>merge_and_unload: A boolean indicating whether to merge and unload the PEFT model after loading. Default is <code>True</code>.</li> </ul> <p>Parameters:</p> <ul> <li> <code>model_config</code>               (<code>str | DictConfig</code>)           \u2013            <p>The configuration for the model. This can be either a string (name of the model) or a DictConfig object containing the model configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code>          \u2013            <p>The loaded model. If the model name is \"pretrained\", it returns a pretrained Seq2Seq language model. Otherwise, it returns a PEFT model.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/PeftModelForSeq2SeqLM.py</code> <pre><code>def load_model(self, model_config: str | DictConfig):\n    \"\"\"\n    Load a model based on the provided configuration.\n\n    The configuration options of `model_config` are:\n\n    - name: The name of the model. If it is \"_pretrained_\", a pretrained Seq2Seq language model is returned.\n    - path: The path where the model is stored.\n    - is_trainable: A boolean indicating whether the model parameters should be trainable. Default is `True`.\n    - merge_and_unload: A boolean indicating whether to merge and unload the PEFT model after loading. Default is `True`.\n\n\n    Args:\n        model_config (str | DictConfig): The configuration for the model. This can be either a string (name of the model) or a DictConfig object containing the model configuration.\n\n\n    Returns:\n        model: The loaded model. If the model name is \"_pretrained_\", it returns a pretrained Seq2Seq language model. Otherwise, it returns a PEFT model.\n    \"\"\"\n    if isinstance(model_config, str):\n        model_config = self.get_model_config(model_config)\n    with timeit_context(f\"Loading model {model_config['name']}\"):\n        if model_config[\"name\"] == \"_pretrained_\":\n            model = AutoModelForSeq2SeqLM.from_pretrained(model_config[\"path\"])\n            return model\n        else:\n            model = self.load_model(\"_pretrained_\")\n            peft_model = PeftModel.from_pretrained(\n                model,\n                model_config[\"path\"],\n                is_trainable=model_config.get(\"is_trainable\", True),\n            )\n            if model_config.get(\"merge_and_unload\", True):\n                return peft_model.merge_and_unload()\n            else:\n                return peft_model\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#causal-language-models-llama-mistral-qwen","title":"Causal Language Models (Llama, Mistral, Qwen...)","text":""},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.CausalLMPool","title":"<code>CausalLMPool</code>","text":"<p>               Bases: <code>BaseModelPool</code></p> <p>A model pool for managing and loading causal language models.</p> <p>This class provides a unified interface for loading and managing multiple causal language models, typically used in model fusion and ensemble scenarios. It supports both eager and lazy loading strategies, and handles model configuration through YAML configs or direct instantiation.</p> <p>The pool can manage models from Hugging Face Hub, local paths, or custom configurations. It also provides tokenizer management and model saving capabilities with optional Hugging Face Hub integration.</p> <p>Parameters:</p> <ul> <li> <code>models</code>           \u2013            <p>Dictionary or configuration specifying the models to be managed. Can contain model names mapped to paths or detailed configurations.</p> </li> <li> <code>tokenizer</code>               (<code>Optional[DictConfig | str]</code>)           \u2013            <p>Tokenizer configuration, either a string path/name or a DictConfig with detailed tokenizer settings.</p> </li> <li> <code>model_kwargs</code>               (<code>Optional[DictConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Additional keyword arguments passed to model loading. Common options include torch_dtype, device_map, etc.</p> </li> <li> <code>enable_lazy_loading</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use lazy loading for models. When True, models are loaded as LazyStateDict objects instead of actual models, which can save memory for large model collections.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to the parent BaseModelPool.</p> </li> </ul> Example <pre><code>&gt;&gt;&gt; pool = CausalLMPool(\n...     models={\n...         \"model_a\": \"microsoft/DialoGPT-medium\",\n...         \"model_b\": \"/path/to/local/model\"\n...     },\n...     tokenizer=\"microsoft/DialoGPT-medium\",\n...     model_kwargs={\"torch_dtype\": \"bfloat16\"}\n... )\n&gt;&gt;&gt; model = pool.load_model(\"model_a\")\n&gt;&gt;&gt; tokenizer = pool.load_tokenizer()\n</code></pre> Source code in <code>fusion_bench/modelpool/causal_lm/causal_lm.py</code> <pre><code>@auto_register_config\nclass CausalLMPool(BaseModelPool):\n    \"\"\"A model pool for managing and loading causal language models.\n\n    This class provides a unified interface for loading and managing multiple\n    causal language models, typically used in model fusion and ensemble scenarios.\n    It supports both eager and lazy loading strategies, and handles model\n    configuration through YAML configs or direct instantiation.\n\n    The pool can manage models from Hugging Face Hub, local paths, or custom\n    configurations. It also provides tokenizer management and model saving\n    capabilities with optional Hugging Face Hub integration.\n\n    Args:\n        models: Dictionary or configuration specifying the models to be managed.\n            Can contain model names mapped to paths or detailed configurations.\n        tokenizer: Tokenizer configuration, either a string path/name or\n            a DictConfig with detailed tokenizer settings.\n        model_kwargs: Additional keyword arguments passed to model loading.\n            Common options include torch_dtype, device_map, etc.\n        enable_lazy_loading: Whether to use lazy loading for models. When True,\n            models are loaded as LazyStateDict objects instead of actual models,\n            which can save memory for large model collections.\n        **kwargs: Additional arguments passed to the parent BaseModelPool.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; pool = CausalLMPool(\n        ...     models={\n        ...         \"model_a\": \"microsoft/DialoGPT-medium\",\n        ...         \"model_b\": \"/path/to/local/model\"\n        ...     },\n        ...     tokenizer=\"microsoft/DialoGPT-medium\",\n        ...     model_kwargs={\"torch_dtype\": \"bfloat16\"}\n        ... )\n        &gt;&gt;&gt; model = pool.load_model(\"model_a\")\n        &gt;&gt;&gt; tokenizer = pool.load_tokenizer()\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        models,\n        *,\n        tokenizer: Optional[DictConfig | str],\n        model_kwargs: Optional[DictConfig] = None,\n        enable_lazy_loading: bool = False,\n        **kwargs,\n    ):\n        super().__init__(models, **kwargs)\n        if model_kwargs is None:\n            self.model_kwargs = DictConfig({})\n\n    def get_model_path(self, model_name: str):\n        \"\"\"Extract the model path from the model configuration.\n\n        Args:\n            model_name: The name of the model as defined in the models configuration.\n\n        Returns:\n            str: The path or identifier for the model. For string configurations,\n                returns the string directly. For dict configurations, extracts\n                the 'pretrained_model_name_or_path' field.\n\n        Raises:\n            RuntimeError: If the model configuration is invalid or the model\n                name is not found in the configuration.\n        \"\"\"\n        model_name_or_config = self._models[model_name]\n        if isinstance(model_name_or_config, str):\n            return model_name_or_config\n        elif isinstance(model_name_or_config, (DictConfig, dict)):\n            return model_name_or_config.get(\"pretrained_model_name_or_path\")\n        else:\n            raise RuntimeError(\"Invalid model configuration\")\n\n    def get_model_kwargs(self):\n        \"\"\"Get processed model keyword arguments for model loading.\n\n        Converts the stored `model_kwargs` from DictConfig to a regular dictionary\n        and processes special arguments like torch_dtype for proper model loading.\n\n        Returns:\n            dict: Processed keyword arguments ready to be passed to model\n                loading functions. The torch_dtype field, if present, is\n                converted from string to the appropriate torch dtype object.\n        \"\"\"\n        model_kwargs = (\n            OmegaConf.to_container(self.model_kwargs, resolve=True)\n            if isinstance(self.model_kwargs, DictConfig)\n            else self.model_kwargs\n        )\n        if \"torch_dtype\" in model_kwargs:\n            model_kwargs[\"torch_dtype\"] = parse_dtype(model_kwargs[\"torch_dtype\"])\n        return model_kwargs\n\n    @override\n    def load_model(\n        self,\n        model_name_or_config: str | DictConfig,\n        *args,\n        **kwargs,\n    ) -&gt; Union[PreTrainedModel, LazyStateDict]:\n        \"\"\"Load a causal language model from the model pool.\n\n        This method supports multiple loading strategies:\n        1. Loading by model name from the configured model pool\n        2. Loading from a direct configuration dictionary\n        3. Lazy loading using LazyStateDict for memory efficiency\n\n        The method automatically handles different model configuration formats\n        and applies the appropriate loading strategy based on the enable_lazy_loading flag.\n\n        Args:\n            model_name_or_config: Either a string model name that exists in the\n                model pool configuration, or a DictConfig/dict containing the\n                model configuration directly.\n            *args: Additional positional arguments passed to the model constructor.\n            **kwargs: Additional keyword arguments passed to the model constructor.\n                These will be merged with the pool's model_kwargs.\n\n        Returns:\n            Union[PreTrainedModel, LazyStateDict]: The loaded model. Returns a\n                PreTrainedModel for normal loading or a LazyStateDict for lazy loading.\n\n        Raises:\n            RuntimeError: If the model configuration is invalid.\n            KeyError: If the model name is not found in the model pool.\n\n        Example YAML configurations:\n            Simple string configuration:\n            ```yaml\n            models:\n              _pretrained_: path_to_pretrained_model\n              model_a: path_to_model_a\n              model_b: path_to_model_b\n            ```\n\n            Detailed configuration:\n            ```yaml\n            models:\n              _pretrained_:\n                _target_: transformers.AutoModelForCausalLM\n                pretrained_model_name_or_path: path_to_pretrained_model\n              model_a:\n                _target_: transformers.AutoModelForCausalLM\n                pretrained_model_name_or_path: path_to_model_a\n            ```\n        \"\"\"\n        model_kwargs = self.get_model_kwargs()\n        model_kwargs.update(kwargs)\n\n        if isinstance(model_name_or_config, str):\n            # If model_name_or_config is a string, it is the name or the path of the model\n            log.info(f\"Loading model: {model_name_or_config}\", stacklevel=2)\n            if model_name_or_config in self._models.keys():\n                model_config = self._models[model_name_or_config]\n                if isinstance(model_config, str):\n                    # model_config is a string\n                    if not self.enable_lazy_loading:\n                        model = AutoModelForCausalLM.from_pretrained(\n                            model_config,\n                            *args,\n                            **model_kwargs,\n                        )\n                    else:\n                        # model_config is a string, but we want to use LazyStateDict\n                        model = LazyStateDict(\n                            checkpoint=model_config,\n                            meta_module_class=AutoModelForCausalLM,\n                            *args,\n                            **model_kwargs,\n                        )\n                    return model\n        elif isinstance(model_name_or_config, (DictConfig, Dict)):\n            model_config = model_name_or_config\n\n        if not self.enable_lazy_loading:\n            model = instantiate(model_config, *args, **model_kwargs)\n        else:\n            meta_module_class = model_config.pop(\"_target_\")\n            checkpoint = model_config.pop(\"pretrained_model_name_or_path\")\n            model = LazyStateDict(\n                checkpoint=checkpoint,\n                meta_module_class=meta_module_class,\n                *args,\n                **model_kwargs,\n            )\n        return model\n\n    def load_tokenizer(self, *args, **kwargs) -&gt; PreTrainedTokenizer:\n        \"\"\"Load the tokenizer associated with this model pool.\n\n        Loads a tokenizer based on the tokenizer configuration provided during\n        pool initialization. Supports both simple string paths and detailed\n        configuration dictionaries.\n\n        Args:\n            *args: Additional positional arguments passed to the tokenizer constructor.\n            **kwargs: Additional keyword arguments passed to the tokenizer constructor.\n\n        Returns:\n            PreTrainedTokenizer: The loaded tokenizer instance.\n\n        Raises:\n            AssertionError: If no tokenizer is defined in the configuration.\n\n        Example YAML configurations:\n            Simple string configuration:\n            ```yaml\n            tokenizer: google/gemma-2-2b-it\n            ```\n\n            Detailed configuration:\n            ```yaml\n            tokenizer:\n              _target_: transformers.AutoTokenizer\n              pretrained_model_name_or_path: google/gemma-2-2b-it\n              use_fast: true\n              padding_side: left\n            ```\n        \"\"\"\n        assert self.tokenizer is not None, \"Tokenizer is not defined in the config\"\n        log.info(\"Loading tokenizer.\", stacklevel=2)\n        if isinstance(self.tokenizer, str):\n            tokenizer = AutoTokenizer.from_pretrained(self.tokenizer, *args, **kwargs)\n        else:\n            tokenizer = instantiate(self.tokenizer, *args, **kwargs)\n        return tokenizer\n\n    @override\n    def save_model(\n        self,\n        model: PreTrainedModel,\n        path: str,\n        push_to_hub: bool = False,\n        model_dtype: Optional[str] = None,\n        save_tokenizer: bool = False,\n        tokenizer_kwargs=None,\n        tokenizer: Optional[PreTrainedTokenizer] = None,\n        algorithm_config: Optional[DictConfig] = None,\n        description: Optional[str] = None,\n        base_model_in_modelcard: bool = True,\n        **kwargs,\n    ):\n        \"\"\"Save a model to the specified path with optional tokenizer and Hub upload.\n\n        This method provides comprehensive model saving capabilities including\n        optional tokenizer saving, dtype conversion, model card creation, and\n        Hugging Face Hub upload. The model is saved in the standard Hugging Face format.\n\n        Args:\n            model: The PreTrainedModel instance to be saved.\n            path: The local path where the model will be saved. Supports tilde\n                expansion for home directory paths.\n            push_to_hub: Whether to push the saved model to the Hugging Face Hub.\n                Requires proper authentication and repository permissions.\n            model_dtype: Optional string specifying the target dtype for the model\n                before saving (e.g., \"float16\", \"bfloat16\"). The model will be\n                converted to this dtype before saving.\n            save_tokenizer: Whether to save the tokenizer alongside the model.\n                If True, the tokenizer will be loaded using the pool's tokenizer\n                configuration and saved to the same path.\n            tokenizer_kwargs: Additional keyword arguments for tokenizer loading\n                when save_tokenizer is True.\n            tokenizer: Optional pre-loaded tokenizer instance. If provided, this\n                tokenizer will be saved regardless of the save_tokenizer flag.\n            algorithm_config: Optional DictConfig containing algorithm configuration.\n                If provided, a model card will be created with algorithm details.\n            description: Optional description for the model card. If not provided\n                and algorithm_config is given, a default description will be generated.\n            **kwargs: Additional keyword arguments passed to the model's\n                save_pretrained method.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; pool = CausalLMPool(models=..., tokenizer=...)\n            &gt;&gt;&gt; model = pool.load_model(\"my_model\")\n            &gt;&gt;&gt; pool.save_model(\n            ...     model,\n            ...     \"/path/to/save\",\n            ...     save_tokenizer=True,\n            ...     model_dtype=\"float16\",\n            ...     push_to_hub=True,\n            ...     algorithm_config=algorithm_config,\n            ...     description=\"Custom merged model\"\n            ... )\n            ```\n        \"\"\"\n        path = os.path.expanduser(path)\n        # NOTE: if tokenizer is provided, it will be saved regardless of `save_tokenizer`\n        if save_tokenizer or tokenizer is not None:\n            if tokenizer is None:\n                if tokenizer_kwargs is None:\n                    tokenizer_kwargs = {}\n                # load the tokenizer\n                tokenizer = self.load_tokenizer(**tokenizer_kwargs)\n            tokenizer.save_pretrained(\n                path,\n                push_to_hub=push_to_hub,\n            )\n        if model_dtype is not None:\n            model.to(dtype=parse_dtype(model_dtype))\n        model.save_pretrained(\n            path,\n            push_to_hub=push_to_hub,\n            **kwargs,\n        )\n\n        # Create and save model card if algorithm_config is provided\n        if algorithm_config is not None:\n            if description is None:\n                description = \"Model created using FusionBench.\"\n            model_card_str = create_default_model_card(\n                base_model=(\n                    self.get_model_path(\"_pretrained_\")\n                    if base_model_in_modelcard and self.has_pretrained\n                    else None\n                ),\n                models=[self.get_model_path(m) for m in self.model_names],\n                description=description,\n                algorithm_config=algorithm_config,\n                modelpool_config=self.config,\n            )\n            with open(os.path.join(path, \"README.md\"), \"w\") as f:\n                f.write(model_card_str)\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.CausalLMPool.get_model_kwargs","title":"<code>get_model_kwargs()</code>","text":"<p>Get processed model keyword arguments for model loading.</p> <p>Converts the stored <code>model_kwargs</code> from DictConfig to a regular dictionary and processes special arguments like torch_dtype for proper model loading.</p> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>Processed keyword arguments ready to be passed to model loading functions. The torch_dtype field, if present, is converted from string to the appropriate torch dtype object.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/causal_lm/causal_lm.py</code> <pre><code>def get_model_kwargs(self):\n    \"\"\"Get processed model keyword arguments for model loading.\n\n    Converts the stored `model_kwargs` from DictConfig to a regular dictionary\n    and processes special arguments like torch_dtype for proper model loading.\n\n    Returns:\n        dict: Processed keyword arguments ready to be passed to model\n            loading functions. The torch_dtype field, if present, is\n            converted from string to the appropriate torch dtype object.\n    \"\"\"\n    model_kwargs = (\n        OmegaConf.to_container(self.model_kwargs, resolve=True)\n        if isinstance(self.model_kwargs, DictConfig)\n        else self.model_kwargs\n    )\n    if \"torch_dtype\" in model_kwargs:\n        model_kwargs[\"torch_dtype\"] = parse_dtype(model_kwargs[\"torch_dtype\"])\n    return model_kwargs\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.CausalLMPool.get_model_path","title":"<code>get_model_path(model_name)</code>","text":"<p>Extract the model path from the model configuration.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model as defined in the models configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>The path or identifier for the model. For string configurations, returns the string directly. For dict configurations, extracts the 'pretrained_model_name_or_path' field.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the model configuration is invalid or the model name is not found in the configuration.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/causal_lm/causal_lm.py</code> <pre><code>def get_model_path(self, model_name: str):\n    \"\"\"Extract the model path from the model configuration.\n\n    Args:\n        model_name: The name of the model as defined in the models configuration.\n\n    Returns:\n        str: The path or identifier for the model. For string configurations,\n            returns the string directly. For dict configurations, extracts\n            the 'pretrained_model_name_or_path' field.\n\n    Raises:\n        RuntimeError: If the model configuration is invalid or the model\n            name is not found in the configuration.\n    \"\"\"\n    model_name_or_config = self._models[model_name]\n    if isinstance(model_name_or_config, str):\n        return model_name_or_config\n    elif isinstance(model_name_or_config, (DictConfig, dict)):\n        return model_name_or_config.get(\"pretrained_model_name_or_path\")\n    else:\n        raise RuntimeError(\"Invalid model configuration\")\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.CausalLMPool.load_model","title":"<code>load_model(model_name_or_config, *args, **kwargs)</code>","text":"<p>Load a causal language model from the model pool.</p> <p>This method supports multiple loading strategies: 1. Loading by model name from the configured model pool 2. Loading from a direct configuration dictionary 3. Lazy loading using LazyStateDict for memory efficiency</p> <p>The method automatically handles different model configuration formats and applies the appropriate loading strategy based on the enable_lazy_loading flag.</p> <p>Parameters:</p> <ul> <li> <code>model_name_or_config</code>               (<code>str | DictConfig</code>)           \u2013            <p>Either a string model name that exists in the model pool configuration, or a DictConfig/dict containing the model configuration directly.</p> </li> <li> <code>*args</code>           \u2013            <p>Additional positional arguments passed to the model constructor.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments passed to the model constructor. These will be merged with the pool's model_kwargs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[PreTrainedModel, LazyStateDict]</code>           \u2013            <p>Union[PreTrainedModel, LazyStateDict]: The loaded model. Returns a PreTrainedModel for normal loading or a LazyStateDict for lazy loading.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the model configuration is invalid.</p> </li> <li> <code>KeyError</code>             \u2013            <p>If the model name is not found in the model pool.</p> </li> </ul> Example YAML configurations <p>Simple string configuration: <pre><code>models:\n  _pretrained_: path_to_pretrained_model\n  model_a: path_to_model_a\n  model_b: path_to_model_b\n</code></pre></p> <p>Detailed configuration: <pre><code>models:\n  _pretrained_:\n    _target_: transformers.AutoModelForCausalLM\n    pretrained_model_name_or_path: path_to_pretrained_model\n  model_a:\n    _target_: transformers.AutoModelForCausalLM\n    pretrained_model_name_or_path: path_to_model_a\n</code></pre></p> Source code in <code>fusion_bench/modelpool/causal_lm/causal_lm.py</code> <pre><code>@override\ndef load_model(\n    self,\n    model_name_or_config: str | DictConfig,\n    *args,\n    **kwargs,\n) -&gt; Union[PreTrainedModel, LazyStateDict]:\n    \"\"\"Load a causal language model from the model pool.\n\n    This method supports multiple loading strategies:\n    1. Loading by model name from the configured model pool\n    2. Loading from a direct configuration dictionary\n    3. Lazy loading using LazyStateDict for memory efficiency\n\n    The method automatically handles different model configuration formats\n    and applies the appropriate loading strategy based on the enable_lazy_loading flag.\n\n    Args:\n        model_name_or_config: Either a string model name that exists in the\n            model pool configuration, or a DictConfig/dict containing the\n            model configuration directly.\n        *args: Additional positional arguments passed to the model constructor.\n        **kwargs: Additional keyword arguments passed to the model constructor.\n            These will be merged with the pool's model_kwargs.\n\n    Returns:\n        Union[PreTrainedModel, LazyStateDict]: The loaded model. Returns a\n            PreTrainedModel for normal loading or a LazyStateDict for lazy loading.\n\n    Raises:\n        RuntimeError: If the model configuration is invalid.\n        KeyError: If the model name is not found in the model pool.\n\n    Example YAML configurations:\n        Simple string configuration:\n        ```yaml\n        models:\n          _pretrained_: path_to_pretrained_model\n          model_a: path_to_model_a\n          model_b: path_to_model_b\n        ```\n\n        Detailed configuration:\n        ```yaml\n        models:\n          _pretrained_:\n            _target_: transformers.AutoModelForCausalLM\n            pretrained_model_name_or_path: path_to_pretrained_model\n          model_a:\n            _target_: transformers.AutoModelForCausalLM\n            pretrained_model_name_or_path: path_to_model_a\n        ```\n    \"\"\"\n    model_kwargs = self.get_model_kwargs()\n    model_kwargs.update(kwargs)\n\n    if isinstance(model_name_or_config, str):\n        # If model_name_or_config is a string, it is the name or the path of the model\n        log.info(f\"Loading model: {model_name_or_config}\", stacklevel=2)\n        if model_name_or_config in self._models.keys():\n            model_config = self._models[model_name_or_config]\n            if isinstance(model_config, str):\n                # model_config is a string\n                if not self.enable_lazy_loading:\n                    model = AutoModelForCausalLM.from_pretrained(\n                        model_config,\n                        *args,\n                        **model_kwargs,\n                    )\n                else:\n                    # model_config is a string, but we want to use LazyStateDict\n                    model = LazyStateDict(\n                        checkpoint=model_config,\n                        meta_module_class=AutoModelForCausalLM,\n                        *args,\n                        **model_kwargs,\n                    )\n                return model\n    elif isinstance(model_name_or_config, (DictConfig, Dict)):\n        model_config = model_name_or_config\n\n    if not self.enable_lazy_loading:\n        model = instantiate(model_config, *args, **model_kwargs)\n    else:\n        meta_module_class = model_config.pop(\"_target_\")\n        checkpoint = model_config.pop(\"pretrained_model_name_or_path\")\n        model = LazyStateDict(\n            checkpoint=checkpoint,\n            meta_module_class=meta_module_class,\n            *args,\n            **model_kwargs,\n        )\n    return model\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.CausalLMPool.load_tokenizer","title":"<code>load_tokenizer(*args, **kwargs)</code>","text":"<p>Load the tokenizer associated with this model pool.</p> <p>Loads a tokenizer based on the tokenizer configuration provided during pool initialization. Supports both simple string paths and detailed configuration dictionaries.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>           \u2013            <p>Additional positional arguments passed to the tokenizer constructor.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments passed to the tokenizer constructor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PreTrainedTokenizer</code> (              <code>PreTrainedTokenizer</code> )          \u2013            <p>The loaded tokenizer instance.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AssertionError</code>             \u2013            <p>If no tokenizer is defined in the configuration.</p> </li> </ul> Example YAML configurations <p>Simple string configuration: <pre><code>tokenizer: google/gemma-2-2b-it\n</code></pre></p> <p>Detailed configuration: <pre><code>tokenizer:\n  _target_: transformers.AutoTokenizer\n  pretrained_model_name_or_path: google/gemma-2-2b-it\n  use_fast: true\n  padding_side: left\n</code></pre></p> Source code in <code>fusion_bench/modelpool/causal_lm/causal_lm.py</code> <pre><code>def load_tokenizer(self, *args, **kwargs) -&gt; PreTrainedTokenizer:\n    \"\"\"Load the tokenizer associated with this model pool.\n\n    Loads a tokenizer based on the tokenizer configuration provided during\n    pool initialization. Supports both simple string paths and detailed\n    configuration dictionaries.\n\n    Args:\n        *args: Additional positional arguments passed to the tokenizer constructor.\n        **kwargs: Additional keyword arguments passed to the tokenizer constructor.\n\n    Returns:\n        PreTrainedTokenizer: The loaded tokenizer instance.\n\n    Raises:\n        AssertionError: If no tokenizer is defined in the configuration.\n\n    Example YAML configurations:\n        Simple string configuration:\n        ```yaml\n        tokenizer: google/gemma-2-2b-it\n        ```\n\n        Detailed configuration:\n        ```yaml\n        tokenizer:\n          _target_: transformers.AutoTokenizer\n          pretrained_model_name_or_path: google/gemma-2-2b-it\n          use_fast: true\n          padding_side: left\n        ```\n    \"\"\"\n    assert self.tokenizer is not None, \"Tokenizer is not defined in the config\"\n    log.info(\"Loading tokenizer.\", stacklevel=2)\n    if isinstance(self.tokenizer, str):\n        tokenizer = AutoTokenizer.from_pretrained(self.tokenizer, *args, **kwargs)\n    else:\n        tokenizer = instantiate(self.tokenizer, *args, **kwargs)\n    return tokenizer\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.CausalLMPool.save_model","title":"<code>save_model(model, path, push_to_hub=False, model_dtype=None, save_tokenizer=False, tokenizer_kwargs=None, tokenizer=None, algorithm_config=None, description=None, base_model_in_modelcard=True, **kwargs)</code>","text":"<p>Save a model to the specified path with optional tokenizer and Hub upload.</p> <p>This method provides comprehensive model saving capabilities including optional tokenizer saving, dtype conversion, model card creation, and Hugging Face Hub upload. The model is saved in the standard Hugging Face format.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>PreTrainedModel</code>)           \u2013            <p>The PreTrainedModel instance to be saved.</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The local path where the model will be saved. Supports tilde expansion for home directory paths.</p> </li> <li> <code>push_to_hub</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to push the saved model to the Hugging Face Hub. Requires proper authentication and repository permissions.</p> </li> <li> <code>model_dtype</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional string specifying the target dtype for the model before saving (e.g., \"float16\", \"bfloat16\"). The model will be converted to this dtype before saving.</p> </li> <li> <code>save_tokenizer</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the tokenizer alongside the model. If True, the tokenizer will be loaded using the pool's tokenizer configuration and saved to the same path.</p> </li> <li> <code>tokenizer_kwargs</code>           \u2013            <p>Additional keyword arguments for tokenizer loading when save_tokenizer is True.</p> </li> <li> <code>tokenizer</code>               (<code>Optional[PreTrainedTokenizer]</code>, default:                   <code>None</code> )           \u2013            <p>Optional pre-loaded tokenizer instance. If provided, this tokenizer will be saved regardless of the save_tokenizer flag.</p> </li> <li> <code>algorithm_config</code>               (<code>Optional[DictConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Optional DictConfig containing algorithm configuration. If provided, a model card will be created with algorithm details.</p> </li> <li> <code>description</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional description for the model card. If not provided and algorithm_config is given, a default description will be generated.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments passed to the model's save_pretrained method.</p> </li> </ul> Example <pre><code>&gt;&gt;&gt; pool = CausalLMPool(models=..., tokenizer=...)\n&gt;&gt;&gt; model = pool.load_model(\"my_model\")\n&gt;&gt;&gt; pool.save_model(\n...     model,\n...     \"/path/to/save\",\n...     save_tokenizer=True,\n...     model_dtype=\"float16\",\n...     push_to_hub=True,\n...     algorithm_config=algorithm_config,\n...     description=\"Custom merged model\"\n... )\n</code></pre> Source code in <code>fusion_bench/modelpool/causal_lm/causal_lm.py</code> <pre><code>@override\ndef save_model(\n    self,\n    model: PreTrainedModel,\n    path: str,\n    push_to_hub: bool = False,\n    model_dtype: Optional[str] = None,\n    save_tokenizer: bool = False,\n    tokenizer_kwargs=None,\n    tokenizer: Optional[PreTrainedTokenizer] = None,\n    algorithm_config: Optional[DictConfig] = None,\n    description: Optional[str] = None,\n    base_model_in_modelcard: bool = True,\n    **kwargs,\n):\n    \"\"\"Save a model to the specified path with optional tokenizer and Hub upload.\n\n    This method provides comprehensive model saving capabilities including\n    optional tokenizer saving, dtype conversion, model card creation, and\n    Hugging Face Hub upload. The model is saved in the standard Hugging Face format.\n\n    Args:\n        model: The PreTrainedModel instance to be saved.\n        path: The local path where the model will be saved. Supports tilde\n            expansion for home directory paths.\n        push_to_hub: Whether to push the saved model to the Hugging Face Hub.\n            Requires proper authentication and repository permissions.\n        model_dtype: Optional string specifying the target dtype for the model\n            before saving (e.g., \"float16\", \"bfloat16\"). The model will be\n            converted to this dtype before saving.\n        save_tokenizer: Whether to save the tokenizer alongside the model.\n            If True, the tokenizer will be loaded using the pool's tokenizer\n            configuration and saved to the same path.\n        tokenizer_kwargs: Additional keyword arguments for tokenizer loading\n            when save_tokenizer is True.\n        tokenizer: Optional pre-loaded tokenizer instance. If provided, this\n            tokenizer will be saved regardless of the save_tokenizer flag.\n        algorithm_config: Optional DictConfig containing algorithm configuration.\n            If provided, a model card will be created with algorithm details.\n        description: Optional description for the model card. If not provided\n            and algorithm_config is given, a default description will be generated.\n        **kwargs: Additional keyword arguments passed to the model's\n            save_pretrained method.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; pool = CausalLMPool(models=..., tokenizer=...)\n        &gt;&gt;&gt; model = pool.load_model(\"my_model\")\n        &gt;&gt;&gt; pool.save_model(\n        ...     model,\n        ...     \"/path/to/save\",\n        ...     save_tokenizer=True,\n        ...     model_dtype=\"float16\",\n        ...     push_to_hub=True,\n        ...     algorithm_config=algorithm_config,\n        ...     description=\"Custom merged model\"\n        ... )\n        ```\n    \"\"\"\n    path = os.path.expanduser(path)\n    # NOTE: if tokenizer is provided, it will be saved regardless of `save_tokenizer`\n    if save_tokenizer or tokenizer is not None:\n        if tokenizer is None:\n            if tokenizer_kwargs is None:\n                tokenizer_kwargs = {}\n            # load the tokenizer\n            tokenizer = self.load_tokenizer(**tokenizer_kwargs)\n        tokenizer.save_pretrained(\n            path,\n            push_to_hub=push_to_hub,\n        )\n    if model_dtype is not None:\n        model.to(dtype=parse_dtype(model_dtype))\n    model.save_pretrained(\n        path,\n        push_to_hub=push_to_hub,\n        **kwargs,\n    )\n\n    # Create and save model card if algorithm_config is provided\n    if algorithm_config is not None:\n        if description is None:\n            description = \"Model created using FusionBench.\"\n        model_card_str = create_default_model_card(\n            base_model=(\n                self.get_model_path(\"_pretrained_\")\n                if base_model_in_modelcard and self.has_pretrained\n                else None\n            ),\n            models=[self.get_model_path(m) for m in self.model_names],\n            description=description,\n            algorithm_config=algorithm_config,\n            modelpool_config=self.config,\n        )\n        with open(os.path.join(path, \"README.md\"), \"w\") as f:\n            f.write(model_card_str)\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.CausalLMBackbonePool","title":"<code>CausalLMBackbonePool</code>","text":"<p>               Bases: <code>CausalLMPool</code></p> <p>A specialized model pool that loads only the transformer backbone layers.</p> <p>This class extends CausalLMPool to provide access to just the transformer layers (backbone) of causal language models, excluding the language modeling head and embeddings. This is useful for model fusion scenarios where only the core transformer layers are needed.</p> <p>The class automatically extracts the <code>model.layers</code> component from loaded AutoModelForCausalLM instances, providing direct access to the transformer blocks. Lazy loading is not supported for this pool type.</p> Note <p>This pool automatically disables lazy loading as it needs to access the internal structure of the model to extract the backbone layers.</p> Example <pre><code>&gt;&gt;&gt; backbone_pool = CausalLMBackbonePool(\n...     models={\"model_a\": \"microsoft/DialoGPT-medium\"},\n...     tokenizer=\"microsoft/DialoGPT-medium\"\n... )\n&gt;&gt;&gt; layers = backbone_pool.load_model(\"model_a\")  # Returns nn.ModuleList of transformer layers\n</code></pre> Source code in <code>fusion_bench/modelpool/causal_lm/causal_lm.py</code> <pre><code>class CausalLMBackbonePool(CausalLMPool):\n    \"\"\"A specialized model pool that loads only the transformer backbone layers.\n\n    This class extends CausalLMPool to provide access to just the transformer\n    layers (backbone) of causal language models, excluding the language modeling\n    head and embeddings. This is useful for model fusion scenarios where only\n    the core transformer layers are needed.\n\n    The class automatically extracts the `model.layers` component from loaded\n    AutoModelForCausalLM instances, providing direct access to the transformer\n    blocks. Lazy loading is not supported for this pool type.\n\n    Note:\n        This pool automatically disables lazy loading as it needs to access\n        the internal structure of the model to extract the backbone layers.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; backbone_pool = CausalLMBackbonePool(\n        ...     models={\"model_a\": \"microsoft/DialoGPT-medium\"},\n        ...     tokenizer=\"microsoft/DialoGPT-medium\"\n        ... )\n        &gt;&gt;&gt; layers = backbone_pool.load_model(\"model_a\")  # Returns nn.ModuleList of transformer layers\n        ```\n    \"\"\"\n\n    def load_model(\n        self, model_name_or_config: str | DictConfig, *args, **kwargs\n    ) -&gt; Module:\n        \"\"\"Load only the transformer backbone layers from a causal language model.\n\n        This method loads a complete causal language model and then extracts\n        only the transformer layers (backbone), discarding the embedding layers\n        and language modeling head. This is useful for model fusion scenarios\n        where only the core transformer computation is needed.\n\n        Args:\n            model_name_or_config: Either a string model name from the pool\n                configuration or a DictConfig with model loading parameters.\n            *args: Additional positional arguments passed to the parent load_model method.\n            **kwargs: Additional keyword arguments passed to the parent load_model method.\n\n        Returns:\n            Module: The transformer layers (typically a nn.ModuleList) containing\n                the core transformer blocks without embeddings or output heads.\n\n        Note:\n            Lazy loading is automatically disabled for this method as it needs\n            to access the internal model structure to extract the layers.\n        \"\"\"\n        if self.enable_lazy_loading:\n            log.warning(\n                \"CausalLMBackbonePool does not support lazy loading. \"\n                \"Falling back to normal loading.\"\n            )\n            self.enable_lazy_loading = False\n        model: AutoModelForCausalLM = super().load_model(\n            model_name_or_config, *args, **kwargs\n        )\n        return model.model.layers\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.CausalLMBackbonePool.load_model","title":"<code>load_model(model_name_or_config, *args, **kwargs)</code>","text":"<p>Load only the transformer backbone layers from a causal language model.</p> <p>This method loads a complete causal language model and then extracts only the transformer layers (backbone), discarding the embedding layers and language modeling head. This is useful for model fusion scenarios where only the core transformer computation is needed.</p> <p>Parameters:</p> <ul> <li> <code>model_name_or_config</code>               (<code>str | DictConfig</code>)           \u2013            <p>Either a string model name from the pool configuration or a DictConfig with model loading parameters.</p> </li> <li> <code>*args</code>           \u2013            <p>Additional positional arguments passed to the parent load_model method.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments passed to the parent load_model method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code> (              <code>Module</code> )          \u2013            <p>The transformer layers (typically a nn.ModuleList) containing the core transformer blocks without embeddings or output heads.</p> </li> </ul> Note <p>Lazy loading is automatically disabled for this method as it needs to access the internal model structure to extract the layers.</p> Source code in <code>fusion_bench/modelpool/causal_lm/causal_lm.py</code> <pre><code>def load_model(\n    self, model_name_or_config: str | DictConfig, *args, **kwargs\n) -&gt; Module:\n    \"\"\"Load only the transformer backbone layers from a causal language model.\n\n    This method loads a complete causal language model and then extracts\n    only the transformer layers (backbone), discarding the embedding layers\n    and language modeling head. This is useful for model fusion scenarios\n    where only the core transformer computation is needed.\n\n    Args:\n        model_name_or_config: Either a string model name from the pool\n            configuration or a DictConfig with model loading parameters.\n        *args: Additional positional arguments passed to the parent load_model method.\n        **kwargs: Additional keyword arguments passed to the parent load_model method.\n\n    Returns:\n        Module: The transformer layers (typically a nn.ModuleList) containing\n            the core transformer blocks without embeddings or output heads.\n\n    Note:\n        Lazy loading is automatically disabled for this method as it needs\n        to access the internal model structure to extract the layers.\n    \"\"\"\n    if self.enable_lazy_loading:\n        log.warning(\n            \"CausalLMBackbonePool does not support lazy loading. \"\n            \"Falling back to normal loading.\"\n        )\n        self.enable_lazy_loading = False\n    model: AutoModelForCausalLM = super().load_model(\n        model_name_or_config, *args, **kwargs\n    )\n    return model.model.layers\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.causal_lm.load_peft_causal_lm","title":"<code>load_peft_causal_lm(base_model_path, peft_model_path, torch_dtype='bfloat16', is_trainable=True, merge_and_unload=False)</code>","text":"<p>Load a causal language model with PEFT (Parameter-Efficient Fine-Tuning) adapters.</p> <p>This function loads a base causal language model and applies PEFT adapters (such as LoRA, AdaLoRA, or other parameter-efficient fine-tuning methods) to create a fine-tuned model. It supports both keeping the adapters separate or merging them into the base model.</p> <p>Parameters:</p> <ul> <li> <code>base_model_path</code>               (<code>str</code>)           \u2013            <p>Path or identifier for the base causal language model. Can be a Hugging Face model name or local path.</p> </li> <li> <code>peft_model_path</code>               (<code>str</code>)           \u2013            <p>Path to the PEFT adapter configuration and weights. This should contain the adapter_config.json and adapter weights.</p> </li> <li> <code>torch_dtype</code>               (<code>str</code>, default:                   <code>'bfloat16'</code> )           \u2013            <p>The torch data type to use for the model. Common options include \"float16\", \"bfloat16\", \"float32\". Defaults to \"bfloat16\".</p> </li> <li> <code>is_trainable</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether the loaded PEFT model should be trainable. Set to False for inference-only usage to save memory.</p> </li> <li> <code>merge_and_unload</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to merge the PEFT adapters into the base model and unload the adapter weights. When True, returns a standard PreTrainedModel instead of a PeftModel.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Union[PeftModel, PreTrainedModel]: The loaded model with PEFT adapters. Returns a PeftModel if merge_and_unload is False, or a PreTrainedModel if the adapters are merged and unloaded.</p> </li> </ul> Example <pre><code>&gt;&gt;&gt; # Load model with adapters for training\n&gt;&gt;&gt; model = load_peft_causal_lm(\n...     \"microsoft/DialoGPT-medium\",\n...     \"/path/to/lora/adapters\",\n...     is_trainable=True\n... )\n\n&gt;&gt;&gt; # Load and merge adapters for inference\n&gt;&gt;&gt; merged_model = load_peft_causal_lm(\n...     \"microsoft/DialoGPT-medium\",\n...     \"/path/to/lora/adapters\",\n...     merge_and_unload=True,\n...     is_trainable=False\n... )\n</code></pre> Source code in <code>fusion_bench/modelpool/causal_lm/causal_lm.py</code> <pre><code>def load_peft_causal_lm(\n    base_model_path: str,\n    peft_model_path: str,\n    torch_dtype: str = \"bfloat16\",\n    is_trainable: bool = True,\n    merge_and_unload: bool = False,\n):\n    \"\"\"Load a causal language model with PEFT (Parameter-Efficient Fine-Tuning) adapters.\n\n    This function loads a base causal language model and applies PEFT adapters\n    (such as LoRA, AdaLoRA, or other parameter-efficient fine-tuning methods)\n    to create a fine-tuned model. It supports both keeping the adapters separate\n    or merging them into the base model.\n\n    Args:\n        base_model_path: Path or identifier for the base causal language model.\n            Can be a Hugging Face model name or local path.\n        peft_model_path: Path to the PEFT adapter configuration and weights.\n            This should contain the adapter_config.json and adapter weights.\n        torch_dtype: The torch data type to use for the model. Common options\n            include \"float16\", \"bfloat16\", \"float32\". Defaults to \"bfloat16\".\n        is_trainable: Whether the loaded PEFT model should be trainable.\n            Set to False for inference-only usage to save memory.\n        merge_and_unload: Whether to merge the PEFT adapters into the base model\n            and unload the adapter weights. When True, returns a standard\n            PreTrainedModel instead of a PeftModel.\n\n    Returns:\n        Union[PeftModel, PreTrainedModel]: The loaded model with PEFT adapters.\n            Returns a PeftModel if merge_and_unload is False, or a PreTrainedModel\n            if the adapters are merged and unloaded.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; # Load model with adapters for training\n        &gt;&gt;&gt; model = load_peft_causal_lm(\n        ...     \"microsoft/DialoGPT-medium\",\n        ...     \"/path/to/lora/adapters\",\n        ...     is_trainable=True\n        ... )\n\n        &gt;&gt;&gt; # Load and merge adapters for inference\n        &gt;&gt;&gt; merged_model = load_peft_causal_lm(\n        ...     \"microsoft/DialoGPT-medium\",\n        ...     \"/path/to/lora/adapters\",\n        ...     merge_and_unload=True,\n        ...     is_trainable=False\n        ... )\n        ```\n    \"\"\"\n    base_model = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch_dtype\n    )\n    model = peft.PeftModel.from_pretrained(\n        base_model,\n        peft_model_path,\n        is_trainable=is_trainable,\n    )\n    if merge_and_unload:\n        model = model.merge_and_unload()\n    return model\n</code></pre>"},{"location":"api/fusion_bench.modelpool/#others","title":"Others","text":""},{"location":"api/fusion_bench.modelpool/#transformers-automodel","title":"Transformers AutoModel","text":""},{"location":"api/fusion_bench.modelpool/#fusion_bench.modelpool.AutoModelPool","title":"<code>AutoModelPool</code>","text":"<p>               Bases: <code>ModelPool</code></p> Source code in <code>fusion_bench/modelpool/huggingface_automodel.py</code> <pre><code>class AutoModelPool(ModelPool):\n    def load_model(self, model_config: str | DictConfig) -&gt; Module:\n        if isinstance(model_config, str):\n            model_config = self.get_model_config(model_config)\n        else:\n            model_config = model_config\n\n        model = AutoModel.from_pretrained(model_config.path)\n        return model\n</code></pre>"},{"location":"api/fusion_bench.models/","title":"fusion_bench.models","text":""},{"location":"api/fusion_bench.models/#task-and-layer-wise-merging-adamerging","title":"Task and Layer-wise Merging (AdaMerging)","text":""},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.layer_wise_fusion","title":"<code>layer_wise_fusion</code>","text":""},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.layer_wise_fusion.LayerWiseMergedModel","title":"<code>LayerWiseMergedModel</code>","text":"<p>               Bases: <code>Module</code>, <code>Generic[TorchModelType]</code></p> Source code in <code>fusion_bench/models/wrappers/layer_wise_fusion.py</code> <pre><code>class LayerWiseMergedModel(nn.Module, Generic[TorchModelType]):\n    _merged_state_dict: StateDictType = None\n\n    def __init__(\n        self,\n        layer_wise_weight: Tensor,\n        pretrained_model: TorchModelType,\n        finetuned_models: List[TorchModelType],\n        clamp_weights: bool = True,\n        tie_weights: bool = False,\n        strict: bool = True,\n        sparsity_ratio: Optional[float] = None,\n        normalized_merging_weights: bool = False,\n    ):\n        R\"\"\"\n        This class wraps a pretrained model and a list of finetuned models, and merges the weights of the finetuned models into the pretrained model using layer-wise fusion.\n\n        Reference:\n\n            (ICLR 2024) Yang E, Wang Z, Shen L, et al. Adamerging: Adaptive model merging for multi-task learning. https://arxiv.org/pdf/2310.02575\n\n        Args:\n            layer_wise_weight (Tensor): A tensor of shape (num_models, num_layers) representing the weight of each layer for each model.\n            pretrained_model (nn.Module): The pretrained model to merge the weights into.\n            finetuned_models (List[nn.Module]): A list of finetuned models to merge the weights from. This should have the same architecture as the pretrained model. We use these models to compute the task vectors.\n            clamp_weights (bool, optional): If True, the layer-wise weights will be clamped to [0, 1]. Defaults to True.\n            tie_weights (bool, optional): This option passes the `tie_weights` argument to the `functional_call` function. Defaults to False.\n            strict (bool, optional): This option passes the `strict` argument to the `functional_call` function. Defaults to True.\n            sparsity_ratio (float, optional): If `sparsity_ratio` is provided, the task vector will be pruned before merging. A high spasity level can save the memory usage during merging.\n            normalized_merging_weights (bool, optional): If True, the layer-wise weights will be normalized for each layer, so that the sum of weights across models for each layer is 1. Defaults to False.\n        \"\"\"\n        super().__init__()\n        self.clamp_weights = clamp_weights\n        self.tie_weights = tie_weights\n        self.strict = strict\n        self.sparsity_ratio = sparsity_ratio\n        self.nromalized_merging_weights = normalized_merging_weights\n\n        self.merge_weight = nn.Parameter(layer_wise_weight, requires_grad=True)\n\n        for name, param in pretrained_model.named_parameters():\n            if not param.requires_grad:\n                for m in finetuned_models:\n                    del_attr(m, name.split(\".\"))\n            else:\n                for m in finetuned_models:\n                    get_attr(m, name.split(\".\")).data = (\n                        get_attr(m, name.split(\".\")) - param\n                    )\n\n        self.pretrained_model = pretrained_model.requires_grad_(False)\n        for m in finetuned_models:\n            m.requires_grad_(False)\n\n        self.task_vectors = nn.ModuleList(finetuned_models)\n\n        # if `sparisty_ratio` is given, pruning the task vectors.\n        if sparsity_ratio is not None:\n            from fusion_bench.method.pruning.prune_utils import (\n                unstructured_magnitude_prune_,\n            )\n\n            for name, param in self.task_vectors.named_parameters():\n                if param.dim() != 2:\n                    continue\n                print(f\"pruning {name}\")\n                pruned_param = unstructured_magnitude_prune_(\n                    param.data.clone(), torch.abs, sparsity_ratio=sparsity_ratio\n                )\n                set_attr(\n                    self.task_vectors,\n                    name.split(\".\"),\n                    nn.Parameter(pruned_param.to_sparse(), requires_grad=False),\n                )\n\n    @property\n    def forward_model(self):\n        return functools.partial(\n            functional_call,\n            self.pretrained_model,\n            self._merged_state_dict,\n            tie_weights=self.tie_weights,\n            strict=self.strict,\n        )\n\n    def merge_and_unload(self, task_vector_mask: Optional[Dict[str, Tensor]] = None):\n        self.merge_weights(task_vector_mask=task_vector_mask)\n        self.pretrained_model.load_state_dict(self._merged_state_dict)\n        return self.pretrained_model\n\n    def merge_weights(self, task_vector_mask: Optional[Dict[str, Tensor]] = None):\n        \"\"\"\n        Merges the weights of the model.\n        Call this after each update step.\n        \"\"\"\n        if self.clamp_weights:\n            layer_wise_weight = self.merge_weight.clamp(0, 1)\n        else:\n            layer_wise_weight = self.merge_weight\n        if self.nromalized_merging_weights:\n            # normalize the weights for each layer, so that the sum of weights across models for each layer is 1.\n            layer_wise_weight = layer_wise_weight.softmax(dim=0)\n\n        state_dict = self.pretrained_model.state_dict(keep_vars=True)\n        # shape of layer_wise_weight: (num_models, num_layers)\n        for weight, task_vector in zip(layer_wise_weight, self.task_vectors):\n            assert len(list(task_vector.named_parameters())) == weight.size(0)\n            if task_vector_mask is not None:\n                weight = [\n                    w * task_vector_mask[name]\n                    for w, (name, param) in zip(weight, task_vector.named_parameters())\n                ]\n            for w, (name, param) in zip(weight, task_vector.named_parameters()):\n                state_dict[name] = state_dict[name] + param * w\n        self._merged_state_dict = state_dict\n\n        return state_dict\n\n    def forward(self, *args, **kwargs):\n        if self._merged_state_dict is None:\n            self.merge_weights()\n        return self.forward_model(args=args, kwargs=kwargs)\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.layer_wise_fusion.LayerWiseMergedModel.__init__","title":"<code>__init__(layer_wise_weight, pretrained_model, finetuned_models, clamp_weights=True, tie_weights=False, strict=True, sparsity_ratio=None, normalized_merging_weights=False)</code>","text":"<p>This class wraps a pretrained model and a list of finetuned models, and merges the weights of the finetuned models into the pretrained model using layer-wise fusion.</p> <p>Reference:</p> <pre><code>(ICLR 2024) Yang E, Wang Z, Shen L, et al. Adamerging: Adaptive model merging for multi-task learning. https://arxiv.org/pdf/2310.02575\n</code></pre> <p>Parameters:</p> <ul> <li> <code>layer_wise_weight</code>               (<code>Tensor</code>)           \u2013            <p>A tensor of shape (num_models, num_layers) representing the weight of each layer for each model.</p> </li> <li> <code>pretrained_model</code>               (<code>Module</code>)           \u2013            <p>The pretrained model to merge the weights into.</p> </li> <li> <code>finetuned_models</code>               (<code>List[Module]</code>)           \u2013            <p>A list of finetuned models to merge the weights from. This should have the same architecture as the pretrained model. We use these models to compute the task vectors.</p> </li> <li> <code>clamp_weights</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, the layer-wise weights will be clamped to [0, 1]. Defaults to True.</p> </li> <li> <code>tie_weights</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>This option passes the <code>tie_weights</code> argument to the <code>functional_call</code> function. Defaults to False.</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>This option passes the <code>strict</code> argument to the <code>functional_call</code> function. Defaults to True.</p> </li> <li> <code>sparsity_ratio</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>If <code>sparsity_ratio</code> is provided, the task vector will be pruned before merging. A high spasity level can save the memory usage during merging.</p> </li> <li> <code>normalized_merging_weights</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the layer-wise weights will be normalized for each layer, so that the sum of weights across models for each layer is 1. Defaults to False.</p> </li> </ul> Source code in <code>fusion_bench/models/wrappers/layer_wise_fusion.py</code> <pre><code>def __init__(\n    self,\n    layer_wise_weight: Tensor,\n    pretrained_model: TorchModelType,\n    finetuned_models: List[TorchModelType],\n    clamp_weights: bool = True,\n    tie_weights: bool = False,\n    strict: bool = True,\n    sparsity_ratio: Optional[float] = None,\n    normalized_merging_weights: bool = False,\n):\n    R\"\"\"\n    This class wraps a pretrained model and a list of finetuned models, and merges the weights of the finetuned models into the pretrained model using layer-wise fusion.\n\n    Reference:\n\n        (ICLR 2024) Yang E, Wang Z, Shen L, et al. Adamerging: Adaptive model merging for multi-task learning. https://arxiv.org/pdf/2310.02575\n\n    Args:\n        layer_wise_weight (Tensor): A tensor of shape (num_models, num_layers) representing the weight of each layer for each model.\n        pretrained_model (nn.Module): The pretrained model to merge the weights into.\n        finetuned_models (List[nn.Module]): A list of finetuned models to merge the weights from. This should have the same architecture as the pretrained model. We use these models to compute the task vectors.\n        clamp_weights (bool, optional): If True, the layer-wise weights will be clamped to [0, 1]. Defaults to True.\n        tie_weights (bool, optional): This option passes the `tie_weights` argument to the `functional_call` function. Defaults to False.\n        strict (bool, optional): This option passes the `strict` argument to the `functional_call` function. Defaults to True.\n        sparsity_ratio (float, optional): If `sparsity_ratio` is provided, the task vector will be pruned before merging. A high spasity level can save the memory usage during merging.\n        normalized_merging_weights (bool, optional): If True, the layer-wise weights will be normalized for each layer, so that the sum of weights across models for each layer is 1. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.clamp_weights = clamp_weights\n    self.tie_weights = tie_weights\n    self.strict = strict\n    self.sparsity_ratio = sparsity_ratio\n    self.nromalized_merging_weights = normalized_merging_weights\n\n    self.merge_weight = nn.Parameter(layer_wise_weight, requires_grad=True)\n\n    for name, param in pretrained_model.named_parameters():\n        if not param.requires_grad:\n            for m in finetuned_models:\n                del_attr(m, name.split(\".\"))\n        else:\n            for m in finetuned_models:\n                get_attr(m, name.split(\".\")).data = (\n                    get_attr(m, name.split(\".\")) - param\n                )\n\n    self.pretrained_model = pretrained_model.requires_grad_(False)\n    for m in finetuned_models:\n        m.requires_grad_(False)\n\n    self.task_vectors = nn.ModuleList(finetuned_models)\n\n    # if `sparisty_ratio` is given, pruning the task vectors.\n    if sparsity_ratio is not None:\n        from fusion_bench.method.pruning.prune_utils import (\n            unstructured_magnitude_prune_,\n        )\n\n        for name, param in self.task_vectors.named_parameters():\n            if param.dim() != 2:\n                continue\n            print(f\"pruning {name}\")\n            pruned_param = unstructured_magnitude_prune_(\n                param.data.clone(), torch.abs, sparsity_ratio=sparsity_ratio\n            )\n            set_attr(\n                self.task_vectors,\n                name.split(\".\"),\n                nn.Parameter(pruned_param.to_sparse(), requires_grad=False),\n            )\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.layer_wise_fusion.LayerWiseMergedModel.merge_weights","title":"<code>merge_weights(task_vector_mask=None)</code>","text":"<p>Merges the weights of the model. Call this after each update step.</p> Source code in <code>fusion_bench/models/wrappers/layer_wise_fusion.py</code> <pre><code>def merge_weights(self, task_vector_mask: Optional[Dict[str, Tensor]] = None):\n    \"\"\"\n    Merges the weights of the model.\n    Call this after each update step.\n    \"\"\"\n    if self.clamp_weights:\n        layer_wise_weight = self.merge_weight.clamp(0, 1)\n    else:\n        layer_wise_weight = self.merge_weight\n    if self.nromalized_merging_weights:\n        # normalize the weights for each layer, so that the sum of weights across models for each layer is 1.\n        layer_wise_weight = layer_wise_weight.softmax(dim=0)\n\n    state_dict = self.pretrained_model.state_dict(keep_vars=True)\n    # shape of layer_wise_weight: (num_models, num_layers)\n    for weight, task_vector in zip(layer_wise_weight, self.task_vectors):\n        assert len(list(task_vector.named_parameters())) == weight.size(0)\n        if task_vector_mask is not None:\n            weight = [\n                w * task_vector_mask[name]\n                for w, (name, param) in zip(weight, task_vector.named_parameters())\n            ]\n        for w, (name, param) in zip(weight, task_vector.named_parameters()):\n            state_dict[name] = state_dict[name] + param * w\n    self._merged_state_dict = state_dict\n\n    return state_dict\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.layer_wise_fusion.fix_other_parts","title":"<code>fix_other_parts(module)</code>","text":"<p>Sets all parameters in the module to not require gradients, except for the merge weights in <code>LayerWiseMergedModel</code> instances.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The module to process.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The module with updated parameter requirements.</p> </li> </ul> Source code in <code>fusion_bench/models/wrappers/layer_wise_fusion.py</code> <pre><code>def fix_other_parts(module: nn.Module):\n    \"\"\"\n    Sets all parameters in the module to not require gradients, except for the merge weights\n    in `LayerWiseMergedModel` instances.\n\n    Args:\n        module (nn.Module): The module to process.\n\n    Returns:\n        nn.Module: The module with updated parameter requirements.\n    \"\"\"\n    module.requires_grad_(False)\n    for submodule in module.modules():\n        if isinstance(submodule, LayerWiseMergedModel):\n            submodule.merge_weight.requires_grad_(True)\n    return module\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.layer_wise_fusion.fuse_weights","title":"<code>fuse_weights(layer_wise_weight, state_dicts)</code>","text":"<p>Fuse the weights of multiple models using layer-wise fusion.</p> <p>Parameters:</p> <ul> <li> <code>layer_wise_weight</code>               (<code>Tensor</code>)           \u2013            <p>A tensor of shape (num_models, num_layers) representing the weight of each layer for each model.</p> </li> <li> <code>state_dicts</code>               (<code>List[StateDict]</code>)           \u2013            <p>A list of state dictionaries, one for each model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>A dictionary mapping each weight tensor key to the fused weight tensor.</p> </li> </ul> Source code in <code>fusion_bench/models/wrappers/layer_wise_fusion.py</code> <pre><code>def fuse_weights(\n    layer_wise_weight: Tensor, state_dicts: List[StateDictType]\n) -&gt; StateDictType:\n    \"\"\"\n    Fuse the weights of multiple models using layer-wise fusion.\n\n    Args:\n        layer_wise_weight (Tensor): A tensor of shape (num_models, num_layers) representing the weight of each layer for each model.\n        state_dicts (List[StateDict]): A list of state dictionaries, one for each model.\n\n    Returns:\n        A dictionary mapping each weight tensor key to the fused weight tensor.\n    \"\"\"\n    num_models = len(state_dicts)\n    num_layers = len(state_dicts[0])\n    assert layer_wise_weight.shape == (\n        num_models,\n        num_layers,\n    ), f\"layer_wise_weight.shape={layer_wise_weight.shape}, expected (num_models, num_layers): ({num_models}, {num_layers})\"\n    return {\n        k: _fuse_weights(\n            layer_wise_weight[:, i], [state_dict[k] for state_dict in state_dicts]\n        )\n        for i, k in enumerate(state_dicts[0].keys())\n    }\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.layer_wise_fusion.get_layer_wise_weights","title":"<code>get_layer_wise_weights(num_models, num_layers, init_values=None, dtype=torch.float32)</code>","text":"<p>Return a tensor of layer-wise weights for the given number of models and layers.</p> <p>Parameters:</p> <ul> <li> <code>num_models</code>               (<code>int</code>)           \u2013            <p>The number of models to fuse.</p> </li> <li> <code>num_layers</code>               (<code>int</code>)           \u2013            <p>The number of layers in each model.</p> </li> <li> <code>init_values</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The initial value for each weight. Defaults to 1.0 / num_models.</p> </li> <li> <code>dtype</code>               (<code>dtype</code>, default:                   <code>float32</code> )           \u2013            <p>dtype of weights. This should be the same with model dtype.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>          \u2013            <p>A tensor of shape (num_models, num_layers) containing the layer-wise weights.</p> </li> </ul> Source code in <code>fusion_bench/models/wrappers/layer_wise_fusion.py</code> <pre><code>def get_layer_wise_weights(\n    num_models: int,\n    num_layers: int,\n    init_values: float = None,\n    dtype: torch.dtype = torch.float32,\n):\n    \"\"\"\n    Return a tensor of layer-wise weights for the given number of models and layers.\n\n    Args:\n        num_models (int): The number of models to fuse.\n        num_layers (int): The number of layers in each model.\n        init_values (float, optional): The initial value for each weight. Defaults to 1.0 / num_models.\n        dtype (torch.dtype): dtype of weights. This should be the same with model dtype.\n\n    Returns:\n        Tensor: A tensor of shape (num_models, num_layers) containing the layer-wise weights.\n    \"\"\"\n    assert num_models &gt;= 1, f\"num_models must be &gt;= 1, got {num_models}\"\n    assert num_layers &gt;= 1, f\"num_layers must be &gt;= 1, got {num_layers}\"\n    if init_values is None:\n        init_values = 1.0 / num_models\n    return torch.full((num_models, num_layers), init_values, dtype=dtype)\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.layer_wise_fusion.merge_and_unload","title":"<code>merge_and_unload(module)</code>","text":"<p>Merges and unloads all <code>LayerWiseMergedModel</code> instances within the given module.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The module to process.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The updated module with merged weights.</p> </li> </ul> Source code in <code>fusion_bench/models/wrappers/layer_wise_fusion.py</code> <pre><code>def merge_and_unload(module: nn.Module):\n    \"\"\"\n    Merges and unloads all `LayerWiseMergedModel` instances within the given module.\n\n    Args:\n        module (nn.Module): The module to process.\n\n    Returns:\n        nn.Module: The updated module with merged weights.\n    \"\"\"\n    if isinstance(module, LayerWiseMergedModel):\n        return module.merge_and_unload()\n    else:\n        for name, submodule in module.named_children():\n            need_merge = isinstance(submodule, LayerWiseMergedModel)\n            submodule = merge_and_unload(submodule)\n            if need_merge:\n                setattr(module, name, submodule)\n        return module\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.layer_wise_fusion.merge_weights","title":"<code>merge_weights(module)</code>","text":"<p>Merges the weights for all <code>LayerWiseMergedModel</code> instances within the given module.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The module to process.</p> </li> </ul> Source code in <code>fusion_bench/models/wrappers/layer_wise_fusion.py</code> <pre><code>def merge_weights(module: nn.Module):\n    \"\"\"\n    Merges the weights for all `LayerWiseMergedModel` instances within the given module.\n\n    Args:\n        module (nn.Module): The module to process.\n    \"\"\"\n    if isinstance(module, LayerWiseMergedModel):\n        module.merge_weights()\n        return\n    else:\n        for submodule in module.children():\n            merge_weights(submodule)\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.task_wise_fusion","title":"<code>task_wise_fusion</code>","text":"<pre><code># Get the task-wise weights\ntask_wise_weights = get_task_wise_weights(num_models)\n\n# Define the task vectors (in this case, we'll use the state_dict of the pretrained model)\ntask_vectors = ...\n\n# Initialize the TaskWiseMergedModel\nmerged_model = TaskWiseMergedModel(pretrained_model, task_wise_weights, task_vectors)\n\n# Now you can use the merged_model like a regular PyTorch model\noutputs = merged_model(inputs)\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.task_wise_fusion.TaskWiseMergedModel","title":"<code>TaskWiseMergedModel</code>","text":"<p>               Bases: <code>Module</code>, <code>Generic[TorchModelType]</code></p> <p>A PyTorch module that dynamically merges multiple fine-tuned models using learnable task-wise weights.</p> <p>This class implements a sophisticated model fusion approach where multiple task-specific models are combined with a pretrained base model using learnable weights. The fusion is performed using task vectors (differences between fine-tuned and pretrained models) that are weighted and added to the base model's parameters.</p> <p>The key innovation is that the merging weights are learnable parameters that can be optimized during training, allowing the model to automatically learn the optimal combination of different task-specific knowledge.</p> Architecture <ul> <li>Base pretrained model (frozen)</li> <li>Multiple task vectors (differences from pretrained model, frozen)</li> <li>Learnable task-wise weights (trainable parameters)</li> <li>Dynamic merging during forward pass</li> </ul> <p>Parameters:</p> <ul> <li> <code>task_wise_weight</code>               (<code>Tensor</code>)           \u2013            <p>Initial weights for each task model. Shape: (num_models,). These become learnable parameters that control the contribution of each task vector.</p> </li> <li> <code>pretrained_model</code>               (<code>TorchModelType</code>)           \u2013            <p>The base pretrained model that serves as the foundation. This model is frozen and used as the starting point for merging.</p> </li> <li> <code>finetuned_models</code>               (<code>List[TorchModelType]</code>)           \u2013            <p>List of fine-tuned models for different tasks. These are converted to task vectors (differences from pretrained model) and frozen.</p> </li> <li> <code>clamp_weights</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to clamp merge weights to [0, 1] range. Defaults to True. When True, ensures weights are non-negative and bounded.</p> </li> <li> <code>tie_weights</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to tie weights during functional call. Defaults to False. Used in the underlying PyTorch functional_call.</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to enforce strict parameter matching. Defaults to True. Used in the underlying PyTorch functional_call.</p> </li> <li> <code>task_vector_dtype</code>               (<code>Optional[dtype]</code>, default:                   <code>None</code> )           \u2013            <p>Data type for task vectors. Defaults to None. Can be used to save memory (e.g., torch.float16).</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>merge_weight</code>               (<code>Parameter</code>)           \u2013            <p>Learnable weights for merging task vectors.</p> </li> <li> <code>pretrained_model</code>               (<code>TorchModelType</code>)           \u2013            <p>The frozen base model.</p> </li> <li> <code>task_vectors</code>               (<code>ModuleList</code>)           \u2013            <p>List of frozen task vector models.</p> </li> <li> <code>_merged_state_dict</code>               (<code>StateDictType</code>)           \u2013            <p>Cached merged state dictionary.</p> </li> </ul> Example <pre><code>import torch\nimport torch.nn as nn\n\n# Create example models\npretrained_model = nn.Linear(10, 5)\nfinetuned_model1 = nn.Linear(10, 5)  # Fine-tuned on task 1\nfinetuned_model2 = nn.Linear(10, 5)  # Fine-tuned on task 2\n\n# Initialize task-wise weights\ntask_weights = torch.tensor([0.3, 0.7])  # Initial weights for 2 tasks\n\n# Create merged model\nmerged_model = TaskWiseMergedModel(\n    task_wise_weight=task_weights,\n    pretrained_model=pretrained_model,\n    finetuned_models=[finetuned_model1, finetuned_model2],\n    clamp_weights=True\n)\n\n# Use like a regular PyTorch model\nx = torch.randn(32, 10)\noutput = merged_model(x)\n\n# Train the merge weights\noptimizer = torch.optim.Adam(merged_model.parameters())\nloss = some_loss_function(output, targets)\nloss.backward()\noptimizer.step()\n\n# Get the final merged model\nfinal_model = merged_model.merge_and_unload()\n</code></pre> Training Workflow <ol> <li>Initialization: Task vectors are computed as differences from pretrained model</li> <li>Forward Pass: Weights are dynamically merged based on current merge_weight values</li> <li>Loss Computation: Standard loss computation on model outputs</li> <li>Backpropagation: Gradients flow through merge_weight parameters</li> <li>Optimization: merge_weight parameters are updated to improve performance</li> </ol> Memory Efficiency <ul> <li>Task vectors can use lower precision (task_vector_dtype)</li> <li>Base model and task vectors are frozen (no gradient computation)</li> <li>Only merge weights require gradients</li> </ul> Note <ul> <li>The pretrained model and task vectors are frozen during training</li> <li>Only the merge weights (task_wise_weight) are trainable parameters</li> <li>Task vectors represent the difference between fine-tuned and pretrained models</li> <li>The merged state dict is cached and recomputed when merge weights change</li> </ul> Source code in <code>fusion_bench/models/wrappers/task_wise_fusion.py</code> <pre><code>class TaskWiseMergedModel(nn.Module, Generic[TorchModelType]):\n    \"\"\"\n    A PyTorch module that dynamically merges multiple fine-tuned models using learnable task-wise weights.\n\n    This class implements a sophisticated model fusion approach where multiple task-specific models\n    are combined with a pretrained base model using learnable weights. The fusion is performed\n    using task vectors (differences between fine-tuned and pretrained models) that are weighted\n    and added to the base model's parameters.\n\n    The key innovation is that the merging weights are learnable parameters that can be optimized\n    during training, allowing the model to automatically learn the optimal combination of different\n    task-specific knowledge.\n\n    Architecture:\n        - Base pretrained model (frozen)\n        - Multiple task vectors (differences from pretrained model, frozen)\n        - Learnable task-wise weights (trainable parameters)\n        - Dynamic merging during forward pass\n\n    Args:\n        task_wise_weight (Tensor): Initial weights for each task model. Shape: (num_models,).\n            These become learnable parameters that control the contribution of each task vector.\n        pretrained_model (TorchModelType): The base pretrained model that serves as the foundation.\n            This model is frozen and used as the starting point for merging.\n        finetuned_models (List[TorchModelType]): List of fine-tuned models for different tasks.\n            These are converted to task vectors (differences from pretrained model) and frozen.\n        clamp_weights (bool, optional): Whether to clamp merge weights to [0, 1] range.\n            Defaults to True. When True, ensures weights are non-negative and bounded.\n        tie_weights (bool, optional): Whether to tie weights during functional call.\n            Defaults to False. Used in the underlying PyTorch functional_call.\n        strict (bool, optional): Whether to enforce strict parameter matching.\n            Defaults to True. Used in the underlying PyTorch functional_call.\n        task_vector_dtype (Optional[torch.dtype], optional): Data type for task vectors.\n            Defaults to None. Can be used to save memory (e.g., torch.float16).\n\n    Attributes:\n        merge_weight (nn.Parameter): Learnable weights for merging task vectors.\n        pretrained_model (TorchModelType): The frozen base model.\n        task_vectors (nn.ModuleList): List of frozen task vector models.\n        _merged_state_dict (StateDictType): Cached merged state dictionary.\n\n    Example:\n        ```python\n        import torch\n        import torch.nn as nn\n\n        # Create example models\n        pretrained_model = nn.Linear(10, 5)\n        finetuned_model1 = nn.Linear(10, 5)  # Fine-tuned on task 1\n        finetuned_model2 = nn.Linear(10, 5)  # Fine-tuned on task 2\n\n        # Initialize task-wise weights\n        task_weights = torch.tensor([0.3, 0.7])  # Initial weights for 2 tasks\n\n        # Create merged model\n        merged_model = TaskWiseMergedModel(\n            task_wise_weight=task_weights,\n            pretrained_model=pretrained_model,\n            finetuned_models=[finetuned_model1, finetuned_model2],\n            clamp_weights=True\n        )\n\n        # Use like a regular PyTorch model\n        x = torch.randn(32, 10)\n        output = merged_model(x)\n\n        # Train the merge weights\n        optimizer = torch.optim.Adam(merged_model.parameters())\n        loss = some_loss_function(output, targets)\n        loss.backward()\n        optimizer.step()\n\n        # Get the final merged model\n        final_model = merged_model.merge_and_unload()\n        ```\n\n    Training Workflow:\n        1. **Initialization**: Task vectors are computed as differences from pretrained model\n        2. **Forward Pass**: Weights are dynamically merged based on current merge_weight values\n        3. **Loss Computation**: Standard loss computation on model outputs\n        4. **Backpropagation**: Gradients flow through merge_weight parameters\n        5. **Optimization**: merge_weight parameters are updated to improve performance\n\n    Memory Efficiency:\n        - Task vectors can use lower precision (task_vector_dtype)\n        - Base model and task vectors are frozen (no gradient computation)\n        - Only merge weights require gradients\n\n    Note:\n        - The pretrained model and task vectors are frozen during training\n        - Only the merge weights (task_wise_weight) are trainable parameters\n        - Task vectors represent the difference between fine-tuned and pretrained models\n        - The merged state dict is cached and recomputed when merge weights change\n    \"\"\"\n\n    _merged_state_dict: StateDictType = None\n\n    def __init__(\n        self,\n        task_wise_weight: Tensor,\n        pretrained_model: TorchModelType,\n        finetuned_models: List[TorchModelType],\n        clamp_weights: bool = True,\n        tie_weights: bool = False,\n        strict: bool = True,\n        task_vector_dtype: Optional[torch.dtype] = None,\n    ):\n        \"\"\"\n        Initialize the TaskWiseMergedModel.\n\n        This constructor sets up the model by:\n        1. Converting fine-tuned models to task vectors (differences from pretrained)\n        2. Freezing the pretrained model and task vectors\n        3. Setting up learnable merge weights as parameters\n        4. Configuring merging behavior options\n\n        Args:\n            task_wise_weight (Tensor): Initial weights for each task model. Shape: (num_models,).\n                These values become the starting point for learnable parameters.\n            pretrained_model (TorchModelType): The base pretrained model.\n                Will be frozen and used as the foundation for merging.\n            finetuned_models (List[TorchModelType]): List of fine-tuned models.\n                Must have the same architecture as pretrained_model.\n            clamp_weights (bool, optional): Whether to clamp weights to [0, 1]. Defaults to True.\n            tie_weights (bool, optional): Whether to tie weights in functional_call. Defaults to False.\n            strict (bool, optional): Whether to use strict parameter matching. Defaults to True.\n            task_vector_dtype (Optional[torch.dtype], optional): Data type for task vectors.\n                Defaults to None (same as original models).\n\n        Raises:\n            ValueError: If the number of task_wise_weights doesn't match the number of fine-tuned models.\n            RuntimeError: If models have incompatible architectures.\n        \"\"\"\n        super().__init__()\n        self.clamp_weights = clamp_weights\n        self.tie_weights = tie_weights\n        self.strict = strict\n        self.task_vector_dtype = task_vector_dtype\n\n        self.merge_weight = nn.Parameter(task_wise_weight, requires_grad=True)\n\n        for name, param in pretrained_model.named_parameters():\n            if not param.requires_grad:\n                for m in finetuned_models:\n                    del_attr(m, name.split(\".\"))\n            else:\n                for m in finetuned_models:\n                    get_attr(m, name.split(\".\")).data = (\n                        get_attr(m, name.split(\".\")) - param\n                    )\n        self.pretrained_model = pretrained_model.requires_grad_(False)\n        for m in finetuned_models:\n            m.requires_grad_(False)\n        self.task_vectors = nn.ModuleList(finetuned_models)\n        if self.task_vector_dtype is not None:\n            self.task_vectors = self.task_vectors.to(self.task_vector_dtype)\n\n    @property\n    def forward_model(self):\n        \"\"\"\n        Get a functional model with merged parameters.\n\n        Returns a partial function that applies the pretrained model with the current\n        merged state dictionary. This allows for efficient forward passes without\n        modifying the original model's parameters.\n\n        Returns:\n            Callable: A partial function that can be called with (args, kwargs) to\n                perform forward pass with merged parameters.\n\n        Example:\n            ```python\n            # Internal usage during forward pass\n            forward_fn = merged_model.forward_model\n            output = forward_fn(args=(x,), kwargs={})\n            ```\n        \"\"\"\n        return functools.partial(\n            functional_call,\n            self.pretrained_model,\n            self._merged_state_dict,\n            tie_weights=self.tie_weights,\n            strict=self.strict,\n        )\n\n    def merge_weights(self, task_vector_mask: Optional[Dict[str, Tensor]] = None):\n        \"\"\"\n        Merge task vectors with the pretrained model using current merge weights.\n\n        This method computes the merged model parameters by combining the pretrained\n        model with weighted task vectors. The resulting state dictionary represents\n        a model that incorporates knowledge from all task-specific models.\n\n        The merging formula for each parameter is:\n        merged_param = pretrained_param + \u03a3(weight_i * task_vector_i * mask_i)\n\n        Args:\n            task_vector_mask (Optional[Dict[str, Tensor]], optional): Optional masks\n                to selectively apply task vectors to specific parameters. Keys should\n                match parameter names, values should be tensors with the same shape\n                as the corresponding parameters. Defaults to None (no masking).\n\n        Returns:\n            StateDictType: The merged state dictionary containing combined parameters.\n\n        Example:\n            ```python\n            # Basic merging\n            merged_state = model.merge_weights()\n\n            # Merging with parameter-specific masks\n            masks = {\n                'layer1.weight': torch.ones_like(model.pretrained_model.layer1.weight),\n                'layer2.weight': torch.zeros_like(model.pretrained_model.layer2.weight),\n            }\n            masked_state = model.merge_weights(task_vector_mask=masks)\n            ```\n\n        Note:\n            - If clamp_weights is True, merge weights are clamped to [0, 1] range\n            - The merged state dict is cached in _merged_state_dict\n            - Task vector masks allow fine-grained control over which parameters are affected\n        \"\"\"\n        if self.clamp_weights:\n            merge_weight = self.merge_weight.clamp(0, 1)\n        else:\n            merge_weight = self.merge_weight\n\n        state_dict = self.pretrained_model.state_dict(keep_vars=True)\n        for weight, task_vector in zip(merge_weight, self.task_vectors):\n            for name, param in task_vector.named_parameters():\n                if task_vector_mask is None:\n                    w = weight\n                else:\n                    w = weight * task_vector_mask[name]\n                state_dict[name] = state_dict[name] + param * w\n        self._merged_state_dict = state_dict\n        return state_dict\n\n    def merge_and_unload(self, task_vector_mask: Optional[Dict[str, Tensor]] = None):\n        \"\"\"\n        Merge models and return the final merged model.\n\n        This method performs the merging operation and then loads the merged parameters\n        into the pretrained model, returning a standard PyTorch model that can be used\n        independently of the TaskWiseMergedModel wrapper.\n\n        Args:\n            task_vector_mask (Optional[Dict[str, Tensor]], optional): Optional masks\n                for selective parameter merging. Defaults to None.\n\n        Returns:\n            TorchModelType: The pretrained model with merged parameters loaded.\n                This is a standalone model that can be used without the wrapper.\n\n        Example:\n            ```python\n            # Train the merged model\n            for epoch in range(num_epochs):\n                # ... training loop ...\n                pass\n\n            # Get the final merged model\n            final_model = merged_model.merge_and_unload()\n\n            # Save or use the final model\n            torch.save(final_model.state_dict(), 'merged_model.pth')\n            output = final_model(new_input)\n            ```\n\n        Warning:\n            This method modifies the pretrained_model's parameters in-place.\n            The original pretrained model parameters will be lost.\n        \"\"\"\n        self.merge_weights(task_vector_mask=task_vector_mask)\n        self.pretrained_model.load_state_dict(self._merged_state_dict)\n        return self.pretrained_model\n\n    def forward(self, *args, **kwargs):\n        \"\"\"\n        Forward pass through the dynamically merged model.\n\n        This method performs the forward pass by first ensuring the model parameters\n        are merged according to the current merge weights, then applying the merged\n        model to the input data.\n\n        The forward pass involves:\n        1. Check if merged state dict is current (recompute if needed)\n        2. Apply the merged model to inputs using functional_call\n        3. Return the model outputs\n\n        Args:\n            *args: Positional arguments to pass to the underlying model.\n            **kwargs: Keyword arguments to pass to the underlying model.\n\n        Returns:\n            Any: The output of the merged model, typically torch.Tensor or tuple of tensors.\n\n        Example:\n            ```python\n            # Single input\n            x = torch.randn(32, 784)\n            output = merged_model(x)\n\n            # Multiple inputs\n            x1, x2 = torch.randn(32, 784), torch.randn(32, 100)\n            output = merged_model(x1, x2)\n\n            # With keyword arguments\n            output = merged_model(input_ids=input_ids, attention_mask=attention_mask)\n            ```\n\n        Note:\n            - The merged state dict is recomputed if merge weights have changed\n            - This allows for dynamic behavior during training as weights are updated\n            - The computation is efficient as merging only happens when needed\n        \"\"\"\n        if self._merged_state_dict is None:\n            self.merge_weights()\n        return self.forward_model(args=args, kwargs=kwargs)\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.task_wise_fusion.TaskWiseMergedModel.forward_model","title":"<code>forward_model</code>  <code>property</code>","text":"<p>Get a functional model with merged parameters.</p> <p>Returns a partial function that applies the pretrained model with the current merged state dictionary. This allows for efficient forward passes without modifying the original model's parameters.</p> <p>Returns:</p> <ul> <li> <code>Callable</code>          \u2013            <p>A partial function that can be called with (args, kwargs) to perform forward pass with merged parameters.</p> </li> </ul> Example <pre><code># Internal usage during forward pass\nforward_fn = merged_model.forward_model\noutput = forward_fn(args=(x,), kwargs={})\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.task_wise_fusion.TaskWiseMergedModel.__init__","title":"<code>__init__(task_wise_weight, pretrained_model, finetuned_models, clamp_weights=True, tie_weights=False, strict=True, task_vector_dtype=None)</code>","text":"<p>Initialize the TaskWiseMergedModel.</p> <p>This constructor sets up the model by: 1. Converting fine-tuned models to task vectors (differences from pretrained) 2. Freezing the pretrained model and task vectors 3. Setting up learnable merge weights as parameters 4. Configuring merging behavior options</p> <p>Parameters:</p> <ul> <li> <code>task_wise_weight</code>               (<code>Tensor</code>)           \u2013            <p>Initial weights for each task model. Shape: (num_models,). These values become the starting point for learnable parameters.</p> </li> <li> <code>pretrained_model</code>               (<code>TorchModelType</code>)           \u2013            <p>The base pretrained model. Will be frozen and used as the foundation for merging.</p> </li> <li> <code>finetuned_models</code>               (<code>List[TorchModelType]</code>)           \u2013            <p>List of fine-tuned models. Must have the same architecture as pretrained_model.</p> </li> <li> <code>clamp_weights</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to clamp weights to [0, 1]. Defaults to True.</p> </li> <li> <code>tie_weights</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to tie weights in functional_call. Defaults to False.</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use strict parameter matching. Defaults to True.</p> </li> <li> <code>task_vector_dtype</code>               (<code>Optional[dtype]</code>, default:                   <code>None</code> )           \u2013            <p>Data type for task vectors. Defaults to None (same as original models).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the number of task_wise_weights doesn't match the number of fine-tuned models.</p> </li> <li> <code>RuntimeError</code>             \u2013            <p>If models have incompatible architectures.</p> </li> </ul> Source code in <code>fusion_bench/models/wrappers/task_wise_fusion.py</code> <pre><code>def __init__(\n    self,\n    task_wise_weight: Tensor,\n    pretrained_model: TorchModelType,\n    finetuned_models: List[TorchModelType],\n    clamp_weights: bool = True,\n    tie_weights: bool = False,\n    strict: bool = True,\n    task_vector_dtype: Optional[torch.dtype] = None,\n):\n    \"\"\"\n    Initialize the TaskWiseMergedModel.\n\n    This constructor sets up the model by:\n    1. Converting fine-tuned models to task vectors (differences from pretrained)\n    2. Freezing the pretrained model and task vectors\n    3. Setting up learnable merge weights as parameters\n    4. Configuring merging behavior options\n\n    Args:\n        task_wise_weight (Tensor): Initial weights for each task model. Shape: (num_models,).\n            These values become the starting point for learnable parameters.\n        pretrained_model (TorchModelType): The base pretrained model.\n            Will be frozen and used as the foundation for merging.\n        finetuned_models (List[TorchModelType]): List of fine-tuned models.\n            Must have the same architecture as pretrained_model.\n        clamp_weights (bool, optional): Whether to clamp weights to [0, 1]. Defaults to True.\n        tie_weights (bool, optional): Whether to tie weights in functional_call. Defaults to False.\n        strict (bool, optional): Whether to use strict parameter matching. Defaults to True.\n        task_vector_dtype (Optional[torch.dtype], optional): Data type for task vectors.\n            Defaults to None (same as original models).\n\n    Raises:\n        ValueError: If the number of task_wise_weights doesn't match the number of fine-tuned models.\n        RuntimeError: If models have incompatible architectures.\n    \"\"\"\n    super().__init__()\n    self.clamp_weights = clamp_weights\n    self.tie_weights = tie_weights\n    self.strict = strict\n    self.task_vector_dtype = task_vector_dtype\n\n    self.merge_weight = nn.Parameter(task_wise_weight, requires_grad=True)\n\n    for name, param in pretrained_model.named_parameters():\n        if not param.requires_grad:\n            for m in finetuned_models:\n                del_attr(m, name.split(\".\"))\n        else:\n            for m in finetuned_models:\n                get_attr(m, name.split(\".\")).data = (\n                    get_attr(m, name.split(\".\")) - param\n                )\n    self.pretrained_model = pretrained_model.requires_grad_(False)\n    for m in finetuned_models:\n        m.requires_grad_(False)\n    self.task_vectors = nn.ModuleList(finetuned_models)\n    if self.task_vector_dtype is not None:\n        self.task_vectors = self.task_vectors.to(self.task_vector_dtype)\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.task_wise_fusion.TaskWiseMergedModel.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Forward pass through the dynamically merged model.</p> <p>This method performs the forward pass by first ensuring the model parameters are merged according to the current merge weights, then applying the merged model to the input data.</p> <p>The forward pass involves: 1. Check if merged state dict is current (recompute if needed) 2. Apply the merged model to inputs using functional_call 3. Return the model outputs</p> <p>Parameters:</p> <ul> <li> <code>*args</code>           \u2013            <p>Positional arguments to pass to the underlying model.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Keyword arguments to pass to the underlying model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>          \u2013            <p>The output of the merged model, typically torch.Tensor or tuple of tensors.</p> </li> </ul> Example <pre><code># Single input\nx = torch.randn(32, 784)\noutput = merged_model(x)\n\n# Multiple inputs\nx1, x2 = torch.randn(32, 784), torch.randn(32, 100)\noutput = merged_model(x1, x2)\n\n# With keyword arguments\noutput = merged_model(input_ids=input_ids, attention_mask=attention_mask)\n</code></pre> Note <ul> <li>The merged state dict is recomputed if merge weights have changed</li> <li>This allows for dynamic behavior during training as weights are updated</li> <li>The computation is efficient as merging only happens when needed</li> </ul> Source code in <code>fusion_bench/models/wrappers/task_wise_fusion.py</code> <pre><code>def forward(self, *args, **kwargs):\n    \"\"\"\n    Forward pass through the dynamically merged model.\n\n    This method performs the forward pass by first ensuring the model parameters\n    are merged according to the current merge weights, then applying the merged\n    model to the input data.\n\n    The forward pass involves:\n    1. Check if merged state dict is current (recompute if needed)\n    2. Apply the merged model to inputs using functional_call\n    3. Return the model outputs\n\n    Args:\n        *args: Positional arguments to pass to the underlying model.\n        **kwargs: Keyword arguments to pass to the underlying model.\n\n    Returns:\n        Any: The output of the merged model, typically torch.Tensor or tuple of tensors.\n\n    Example:\n        ```python\n        # Single input\n        x = torch.randn(32, 784)\n        output = merged_model(x)\n\n        # Multiple inputs\n        x1, x2 = torch.randn(32, 784), torch.randn(32, 100)\n        output = merged_model(x1, x2)\n\n        # With keyword arguments\n        output = merged_model(input_ids=input_ids, attention_mask=attention_mask)\n        ```\n\n    Note:\n        - The merged state dict is recomputed if merge weights have changed\n        - This allows for dynamic behavior during training as weights are updated\n        - The computation is efficient as merging only happens when needed\n    \"\"\"\n    if self._merged_state_dict is None:\n        self.merge_weights()\n    return self.forward_model(args=args, kwargs=kwargs)\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.task_wise_fusion.TaskWiseMergedModel.merge_and_unload","title":"<code>merge_and_unload(task_vector_mask=None)</code>","text":"<p>Merge models and return the final merged model.</p> <p>This method performs the merging operation and then loads the merged parameters into the pretrained model, returning a standard PyTorch model that can be used independently of the TaskWiseMergedModel wrapper.</p> <p>Parameters:</p> <ul> <li> <code>task_vector_mask</code>               (<code>Optional[Dict[str, Tensor]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional masks for selective parameter merging. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TorchModelType</code>          \u2013            <p>The pretrained model with merged parameters loaded. This is a standalone model that can be used without the wrapper.</p> </li> </ul> Example <pre><code># Train the merged model\nfor epoch in range(num_epochs):\n    # ... training loop ...\n    pass\n\n# Get the final merged model\nfinal_model = merged_model.merge_and_unload()\n\n# Save or use the final model\ntorch.save(final_model.state_dict(), 'merged_model.pth')\noutput = final_model(new_input)\n</code></pre> Warning <p>This method modifies the pretrained_model's parameters in-place. The original pretrained model parameters will be lost.</p> Source code in <code>fusion_bench/models/wrappers/task_wise_fusion.py</code> <pre><code>def merge_and_unload(self, task_vector_mask: Optional[Dict[str, Tensor]] = None):\n    \"\"\"\n    Merge models and return the final merged model.\n\n    This method performs the merging operation and then loads the merged parameters\n    into the pretrained model, returning a standard PyTorch model that can be used\n    independently of the TaskWiseMergedModel wrapper.\n\n    Args:\n        task_vector_mask (Optional[Dict[str, Tensor]], optional): Optional masks\n            for selective parameter merging. Defaults to None.\n\n    Returns:\n        TorchModelType: The pretrained model with merged parameters loaded.\n            This is a standalone model that can be used without the wrapper.\n\n    Example:\n        ```python\n        # Train the merged model\n        for epoch in range(num_epochs):\n            # ... training loop ...\n            pass\n\n        # Get the final merged model\n        final_model = merged_model.merge_and_unload()\n\n        # Save or use the final model\n        torch.save(final_model.state_dict(), 'merged_model.pth')\n        output = final_model(new_input)\n        ```\n\n    Warning:\n        This method modifies the pretrained_model's parameters in-place.\n        The original pretrained model parameters will be lost.\n    \"\"\"\n    self.merge_weights(task_vector_mask=task_vector_mask)\n    self.pretrained_model.load_state_dict(self._merged_state_dict)\n    return self.pretrained_model\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.task_wise_fusion.TaskWiseMergedModel.merge_weights","title":"<code>merge_weights(task_vector_mask=None)</code>","text":"<p>Merge task vectors with the pretrained model using current merge weights.</p> <p>This method computes the merged model parameters by combining the pretrained model with weighted task vectors. The resulting state dictionary represents a model that incorporates knowledge from all task-specific models.</p> <p>The merging formula for each parameter is: merged_param = pretrained_param + \u03a3(weight_i * task_vector_i * mask_i)</p> <p>Parameters:</p> <ul> <li> <code>task_vector_mask</code>               (<code>Optional[Dict[str, Tensor]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional masks to selectively apply task vectors to specific parameters. Keys should match parameter names, values should be tensors with the same shape as the corresponding parameters. Defaults to None (no masking).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>          \u2013            <p>The merged state dictionary containing combined parameters.</p> </li> </ul> Example <pre><code># Basic merging\nmerged_state = model.merge_weights()\n\n# Merging with parameter-specific masks\nmasks = {\n    'layer1.weight': torch.ones_like(model.pretrained_model.layer1.weight),\n    'layer2.weight': torch.zeros_like(model.pretrained_model.layer2.weight),\n}\nmasked_state = model.merge_weights(task_vector_mask=masks)\n</code></pre> Note <ul> <li>If clamp_weights is True, merge weights are clamped to [0, 1] range</li> <li>The merged state dict is cached in _merged_state_dict</li> <li>Task vector masks allow fine-grained control over which parameters are affected</li> </ul> Source code in <code>fusion_bench/models/wrappers/task_wise_fusion.py</code> <pre><code>def merge_weights(self, task_vector_mask: Optional[Dict[str, Tensor]] = None):\n    \"\"\"\n    Merge task vectors with the pretrained model using current merge weights.\n\n    This method computes the merged model parameters by combining the pretrained\n    model with weighted task vectors. The resulting state dictionary represents\n    a model that incorporates knowledge from all task-specific models.\n\n    The merging formula for each parameter is:\n    merged_param = pretrained_param + \u03a3(weight_i * task_vector_i * mask_i)\n\n    Args:\n        task_vector_mask (Optional[Dict[str, Tensor]], optional): Optional masks\n            to selectively apply task vectors to specific parameters. Keys should\n            match parameter names, values should be tensors with the same shape\n            as the corresponding parameters. Defaults to None (no masking).\n\n    Returns:\n        StateDictType: The merged state dictionary containing combined parameters.\n\n    Example:\n        ```python\n        # Basic merging\n        merged_state = model.merge_weights()\n\n        # Merging with parameter-specific masks\n        masks = {\n            'layer1.weight': torch.ones_like(model.pretrained_model.layer1.weight),\n            'layer2.weight': torch.zeros_like(model.pretrained_model.layer2.weight),\n        }\n        masked_state = model.merge_weights(task_vector_mask=masks)\n        ```\n\n    Note:\n        - If clamp_weights is True, merge weights are clamped to [0, 1] range\n        - The merged state dict is cached in _merged_state_dict\n        - Task vector masks allow fine-grained control over which parameters are affected\n    \"\"\"\n    if self.clamp_weights:\n        merge_weight = self.merge_weight.clamp(0, 1)\n    else:\n        merge_weight = self.merge_weight\n\n    state_dict = self.pretrained_model.state_dict(keep_vars=True)\n    for weight, task_vector in zip(merge_weight, self.task_vectors):\n        for name, param in task_vector.named_parameters():\n            if task_vector_mask is None:\n                w = weight\n            else:\n                w = weight * task_vector_mask[name]\n            state_dict[name] = state_dict[name] + param * w\n    self._merged_state_dict = state_dict\n    return state_dict\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.task_wise_fusion.fuse_weights","title":"<code>fuse_weights(task_wise_weight, state_dicts)</code>","text":"<p>This function fuses the weights of the models and returns a state dictionary.</p> <p>Parameters:</p> <ul> <li> <code>task_wise_weight</code>               (<code>Tensor</code>)           \u2013            <p>The weights for each model. on cuda or cpu.</p> </li> <li> <code>state_dicts</code>               (<code>List[StateDictType]</code>)           \u2013            <p>The list of state dictionaries. on cpu.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code> (              <code>StateDictType</code> )          \u2013            <p>The fused state dictionary.</p> </li> </ul> Source code in <code>fusion_bench/models/wrappers/task_wise_fusion.py</code> <pre><code>def fuse_weights(\n    task_wise_weight: Tensor, state_dicts: List[StateDictType]\n) -&gt; StateDictType:\n    \"\"\"\n    This function fuses the weights of the models and returns a state dictionary.\n\n    Args:\n        task_wise_weight (Tensor): The weights for each model. on cuda or cpu.\n        state_dicts (List[StateDictType]): The list of state dictionaries. on cpu.\n\n    Returns:\n        StateDictType: The fused state dictionary.\n    \"\"\"\n    num_models = len(state_dicts)\n    assert (\n        task_wise_weight.dim() == 1\n    ), f\"task_wise_weight must be a 1D tensor, got {task_wise_weight.dim()}\"\n    assert num_models == task_wise_weight.size(\n        0\n    ), f\"num_models must be equal to the number of state_dicts, got {num_models} and {task_wise_weight.size(0)}\"\n    return {\n        k: _fuse_weights(task_wise_weight, [sd[k] for sd in state_dicts])\n        for k in state_dicts[0].keys()\n    }\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.task_wise_fusion.get_task_wise_weights","title":"<code>get_task_wise_weights(num_models, init_values=None)</code>","text":"<p>This function generates a tensor of weights for each model.</p> <p>Parameters:</p> <ul> <li> <code>num_models</code>               (<code>int</code>)           \u2013            <p>The number of models.</p> </li> <li> <code>init_values</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The initial value for each weight. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>A tensor of weights for each model.</p> </li> </ul> Source code in <code>fusion_bench/models/wrappers/task_wise_fusion.py</code> <pre><code>def get_task_wise_weights(num_models: int, init_values: float = None) -&gt; Tensor:\n    \"\"\"\n    This function generates a tensor of weights for each model.\n\n    Args:\n        num_models (int): The number of models.\n        init_values (float, optional): The initial value for each weight. Defaults to None.\n\n    Returns:\n        Tensor: A tensor of weights for each model.\n    \"\"\"\n    assert num_models &gt;= 1, f\"num_models must be &gt;= 1, got {num_models}\"\n    if init_values is None:\n        init_values = 1.0 / num_models\n    return torch.full((num_models,), init_values, dtype=torch.float32)\n</code></pre>"},{"location":"api/fusion_bench.models/#model-ensemble","title":"Model Ensemble","text":""},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.ensemble","title":"<code>ensemble</code>","text":""},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.ensemble.EnsembleModule","title":"<code>EnsembleModule</code>","text":"<p>               Bases: <code>Module</code>, <code>Generic[TorchModelType]</code></p> <p>Ensemble module that averages the outputs of multiple models.</p> Source code in <code>fusion_bench/models/wrappers/ensemble.py</code> <pre><code>class EnsembleModule(nn.Module, Generic[TorchModelType]):\n    \"\"\"\n    Ensemble module that averages the outputs of multiple models.\n    \"\"\"\n\n    def __init__(\n        self,\n        models: List[TorchModelType],\n        device_map: Dict[int, Union[int, str]] | None = None,\n    ):\n        \"\"\"\n        Initializes the EnsembleModule with a list of models.\n\n        Args:\n            models (List[nn.Module]): List of models to ensemble.\n        \"\"\"\n        super().__init__()\n        # TODO: distribute models to devices\n        self.model_list = nn.ModuleList(models)\n        self.device_map = device_map\n        if self.device_map is not None:\n            self._move_models_to_devices()\n\n    def _move_models_to_devices(self):\n        for model_idx, device_id in self.device_map.items():\n            log.info(f\"Moving model {model_idx} to device {device_id}\")\n            self.model_list[model_idx] = self.model_list[model_idx].to(\n                device_id, non_blocking=True\n            )\n\n    def _aggregate_tensors(self, outputs: List[Tensor]) -&gt; Tensor:\n        \"\"\"\n        Aggregates a list of tensors by computing their mean.\n\n        Args:\n            outputs (List[Tensor]): List of tensors to aggregate.\n\n        Returns:\n            Tensor: The mean tensor.\n        \"\"\"\n        return torch.stack(outputs).mean(dim=0)\n\n    def _parallel_forward_with_device_map(self, *args: Any, **kwargs: Any) -&gt; List[Any]:\n        \"\"\"\n        Performs parallel forward pass using device mapping with futures.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            List[Any]: List of outputs from all models, all moved to the same device.\n        \"\"\"\n        futures = []\n\n        device_data_cache = {}\n        for i, model in enumerate(self.model_list):\n            device_id = self.device_map.get(i, \"cpu\")\n\n            if device_id not in device_data_cache:\n                # Move inputs to the same device as the model\n                device_args = to_device(\n                    args, device_id, copy_on_move=True, non_blocking=True\n                )\n                device_kwargs = to_device(\n                    kwargs, device_id, copy_on_move=True, non_blocking=True\n                )\n                device_data_cache[device_id] = (device_args, device_kwargs)\n            else:\n                device_args, device_kwargs = device_data_cache[device_id]\n\n            # Create a future for asynchronous execution\n            future = torch.jit.fork(model, *device_args, **device_kwargs)\n            futures.append(future)\n\n        # Wait for all futures to complete and collect results\n        outputs = [torch.jit.wait(future) for future in futures]\n\n        # Move all outputs to the same device (use the device of the first model or cpu as fallback)\n        target_device = self.device_map.get(0, \"cpu\") if self.device_map else \"cpu\"\n        outputs = [\n            to_device(output, target_device, non_blocking=True) for output in outputs\n        ]\n        return outputs\n\n    def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"\n        Performs a forward pass by averaging the outputs of the models.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            Aggregated output from the ensemble of models.\n        \"\"\"\n        if self.device_map is None:\n            outputs = [model(*args, **kwargs) for model in self.model_list]\n        else:\n            # Parallel execution with device mapping\n            outputs = self._parallel_forward_with_device_map(*args, **kwargs)\n        return aggregate_tensors(outputs, self._aggregate_tensors)\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.ensemble.EnsembleModule.__init__","title":"<code>__init__(models, device_map=None)</code>","text":"<p>Initializes the EnsembleModule with a list of models.</p> <p>Parameters:</p> <ul> <li> <code>models</code>               (<code>List[Module]</code>)           \u2013            <p>List of models to ensemble.</p> </li> </ul> Source code in <code>fusion_bench/models/wrappers/ensemble.py</code> <pre><code>def __init__(\n    self,\n    models: List[TorchModelType],\n    device_map: Dict[int, Union[int, str]] | None = None,\n):\n    \"\"\"\n    Initializes the EnsembleModule with a list of models.\n\n    Args:\n        models (List[nn.Module]): List of models to ensemble.\n    \"\"\"\n    super().__init__()\n    # TODO: distribute models to devices\n    self.model_list = nn.ModuleList(models)\n    self.device_map = device_map\n    if self.device_map is not None:\n        self._move_models_to_devices()\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.ensemble.EnsembleModule.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Performs a forward pass by averaging the outputs of the models.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Variable length argument list.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Arbitrary keyword arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>Aggregated output from the ensemble of models.</p> </li> </ul> Source code in <code>fusion_bench/models/wrappers/ensemble.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Performs a forward pass by averaging the outputs of the models.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        Aggregated output from the ensemble of models.\n    \"\"\"\n    if self.device_map is None:\n        outputs = [model(*args, **kwargs) for model in self.model_list]\n    else:\n        # Parallel execution with device mapping\n        outputs = self._parallel_forward_with_device_map(*args, **kwargs)\n    return aggregate_tensors(outputs, self._aggregate_tensors)\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.ensemble.MaxModelPredictor","title":"<code>MaxModelPredictor</code>","text":"<p>               Bases: <code>EnsembleModule</code></p> <p>Ensemble module that selects the maximum output among multiple models.</p> Source code in <code>fusion_bench/models/wrappers/ensemble.py</code> <pre><code>class MaxModelPredictor(EnsembleModule):\n    \"\"\"\n    Ensemble module that selects the maximum output among multiple models.\n    \"\"\"\n\n    def _aggregate_tensors(self, outputs: List[Tensor]) -&gt; Tensor:\n        \"\"\"\n        Aggregates a list of tensors by selecting the maximum value at each position.\n\n        Args:\n            outputs (List[Tensor]): List of tensors to aggregate.\n\n        Returns:\n            Tensor: Tensor with the maximum values.\n        \"\"\"\n        return torch.stack(outputs).max(dim=0).values\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.ensemble.WeightedEnsembleModule","title":"<code>WeightedEnsembleModule</code>","text":"<p>               Bases: <code>Module</code>, <code>Generic[TorchModelType]</code></p> <p>Ensemble module that computes a weighted average of the outputs from multiple models.</p> Source code in <code>fusion_bench/models/wrappers/ensemble.py</code> <pre><code>class WeightedEnsembleModule(nn.Module, Generic[TorchModelType]):\n    \"\"\"\n    Ensemble module that computes a weighted average of the outputs from multiple models.\n    \"\"\"\n\n    def __init__(\n        self,\n        models: List[TorchModelType],\n        weights: List[float] | Tensor | np.ndarray,\n        normalize: bool = True,\n        device_map: Dict[int, Union[int, str]] | None = None,\n    ):\n        \"\"\"\n        Initializes the WeightedEnsembleModule with models and their corresponding weights.\n\n        Args:\n            models (List[nn.Module]): List of models to ensemble.\n            weights (List[float] | Tensor | np.ndarray): Weights for each model.\n            normalize (bool, optional): If True, normalizes the weights. Defaults to True.\n            device_map (Dict[int, Union[int, str]] | None, optional): Device mapping for parallel execution. Defaults to None.\n        \"\"\"\n        super().__init__()\n        self.model_list = nn.ModuleList(models)\n        self.device_map = device_map\n\n        if isinstance(weights, (list, tuple, ListConfig)):\n            weights = torch.tensor(weights)\n        elif isinstance(weights, Tensor):\n            weights = weights\n        elif isinstance(weights, np.ndarray):\n            weights = torch.from_numpy(weights)\n        else:\n            raise ValueError(f\"Unsupported type for weights: {type(weights)=}\")\n\n        assert len(models) == len(weights) and weights.dim() == 1, (\n            \"weights must be a 1D tensor of the same length as models.\"\n            f\"But got {len(models)=}, {weights.dim()=}\"\n        )\n        if normalize:\n            weights = weights / weights.sum()\n        self.register_buffer(\"weights\", weights)\n\n        if self.device_map is not None:\n            self._move_models_to_devices()\n\n    def _move_models_to_devices(self):\n        \"\"\"Move models to their assigned devices according to device_map.\"\"\"\n        for model_idx, device_id in self.device_map.items():\n            log.info(f\"Moving model {model_idx} to device {device_id}\")\n            self.model_list[model_idx] = self.model_list[model_idx].to(\n                device_id, non_blocking=True\n            )\n\n    def _aggregate_tensors(self, outputs: List[Tensor]) -&gt; Tensor:\n        \"\"\"\n        Aggregates a list of tensors using the provided weights.\n\n        Args:\n            outputs (List[Tensor]): List of tensors to aggregate.\n\n        Returns:\n            Tensor: The weighted sum of the tensors.\n        \"\"\"\n        weights = cast(Tensor, self.weights).view(-1, *([1] * outputs[0].dim()))\n        return (torch.stack(outputs) * weights).sum(dim=0)\n\n    def _parallel_forward_with_device_map(self, *args: Any, **kwargs: Any) -&gt; List[Any]:\n        \"\"\"\n        Performs parallel forward pass using device mapping with futures.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            List[Any]: List of outputs from all models, all moved to the same device.\n        \"\"\"\n        futures = []\n\n        device_data_cache = {}\n        for i, model in enumerate(self.model_list):\n            device_id = self.device_map.get(i, \"cpu\")\n\n            if device_id not in device_data_cache:\n                # Move inputs to the same device as the model\n                device_args = to_device(\n                    args, device_id, copy_on_move=True, non_blocking=True\n                )\n                device_kwargs = to_device(\n                    kwargs, device_id, copy_on_move=True, non_blocking=True\n                )\n                device_data_cache[device_id] = (device_args, device_kwargs)\n            else:\n                device_args, device_kwargs = device_data_cache[device_id]\n\n            # Create a future for asynchronous execution\n            future = torch.jit.fork(model, *device_args, **device_kwargs)\n            futures.append(future)\n\n        # Wait for all futures to complete and collect results\n        outputs = [torch.jit.wait(future) for future in futures]\n\n        # Move all outputs to the same device (use the device of the first model or cpu as fallback)\n        target_device = self.device_map.get(0, \"cpu\") if self.device_map else \"cpu\"\n        outputs = [to_device(output, target_device) for output in outputs]\n\n        return outputs\n\n    def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"\n        Performs a forward pass by computing the weighted average of the models' outputs.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            Weighted aggregated output from the ensemble of models.\n        \"\"\"\n        if self.device_map is None:\n            outputs = [model(*args, **kwargs) for model in self.model_list]\n        else:\n            # Parallel execution with device mapping\n            outputs = self._parallel_forward_with_device_map(*args, **kwargs)\n        return aggregate_tensors(outputs, self._aggregate_tensors)\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.ensemble.WeightedEnsembleModule.__init__","title":"<code>__init__(models, weights, normalize=True, device_map=None)</code>","text":"<p>Initializes the WeightedEnsembleModule with models and their corresponding weights.</p> <p>Parameters:</p> <ul> <li> <code>models</code>               (<code>List[Module]</code>)           \u2013            <p>List of models to ensemble.</p> </li> <li> <code>weights</code>               (<code>List[float] | Tensor | ndarray</code>)           \u2013            <p>Weights for each model.</p> </li> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, normalizes the weights. Defaults to True.</p> </li> <li> <code>device_map</code>               (<code>Dict[int, Union[int, str]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Device mapping for parallel execution. Defaults to None.</p> </li> </ul> Source code in <code>fusion_bench/models/wrappers/ensemble.py</code> <pre><code>def __init__(\n    self,\n    models: List[TorchModelType],\n    weights: List[float] | Tensor | np.ndarray,\n    normalize: bool = True,\n    device_map: Dict[int, Union[int, str]] | None = None,\n):\n    \"\"\"\n    Initializes the WeightedEnsembleModule with models and their corresponding weights.\n\n    Args:\n        models (List[nn.Module]): List of models to ensemble.\n        weights (List[float] | Tensor | np.ndarray): Weights for each model.\n        normalize (bool, optional): If True, normalizes the weights. Defaults to True.\n        device_map (Dict[int, Union[int, str]] | None, optional): Device mapping for parallel execution. Defaults to None.\n    \"\"\"\n    super().__init__()\n    self.model_list = nn.ModuleList(models)\n    self.device_map = device_map\n\n    if isinstance(weights, (list, tuple, ListConfig)):\n        weights = torch.tensor(weights)\n    elif isinstance(weights, Tensor):\n        weights = weights\n    elif isinstance(weights, np.ndarray):\n        weights = torch.from_numpy(weights)\n    else:\n        raise ValueError(f\"Unsupported type for weights: {type(weights)=}\")\n\n    assert len(models) == len(weights) and weights.dim() == 1, (\n        \"weights must be a 1D tensor of the same length as models.\"\n        f\"But got {len(models)=}, {weights.dim()=}\"\n    )\n    if normalize:\n        weights = weights / weights.sum()\n    self.register_buffer(\"weights\", weights)\n\n    if self.device_map is not None:\n        self._move_models_to_devices()\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.ensemble.WeightedEnsembleModule.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Performs a forward pass by computing the weighted average of the models' outputs.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Variable length argument list.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Arbitrary keyword arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>Weighted aggregated output from the ensemble of models.</p> </li> </ul> Source code in <code>fusion_bench/models/wrappers/ensemble.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Performs a forward pass by computing the weighted average of the models' outputs.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        Weighted aggregated output from the ensemble of models.\n    \"\"\"\n    if self.device_map is None:\n        outputs = [model(*args, **kwargs) for model in self.model_list]\n    else:\n        # Parallel execution with device mapping\n        outputs = self._parallel_forward_with_device_map(*args, **kwargs)\n    return aggregate_tensors(outputs, self._aggregate_tensors)\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.wrappers.ensemble.aggregate_tensors","title":"<code>aggregate_tensors(outputs, aggregate_fn)</code>","text":"<p>Aggregates a list of outputs using the provided aggregation function.</p> <p>This function handles different types of outputs: - If the outputs are Tensors, it applies the aggregation function directly. - If the outputs are dictionaries, it recursively aggregates each value. - If the outputs are tuples or lists, it recursively aggregates each element. - If all outputs are None, it returns None. - If the outputs are of an unsupported type, it raises a ValueError.</p> <p>Parameters:</p> <ul> <li> <code>outputs</code>               (<code>list</code>)           \u2013            <p>A list of outputs to be aggregated. The outputs can be Tensors, dictionaries, tuples, lists, or None.</p> </li> <li> <code>aggregate_fn</code>               (<code>callable</code>)           \u2013            <p>A function to aggregate the outputs. Typically, this could be a function like <code>torch.mean</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Tensor, Dict, List, None]</code>           \u2013            <p>Tensor or dict or tuple or list or None: The aggregated output, matching the type of the input outputs.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the outputs are of an unsupported type.</p> </li> </ul> Source code in <code>fusion_bench/models/wrappers/ensemble.py</code> <pre><code>def aggregate_tensors(\n    outputs: List[Any], aggregate_fn: Callable\n) -&gt; Union[Tensor, Dict, List, None]:\n    \"\"\"\n    Aggregates a list of outputs using the provided aggregation function.\n\n    This function handles different types of outputs:\n    - If the outputs are Tensors, it applies the aggregation function directly.\n    - If the outputs are dictionaries, it recursively aggregates each value.\n    - If the outputs are tuples or lists, it recursively aggregates each element.\n    - If all outputs are None, it returns None.\n    - If the outputs are of an unsupported type, it raises a ValueError.\n\n    Args:\n        outputs (list): A list of outputs to be aggregated. The outputs can be Tensors, dictionaries, tuples, lists, or None.\n        aggregate_fn (callable): A function to aggregate the outputs. Typically, this could be a function like `torch.mean`.\n\n    Returns:\n        Tensor or dict or tuple or list or None: The aggregated output, matching the type of the input outputs.\n\n    Raises:\n        ValueError: If the outputs are of an unsupported type.\n    \"\"\"\n    # If the output is a Tensor, take the mean\n    if isinstance(outputs[0], torch.Tensor):\n        return aggregate_fn(outputs)\n\n    # If the output is a dict, take the mean of each value\n    elif isinstance(outputs[0], Dict):\n        result = type(outputs[0])()\n        for key in outputs[0]:\n            result[key] = aggregate_tensors(\n                [output[key] for output in outputs], aggregate_fn\n            )\n        return result\n\n    # If the output is a tuple or list, take the mean of each element\n    elif isinstance(outputs[0], (tuple, list)):\n        return tuple(\n            aggregate_tensors([output[i] for output in outputs], aggregate_fn)\n            for i in range(len(outputs[0]))\n        )\n\n    # If the output is None, return None\n    elif all(output is None for output in outputs):\n        return None\n\n    # If the output is none of the above, return as is\n    else:\n        raise ValueError(\"Unsupported type for outputs\")\n</code></pre>"},{"location":"api/fusion_bench.models/#model-linearization-ntk","title":"Model Linearization (NTK)","text":""},{"location":"api/fusion_bench.models/#fusion_bench.models.linearized.linearized_model_utils.LinearizedModelWraper","title":"<code>LinearizedModelWraper</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fusion_bench/models/linearized/linearized_model_utils.py</code> <pre><code>class LinearizedModelWraper(nn.Module):\n    def __init__(self, model: nn.Module, init_model: Optional[nn.Module] = None):\n        \"\"\"\n        Initializes a linearized model.\n\n        Args:\n            model (nn.Module): The underlying PyTorch model to be linearized.\n            init_model (nn.Module): The initial PyTorch model used to compute the linearization parameters (default: None).\n        \"\"\"\n        super().__init__()\n        self.model = model\n        if init_model is None:\n            init_model = model\n        assert not hasattr(self, \"params0\")\n        params0 = deepcopy([(k, v.detach()) for k, v in init_model.named_parameters()])\n        self.params0_keys = [k for k, v in params0]\n        self.params0_values = nn.ParameterList([v for k, v in params0])\n        for p in self.params0_values:\n            p.requires_grad_(False)\n\n    def tuple_params_to_dict(self, tuple_params) -&gt; Dict[str, Any]:\n        \"\"\"\n        Converts a tuple of parameters to a dictionary with keys corresponding to the parameter names.\n\n        Args:\n            tuple_params (Tuple[Tensor, ...]): A tuple of parameters.\n\n        Returns:\n            Dict[str, Tensor]: A dictionary with keys corresponding to the parameter names and values corresponding to the\n            parameter values.\n        \"\"\"\n        assert len(tuple_params) == len(self.params0_keys)\n        state_dict = {}\n        for k, p in zip(self.params0_keys, tuple_params):\n            state_dict[k] = p\n        return state_dict\n\n    def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"\n        Computes the linearized model output using a first-order Taylor decomposition.\n\n        Args:\n            *args: Positional arguments to be passed to the model.\n            **kwargs: Keyword arguments to be passed to the model.\n\n        Returns:\n            torch.Tensor: The output of the linearized model, computed using a first-order Taylor decomposition.\n        \"\"\"\n        params0 = tuple(self.params0_values)\n        params = dict_params_to_tuple(OrderedDict(self.model.named_parameters()))\n        dparams = tuple(p - p0 for p, p0 in zip(params, params0))\n        out, dp = jvp(\n            lambda *param: functional_call(\n                self.model, self.tuple_params_to_dict(param), args, kwargs\n            ),\n            params0,\n            dparams,\n        )\n        return out + dp\n\n    @staticmethod\n    def unload_linearized_modules_(module: nn.Module):\n        \"\"\"\n        Unloads the linearized module and returns the original module.\n\n        Args:\n            module (nn.Module): The linearized module to be unloaded.\n\n        Returns:\n            nn.Module: The original module.\n        \"\"\"\n        for name, model in module.named_children():\n            if isinstance(model, LinearizedModelWraper):\n                setattr(module, name, model.model)\n            else:\n                LinearizedModelWraper.unload_linearized_modules_(model)\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.linearized.linearized_model_utils.LinearizedModelWraper.__init__","title":"<code>__init__(model, init_model=None)</code>","text":"<p>Initializes a linearized model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The underlying PyTorch model to be linearized.</p> </li> <li> <code>init_model</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>The initial PyTorch model used to compute the linearization parameters (default: None).</p> </li> </ul> Source code in <code>fusion_bench/models/linearized/linearized_model_utils.py</code> <pre><code>def __init__(self, model: nn.Module, init_model: Optional[nn.Module] = None):\n    \"\"\"\n    Initializes a linearized model.\n\n    Args:\n        model (nn.Module): The underlying PyTorch model to be linearized.\n        init_model (nn.Module): The initial PyTorch model used to compute the linearization parameters (default: None).\n    \"\"\"\n    super().__init__()\n    self.model = model\n    if init_model is None:\n        init_model = model\n    assert not hasattr(self, \"params0\")\n    params0 = deepcopy([(k, v.detach()) for k, v in init_model.named_parameters()])\n    self.params0_keys = [k for k, v in params0]\n    self.params0_values = nn.ParameterList([v for k, v in params0])\n    for p in self.params0_values:\n        p.requires_grad_(False)\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.linearized.linearized_model_utils.LinearizedModelWraper.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Computes the linearized model output using a first-order Taylor decomposition.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Positional arguments to be passed to the model.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to be passed to the model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>torch.Tensor: The output of the linearized model, computed using a first-order Taylor decomposition.</p> </li> </ul> Source code in <code>fusion_bench/models/linearized/linearized_model_utils.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Computes the linearized model output using a first-order Taylor decomposition.\n\n    Args:\n        *args: Positional arguments to be passed to the model.\n        **kwargs: Keyword arguments to be passed to the model.\n\n    Returns:\n        torch.Tensor: The output of the linearized model, computed using a first-order Taylor decomposition.\n    \"\"\"\n    params0 = tuple(self.params0_values)\n    params = dict_params_to_tuple(OrderedDict(self.model.named_parameters()))\n    dparams = tuple(p - p0 for p, p0 in zip(params, params0))\n    out, dp = jvp(\n        lambda *param: functional_call(\n            self.model, self.tuple_params_to_dict(param), args, kwargs\n        ),\n        params0,\n        dparams,\n    )\n    return out + dp\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.linearized.linearized_model_utils.LinearizedModelWraper.tuple_params_to_dict","title":"<code>tuple_params_to_dict(tuple_params)</code>","text":"<p>Converts a tuple of parameters to a dictionary with keys corresponding to the parameter names.</p> <p>Parameters:</p> <ul> <li> <code>tuple_params</code>               (<code>Tuple[Tensor, ...]</code>)           \u2013            <p>A tuple of parameters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dict[str, Tensor]: A dictionary with keys corresponding to the parameter names and values corresponding to the</p> </li> <li> <code>Dict[str, Any]</code>           \u2013            <p>parameter values.</p> </li> </ul> Source code in <code>fusion_bench/models/linearized/linearized_model_utils.py</code> <pre><code>def tuple_params_to_dict(self, tuple_params) -&gt; Dict[str, Any]:\n    \"\"\"\n    Converts a tuple of parameters to a dictionary with keys corresponding to the parameter names.\n\n    Args:\n        tuple_params (Tuple[Tensor, ...]): A tuple of parameters.\n\n    Returns:\n        Dict[str, Tensor]: A dictionary with keys corresponding to the parameter names and values corresponding to the\n        parameter values.\n    \"\"\"\n    assert len(tuple_params) == len(self.params0_keys)\n    state_dict = {}\n    for k, p in zip(self.params0_keys, tuple_params):\n        state_dict[k] = p\n    return state_dict\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.linearized.linearized_model_utils.LinearizedModelWraper.unload_linearized_modules_","title":"<code>unload_linearized_modules_(module)</code>  <code>staticmethod</code>","text":"<p>Unloads the linearized module and returns the original module.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The linearized module to be unloaded.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The original module.</p> </li> </ul> Source code in <code>fusion_bench/models/linearized/linearized_model_utils.py</code> <pre><code>@staticmethod\ndef unload_linearized_modules_(module: nn.Module):\n    \"\"\"\n    Unloads the linearized module and returns the original module.\n\n    Args:\n        module (nn.Module): The linearized module to be unloaded.\n\n    Returns:\n        nn.Module: The original module.\n    \"\"\"\n    for name, model in module.named_children():\n        if isinstance(model, LinearizedModelWraper):\n            setattr(module, name, model.model)\n        else:\n            LinearizedModelWraper.unload_linearized_modules_(model)\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.linearized.vision_model.load_lora_vision_model_hf","title":"<code>load_lora_vision_model_hf(base_model_name, peft_name, merge_and_unload=False, return_vision_model=True)</code>","text":"<p>Load a LoRA (Low-Rank Adaptation) vision model from Hugging Face.</p> <p>This function loads a vision model and applies a LoRA adaptation to it. The model can be optionally merged and unloaded.</p> <p>Parameters:</p> <ul> <li> <code>base_model_name</code>               (<code>str</code>)           \u2013            <p>The name of the base vision model to load from Hugging Face.</p> </li> <li> <code>peft_name</code>               (<code>str</code>)           \u2013            <p>The name of the LoRA adaptation to apply to the base model.</p> </li> <li> <code>merge_and_unload</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the LoRA adaptation is merged into the base model and the LoRA layers are removed. Defaults to False.</p> </li> <li> <code>return_vision_model</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If False, the full CLIPVisionModel is returned. If True, only the vision model (<code>CLIPVisionTransformer</code>) is returned. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PeftModel</code> (              <code>PeftModel</code> )          \u2013            <p>The adapted vision model, optionally merged and unloaded.</p> </li> </ul> Source code in <code>fusion_bench/models/linearized/vision_model.py</code> <pre><code>def load_lora_vision_model_hf(\n    base_model_name: str,\n    peft_name: str,\n    merge_and_unload: bool = False,\n    return_vision_model=True,\n) -&gt; PeftModel:\n    \"\"\"\n    Load a LoRA (Low-Rank Adaptation) vision model from Hugging Face.\n\n    This function loads a vision model and applies a LoRA adaptation to it. The model can be optionally merged and unloaded.\n\n    Parameters:\n        base_model_name (str): The name of the base vision model to load from Hugging Face.\n        peft_name (str): The name of the LoRA adaptation to apply to the base model.\n        merge_and_unload (bool, optional): If True, the LoRA adaptation is merged into the base model and the LoRA layers are removed. Defaults to False.\n        return_vision_model (bool, optional): If False, the full CLIPVisionModel is returned. If True, only the vision model (`CLIPVisionTransformer`) is returned. Defaults to True.\n\n    Returns:\n        PeftModel: The adapted vision model, optionally merged and unloaded.\n    \"\"\"\n    model = CLIPVisionModel.from_pretrained(base_model_name)\n\n    # Load the Peft model\n    # note that we apply lora on type `CLIPVisionTransformer` instead of `CLIPVisionModel`\n    vision_model = model.vision_model\n    peft_model = PeftModel.from_pretrained(vision_model, peft_name, is_trainable=True)\n    if merge_and_unload:\n        vision_model = peft_model.merge_and_unload()\n    else:\n        vision_model = peft_model\n\n    # Return the vision model\n    if return_vision_model:\n        return vision_model\n    else:\n        model.vision_model = vision_model\n        return model\n</code></pre>"},{"location":"api/fusion_bench.models/#fusion_bench.models.linearized.vision_model.load_l_lora_vision_model_hf","title":"<code>load_l_lora_vision_model_hf(base_model_name, peft_name)</code>","text":"<p>Load a linearized L-LoRA model from a base model and a Peft model (HuggingFace).</p> Source code in <code>fusion_bench/models/linearized/vision_model.py</code> <pre><code>def load_l_lora_vision_model_hf(base_model_name: str, peft_name: str):\n    \"\"\"\n    Load a linearized L-LoRA model from a base model and a Peft model (HuggingFace).\n    \"\"\"\n    base_model = CLIPVisionModel.from_pretrained(base_model_name).vision_model\n    peft_config = LoraConfig.from_pretrained(peft_name)\n    peft_config.inference_mode = False  # This is important, make the model trainable\n    model = get_peft_model(base_model, peft_config)\n    linearize_lora_model_(model)\n    for filename in [\"linearized_adapter_model.safetensors\"]:\n        path = get_file_path(peft_name, filename)\n        state_dict = load_file(path)\n        for name, param in state_dict.items():\n            model.get_parameter(name).data = param\n\n    return model\n</code></pre>"},{"location":"api/fusion_bench.optim/","title":"fusion_bench.optim","text":""},{"location":"api/fusion_bench.optim/#mezo-optimizer","title":"MeZO optimizer","text":""},{"location":"api/fusion_bench.optim/#fusion_bench.optim.MeZO","title":"<code>MeZO</code>","text":"<p>               Bases: <code>Optimizer</code></p> Source code in <code>fusion_bench/optim/mezo.py</code> <pre><code>class MeZO(Optimizer):\n    def __init__(\n        self,\n        params,\n        lr: float = 1e-5,\n        weight_decay: float = 0,\n        eps: float = 1e-3,\n    ):\n        defaults = dict(eps=eps, lr=lr, weight_decay=weight_decay)\n        super(MeZO, self).__init__(params, defaults)\n\n    def step(\n        self,\n        closure: Callable,\n        prune_ratio: float = 0.02,\n    ):\n        assert isinstance(closure, Callable), \"closure should be provided\"\n        zo_random_seed = np.random.randint(1000000000)\n\n        # First function evaluation\n        self._zo_perturb_parameters(\n            zo_random_seed, scaling_factor=1, prune_ratio=prune_ratio\n        )\n\n        with torch.inference_mode():\n            loss1 = closure()\n            if loss1 is None:\n                raise ValueError(\"closure returned None (should return loss)\")\n\n        # Second function evaluation\n        self._zo_perturb_parameters(\n            zo_random_seed, scaling_factor=-2, prune_ratio=prune_ratio\n        )\n\n        with torch.inference_mode():\n            loss2 = closure()\n\n        # Compute projected gradient\n        projected_grad = (loss1 - loss2) / (2 * self.defaults[\"eps\"])\n        if isinstance(projected_grad, torch.Tensor):\n            projected_grad = projected_grad.item()\n\n        # Reset model back to its parameters at start of step\n        self._zo_perturb_parameters(\n            zo_random_seed, scaling_factor=1, prune_ratio=prune_ratio\n        )\n\n        self._zo_update_parameters(\n            zo_random_seed, projected_grad, prune_ratio=prune_ratio\n        )\n        return loss1, projected_grad\n\n    def _zo_perturb_parameters(\n        self,\n        random_seed: int,\n        scaling_factor: float,\n        prune_ratio: float = 0.1,\n    ):\n        \"\"\"\n        Perturbs the parameters of the Zeroth Order Optimization algorithm.\n\n        Args:\n            random_seed (int): The random seed to use for the perturbation.\n            scaling_factor (float): The scaling factor to use for the perturbation.\n\n        Returns:\n            None\n        \"\"\"\n        torch.manual_seed(random_seed)\n\n        for group in self.param_groups:\n            for param in group[\"params\"]:\n                z = torch.normal(\n                    mean=0,\n                    std=1,\n                    size=param.data.size(),\n                    device=param.data.device,\n                    dtype=param.data.dtype,\n                )\n                if prune_ratio &lt; 1:\n                    m = torch.bernoulli(prune_ratio * torch.ones_like(param))\n                param.data = param.data + group[\"eps\"] * scaling_factor * z * m\n\n    def _zo_update_parameters(\n        self,\n        random_seed: int,\n        projected_grad: float,\n        prune_ratio: float = 0.1,\n    ):\n        # Update parameters\n        torch.manual_seed(random_seed)\n        for group in self.param_groups:\n            for param in group[\"params\"]:\n                z = torch.normal(\n                    mean=0,\n                    std=1,\n                    size=param.data.size(),\n                    device=param.data.device,\n                    dtype=param.data.dtype,\n                )\n                if prune_ratio &lt; 1:\n                    m = torch.bernoulli(prune_ratio * torch.ones_like(param))\n                param.data = (\n                    param.data\n                    - group[\"lr\"]\n                    * (projected_grad * z + group[\"weight_decay\"] * param.data)\n                    * m\n                )\n</code></pre>"},{"location":"api/fusion_bench.program/","title":"fusion_bench.program","text":""},{"location":"api/fusion_bench.program/#class-definitions","title":"Class Definitions","text":"<ul> <li>fusion_bench.programs.BaseHydraProgram: Base class for Hydra-based programs in FusionBench.</li> <li>fusion_bench.programs.FabricModelFusionProgram: A program for fusing models using Lightning Fabric.</li> </ul>"},{"location":"api/fusion_bench.program/#references","title":"References","text":""},{"location":"api/fusion_bench.program/#fusion_bench.programs.BaseHydraProgram","title":"<code>BaseHydraProgram</code>","text":"<p>               Bases: <code>BaseYAMLSerializable</code></p> <p>Abstract base class for all FusionBench programs that use Hydra configuration.</p> <p>This class serves as the foundation for all FusionBench execution programs, providing a standardized interface for configuration-driven model fusion workflows. It combines the serialization capabilities of BaseYAMLSerializable with the requirement for a main execution method.</p> <p>The class is designed to work seamlessly with Hydra's configuration management system, allowing programs to be instantiated and configured through YAML files. This enables flexible, reproducible experiments with different fusion algorithms, model pools, and evaluation tasks.</p> <p>Key Features:</p> <ul> <li>Configuration-driven execution through Hydra integration</li> <li>YAML serialization support for experiment reproducibility</li> <li>Abstract interface ensuring consistent program structure</li> <li>Integration with FusionBench's modular architecture</li> </ul> Typical Usage <p>Subclasses should implement the <code>run()</code> method to define their specific fusion workflow. The program can then be executed through the FusionBench CLI or instantiated directly from configuration files.</p> Example <pre><code>class MyFusionProgram(BaseHydraProgram):\n    def __init__(self, method_config, modelpool_config, taskpool_config):\n        self.method_config = method_config\n        self.modelpool_config = modelpool_config\n        self.taskpool_config = taskpool_config\n\n    def run(self):\n        # Load components\n        algorithm = load_algorithm(self.method_config)\n        modelpool = load_modelpool(self.modelpool_config)\n        taskpool = load_taskpool(self.taskpool_config)\n\n        # Execute fusion\n        merged_model = algorithm.run(modelpool)\n\n        # Evaluate results\n        report = taskpool.evaluate(merged_model)\n        return report\n</code></pre> Note <p>This is an abstract base class and cannot be instantiated directly. Subclasses must implement the <code>run()</code> method to provide concrete functionality.</p> <p>See Also:</p> <ul> <li>FabricModelFusionProgram: Lightning Fabric-based implementation</li> <li>BaseYAMLSerializable: Parent class providing serialization</li> <li>FusionBench CLI documentation for program execution details</li> </ul> Source code in <code>fusion_bench/programs/base_program.py</code> <pre><code>class BaseHydraProgram(BaseYAMLSerializable):\n    \"\"\"\n    Abstract base class for all FusionBench programs that use Hydra configuration.\n\n    This class serves as the foundation for all FusionBench execution programs,\n    providing a standardized interface for configuration-driven model fusion\n    workflows. It combines the serialization capabilities of BaseYAMLSerializable\n    with the requirement for a main execution method.\n\n    The class is designed to work seamlessly with Hydra's configuration management\n    system, allowing programs to be instantiated and configured through YAML files.\n    This enables flexible, reproducible experiments with different fusion algorithms,\n    model pools, and evaluation tasks.\n\n    Key Features:\n\n    - Configuration-driven execution through Hydra integration\n    - YAML serialization support for experiment reproducibility\n    - Abstract interface ensuring consistent program structure\n    - Integration with FusionBench's modular architecture\n\n    Typical Usage:\n        Subclasses should implement the `run()` method to define their specific\n        fusion workflow. The program can then be executed through the FusionBench\n        CLI or instantiated directly from configuration files.\n\n    Example:\n        ```python\n        class MyFusionProgram(BaseHydraProgram):\n            def __init__(self, method_config, modelpool_config, taskpool_config):\n                self.method_config = method_config\n                self.modelpool_config = modelpool_config\n                self.taskpool_config = taskpool_config\n\n            def run(self):\n                # Load components\n                algorithm = load_algorithm(self.method_config)\n                modelpool = load_modelpool(self.modelpool_config)\n                taskpool = load_taskpool(self.taskpool_config)\n\n                # Execute fusion\n                merged_model = algorithm.run(modelpool)\n\n                # Evaluate results\n                report = taskpool.evaluate(merged_model)\n                return report\n        ```\n\n    Note:\n        This is an abstract base class and cannot be instantiated directly.\n        Subclasses must implement the `run()` method to provide concrete\n        functionality.\n\n    See Also:\n\n    - [FabricModelFusionProgram][fusion_bench.programs.FabricModelFusionProgram]: Lightning Fabric-based implementation\n    - [BaseYAMLSerializable][fusion_bench.mixins.BaseYAMLSerializable]: Parent class providing serialization\n    - FusionBench CLI documentation for program execution details\n    \"\"\"\n\n    @abstractmethod\n    def run(self):\n        \"\"\"\n        Execute the main program workflow.\n\n        This abstract method defines the primary entry point for program execution.\n        Subclasses must implement this method to define their specific fusion\n        workflow, including model loading, fusion algorithm execution, and\n        result evaluation.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/fusion_bench.program/#fusion_bench.programs.BaseHydraProgram.run","title":"<code>run()</code>  <code>abstractmethod</code>","text":"<p>Execute the main program workflow.</p> <p>This abstract method defines the primary entry point for program execution. Subclasses must implement this method to define their specific fusion workflow, including model loading, fusion algorithm execution, and result evaluation.</p> Source code in <code>fusion_bench/programs/base_program.py</code> <pre><code>@abstractmethod\ndef run(self):\n    \"\"\"\n    Execute the main program workflow.\n\n    This abstract method defines the primary entry point for program execution.\n    Subclasses must implement this method to define their specific fusion\n    workflow, including model loading, fusion algorithm execution, and\n    result evaluation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.program/#fusion_bench.programs.FabricModelFusionProgram","title":"<code>FabricModelFusionProgram</code>","text":"<p>               Bases: <code>LightningFabricMixin</code>, <code>BaseHydraProgram</code></p> Source code in <code>fusion_bench/programs/fabric_fusion_program.py</code> <pre><code>class FabricModelFusionProgram(\n    LightningFabricMixin,\n    BaseHydraProgram,\n):\n    method: BaseAlgorithm\n    modelpool: BaseModelPool\n    taskpool: Optional[BaseTaskPool] = None\n\n    _config_mapping = BaseHydraProgram._config_mapping | {\n        \"_method\": \"method\",\n        \"_modelpool\": \"modelpool\",\n        \"_taskpool\": \"taskpool\",\n        \"_fabric\": \"fabric\",\n        \"fast_dev_run\": \"fast_dev_run\",\n        \"seed\": \"seed\",\n        \"path\": \"path\",\n    }\n\n    def __init__(\n        self,\n        method: DictConfig,\n        modelpool: DictConfig,\n        taskpool: Optional[DictConfig] = None,\n        *,\n        fabric: Optional[DictConfig] = None,\n        print_config: bool = True,\n        dry_run: bool = False,\n        report_save_path: Optional[str] = None,\n        merged_model_save_path: Optional[str] = None,\n        merged_model_save_kwargs: Optional[DictConfig] = None,\n        fast_dev_run: bool = False,\n        seed: Optional[int] = None,\n        print_function_call: bool = True,\n        path: DictConfig = None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self._method = method\n        self._modelpool = modelpool\n        self._taskpool = taskpool\n        self._fabric = fabric\n        self.report_save_path = report_save_path\n        self.merged_model_save_path = merged_model_save_path\n        self.merged_model_save_kwargs = merged_model_save_kwargs\n        self.fast_dev_run = fast_dev_run\n        self.seed = seed\n        self.path = path\n        RuntimeConstants.debug = fast_dev_run\n        RuntimeConstants.print_function_call = print_function_call\n        if path is not None:\n            RuntimeConstants.cache_dir = path.get(\"cache_dir\", None)\n\n        if print_config:\n            print_config_tree(\n                self.config,\n                print_order=[\"method\", \"modelpool\", \"taskpool\"],\n            )\n        if dry_run:\n            log.info(\"The program is running in dry-run mode. Exiting.\")\n            exit(0)\n\n    def _instantiate_and_setup(\n        self, config: DictConfig, compat_load_fn: Optional[str] = None\n    ):\n        R\"\"\"\n        Instantiates and sets up an object based on the provided configuration.\n\n        This method performs the following steps:\n        1. Checks if the configuration dictionary contains the key \"_target_\".\n        2. If \"_target_\" is not found (for v0.1.x), attempts to instantiate the object using a compatible load function if provided.\n           - Logs a warning if \"_target_\" is missing.\n           - If `compat_load_fn` is provided, imports the function and uses it to instantiate the object.\n           - If `compat_load_fn` is not provided, raises a ValueError.\n        3. If \"_target_\" is found (for v.0.2.0 and above), attempts to import and instantiate the object using the `instantiate` function.\n           - Ensures the target can be imported.\n           - Uses the `instantiate` function with `_recursive_` set based on the configuration.\n        4. Sets the `_program` attribute of the instantiated object to `self` if the object has this attribute.\n        5. Sets the `_fabric_instance` attribute of the instantiated object to `self.fabric` if the object has this attribute and `self.fabric` is not None.\n        6. Returns the instantiated and set up object.\n        \"\"\"\n        if \"_target_\" not in config:\n            log.warning(\n                \"No '_target_' key found in config. Attempting to instantiate the object in a compatible way.\"\n            )\n            if compat_load_fn is not None:\n                compat_load_fn = import_object(compat_load_fn)\n                if rank_zero_only.rank == 0:\n                    print_bordered(\n                        OmegaConf.to_yaml(config),\n                        title=\"instantiate compat object\",\n                        style=\"magenta\",\n                        code_style=\"yaml\",\n                    )\n                obj = compat_load_fn(config)\n            else:\n                raise ValueError(\n                    \"No load function provided. Please provide a load function to instantiate the object.\"\n                )\n        else:\n            # try to import the object from the target\n            # this checks if the target is valid and can be imported\n            import_object(config._target_)\n            obj = instantiate(\n                config,\n                _recursive_=config.get(\"_recursive_\", False),\n            )\n        if hasattr(obj, \"_program\"):\n            obj._program = self\n        if hasattr(obj, \"_fabric_instance\") and self.fabric is not None:\n            obj._fabric_instance = self.fabric\n        if hasattr(obj, \"_fabric\") and self.fabric is not None:\n            # for v0.1.x compatibility\n            obj._fabric = self.fabric\n        return obj\n\n    def save_merged_model(self, merged_model):\n        \"\"\"\n        Saves the merged model to the specified path.\n        \"\"\"\n        if self.merged_model_save_path is not None:\n            # path to save the merged model, use \"{log_dir}\" to refer to the logger directory\n            save_path: str = self.merged_model_save_path\n            if \"{log_dir}\" in save_path and self.log_dir is not None:\n                save_path = save_path.format(log_dir=self.log_dir)\n\n            if os.path.dirname(save_path):\n                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n            # save the merged model\n            if self.merged_model_save_kwargs is not None:\n                merged_model_save_kwargs = self.merged_model_save_kwargs\n            else:\n                merged_model_save_kwargs = {}\n            with timeit_context(f\"Saving the merged model to {save_path}\"):\n                self.modelpool.save_model(\n                    merged_model,\n                    save_path,\n                    **merged_model_save_kwargs,\n                )\n        else:\n            print(\"No save path specified for the merged model. Skipping saving.\")\n\n    def evaluate_merged_model(\n        self,\n        taskpool: BaseTaskPool,\n        merged_model: Union[nn.Module, Dict, Iterable],\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; Union[Dict, List, Any]:\n        \"\"\"\n        Evaluates the merged model using the provided task pool.\n\n        Depending on the type of the merged model, this function handles the evaluation differently:\n        - If the merged model is an instance of `nn.Module`, it directly evaluates the model.\n        - If the merged model is a dictionary, it extracts the model from the dictionary and evaluates it.\n          The evaluation report is then updated with the remaining dictionary items.\n        - If the merged model is an iterable, it recursively evaluates each model in the iterable.\n        - Raises a `ValueError` if the merged model is of an invalid type.\n\n        Args:\n            taskpool: The task pool used for evaluating the merged model.\n            merged_model: The merged model to be evaluated. It can be an instance of `nn.Module`, a dictionary, or an iterable.\n            *args: Additional positional arguments to be passed to the `evaluate` method of the taskpool.\n            **kwargs: Additional keyword arguments to be passed to the `evaluate` method of the taskpool.\n\n        Returns:\n            The evaluation report. The type of the report depends on the type of the merged model:\n            - If the merged model is an instance of `nn.Module`, the report is a dictionary.\n            - If the merged model is a dictionary, the report is a dictionary updated with the remaining dictionary items.\n            - If the merged model is an iterable, the report is a list of evaluation reports.\n        \"\"\"\n        if isinstance(merged_model, nn.Module):\n            report = taskpool.evaluate(merged_model, *args, **kwargs)\n            return report\n        elif isinstance(merged_model, Dict):\n            report = {}\n            for key, item in merged_model.items():\n                if isinstance(item, nn.Module):\n                    report[key] = taskpool.evaluate(item, *args, **kwargs)\n                elif key == \"models\":\n                    # for multi-model evaluation\n                    report[key] = self.evaluate_merged_model(\n                        taskpool, item, *args, **kwargs\n                    )\n                else:\n                    # metadata\n                    report[key] = item\n            return report\n        elif isinstance(merged_model, Iterable):\n            return [\n                self.evaluate_merged_model(taskpool, m, *args, **kwargs)\n                for m in tqdm(merged_model, desc=\"Evaluating models\")\n            ]\n        else:\n            raise ValueError(f\"Invalid type for merged model: {type(merged_model)}\")\n\n    def run(self):\n        \"\"\"\n        Executes the model fusion program.\n        \"\"\"\n        fabric = self.fabric\n        if self.seed is not None:\n            L.seed_everything(self.seed)\n\n        # create symbol link to hydra output directory\n        if (\n            rank_zero_only.rank == 0\n            and self.log_dir is not None\n            and os.path.abspath(self.log_dir) != os.path.abspath(get_hydra_output_dir())\n        ):\n            create_symlink(\n                get_hydra_output_dir(), self.log_dir, link_name=\"hydra_output\"\n            )\n\n        log.info(\"Running the model fusion program.\")\n        # setup the modelpool, method, and taskpool\n        log.info(\"loading model pool\")\n        self.modelpool = self._instantiate_and_setup(\n            self._modelpool,\n            compat_load_fn=\"fusion_bench.compat.modelpool.load_modelpool_from_config\",\n        )\n        log.info(\"loading method\")\n        self.method = self._instantiate_and_setup(\n            self._method,\n            compat_load_fn=\"fusion_bench.compat.method.load_algorithm_from_config\",\n        )\n        if self._taskpool is not None:\n            log.info(\"loading task pool\")\n            self.taskpool = self._instantiate_and_setup(\n                self._taskpool,\n                compat_load_fn=\"fusion_bench.compat.taskpool.load_taskpool_from_config\",\n            )\n\n        self.method.on_run_start()\n        merged_model = self.method.run(self.modelpool)\n        self.method.on_run_end()\n\n        if merged_model is None:\n            log.info(\n                \"No merged model returned by the method. Skipping saving and evaluation.\"\n            )\n        else:\n            self.save_merged_model(merged_model)\n            if self.taskpool is not None:\n                report = self.evaluate_merged_model(self.taskpool, merged_model)\n                try:\n                    if rank_zero_only.rank == 0:\n                        print_json(report, print_type=False)\n                except Exception as e:\n                    log.warning(f\"Failed to pretty print the report: {e}\")\n                    log.info(report)\n                if self.report_save_path is not None:\n                    # save report (Dict) to a file\n                    # if the directory of `save_report` does not exists, create it\n                    if (\n                        \"{log_dir}\" in self.report_save_path\n                        and self.log_dir is not None\n                    ):\n                        self.report_save_path = self.report_save_path.format(\n                            log_dir=self.log_dir\n                        )\n                    os.makedirs(os.path.dirname(self.report_save_path), exist_ok=True)\n                    json.dump(report, open(self.report_save_path, \"w\"))\n            else:\n                log.info(\"No task pool specified. Skipping evaluation.\")\n</code></pre>"},{"location":"api/fusion_bench.program/#fusion_bench.programs.FabricModelFusionProgram.evaluate_merged_model","title":"<code>evaluate_merged_model(taskpool, merged_model, *args, **kwargs)</code>","text":"<p>Evaluates the merged model using the provided task pool.</p> <p>Depending on the type of the merged model, this function handles the evaluation differently: - If the merged model is an instance of <code>nn.Module</code>, it directly evaluates the model. - If the merged model is a dictionary, it extracts the model from the dictionary and evaluates it.   The evaluation report is then updated with the remaining dictionary items. - If the merged model is an iterable, it recursively evaluates each model in the iterable. - Raises a <code>ValueError</code> if the merged model is of an invalid type.</p> <p>Parameters:</p> <ul> <li> <code>taskpool</code>               (<code>BaseTaskPool</code>)           \u2013            <p>The task pool used for evaluating the merged model.</p> </li> <li> <code>merged_model</code>               (<code>Union[Module, Dict, Iterable]</code>)           \u2013            <p>The merged model to be evaluated. It can be an instance of <code>nn.Module</code>, a dictionary, or an iterable.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional positional arguments to be passed to the <code>evaluate</code> method of the taskpool.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments to be passed to the <code>evaluate</code> method of the taskpool.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Dict, List, Any]</code>           \u2013            <p>The evaluation report. The type of the report depends on the type of the merged model:</p> </li> <li> <code>Union[Dict, List, Any]</code>           \u2013            <ul> <li>If the merged model is an instance of <code>nn.Module</code>, the report is a dictionary.</li> </ul> </li> <li> <code>Union[Dict, List, Any]</code>           \u2013            <ul> <li>If the merged model is a dictionary, the report is a dictionary updated with the remaining dictionary items.</li> </ul> </li> <li> <code>Union[Dict, List, Any]</code>           \u2013            <ul> <li>If the merged model is an iterable, the report is a list of evaluation reports.</li> </ul> </li> </ul> Source code in <code>fusion_bench/programs/fabric_fusion_program.py</code> <pre><code>def evaluate_merged_model(\n    self,\n    taskpool: BaseTaskPool,\n    merged_model: Union[nn.Module, Dict, Iterable],\n    *args: Any,\n    **kwargs: Any,\n) -&gt; Union[Dict, List, Any]:\n    \"\"\"\n    Evaluates the merged model using the provided task pool.\n\n    Depending on the type of the merged model, this function handles the evaluation differently:\n    - If the merged model is an instance of `nn.Module`, it directly evaluates the model.\n    - If the merged model is a dictionary, it extracts the model from the dictionary and evaluates it.\n      The evaluation report is then updated with the remaining dictionary items.\n    - If the merged model is an iterable, it recursively evaluates each model in the iterable.\n    - Raises a `ValueError` if the merged model is of an invalid type.\n\n    Args:\n        taskpool: The task pool used for evaluating the merged model.\n        merged_model: The merged model to be evaluated. It can be an instance of `nn.Module`, a dictionary, or an iterable.\n        *args: Additional positional arguments to be passed to the `evaluate` method of the taskpool.\n        **kwargs: Additional keyword arguments to be passed to the `evaluate` method of the taskpool.\n\n    Returns:\n        The evaluation report. The type of the report depends on the type of the merged model:\n        - If the merged model is an instance of `nn.Module`, the report is a dictionary.\n        - If the merged model is a dictionary, the report is a dictionary updated with the remaining dictionary items.\n        - If the merged model is an iterable, the report is a list of evaluation reports.\n    \"\"\"\n    if isinstance(merged_model, nn.Module):\n        report = taskpool.evaluate(merged_model, *args, **kwargs)\n        return report\n    elif isinstance(merged_model, Dict):\n        report = {}\n        for key, item in merged_model.items():\n            if isinstance(item, nn.Module):\n                report[key] = taskpool.evaluate(item, *args, **kwargs)\n            elif key == \"models\":\n                # for multi-model evaluation\n                report[key] = self.evaluate_merged_model(\n                    taskpool, item, *args, **kwargs\n                )\n            else:\n                # metadata\n                report[key] = item\n        return report\n    elif isinstance(merged_model, Iterable):\n        return [\n            self.evaluate_merged_model(taskpool, m, *args, **kwargs)\n            for m in tqdm(merged_model, desc=\"Evaluating models\")\n        ]\n    else:\n        raise ValueError(f\"Invalid type for merged model: {type(merged_model)}\")\n</code></pre>"},{"location":"api/fusion_bench.program/#fusion_bench.programs.FabricModelFusionProgram.run","title":"<code>run()</code>","text":"<p>Executes the model fusion program.</p> Source code in <code>fusion_bench/programs/fabric_fusion_program.py</code> <pre><code>def run(self):\n    \"\"\"\n    Executes the model fusion program.\n    \"\"\"\n    fabric = self.fabric\n    if self.seed is not None:\n        L.seed_everything(self.seed)\n\n    # create symbol link to hydra output directory\n    if (\n        rank_zero_only.rank == 0\n        and self.log_dir is not None\n        and os.path.abspath(self.log_dir) != os.path.abspath(get_hydra_output_dir())\n    ):\n        create_symlink(\n            get_hydra_output_dir(), self.log_dir, link_name=\"hydra_output\"\n        )\n\n    log.info(\"Running the model fusion program.\")\n    # setup the modelpool, method, and taskpool\n    log.info(\"loading model pool\")\n    self.modelpool = self._instantiate_and_setup(\n        self._modelpool,\n        compat_load_fn=\"fusion_bench.compat.modelpool.load_modelpool_from_config\",\n    )\n    log.info(\"loading method\")\n    self.method = self._instantiate_and_setup(\n        self._method,\n        compat_load_fn=\"fusion_bench.compat.method.load_algorithm_from_config\",\n    )\n    if self._taskpool is not None:\n        log.info(\"loading task pool\")\n        self.taskpool = self._instantiate_and_setup(\n            self._taskpool,\n            compat_load_fn=\"fusion_bench.compat.taskpool.load_taskpool_from_config\",\n        )\n\n    self.method.on_run_start()\n    merged_model = self.method.run(self.modelpool)\n    self.method.on_run_end()\n\n    if merged_model is None:\n        log.info(\n            \"No merged model returned by the method. Skipping saving and evaluation.\"\n        )\n    else:\n        self.save_merged_model(merged_model)\n        if self.taskpool is not None:\n            report = self.evaluate_merged_model(self.taskpool, merged_model)\n            try:\n                if rank_zero_only.rank == 0:\n                    print_json(report, print_type=False)\n            except Exception as e:\n                log.warning(f\"Failed to pretty print the report: {e}\")\n                log.info(report)\n            if self.report_save_path is not None:\n                # save report (Dict) to a file\n                # if the directory of `save_report` does not exists, create it\n                if (\n                    \"{log_dir}\" in self.report_save_path\n                    and self.log_dir is not None\n                ):\n                    self.report_save_path = self.report_save_path.format(\n                        log_dir=self.log_dir\n                    )\n                os.makedirs(os.path.dirname(self.report_save_path), exist_ok=True)\n                json.dump(report, open(self.report_save_path, \"w\"))\n        else:\n            log.info(\"No task pool specified. Skipping evaluation.\")\n</code></pre>"},{"location":"api/fusion_bench.program/#fusion_bench.programs.FabricModelFusionProgram.save_merged_model","title":"<code>save_merged_model(merged_model)</code>","text":"<p>Saves the merged model to the specified path.</p> Source code in <code>fusion_bench/programs/fabric_fusion_program.py</code> <pre><code>def save_merged_model(self, merged_model):\n    \"\"\"\n    Saves the merged model to the specified path.\n    \"\"\"\n    if self.merged_model_save_path is not None:\n        # path to save the merged model, use \"{log_dir}\" to refer to the logger directory\n        save_path: str = self.merged_model_save_path\n        if \"{log_dir}\" in save_path and self.log_dir is not None:\n            save_path = save_path.format(log_dir=self.log_dir)\n\n        if os.path.dirname(save_path):\n            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n        # save the merged model\n        if self.merged_model_save_kwargs is not None:\n            merged_model_save_kwargs = self.merged_model_save_kwargs\n        else:\n            merged_model_save_kwargs = {}\n        with timeit_context(f\"Saving the merged model to {save_path}\"):\n            self.modelpool.save_model(\n                merged_model,\n                save_path,\n                **merged_model_save_kwargs,\n            )\n    else:\n        print(\"No save path specified for the merged model. Skipping saving.\")\n</code></pre>"},{"location":"api/fusion_bench.taskpool/","title":"fusion_bench.taskpool","text":""},{"location":"api/fusion_bench.taskpool/#base-class","title":"Base Class","text":""},{"location":"api/fusion_bench.taskpool/#fusion_bench.BaseTaskPool","title":"<code>BaseTaskPool</code>","text":"<p>               Bases: <code>BaseYAMLSerializable</code></p> <p>Abstract base class for task pools in the FusionBench framework.</p> <p>A task pool represents a collection of evaluation tasks that can be used to assess model performance across multiple benchmarks or datasets. This base class defines the common interface that all task pool implementations must follow, ensuring consistency across different task types and evaluation scenarios.</p> <p>Task pools are designed to be configurable through YAML files and can be used in various model fusion and evaluation workflows. They provide a standardized way to evaluate models on multiple tasks and aggregate results.</p> <p>The class inherits from BaseYAMLSerializable to support configuration management and serialization capabilities.</p> <p>Attributes:</p> <ul> <li> <code>_program</code>           \u2013            <p>Optional program reference for execution context.</p> </li> <li> <code>_config_key</code>           \u2013            <p>Configuration key used for YAML configuration (\"taskpool\").</p> </li> </ul> Abstract Methods <p>evaluate: Must be implemented by subclasses to define task-specific     evaluation logic.</p> Example <p>Implementing a custom task pool:</p> <pre><code>class MyTaskPool(BaseTaskPool):\n\n\n    def evaluate(self, model, **kwargs):\n        results = {}\n        for task_name in self.tasks:\n            # Implement task-specific evaluation\n            results[task_name] = self._evaluate_task(model, task_name)\n        return results\n</code></pre> Source code in <code>fusion_bench/taskpool/base_pool.py</code> <pre><code>class BaseTaskPool(BaseYAMLSerializable):\n    \"\"\"Abstract base class for task pools in the FusionBench framework.\n\n    A task pool represents a collection of evaluation tasks that can be used to\n    assess model performance across multiple benchmarks or datasets. This base\n    class defines the common interface that all task pool implementations must\n    follow, ensuring consistency across different task types and evaluation\n    scenarios.\n\n    Task pools are designed to be configurable through YAML files and can be\n    used in various model fusion and evaluation workflows. They provide a\n    standardized way to evaluate models on multiple tasks and aggregate results.\n\n    The class inherits from BaseYAMLSerializable to support configuration\n    management and serialization capabilities.\n\n    Attributes:\n        _program: Optional program reference for execution context.\n        _config_key: Configuration key used for YAML configuration (\"taskpool\").\n\n    Abstract Methods:\n        evaluate: Must be implemented by subclasses to define task-specific\n            evaluation logic.\n\n    Example:\n        Implementing a custom task pool:\n\n        ```python\n        class MyTaskPool(BaseTaskPool):\n\n\n            def evaluate(self, model, **kwargs):\n                results = {}\n                for task_name in self.tasks:\n                    # Implement task-specific evaluation\n                    results[task_name] = self._evaluate_task(model, task_name)\n                return results\n        ```\n    \"\"\"\n\n    _program = None\n    _config_key = \"taskpool\"\n\n    @abstractmethod\n    def evaluate(self, model: Any, *args: Any, **kwargs: Any) -&gt; Dict[str, Any]:\n        \"\"\"Evaluate a model on all tasks in the task pool and return aggregated results.\n\n        This abstract method defines the core evaluation interface that all task pool\n        implementations must provide. It should evaluate the given model on all tasks\n        managed by the pool and return a structured report of the results.\n\n        The evaluation process typically involves:\n        1. Iterating through all tasks in the pool\n        2. Running model inference on each task's dataset\n        3. Computing task-specific metrics\n        4. Aggregating results into a standardized report format\n\n        Args:\n            model: The model to evaluate. Can be any model type (PyTorch model,\n                Hugging Face model, etc.) that is compatible with the specific\n                task pool implementation.\n            *args: Additional positional arguments that may be needed for\n                task-specific evaluation procedures.\n            **kwargs: Additional keyword arguments for evaluation configuration,\n                such as batch_size, device, evaluation metrics, etc.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing evaluation results for each task.\n                The structure follows the pattern:\n\n                ```python\n                {\n                    \"task_name_1\": {\n                        \"metric_1\": value,\n                        \"metric_2\": value,\n                        ...\n                    },\n                    \"task_name_2\": {\n                        \"metric_1\": value,\n                        \"metric_2\": value,\n                        ...\n                    },\n                    ...\n                }\n                ```\n\n        Example:\n            For an image classification task pool:\n\n            ```python\n            results = task_pool.evaluate(model)\n            # Returns:\n            # {\n            #     \"mnist\": {\n            #         \"accuracy\": 0.95,\n            #         \"loss\": 0.15,\n            #     },\n            #     \"cifar10\": {\n            #         \"accuracy\": 0.87,\n            #         \"loss\": 0.42,\n            #     }\n            # }\n            ```\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n\n        Note:\n            Implementations should ensure that the returned dictionary structure\n            is consistent and that metric names are standardized across similar\n            task types to enable meaningful comparison and aggregation.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#fusion_bench.BaseTaskPool.evaluate","title":"<code>evaluate(model, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Evaluate a model on all tasks in the task pool and return aggregated results.</p> <p>This abstract method defines the core evaluation interface that all task pool implementations must provide. It should evaluate the given model on all tasks managed by the pool and return a structured report of the results.</p> <p>The evaluation process typically involves: 1. Iterating through all tasks in the pool 2. Running model inference on each task's dataset 3. Computing task-specific metrics 4. Aggregating results into a standardized report format</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Any</code>)           \u2013            <p>The model to evaluate. Can be any model type (PyTorch model, Hugging Face model, etc.) that is compatible with the specific task pool implementation.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional positional arguments that may be needed for task-specific evaluation procedures.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for evaluation configuration, such as batch_size, device, evaluation metrics, etc.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dict[str, Any]: A dictionary containing evaluation results for each task. The structure follows the pattern:</p> <pre><code>{\n    \"task_name_1\": {\n        \"metric_1\": value,\n        \"metric_2\": value,\n        ...\n    },\n    \"task_name_2\": {\n        \"metric_1\": value,\n        \"metric_2\": value,\n        ...\n    },\n    ...\n}\n</code></pre> </li> </ul> Example <p>For an image classification task pool:</p> <pre><code>results = task_pool.evaluate(model)\n# Returns:\n# {\n#     \"mnist\": {\n#         \"accuracy\": 0.95,\n#         \"loss\": 0.15,\n#     },\n#     \"cifar10\": {\n#         \"accuracy\": 0.87,\n#         \"loss\": 0.42,\n#     }\n# }\n</code></pre> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>This method must be implemented by subclasses.</p> </li> </ul> Note <p>Implementations should ensure that the returned dictionary structure is consistent and that metric names are standardized across similar task types to enable meaningful comparison and aggregation.</p> Source code in <code>fusion_bench/taskpool/base_pool.py</code> <pre><code>@abstractmethod\ndef evaluate(self, model: Any, *args: Any, **kwargs: Any) -&gt; Dict[str, Any]:\n    \"\"\"Evaluate a model on all tasks in the task pool and return aggregated results.\n\n    This abstract method defines the core evaluation interface that all task pool\n    implementations must provide. It should evaluate the given model on all tasks\n    managed by the pool and return a structured report of the results.\n\n    The evaluation process typically involves:\n    1. Iterating through all tasks in the pool\n    2. Running model inference on each task's dataset\n    3. Computing task-specific metrics\n    4. Aggregating results into a standardized report format\n\n    Args:\n        model: The model to evaluate. Can be any model type (PyTorch model,\n            Hugging Face model, etc.) that is compatible with the specific\n            task pool implementation.\n        *args: Additional positional arguments that may be needed for\n            task-specific evaluation procedures.\n        **kwargs: Additional keyword arguments for evaluation configuration,\n            such as batch_size, device, evaluation metrics, etc.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing evaluation results for each task.\n            The structure follows the pattern:\n\n            ```python\n            {\n                \"task_name_1\": {\n                    \"metric_1\": value,\n                    \"metric_2\": value,\n                    ...\n                },\n                \"task_name_2\": {\n                    \"metric_1\": value,\n                    \"metric_2\": value,\n                    ...\n                },\n                ...\n            }\n            ```\n\n    Example:\n        For an image classification task pool:\n\n        ```python\n        results = task_pool.evaluate(model)\n        # Returns:\n        # {\n        #     \"mnist\": {\n        #         \"accuracy\": 0.95,\n        #         \"loss\": 0.15,\n        #     },\n        #     \"cifar10\": {\n        #         \"accuracy\": 0.87,\n        #         \"loss\": 0.42,\n        #     }\n        # }\n        ```\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n\n    Note:\n        Implementations should ensure that the returned dictionary structure\n        is consistent and that metric names are standardized across similar\n        task types to enable meaningful comparison and aggregation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#vision-task-pool","title":"Vision Task Pool","text":""},{"location":"api/fusion_bench.taskpool/#nyuv2-tasks","title":"NYUv2 Tasks","text":""},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.NYUv2TaskPool","title":"<code>NYUv2TaskPool</code>","text":"<p>               Bases: <code>TaskPool</code></p> <p>Task pool for multi-task learning evaluation on the NYUv2 dataset.</p> <p>This task pool provides evaluation capabilities for multi-task learning models on the NYU Depth V2 (NYUv2) dataset, which is a popular benchmark for indoor scene understanding. The dataset supports multiple computer vision tasks including semantic segmentation, depth estimation, and surface normal prediction.</p> <p>The task pool is designed to work with encoder-decoder architectures where a shared encoder processes input images and task-specific decoders generate predictions for different tasks. It integrates with PyTorch Lightning for streamlined training and evaluation workflows.</p> Supported Tasks <ul> <li>Semantic segmentation</li> <li>Depth estimation</li> <li>Surface normal prediction</li> </ul> Source code in <code>fusion_bench/taskpool/nyuv2_taskpool.py</code> <pre><code>class NYUv2TaskPool(TaskPool):\n    \"\"\"Task pool for multi-task learning evaluation on the NYUv2 dataset.\n\n    This task pool provides evaluation capabilities for multi-task learning models\n    on the NYU Depth V2 (NYUv2) dataset, which is a popular benchmark for indoor\n    scene understanding. The dataset supports multiple computer vision tasks\n    including semantic segmentation, depth estimation, and surface normal prediction.\n\n    The task pool is designed to work with encoder-decoder architectures where\n    a shared encoder processes input images and task-specific decoders generate\n    predictions for different tasks. It integrates with PyTorch Lightning for\n    streamlined training and evaluation workflows.\n\n    Supported Tasks:\n        - Semantic segmentation\n        - Depth estimation\n        - Surface normal prediction\n    \"\"\"\n\n    _trainer: L.Trainer = None\n\n    def __init__(self, taskpool_config: DictConfig):\n        \"\"\"Initialize the NYUv2 task pool with configuration settings.\n\n        Args:\n            taskpool_config: Configuration object containing all necessary\n                parameters for the task pool, including:\n                - data_dir: Path to the directory containing NYUv2 dataset\n                - tasks: List of tasks to evaluate (e.g., [\"semantic\", \"depth\"])\n                - batch_size: Batch size for evaluation data loader\n                - num_workers: Number of worker processes for data loading\n        \"\"\"\n        self.config = taskpool_config\n\n    def load_datasets(self):\n        log.info(\"Loading NYUv2 dataset\")\n        data_path = str(Path(self.config.data_dir) / \"nyuv2\")\n\n        train_dataset = NYUv2(root=data_path, train=True)\n        val_dataset = NYUv2(root=data_path, train=False)\n        return train_dataset, val_dataset\n\n    @property\n    def trainer(self):\n        if self._trainer is None:\n            self._trainer = L.Trainer(devices=1)\n        return self._trainer\n\n    def get_decoders(self):\n        from fusion_bench.modelpool.nyuv2_modelpool import NYUv2ModelPool\n\n        modelpool: NYUv2ModelPool = self._program.modelpool\n        decoders = nn.ModuleDict()\n        for task in self.config.tasks:\n            decoders[task] = modelpool.load_model(task, encoder_only=False).decoders[\n                task\n            ]\n        return decoders\n\n    def evaluate(self, encoder: ResnetDilated):\n        model = NYUv2MTLModule(\n            encoder,\n            self.get_decoders(),\n            tasks=self.config.tasks,\n            task_weights=[1] * len(self.config.tasks),\n        )\n        _, val_dataset = self.load_datasets()\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=False,\n            num_workers=self.config.num_workers,\n        )\n        report = self.trainer.validate(model, val_loader)\n        if isinstance(report, list) and len(report) == 1:\n            report = report[0]\n        return report\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.NYUv2TaskPool.__init__","title":"<code>__init__(taskpool_config)</code>","text":"<p>Initialize the NYUv2 task pool with configuration settings.</p> <p>Parameters:</p> <ul> <li> <code>taskpool_config</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration object containing all necessary parameters for the task pool, including: - data_dir: Path to the directory containing NYUv2 dataset - tasks: List of tasks to evaluate (e.g., [\"semantic\", \"depth\"]) - batch_size: Batch size for evaluation data loader - num_workers: Number of worker processes for data loading</p> </li> </ul> Source code in <code>fusion_bench/taskpool/nyuv2_taskpool.py</code> <pre><code>def __init__(self, taskpool_config: DictConfig):\n    \"\"\"Initialize the NYUv2 task pool with configuration settings.\n\n    Args:\n        taskpool_config: Configuration object containing all necessary\n            parameters for the task pool, including:\n            - data_dir: Path to the directory containing NYUv2 dataset\n            - tasks: List of tasks to evaluate (e.g., [\"semantic\", \"depth\"])\n            - batch_size: Batch size for evaluation data loader\n            - num_workers: Number of worker processes for data loading\n    \"\"\"\n    self.config = taskpool_config\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#clip-task-pool","title":"CLIP Task Pool","text":""},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.CLIPVisionModelTaskPool","title":"<code>CLIPVisionModelTaskPool</code>","text":"<p>               Bases: <code>HydraConfigMixin</code>, <code>LightningFabricMixin</code>, <code>BaseTaskPool</code></p> <p>This class is used to define the image classification task for CLIP models.</p> <p>Attributes:</p> <ul> <li> <code>test_datasets</code>               (<code>Union[DictConfig, Dict[str, Dataset]]</code>)           \u2013            <p>The test datasets to evaluate the model on.</p> </li> <li> <code>processor</code>               (<code>Union[DictConfig, CLIPProcessor]</code>)           \u2013            <p>The processor used for preprocessing the input data.</p> </li> <li> <code>data_processor</code>               (<code>Union[DictConfig, CLIPProcessor]</code>)           \u2013            <p>The data processor used for processing the input data.</p> </li> <li> <code>clip_model</code>               (<code>Union[DictConfig, CLIPModel]</code>)           \u2013            <p>The CLIP model used for evaluation.</p> </li> <li> <code>dataloader_kwargs</code>               (<code>DictConfig</code>)           \u2013            <p>Keyword arguments for the data loader.</p> </li> <li> <code>layer_wise_feature_save_path</code>               (<code>Optional[str]</code>)           \u2013            <p>Path to save the layer-wise features.</p> </li> <li> <code>layer_wise_feature_first_token_only</code>               (<code>bool</code>)           \u2013            <p>Boolean indicating whether to save only the first token of the features.</p> </li> <li> <code>layer_wise_feature_max_num</code>               (<code>Optional[int]</code>)           \u2013            <p>Maximum number of features to save.</p> </li> <li> <code>fast_dev_run</code>               (<code>bool</code>)           \u2013            <p>Boolean indicating whether to run in fast development mode.</p> </li> </ul> Source code in <code>fusion_bench/taskpool/clip_vision/taskpool.py</code> <pre><code>@auto_register_config\nclass CLIPVisionModelTaskPool(\n    HydraConfigMixin,\n    LightningFabricMixin,\n    BaseTaskPool,\n):\n    \"\"\"\n    This class is used to define the image classification task for CLIP models.\n\n    Attributes:\n        test_datasets (Union[DictConfig, Dict[str, Dataset]]): The test datasets to evaluate the model on.\n        processor (Union[DictConfig, CLIPProcessor]): The processor used for preprocessing the input data.\n        data_processor (Union[DictConfig, CLIPProcessor]): The data processor used for processing the input data.\n        clip_model (Union[DictConfig, CLIPModel]): The CLIP model used for evaluation.\n        dataloader_kwargs (DictConfig): Keyword arguments for the data loader.\n        layer_wise_feature_save_path (Optional[str]): Path to save the layer-wise features.\n        layer_wise_feature_first_token_only (bool): Boolean indicating whether to save only the first token of the features.\n        layer_wise_feature_max_num (Optional[int]): Maximum number of features to save.\n        fast_dev_run (bool): Boolean indicating whether to run in fast development mode.\n    \"\"\"\n\n    _is_setup = False\n\n    # hooks and handles for saving layer-wise features\n    _layer_wise_feature_save_hooks: Dict[int, LayerWiseFeatureSaver] = {}\n    _layer_wise_feature_save_hook_handles: Dict[int, RemovableHandle] = {}\n\n    _config_mapping = BaseTaskPool._config_mapping | {\n        \"_test_datasets\": \"test_datasets\",\n        \"_processor\": \"processor\",\n        \"_data_processor\": \"data_processor\",\n        \"_clip_model\": \"clip_model\",\n        \"_dataloader_kwargs\": \"dataloader_kwargs\",\n        \"_layer_wise_feature_save_path\": \"layer_wise_feature_save_path\",\n        \"fast_dev_run\": \"fast_dev_run\",\n    }\n\n    def __init__(\n        self,\n        test_datasets: Union[DictConfig, Dict[str, Dataset]],\n        *,\n        processor: Union[str, DictConfig, CLIPProcessor],\n        clip_model: Union[str, DictConfig, CLIPModel],\n        data_processor: Union[DictConfig, CLIPProcessor] = None,\n        dataloader_kwargs: DictConfig = None,\n        layer_wise_feature_save_path: Optional[str] = None,\n        layer_wise_feature_first_token_only: bool = True,\n        layer_wise_feature_max_num: Optional[int] = None,\n        fast_dev_run: Optional[bool] = None,\n        move_to_device: bool = True,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the CLIPVisionModelTaskPool.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._test_datasets = test_datasets\n        self._processor = processor\n        self._data_processor = data_processor\n        self._clip_model = clip_model\n        self._dataloader_kwargs = dataloader_kwargs or {}\n\n        # layer-wise feature saving\n        self._layer_wise_feature_save_path = layer_wise_feature_save_path\n        self.layer_wise_feature_save_path = (\n            Path(layer_wise_feature_save_path)\n            if layer_wise_feature_save_path is not None\n            else None\n        )\n        self.layer_wise_feature_first_token_only = layer_wise_feature_first_token_only\n        self.layer_wise_feature_max_num = layer_wise_feature_max_num\n\n        if self.fast_dev_run is None:\n            self.fast_dev_run = RuntimeConstants().debug\n        else:\n            self.fast_dev_run = fast_dev_run\n\n    def setup(self):\n        \"\"\"\n        Set up the processor, data processor, CLIP model, test datasets, and data loaders.\n        \"\"\"\n        # setup processor and clip model\n        if isinstance(self._processor, str):\n            self.processor = CLIPProcessor.from_pretrained(self._processor)\n        elif (\n            isinstance(self._processor, (dict, DictConfig))\n            and \"_target_\" in self._processor\n        ):\n            self.processor = instantiate(self._processor)\n        else:\n            self.processor = self._processor\n\n        if self._data_processor is None:\n            self.data_processor = self.processor\n        else:\n            self.data_processor = (\n                instantiate(self._data_processor)\n                if isinstance(self._data_processor, DictConfig)\n                else self._data_processor\n            )\n\n        if isinstance(self._clip_model, str):\n            self.clip_model = CLIPModel.from_pretrained(self._clip_model)\n        elif (\n            isinstance(self._clip_model, (dict, DictConfig))\n            and \"_target_\" in self._clip_model\n        ):\n            self.clip_model = instantiate(self._clip_model)\n        else:\n            self.clip_model = self._clip_model\n\n        self.clip_model = self.fabric.to_device(self.clip_model)\n        self.clip_model.requires_grad_(False)\n        self.clip_model.eval()\n\n        # Load the test datasets\n        self.test_datasets = {\n            name: instantiate(dataset) if isinstance(dataset, DictConfig) else dataset\n            for name, dataset in self._test_datasets.items()\n        }\n        self.test_datasets = {\n            name: CLIPDataset(dataset, self.data_processor)\n            for name, dataset in self.test_datasets.items()\n        }\n        # Setup the dataloaders\n        self.test_dataloaders = {\n            name: DataLoader(\n                dataset,\n                **self._dataloader_kwargs,\n                collate_fn=(\n                    raw_image_collate_fn if self.data_processor is None else None\n                ),\n            )\n            for name, dataset in self.test_datasets.items()\n        }\n        self.test_dataloaders = {\n            name: self.fabric.setup_dataloaders(\n                dataloader, move_to_device=self.move_to_device\n            )\n            for name, dataloader in self.test_dataloaders.items()\n        }\n\n        self._is_setup = True\n\n    @torch.no_grad()\n    def _evaluate(\n        self,\n        classifier: HFCLIPClassifier,\n        test_loader: DataLoader,\n        num_classes: int,\n        task_name: str = None,\n    ):\n        \"\"\"\n        Evaluate the classifier on the test dataset (single-task evaluation).\n\n        Args:\n            classifier (HFCLIPClassifier): The classifier to evaluate.\n            test_loader (DataLoader): The data loader for the test dataset.\n            num_classes (int): The number of classes in the classification task.\n            task_name (str): The name of the task.\n\n        Returns:\n            Dict[str, float]: A dictionary containing the accuracy and loss of the classifier on the test dataset.\n        \"\"\"\n        accuracy: MulticlassAccuracy = Accuracy(\n            task=\"multiclass\", num_classes=num_classes\n        )\n        classifier.eval()\n        loss_metric = MeanMetric()\n        # if fast_dev_run is set, we only evaluate on a batch of the data\n        if self.fast_dev_run:\n            log.info(\"Running under fast_dev_run mode, evaluating on a single batch.\")\n            test_loader = itertools.islice(test_loader, 1)\n        else:\n            test_loader = test_loader\n\n        pbar = tqdm(\n            test_loader,\n            desc=f\"Evaluating {task_name}\",\n            leave=False,\n            dynamic_ncols=True,\n        )\n        for batch in pbar:\n            inputs, targets = batch\n            outputs = classifier(\n                inputs,\n                return_image_embeds=True,\n                return_dict=True,\n                task_name=task_name,\n            )\n            logits: Tensor = outputs[\"logits\"]\n            if logits.device != targets.device:\n                targets = targets.to(logits.device)\n\n            loss = F.cross_entropy(logits, targets)\n            loss_metric.update(loss.detach().cpu())\n            acc = accuracy(logits.detach().cpu(), targets.detach().cpu())\n            pbar.set_postfix(\n                {\n                    \"accuracy\": accuracy.compute().item(),\n                    \"loss\": loss_metric.compute().item(),\n                }\n            )\n\n        acc = accuracy.compute().item()\n        loss = loss_metric.compute().item()\n        results = {\"accuracy\": acc, \"loss\": loss}\n        return results\n\n    def evaluate(\n        self,\n        model: Union[CLIPVisionModel, CLIPVisionTransformer],\n        name=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Evaluate the model on the image classification task.\n\n        Args:\n            model (Union[CLIPVisionModel, CLIPVisionTransformer]): The model to evaluate.\n            name (Optional[str]): The name of the model. This will be logged into the report if not None.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the evaluation results for each task.\n        \"\"\"\n        if not self._is_setup:\n            self.setup()\n\n        report = {}\n        # CLIPVisionModel works the same with CLIPVisionTransformer, so we can use it directly\n        if hasattr(model, \"is_surgery_model\") and model.is_surgery_model:\n            log.info(\"running evaluation on a surgery model.\")\n            model: \"SurgeryModelWrapper\" = model\n            self.clip_model.vision_model = model\n        else:\n            # replace the vision encoder with the model\n            self.clip_model.vision_model = model\n        classifier = HFCLIPClassifier(\n            self.clip_model,\n            processor=self.processor,\n        )\n        if self.move_to_device:\n            classifier = cast(HFCLIPClassifier, self.fabric.to_device(classifier))\n        # collect basic model information\n        training_params, all_params = count_parameters(model)\n        report[\"model_info\"] = {\n            \"trainable_params\": training_params,\n            \"all_params\": all_params,\n            \"trainable_percentage\": training_params / all_params,\n        }\n        if name is not None:\n            report[\"model_info\"][\"name\"] = name\n\n        # evaluate on each task\n        pbar = tqdm(\n            self.test_dataloaders.items(),\n            desc=\"Evaluating tasks\",\n            total=len(self.test_dataloaders),\n        )\n        for task_name, test_dataloader in pbar:\n            classnames, templates = get_classnames_and_templates(task_name)\n            self.on_task_evaluation_begin(classifier, task_name)\n            classifier.set_classification_task(classnames, templates)\n            result = self._evaluate(\n                classifier,\n                test_dataloader,\n                num_classes=len(classnames),\n                task_name=task_name,\n            )\n            report[task_name] = result\n            self.on_task_evaluation_end()\n\n        # calculate the average accuracy and loss\n        if \"average\" not in report:\n            report[\"average\"] = {}\n            accuracies = [\n                value[\"accuracy\"]\n                for key, value in report.items()\n                if \"accuracy\" in value\n            ]\n            if len(accuracies) &gt; 0:\n                average_accuracy = sum(accuracies) / len(accuracies)\n                report[\"average\"][\"accuracy\"] = average_accuracy\n            losses = [value[\"loss\"] for key, value in report.items() if \"loss\" in value]\n            if len(losses) &gt; 0:\n                average_loss = sum(losses) / len(losses)\n                report[\"average\"][\"loss\"] = average_loss\n\n        log.info(f\"Evaluation Result: {report}\")\n        if self.fabric.is_global_zero and len(self.fabric._loggers) &gt; 0:\n            save_path = os.path.join(self.log_dir, \"report.json\")\n            for version in itertools.count(1):\n                if not os.path.exists(save_path):\n                    break\n                # if the file already exists, increment the version to avoid overwriting\n                save_path = os.path.join(self.log_dir, f\"report_{version}.json\")\n            with open(save_path, \"w\") as fp:\n                json.dump(report, fp)\n            log.info(f\"Evaluation report saved to {save_path}\")\n        return report\n\n    def on_task_evaluation_begin(self, classifier: HFCLIPClassifier, task_name: str):\n        \"\"\"\n        Called at the beginning of task evaluation to set up hooks for saving layer-wise features.\n\n        Args:\n            classifier (HFCLIPClassifier): The classifier being evaluated.\n            task_name (str): The name of the task being evaluated.\n        \"\"\"\n        if self.layer_wise_feature_save_path is not None:\n            # setup hooks for saving layer-wise features\n            assert isinstance(\n                classifier.clip_model.vision_model,\n                (CLIPVisionTransformer, CLIPVisionModel),\n            ), \"Vision model is expected to be a CLIPVisionTransformer\"\n            vision_model = classifier.clip_model.vision_model\n            if isinstance(vision_model, CLIPVisionModel):\n                vision_model = vision_model.vision_model\n                # assign forward hooks for each layer\n            for i, layer in enumerate(vision_model.encoder.layers):\n                self._layer_wise_feature_save_hooks[i] = LayerWiseFeatureSaver(\n                    self.layer_wise_feature_save_path / task_name / f\"layer_{i}.pth\",\n                    first_token_only=self.layer_wise_feature_first_token_only,\n                    max_num=self.layer_wise_feature_max_num,\n                )\n                self._layer_wise_feature_save_hook_handles[i] = (\n                    layer.register_forward_hook(self._layer_wise_feature_save_hooks[i])\n                )\n\n    def on_task_evaluation_end(self):\n        \"\"\"\n        Called at the end of task evaluation to save features and remove hooks.\n        \"\"\"\n        if self.layer_wise_feature_save_path is not None:\n            # save features and remove hooks after evaluation\n            for i, hook in self._layer_wise_feature_save_hooks.items():\n                hook.save_features()\n                self._layer_wise_feature_save_hook_handles[i].remove()\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.CLIPVisionModelTaskPool.__init__","title":"<code>__init__(test_datasets, *, processor, clip_model, data_processor=None, dataloader_kwargs=None, layer_wise_feature_save_path=None, layer_wise_feature_first_token_only=True, layer_wise_feature_max_num=None, fast_dev_run=None, move_to_device=True, **kwargs)</code>","text":"<p>Initialize the CLIPVisionModelTaskPool.</p> Source code in <code>fusion_bench/taskpool/clip_vision/taskpool.py</code> <pre><code>def __init__(\n    self,\n    test_datasets: Union[DictConfig, Dict[str, Dataset]],\n    *,\n    processor: Union[str, DictConfig, CLIPProcessor],\n    clip_model: Union[str, DictConfig, CLIPModel],\n    data_processor: Union[DictConfig, CLIPProcessor] = None,\n    dataloader_kwargs: DictConfig = None,\n    layer_wise_feature_save_path: Optional[str] = None,\n    layer_wise_feature_first_token_only: bool = True,\n    layer_wise_feature_max_num: Optional[int] = None,\n    fast_dev_run: Optional[bool] = None,\n    move_to_device: bool = True,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the CLIPVisionModelTaskPool.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._test_datasets = test_datasets\n    self._processor = processor\n    self._data_processor = data_processor\n    self._clip_model = clip_model\n    self._dataloader_kwargs = dataloader_kwargs or {}\n\n    # layer-wise feature saving\n    self._layer_wise_feature_save_path = layer_wise_feature_save_path\n    self.layer_wise_feature_save_path = (\n        Path(layer_wise_feature_save_path)\n        if layer_wise_feature_save_path is not None\n        else None\n    )\n    self.layer_wise_feature_first_token_only = layer_wise_feature_first_token_only\n    self.layer_wise_feature_max_num = layer_wise_feature_max_num\n\n    if self.fast_dev_run is None:\n        self.fast_dev_run = RuntimeConstants().debug\n    else:\n        self.fast_dev_run = fast_dev_run\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.CLIPVisionModelTaskPool.evaluate","title":"<code>evaluate(model, name=None, **kwargs)</code>","text":"<p>Evaluate the model on the image classification task.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Union[CLIPVisionModel, CLIPVisionTransformer]</code>)           \u2013            <p>The model to evaluate.</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The name of the model. This will be logged into the report if not None.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Dict[str, Any]: A dictionary containing the evaluation results for each task.</p> </li> </ul> Source code in <code>fusion_bench/taskpool/clip_vision/taskpool.py</code> <pre><code>def evaluate(\n    self,\n    model: Union[CLIPVisionModel, CLIPVisionTransformer],\n    name=None,\n    **kwargs,\n):\n    \"\"\"\n    Evaluate the model on the image classification task.\n\n    Args:\n        model (Union[CLIPVisionModel, CLIPVisionTransformer]): The model to evaluate.\n        name (Optional[str]): The name of the model. This will be logged into the report if not None.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the evaluation results for each task.\n    \"\"\"\n    if not self._is_setup:\n        self.setup()\n\n    report = {}\n    # CLIPVisionModel works the same with CLIPVisionTransformer, so we can use it directly\n    if hasattr(model, \"is_surgery_model\") and model.is_surgery_model:\n        log.info(\"running evaluation on a surgery model.\")\n        model: \"SurgeryModelWrapper\" = model\n        self.clip_model.vision_model = model\n    else:\n        # replace the vision encoder with the model\n        self.clip_model.vision_model = model\n    classifier = HFCLIPClassifier(\n        self.clip_model,\n        processor=self.processor,\n    )\n    if self.move_to_device:\n        classifier = cast(HFCLIPClassifier, self.fabric.to_device(classifier))\n    # collect basic model information\n    training_params, all_params = count_parameters(model)\n    report[\"model_info\"] = {\n        \"trainable_params\": training_params,\n        \"all_params\": all_params,\n        \"trainable_percentage\": training_params / all_params,\n    }\n    if name is not None:\n        report[\"model_info\"][\"name\"] = name\n\n    # evaluate on each task\n    pbar = tqdm(\n        self.test_dataloaders.items(),\n        desc=\"Evaluating tasks\",\n        total=len(self.test_dataloaders),\n    )\n    for task_name, test_dataloader in pbar:\n        classnames, templates = get_classnames_and_templates(task_name)\n        self.on_task_evaluation_begin(classifier, task_name)\n        classifier.set_classification_task(classnames, templates)\n        result = self._evaluate(\n            classifier,\n            test_dataloader,\n            num_classes=len(classnames),\n            task_name=task_name,\n        )\n        report[task_name] = result\n        self.on_task_evaluation_end()\n\n    # calculate the average accuracy and loss\n    if \"average\" not in report:\n        report[\"average\"] = {}\n        accuracies = [\n            value[\"accuracy\"]\n            for key, value in report.items()\n            if \"accuracy\" in value\n        ]\n        if len(accuracies) &gt; 0:\n            average_accuracy = sum(accuracies) / len(accuracies)\n            report[\"average\"][\"accuracy\"] = average_accuracy\n        losses = [value[\"loss\"] for key, value in report.items() if \"loss\" in value]\n        if len(losses) &gt; 0:\n            average_loss = sum(losses) / len(losses)\n            report[\"average\"][\"loss\"] = average_loss\n\n    log.info(f\"Evaluation Result: {report}\")\n    if self.fabric.is_global_zero and len(self.fabric._loggers) &gt; 0:\n        save_path = os.path.join(self.log_dir, \"report.json\")\n        for version in itertools.count(1):\n            if not os.path.exists(save_path):\n                break\n            # if the file already exists, increment the version to avoid overwriting\n            save_path = os.path.join(self.log_dir, f\"report_{version}.json\")\n        with open(save_path, \"w\") as fp:\n            json.dump(report, fp)\n        log.info(f\"Evaluation report saved to {save_path}\")\n    return report\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.CLIPVisionModelTaskPool.on_task_evaluation_begin","title":"<code>on_task_evaluation_begin(classifier, task_name)</code>","text":"<p>Called at the beginning of task evaluation to set up hooks for saving layer-wise features.</p> <p>Parameters:</p> <ul> <li> <code>classifier</code>               (<code>HFCLIPClassifier</code>)           \u2013            <p>The classifier being evaluated.</p> </li> <li> <code>task_name</code>               (<code>str</code>)           \u2013            <p>The name of the task being evaluated.</p> </li> </ul> Source code in <code>fusion_bench/taskpool/clip_vision/taskpool.py</code> <pre><code>def on_task_evaluation_begin(self, classifier: HFCLIPClassifier, task_name: str):\n    \"\"\"\n    Called at the beginning of task evaluation to set up hooks for saving layer-wise features.\n\n    Args:\n        classifier (HFCLIPClassifier): The classifier being evaluated.\n        task_name (str): The name of the task being evaluated.\n    \"\"\"\n    if self.layer_wise_feature_save_path is not None:\n        # setup hooks for saving layer-wise features\n        assert isinstance(\n            classifier.clip_model.vision_model,\n            (CLIPVisionTransformer, CLIPVisionModel),\n        ), \"Vision model is expected to be a CLIPVisionTransformer\"\n        vision_model = classifier.clip_model.vision_model\n        if isinstance(vision_model, CLIPVisionModel):\n            vision_model = vision_model.vision_model\n            # assign forward hooks for each layer\n        for i, layer in enumerate(vision_model.encoder.layers):\n            self._layer_wise_feature_save_hooks[i] = LayerWiseFeatureSaver(\n                self.layer_wise_feature_save_path / task_name / f\"layer_{i}.pth\",\n                first_token_only=self.layer_wise_feature_first_token_only,\n                max_num=self.layer_wise_feature_max_num,\n            )\n            self._layer_wise_feature_save_hook_handles[i] = (\n                layer.register_forward_hook(self._layer_wise_feature_save_hooks[i])\n            )\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.CLIPVisionModelTaskPool.on_task_evaluation_end","title":"<code>on_task_evaluation_end()</code>","text":"<p>Called at the end of task evaluation to save features and remove hooks.</p> Source code in <code>fusion_bench/taskpool/clip_vision/taskpool.py</code> <pre><code>def on_task_evaluation_end(self):\n    \"\"\"\n    Called at the end of task evaluation to save features and remove hooks.\n    \"\"\"\n    if self.layer_wise_feature_save_path is not None:\n        # save features and remove hooks after evaluation\n        for i, hook in self._layer_wise_feature_save_hooks.items():\n            hook.save_features()\n            self._layer_wise_feature_save_hook_handles[i].remove()\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.CLIPVisionModelTaskPool.setup","title":"<code>setup()</code>","text":"<p>Set up the processor, data processor, CLIP model, test datasets, and data loaders.</p> Source code in <code>fusion_bench/taskpool/clip_vision/taskpool.py</code> <pre><code>def setup(self):\n    \"\"\"\n    Set up the processor, data processor, CLIP model, test datasets, and data loaders.\n    \"\"\"\n    # setup processor and clip model\n    if isinstance(self._processor, str):\n        self.processor = CLIPProcessor.from_pretrained(self._processor)\n    elif (\n        isinstance(self._processor, (dict, DictConfig))\n        and \"_target_\" in self._processor\n    ):\n        self.processor = instantiate(self._processor)\n    else:\n        self.processor = self._processor\n\n    if self._data_processor is None:\n        self.data_processor = self.processor\n    else:\n        self.data_processor = (\n            instantiate(self._data_processor)\n            if isinstance(self._data_processor, DictConfig)\n            else self._data_processor\n        )\n\n    if isinstance(self._clip_model, str):\n        self.clip_model = CLIPModel.from_pretrained(self._clip_model)\n    elif (\n        isinstance(self._clip_model, (dict, DictConfig))\n        and \"_target_\" in self._clip_model\n    ):\n        self.clip_model = instantiate(self._clip_model)\n    else:\n        self.clip_model = self._clip_model\n\n    self.clip_model = self.fabric.to_device(self.clip_model)\n    self.clip_model.requires_grad_(False)\n    self.clip_model.eval()\n\n    # Load the test datasets\n    self.test_datasets = {\n        name: instantiate(dataset) if isinstance(dataset, DictConfig) else dataset\n        for name, dataset in self._test_datasets.items()\n    }\n    self.test_datasets = {\n        name: CLIPDataset(dataset, self.data_processor)\n        for name, dataset in self.test_datasets.items()\n    }\n    # Setup the dataloaders\n    self.test_dataloaders = {\n        name: DataLoader(\n            dataset,\n            **self._dataloader_kwargs,\n            collate_fn=(\n                raw_image_collate_fn if self.data_processor is None else None\n            ),\n        )\n        for name, dataset in self.test_datasets.items()\n    }\n    self.test_dataloaders = {\n        name: self.fabric.setup_dataloaders(\n            dataloader, move_to_device=self.move_to_device\n        )\n        for name, dataloader in self.test_dataloaders.items()\n    }\n\n    self._is_setup = True\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.SparseWEMoECLIPVisionModelTaskPool","title":"<code>SparseWEMoECLIPVisionModelTaskPool</code>","text":"<p>               Bases: <code>CLIPVisionModelTaskPool</code></p> Source code in <code>fusion_bench/taskpool/clip_vision/clip_sparse_wemoe_taskpool.py</code> <pre><code>class SparseWEMoECLIPVisionModelTaskPool(CLIPVisionModelTaskPool):\n\n    # hooks and handles for saving layer-wise routing weights\n    _layer_wise_routing_weights_save_hooks: Dict[Any, LayerWiseRoutingWeightSaver] = {}\n    _layer_wise_routing_weights_save_hook_handles: Dict[Any, RemovableHandle] = {}\n\n    _config_mapping = CLIPVisionModelTaskPool._config_mapping | {\n        \"_layer_wise_routing_weights_save_path\": \"layer_wise_routing_weights_save_path\",\n    }\n\n    def __init__(\n        self,\n        layer_wise_routing_weights_save_path: Optional[str],\n        layer_wise_routing_weights_max_num: Optional[int] = None,\n        **kwargs,\n    ):\n        # save path for layer-wise routing weights\n        self._layer_wise_routing_weights_save_path = (\n            layer_wise_routing_weights_save_path\n        )\n        self.layer_wise_routing_weights_save_path = (\n            Path(layer_wise_routing_weights_save_path)\n            if layer_wise_routing_weights_save_path is not None\n            else None\n        )\n        self.layer_wise_routing_weights_max_num = layer_wise_routing_weights_max_num\n        super().__init__(**kwargs)\n\n    def on_task_evaluation_begin(self, classifier: HFCLIPClassifier, task_name: str):\n        super().on_task_evaluation_begin(classifier, task_name)\n        if self.layer_wise_routing_weights_save_path is not None:\n            # setup hooks for saving layer-wise routing weights\n            assert isinstance(\n                classifier.clip_model.vision_model,\n                (CLIPVisionTransformer, CLIPVisionModel),\n            ), \"Vision model is expected to be a CLIPVisionTransformer\"\n            vision_model = classifier.clip_model.vision_model\n            if isinstance(vision_model, CLIPVisionModel):\n                vision_model = vision_model.vision_model\n                # assign forward hooks for each layer\n            shared_gate = None\n            for i, layer in enumerate(vision_model.encoder.layers):\n                mlp = layer.mlp\n                assert isinstance(\n                    mlp,\n                    (SparseWeightEnsemblingMoE, SparseWeightEnsemblingMoE_ShardGate),\n                ), f\"MLP is expected to be a SparseWeightEnsemblingMoE or SparseWeightEnsemblingMoE_ShardGate, but got {type(mlp)}\"\n                # layer-wise routing weights\n                hook = LayerWiseRoutingWeightSaver(\n                    self.layer_wise_routing_weights_save_path\n                    / task_name\n                    / f\"layer_{i}.pt\",\n                    max_num=self.layer_wise_routing_weights_max_num,\n                )\n                self._layer_wise_routing_weights_save_hooks[i] = hook\n                if isinstance(mlp, SparseWeightEnsemblingMoE_ShardGate):\n                    # if use shared gate, copy the gate to all layers to avoid multiple hooks\n                    if shared_gate is None:\n                        shared_gate = mlp.gate\n                    mlp.gate = deepcopy(shared_gate)\n                self._layer_wise_routing_weights_save_hook_handles[i] = (\n                    mlp.gate.register_forward_hook(hook)\n                )\n\n    def on_task_evaluation_end(self):\n        super().on_task_evaluation_end()\n        if self.layer_wise_routing_weights_save_path is not None:\n            # remove hooks for saving layer-wise routing weights\n            for i, handle in self._layer_wise_routing_weights_save_hook_handles.items():\n                self._layer_wise_routing_weights_save_hooks[i].save_routing_weights()\n                self._layer_wise_routing_weights_save_hook_handles.pop(i)\n                handle.remove()\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.RankoneMoECLIPVisionModelTaskPool","title":"<code>RankoneMoECLIPVisionModelTaskPool</code>","text":"<p>               Bases: <code>CLIPVisionModelTaskPool</code></p> Source code in <code>fusion_bench/taskpool/clip_vision/clip_rankone_moe_taskpool.py</code> <pre><code>class RankoneMoECLIPVisionModelTaskPool(CLIPVisionModelTaskPool):\n\n    # hooks and handles for saving layer-wise routing weights\n    _layer_wise_routing_weights_save_hooks: Dict[Any, LayerWiseRoutingWeightSaver] = {}\n    _layer_wise_routing_weights_save_hook_handles: Dict[Any, RemovableHandle] = {}\n\n    _config_mapping = CLIPVisionModelTaskPool._config_mapping | {\n        \"_layer_wise_routing_weights_save_path\": \"layer_wise_routing_weights_save_path\",\n    }\n\n    def __init__(\n        self,\n        layer_wise_routing_weights_save_path: Optional[str],\n        layer_wise_routing_weights_max_num: Optional[int] = None,\n        **kwargs,\n    ):\n        # save path for layer-wise routing weights\n        self._layer_wise_routing_weights_save_path = (\n            layer_wise_routing_weights_save_path\n        )\n        self.layer_wise_routing_weights_save_path = (\n            Path(layer_wise_routing_weights_save_path)\n            if layer_wise_routing_weights_save_path is not None\n            else None\n        )\n        self.layer_wise_routing_weights_max_num = layer_wise_routing_weights_max_num\n        super().__init__(**kwargs)\n\n    def on_task_evaluation_begin(self, classifier: HFCLIPClassifier, task_name: str):\n        super().on_task_evaluation_begin(classifier, task_name)\n        if self.layer_wise_routing_weights_save_path is not None:\n            # setup hooks for saving layer-wise routing weights\n            assert isinstance(\n                classifier.clip_model.vision_model,\n                (CLIPVisionTransformer, CLIPVisionModel),\n            ), \"Vision model is expected to be a CLIPVisionTransformer\"\n            vision_model = classifier.clip_model.vision_model\n            if isinstance(vision_model, CLIPVisionModel):\n                vision_model = vision_model.vision_model\n                # assign forward hooks for each layer\n\n            for i, layer in enumerate(vision_model.encoder.layers):\n                mlp = layer.mlp\n                assert isinstance(\n                    mlp,\n                    (RankOneMoE),\n                ), f\"MLP is expected to be a RankOneWeightEnsemblingMoE, but got {type(mlp)}\"\n                # layer-wise routing weights\n                hook = LayerWiseRoutingWeightSaver(\n                    self.layer_wise_routing_weights_save_path\n                    / task_name\n                    / f\"layer_{i}.pt\",\n                    max_num=self.layer_wise_routing_weights_max_num,\n                )\n                self._layer_wise_routing_weights_save_hooks[i] = hook\n                self._layer_wise_routing_weights_save_hook_handles[i] = (\n                    mlp.gate.register_forward_hook(hook)\n                )\n\n    def on_task_evaluation_end(self):\n        super().on_task_evaluation_end()\n        if self.layer_wise_routing_weights_save_path is not None:\n            # remove hooks for saving layer-wise routing weights\n            for i, handle in self._layer_wise_routing_weights_save_hook_handles.items():\n                self._layer_wise_routing_weights_save_hooks[i].save_routing_weights()\n                self._layer_wise_routing_weights_save_hook_handles.pop(i)\n                handle.remove()\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#natural-language-processing-nlp-tasks","title":"Natural Language Processing (NLP) Tasks","text":""},{"location":"api/fusion_bench.taskpool/#gpt-2","title":"GPT-2","text":""},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.GPT2TextClassificationTaskPool","title":"<code>GPT2TextClassificationTaskPool</code>","text":"<p>               Bases: <code>BaseTaskPool</code>, <code>LightningFabricMixin</code></p> <p>A task pool for GPT2 text classification tasks. This class manages the tasks and provides methods for loading test dataset and evaluation.</p> Source code in <code>fusion_bench/taskpool/gpt2_text_classification.py</code> <pre><code>class GPT2TextClassificationTaskPool(BaseTaskPool, LightningFabricMixin):\n    \"\"\"\n    A task pool for GPT2 text classification tasks.\n    This class manages the tasks and provides methods for loading test dataset and evaluation.\n    \"\"\"\n\n    _config_mapping = BaseTaskPool._config_mapping | {\n        \"_test_datasets\": \"test_datasets\",\n        \"_tokenizer\": \"tokenizer\",\n        \"dataloader_kwargs\": \"dataloader_kwargs\",\n        \"fast_dev_run\": \"fast_dev_run\",\n    }\n\n    def __init__(\n        self,\n        test_datasets: DictConfig,\n        tokenizer: DictConfig,\n        dataloader_kwargs: DictConfig,\n        fast_dev_run: bool,\n        **kwargs,\n    ):\n        self._test_datasets = test_datasets\n        self._tokenizer = tokenizer\n        self.dataloader_kwargs = dataloader_kwargs\n        self.fast_dev_run = fast_dev_run\n        super().__init__(**kwargs)\n\n        self.setup()\n\n    def setup(self):\n        global tokenizer\n        self.tokenizer = tokenizer = instantiate(self._tokenizer)\n\n    def get_classifier(\n        self, task_name: str, model: GPT2Model\n    ) -&gt; GPT2ForSequenceClassification:\n        modelpool = self._program.modelpool\n        classifier = modelpool.load_classifier(task_name)\n        classifier.transformer = deepcopy(model)\n        return classifier\n\n    @torch.no_grad()\n    def evaluate_single_task(\n        self,\n        task_name: str,\n        model: GPT2Model,\n        test_loader: DataLoader,\n    ):\n        loss_metric = MeanMetric()\n        # load classifier and replace the backbone with the passed model\n        model: GPT2ForSequenceClassification = self.get_classifier(task_name, model)\n        accuracy = Accuracy(\"multiclass\", num_classes=model.num_labels)\n        model = self.fabric.setup(model)\n\n        if self.config.get(\"fast_dev_run\", False):\n            log.info(\"Running under fast_dev_run mode, evaluating on a single batch.\")\n            test_loader = itertools.islice(test_loader, 1)\n        else:\n            test_loader = test_loader\n\n        for batch in (\n            pbar := tqdm(\n                test_loader, desc=\"Evaluating\", leave=False, dynamic_ncols=True\n            )\n        ):\n            input_ids = batch[\"input_ids\"]\n            attention_mask = batch[\"attention_mask\"]\n            labels = batch[\"labels\"]\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            loss = F.cross_entropy(logits, labels)\n\n            accuracy(logits.detach().cpu(), labels.detach().cpu())\n            loss_metric.update(loss.detach().cpu())\n            pbar.set_postfix(\n                {\n                    \"accuracy\": accuracy.compute().item(),\n                    \"loss\": loss_metric.compute().item(),\n                }\n            )\n\n        acc = accuracy.compute().item()\n        loss = loss_metric.compute().item()\n        results = {\"accuracy\": acc, \"loss\": loss}\n        log.info(f\"Results for task {task_name}: {results}\")\n        return results\n\n    def get_test_dataloader(self, task_name: str):\n        dataset = instantiate(self._test_datasets[task_name])\n        dataloader_kwargs = {\n            \"shuffle\": False,\n        }\n        dataloader_kwargs.update(self.dataloader_kwargs)\n        dataloader = DataLoader(\n            dataset, collate_fn=default_data_collator, **dataloader_kwargs\n        )\n        if self.fabric is not None:\n            dataloader = self.fabric.setup_dataloaders(dataloader)\n        return dataloader\n\n    @override\n    def evaluate(self, model: GPT2Model, name: str = None):\n        \"\"\"Evaluate the model on the test datasets.\n\n        Args:\n            model (GPT2Model): The model to evaluate.\n            name (str, optional): The name of the model. Defaults to None. This is used to identify the model in the report.\n\n        Returns:\n            dict: A dictionary containing the evaluation results for each task.\n        \"\"\"\n        report = {}\n        if name is not None:\n            report[\"name\"] = name\n        for task_name in (pbar := tqdm(self._test_datasets, desc=\"Evaluating tasks\")):\n            pbar.set_description(f\"Evaluating task {task_name}\")\n            dataloader = self.get_test_dataloader(task_name)\n            result = self.evaluate_single_task(task_name, model, dataloader)\n            report[task_name] = result\n\n        # calculate the average accuracy and loss\n        if \"average\" not in report:\n            report[\"average\"] = {}\n            accuracies = [\n                value[\"accuracy\"]\n                for key, value in report.items()\n                if isinstance(value, dict) and \"accuracy\" in value\n            ]\n            if len(accuracies) &gt; 0:\n                average_accuracy = sum(accuracies) / len(accuracies)\n                report[\"average\"][\"accuracy\"] = average_accuracy\n            losses = [value[\"loss\"] for key, value in report.items() if \"loss\" in value]\n            if len(losses) &gt; 0:\n                average_loss = sum(losses) / len(losses)\n                report[\"average\"][\"loss\"] = average_loss\n\n        log.info(f\"Evaluation Result: {report}\")\n        return report\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.GPT2TextClassificationTaskPool.evaluate","title":"<code>evaluate(model, name=None)</code>","text":"<p>Evaluate the model on the test datasets.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>GPT2Model</code>)           \u2013            <p>The model to evaluate.</p> </li> <li> <code>name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The name of the model. Defaults to None. This is used to identify the model in the report.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>A dictionary containing the evaluation results for each task.</p> </li> </ul> Source code in <code>fusion_bench/taskpool/gpt2_text_classification.py</code> <pre><code>@override\ndef evaluate(self, model: GPT2Model, name: str = None):\n    \"\"\"Evaluate the model on the test datasets.\n\n    Args:\n        model (GPT2Model): The model to evaluate.\n        name (str, optional): The name of the model. Defaults to None. This is used to identify the model in the report.\n\n    Returns:\n        dict: A dictionary containing the evaluation results for each task.\n    \"\"\"\n    report = {}\n    if name is not None:\n        report[\"name\"] = name\n    for task_name in (pbar := tqdm(self._test_datasets, desc=\"Evaluating tasks\")):\n        pbar.set_description(f\"Evaluating task {task_name}\")\n        dataloader = self.get_test_dataloader(task_name)\n        result = self.evaluate_single_task(task_name, model, dataloader)\n        report[task_name] = result\n\n    # calculate the average accuracy and loss\n    if \"average\" not in report:\n        report[\"average\"] = {}\n        accuracies = [\n            value[\"accuracy\"]\n            for key, value in report.items()\n            if isinstance(value, dict) and \"accuracy\" in value\n        ]\n        if len(accuracies) &gt; 0:\n            average_accuracy = sum(accuracies) / len(accuracies)\n            report[\"average\"][\"accuracy\"] = average_accuracy\n        losses = [value[\"loss\"] for key, value in report.items() if \"loss\" in value]\n        if len(losses) &gt; 0:\n            average_loss = sum(losses) / len(losses)\n            report[\"average\"][\"loss\"] = average_loss\n\n    log.info(f\"Evaluation Result: {report}\")\n    return report\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#flan-t5","title":"Flan-T5","text":""},{"location":"api/fusion_bench.taskpool/#fusion_bench.compat.taskpool.flan_t5_glue_text_generation.FlanT5GLUETextGenerationTask","title":"<code>fusion_bench.compat.taskpool.flan_t5_glue_text_generation.FlanT5GLUETextGenerationTask</code>","text":"<p>               Bases: <code>BaseTask</code></p> Source code in <code>fusion_bench/compat/taskpool/flan_t5_glue_text_generation.py</code> <pre><code>class FlanT5GLUETextGenerationTask(BaseTask):\n    _taskpool: \"FlanT5GLUETextGenerationTaskPool\" = None\n\n    @property\n    def taskpool(self):\n        if self._taskpool is not None:\n            return self._taskpool\n        else:\n            raise ValueError(\"Taskpool not set\")\n\n    @property\n    def fabric(self):\n        return self.taskpool.fabric\n\n    @property\n    def tokenizer(self):\n        return self.taskpool.tokenizer\n\n    @functools.cached_property\n    def dataset(self):\n        log.info(f'Loading dataset: \"{self.config.dataset.name}\"')\n        dataset = load_glue_dataset(\n            self.config.dataset.name, self.tokenizer, self.taskpool.config.cache_dir\n        )\n        return dataset\n\n    @functools.cached_property\n    def test_dataset(self):\n        return self.dataset[self.config.dataset.split]\n\n    @property\n    def test_loader(self):\n        loader = DataLoader(\n            self.test_dataset,\n            batch_size=self.taskpool.config.batch_size,\n            num_workers=self.taskpool.config.num_workers,\n            shuffle=False,\n            collate_fn=default_data_collator,\n        )\n        loader = self.fabric.setup_dataloaders(loader)\n        return loader\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#lm-eval-harness-integration-llm","title":"LM-Eval-Harness Integration (LLM)","text":""},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.LMEvalHarnessTaskPool","title":"<code>LMEvalHarnessTaskPool</code>","text":"<p>               Bases: <code>BaseTaskPool</code>, <code>LightningFabricMixin</code></p> <p>A task pool implementation that interfaces with the LM Evaluation Harness framework.</p> <p>This class provides a wrapper around the LM Evaluation Harness (lm-eval) library, enabling evaluation of language models on various standardized benchmarks and tasks. It inherits from BaseTaskPool and LightningFabricMixin to provide distributed computing capabilities through PyTorch Lightning Fabric.</p> <p>The task pool supports evaluation on multiple tasks simultaneously and provides flexible configuration options for batch processing, output formatting, and logging. It automatically handles model setup and wrapping for distributed evaluation when using Lightning Fabric.</p> <p>Parameters:</p> <ul> <li> <code>tasks</code>               (<code>Union[str, List[str]]</code>)           \u2013            <p>A single task name or list of task names to evaluate on. Examples: \"hellaswag\", [\"arc_easy\", \"arc_challenge\", \"hellaswag\"]</p> </li> <li> <code>apply_chat_template</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to apply chat template formatting to inputs. Useful for instruction-tuned or chat models.</p> </li> <li> <code>include_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Path to additional task definitions or custom tasks.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of samples to process in each batch. Larger values may improve throughput but require more memory.</p> </li> <li> <code>metadata</code>               (<code>Optional[DictConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Additional metadata to include in evaluation results.</p> </li> <li> <code>verbosity</code>               (<code>Optional[Literal['CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG']]</code>, default:                   <code>None</code> )           \u2013            <p>Logging verbosity level for the evaluation process.</p> </li> <li> <code>output_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Custom path for saving evaluation results. If None, results are saved to the default log directory.</p> </li> <li> <code>log_samples</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to log individual sample predictions and targets. Useful for debugging but increases output size significantly.</p> </li> <li> <code>_usage_</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Internal usage tracking string.</p> </li> <li> <code>_version_</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Internal version tracking string.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to the LM Evaluation Harness.</p> </li> </ul> Example <pre><code>&gt;&gt;&gt; taskpool = LMEvalHarnessTaskPool(\n...     tasks=[\"arc_easy\", \"hellaswag\"],\n...     batch_size=8,\n...     verbosity=\"INFO\"\n... )\n&gt;&gt;&gt; results = taskpool.evaluate(model)\n</code></pre> Source code in <code>fusion_bench/taskpool/lm_eval_harness/taskpool.py</code> <pre><code>class LMEvalHarnessTaskPool(BaseTaskPool, LightningFabricMixin):\n    \"\"\"A task pool implementation that interfaces with the LM Evaluation Harness framework.\n\n    This class provides a wrapper around the LM Evaluation Harness (lm-eval) library,\n    enabling evaluation of language models on various standardized benchmarks and tasks.\n    It inherits from BaseTaskPool and LightningFabricMixin to provide distributed\n    computing capabilities through PyTorch Lightning Fabric.\n\n    The task pool supports evaluation on multiple tasks simultaneously and provides\n    flexible configuration options for batch processing, output formatting, and\n    logging. It automatically handles model setup and wrapping for distributed\n    evaluation when using Lightning Fabric.\n\n    Args:\n        tasks: A single task name or list of task names to evaluate on.\n            Examples: \"hellaswag\", [\"arc_easy\", \"arc_challenge\", \"hellaswag\"]\n        apply_chat_template: Whether to apply chat template formatting to inputs.\n            Useful for instruction-tuned or chat models.\n        include_path: Path to additional task definitions or custom tasks.\n        batch_size: Number of samples to process in each batch. Larger values\n            may improve throughput but require more memory.\n        metadata: Additional metadata to include in evaluation results.\n        verbosity: Logging verbosity level for the evaluation process.\n        output_path: Custom path for saving evaluation results. If None,\n            results are saved to the default log directory.\n        log_samples: Whether to log individual sample predictions and targets.\n            Useful for debugging but increases output size significantly.\n        _usage_: Internal usage tracking string.\n        _version_: Internal version tracking string.\n        **kwargs: Additional arguments passed to the LM Evaluation Harness.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; taskpool = LMEvalHarnessTaskPool(\n        ...     tasks=[\"arc_easy\", \"hellaswag\"],\n        ...     batch_size=8,\n        ...     verbosity=\"INFO\"\n        ... )\n        &gt;&gt;&gt; results = taskpool.evaluate(model)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        tasks: Union[str, List[str]],\n        apply_chat_template: bool = False,\n        include_path: Optional[str] = None,\n        batch_size: int = 1,\n        metadata: Optional[DictConfig] = None,\n        verbosity: Optional[\n            Literal[\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\"]\n        ] = None,\n        output_path: Optional[str] = None,\n        log_samples: bool = False,\n        _usage_: Optional[str] = None,\n        _version_: Optional[str] = None,\n        **kwargs,\n    ):\n        super().__init__(_usage_=_usage_, _version_=_version_)\n        self.tasks = tasks\n        self.include_path = include_path\n        self.batch_size = batch_size\n        self.metadata = metadata\n        self.apply_chat_template = apply_chat_template\n        self.verbosity = verbosity\n        self.kwargs = kwargs\n        self.output_path = output_path\n        self.log_samples = log_samples\n\n    def evaluate(self, model, *command_line_args, **kwargs):\n        \"\"\"Evaluate a language model on the configured tasks using LM Evaluation Harness.\n\n        This method wraps the model with the LM Evaluation Harness framework and\n        executes evaluation on all configured tasks. It automatically handles\n        command-line argument construction, model wrapping with Lightning Fabric\n        for distributed evaluation, and result logging.\n\n        The evaluation process includes:\n        1. Building command-line arguments from instance configuration\n        2. Setting up the LM Evaluation Harness argument parser\n        3. Wrapping the model with Lightning Fabric if not already wrapped\n        4. Creating an HFLM (Hugging Face Language Model) wrapper\n        5. Executing the evaluation through the LM-Eval CLI interface\n\n        Args:\n            model: The language model to evaluate. Can be a Hugging Face model,\n                PyTorch model, or any model compatible with the LM Evaluation Harness.\n                The model will be automatically wrapped with Lightning Fabric for\n                distributed evaluation if not already wrapped.\n            *command_line_args: Additional positional command-line arguments\n                (currently unused but preserved for interface compatibility).\n            **kwargs: Additional keyword arguments that will be converted to\n                command-line flags and passed to the LM Evaluation Harness.\n                Keys will be prefixed with '--' and values converted to strings.\n\n        Returns:\n            None: Results are written to the configured output path and logged.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; taskpool = LMEvalHarnessTaskPool(tasks=[\"arc_easy\"])\n            &gt;&gt;&gt; taskpool.evaluate(model, limit=100, device=\"cuda\")\n            ```\n\n        Note:\n            The method leverages the LM Evaluation Harness's command-line interface\n            internally, which provides standardized evaluation procedures and\n            ensures compatibility with the broader evaluation ecosystem.\n        \"\"\"\n        command_line_args = []\n        if self.include_path is not None:\n            command_line_args.extend([\"--include_path\", self.include_path])\n        if isinstance(self.tasks, (list, ListConfig)):\n            command_line_args.extend([\"--tasks\", \",\".join(self.tasks)])\n        elif isinstance(self.tasks, str):\n            command_line_args.extend([\"--tasks\", self.tasks])\n        if self.apply_chat_template:\n            command_line_args.extend(\n                [\"--apply_chat_template\", str(self.apply_chat_template)]\n            )\n        if self.batch_size is not None:\n            command_line_args.extend([\"--batch_size\", str(self.batch_size)])\n        if self.verbosity is not None:\n            command_line_args.extend([\"--verbosity\", str(self.verbosity)])\n        if self.metadata is not None:\n            command_line_args.extend([\"--metadata\", str(self.metadata)])\n        if self.output_path is None:\n            command_line_args.extend(\n                [\n                    \"--output_path\",\n                    os.path.join(self.log_dir, \"lm_eval_results\"),\n                ]\n            )\n        else:\n            command_line_args.extend([\"--output_path\", self.output_path])\n        if self.log_samples:\n            command_line_args.extend([\"--log_samples\"])\n        for key, value in kwargs.items():\n            command_line_args.extend([f\"--{key}\", str(value)])\n\n        parser = setup_parser()\n        check_argument_types(parser)\n        args = parser.parse_args(args=command_line_args)\n        log.info(\"LM-Eval Harness arguments:\\n%s\", args)\n\n        if not lightning.fabric.is_wrapped(model):\n            model = self.fabric.setup(model)\n        args.model = lm_eval.models.huggingface.HFLM(pretrained=model)\n        cli_evaluate(args)\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.LMEvalHarnessTaskPool.evaluate","title":"<code>evaluate(model, *command_line_args, **kwargs)</code>","text":"<p>Evaluate a language model on the configured tasks using LM Evaluation Harness.</p> <p>This method wraps the model with the LM Evaluation Harness framework and executes evaluation on all configured tasks. It automatically handles command-line argument construction, model wrapping with Lightning Fabric for distributed evaluation, and result logging.</p> <p>The evaluation process includes: 1. Building command-line arguments from instance configuration 2. Setting up the LM Evaluation Harness argument parser 3. Wrapping the model with Lightning Fabric if not already wrapped 4. Creating an HFLM (Hugging Face Language Model) wrapper 5. Executing the evaluation through the LM-Eval CLI interface</p> <p>Parameters:</p> <ul> <li> <code>model</code>           \u2013            <p>The language model to evaluate. Can be a Hugging Face model, PyTorch model, or any model compatible with the LM Evaluation Harness. The model will be automatically wrapped with Lightning Fabric for distributed evaluation if not already wrapped.</p> </li> <li> <code>*command_line_args</code>           \u2013            <p>Additional positional command-line arguments (currently unused but preserved for interface compatibility).</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments that will be converted to command-line flags and passed to the LM Evaluation Harness. Keys will be prefixed with '--' and values converted to strings.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>          \u2013            <p>Results are written to the configured output path and logged.</p> </li> </ul> Example <pre><code>&gt;&gt;&gt; taskpool = LMEvalHarnessTaskPool(tasks=[\"arc_easy\"])\n&gt;&gt;&gt; taskpool.evaluate(model, limit=100, device=\"cuda\")\n</code></pre> Note <p>The method leverages the LM Evaluation Harness's command-line interface internally, which provides standardized evaluation procedures and ensures compatibility with the broader evaluation ecosystem.</p> Source code in <code>fusion_bench/taskpool/lm_eval_harness/taskpool.py</code> <pre><code>def evaluate(self, model, *command_line_args, **kwargs):\n    \"\"\"Evaluate a language model on the configured tasks using LM Evaluation Harness.\n\n    This method wraps the model with the LM Evaluation Harness framework and\n    executes evaluation on all configured tasks. It automatically handles\n    command-line argument construction, model wrapping with Lightning Fabric\n    for distributed evaluation, and result logging.\n\n    The evaluation process includes:\n    1. Building command-line arguments from instance configuration\n    2. Setting up the LM Evaluation Harness argument parser\n    3. Wrapping the model with Lightning Fabric if not already wrapped\n    4. Creating an HFLM (Hugging Face Language Model) wrapper\n    5. Executing the evaluation through the LM-Eval CLI interface\n\n    Args:\n        model: The language model to evaluate. Can be a Hugging Face model,\n            PyTorch model, or any model compatible with the LM Evaluation Harness.\n            The model will be automatically wrapped with Lightning Fabric for\n            distributed evaluation if not already wrapped.\n        *command_line_args: Additional positional command-line arguments\n            (currently unused but preserved for interface compatibility).\n        **kwargs: Additional keyword arguments that will be converted to\n            command-line flags and passed to the LM Evaluation Harness.\n            Keys will be prefixed with '--' and values converted to strings.\n\n    Returns:\n        None: Results are written to the configured output path and logged.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; taskpool = LMEvalHarnessTaskPool(tasks=[\"arc_easy\"])\n        &gt;&gt;&gt; taskpool.evaluate(model, limit=100, device=\"cuda\")\n        ```\n\n    Note:\n        The method leverages the LM Evaluation Harness's command-line interface\n        internally, which provides standardized evaluation procedures and\n        ensures compatibility with the broader evaluation ecosystem.\n    \"\"\"\n    command_line_args = []\n    if self.include_path is not None:\n        command_line_args.extend([\"--include_path\", self.include_path])\n    if isinstance(self.tasks, (list, ListConfig)):\n        command_line_args.extend([\"--tasks\", \",\".join(self.tasks)])\n    elif isinstance(self.tasks, str):\n        command_line_args.extend([\"--tasks\", self.tasks])\n    if self.apply_chat_template:\n        command_line_args.extend(\n            [\"--apply_chat_template\", str(self.apply_chat_template)]\n        )\n    if self.batch_size is not None:\n        command_line_args.extend([\"--batch_size\", str(self.batch_size)])\n    if self.verbosity is not None:\n        command_line_args.extend([\"--verbosity\", str(self.verbosity)])\n    if self.metadata is not None:\n        command_line_args.extend([\"--metadata\", str(self.metadata)])\n    if self.output_path is None:\n        command_line_args.extend(\n            [\n                \"--output_path\",\n                os.path.join(self.log_dir, \"lm_eval_results\"),\n            ]\n        )\n    else:\n        command_line_args.extend([\"--output_path\", self.output_path])\n    if self.log_samples:\n        command_line_args.extend([\"--log_samples\"])\n    for key, value in kwargs.items():\n        command_line_args.extend([f\"--{key}\", str(value)])\n\n    parser = setup_parser()\n    check_argument_types(parser)\n    args = parser.parse_args(args=command_line_args)\n    log.info(\"LM-Eval Harness arguments:\\n%s\", args)\n\n    if not lightning.fabric.is_wrapped(model):\n        model = self.fabric.setup(model)\n    args.model = lm_eval.models.huggingface.HFLM(pretrained=model)\n    cli_evaluate(args)\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#task-agnostic","title":"Task Agnostic","text":""},{"location":"api/fusion_bench.taskpool/#utility-classes","title":"Utility Classes","text":""},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.DummyTaskPool","title":"<code>DummyTaskPool</code>","text":"<p>               Bases: <code>BaseTaskPool</code></p> <p>A lightweight task pool implementation for debugging and development workflows.</p> <p>This dummy task pool provides a minimal evaluation interface that focuses on model introspection rather than task-specific performance evaluation. It's designed for development scenarios where you need to test model fusion pipelines, validate architectures, or debug workflows without the overhead of running actual evaluation tasks.</p> The task pool is particularly useful when <ul> <li>You want to verify model fusion works correctly</li> <li>You need to check parameter counts after fusion</li> <li>You're developing new fusion algorithms</li> <li>You want to test infrastructure without expensive evaluations</li> </ul> Example <pre><code>&gt;&gt;&gt; taskpool = DummyTaskPool(model_save_path=\"/tmp/fused_model\")\n&gt;&gt;&gt; results = taskpool.evaluate(fused_model)\n&gt;&gt;&gt; print(f\"Model has {results['model_info']['trainable_params']} parameters\")\n</code></pre> Source code in <code>fusion_bench/taskpool/dummy.py</code> <pre><code>class DummyTaskPool(BaseTaskPool):\n    \"\"\"A lightweight task pool implementation for debugging and development workflows.\n\n    This dummy task pool provides a minimal evaluation interface that focuses on\n    model introspection rather than task-specific performance evaluation. It's\n    designed for development scenarios where you need to test model fusion\n    pipelines, validate architectures, or debug workflows without the overhead\n    of running actual evaluation tasks.\n\n    The task pool is particularly useful when:\n        - You want to verify model fusion works correctly\n        - You need to check parameter counts after fusion\n        - You're developing new fusion algorithms\n        - You want to test infrastructure without expensive evaluations\n\n    Example:\n        ```python\n        &gt;&gt;&gt; taskpool = DummyTaskPool(model_save_path=\"/tmp/fused_model\")\n        &gt;&gt;&gt; results = taskpool.evaluate(fused_model)\n        &gt;&gt;&gt; print(f\"Model has {results['model_info']['trainable_params']} parameters\")\n        ```\n    \"\"\"\n\n    def __init__(self, model_save_path: Optional[str] = None, **kwargs):\n        \"\"\"Initialize the dummy task pool with optional model saving capability.\n\n        Args:\n            model_save_path: Optional path where the evaluated model should be saved.\n                If provided, the model will be serialized and saved to this location\n                after evaluation using the separate_save utility. If None, no model\n                saving will be performed.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; # Create taskpool without saving\n            &gt;&gt;&gt; taskpool = DummyTaskPool()\n\n            &gt;&gt;&gt; # Create taskpool with model saving\n            &gt;&gt;&gt; taskpool = DummyTaskPool(model_save_path=\"/path/to/save/model.pth\")\n            ```\n        \"\"\"\n        super().__init__(**kwargs)\n        self.model_save_path = model_save_path\n\n    def evaluate(self, model):\n        \"\"\"Perform lightweight evaluation and analysis of the given model.\n\n        This method provides a minimal evaluation that focuses on model introspection\n        rather than task-specific performance metrics. It performs parameter analysis,\n        optionally saves the model, and returns a summary report.\n\n        The evaluation process includes:\n        1. Printing human-readable parameter information (rank-zero only)\n        2. Optionally saving the model if a save path was configured\n        3. Generating and returning a model summary report\n\n        Args:\n            model: The model to evaluate. Can be any PyTorch nn.Module including\n                fusion models, pre-trained models, or custom architectures.\n\n        Returns:\n            dict: A model summary report containing parameter statistics and\n                architecture information. See get_model_summary() for detailed\n                format specification.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; taskpool = DummyTaskPool(model_save_path=\"/tmp/model.pth\")\n            &gt;&gt;&gt; model = torch.nn.Linear(10, 5)\n            &gt;&gt;&gt; results = taskpool.evaluate(model)\n            &gt;&gt;&gt; print(f\"Trainable params: {results['model_info']['trainable_params']}\")\n            ```\n        \"\"\"\n        if rank_zero_only.rank == 0:\n            print_parameters(model, is_human_readable=True)\n\n            if self.model_save_path is not None:\n                with timeit_context(f\"Saving the model to {self.model_save_path}\"):\n                    separate_save(model, self.model_save_path)\n\n        return get_model_summary(model)\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.DummyTaskPool.__init__","title":"<code>__init__(model_save_path=None, **kwargs)</code>","text":"<p>Initialize the dummy task pool with optional model saving capability.</p> <p>Parameters:</p> <ul> <li> <code>model_save_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional path where the evaluated model should be saved. If provided, the model will be serialized and saved to this location after evaluation using the separate_save utility. If None, no model saving will be performed.</p> </li> </ul> Example <pre><code>&gt;&gt;&gt; # Create taskpool without saving\n&gt;&gt;&gt; taskpool = DummyTaskPool()\n\n&gt;&gt;&gt; # Create taskpool with model saving\n&gt;&gt;&gt; taskpool = DummyTaskPool(model_save_path=\"/path/to/save/model.pth\")\n</code></pre> Source code in <code>fusion_bench/taskpool/dummy.py</code> <pre><code>def __init__(self, model_save_path: Optional[str] = None, **kwargs):\n    \"\"\"Initialize the dummy task pool with optional model saving capability.\n\n    Args:\n        model_save_path: Optional path where the evaluated model should be saved.\n            If provided, the model will be serialized and saved to this location\n            after evaluation using the separate_save utility. If None, no model\n            saving will be performed.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; # Create taskpool without saving\n        &gt;&gt;&gt; taskpool = DummyTaskPool()\n\n        &gt;&gt;&gt; # Create taskpool with model saving\n        &gt;&gt;&gt; taskpool = DummyTaskPool(model_save_path=\"/path/to/save/model.pth\")\n        ```\n    \"\"\"\n    super().__init__(**kwargs)\n    self.model_save_path = model_save_path\n</code></pre>"},{"location":"api/fusion_bench.taskpool/#fusion_bench.taskpool.DummyTaskPool.evaluate","title":"<code>evaluate(model)</code>","text":"<p>Perform lightweight evaluation and analysis of the given model.</p> <p>This method provides a minimal evaluation that focuses on model introspection rather than task-specific performance metrics. It performs parameter analysis, optionally saves the model, and returns a summary report.</p> <p>The evaluation process includes: 1. Printing human-readable parameter information (rank-zero only) 2. Optionally saving the model if a save path was configured 3. Generating and returning a model summary report</p> <p>Parameters:</p> <ul> <li> <code>model</code>           \u2013            <p>The model to evaluate. Can be any PyTorch nn.Module including fusion models, pre-trained models, or custom architectures.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>A model summary report containing parameter statistics and architecture information. See get_model_summary() for detailed format specification.</p> </li> </ul> Example <pre><code>&gt;&gt;&gt; taskpool = DummyTaskPool(model_save_path=\"/tmp/model.pth\")\n&gt;&gt;&gt; model = torch.nn.Linear(10, 5)\n&gt;&gt;&gt; results = taskpool.evaluate(model)\n&gt;&gt;&gt; print(f\"Trainable params: {results['model_info']['trainable_params']}\")\n</code></pre> Source code in <code>fusion_bench/taskpool/dummy.py</code> <pre><code>def evaluate(self, model):\n    \"\"\"Perform lightweight evaluation and analysis of the given model.\n\n    This method provides a minimal evaluation that focuses on model introspection\n    rather than task-specific performance metrics. It performs parameter analysis,\n    optionally saves the model, and returns a summary report.\n\n    The evaluation process includes:\n    1. Printing human-readable parameter information (rank-zero only)\n    2. Optionally saving the model if a save path was configured\n    3. Generating and returning a model summary report\n\n    Args:\n        model: The model to evaluate. Can be any PyTorch nn.Module including\n            fusion models, pre-trained models, or custom architectures.\n\n    Returns:\n        dict: A model summary report containing parameter statistics and\n            architecture information. See get_model_summary() for detailed\n            format specification.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; taskpool = DummyTaskPool(model_save_path=\"/tmp/model.pth\")\n        &gt;&gt;&gt; model = torch.nn.Linear(10, 5)\n        &gt;&gt;&gt; results = taskpool.evaluate(model)\n        &gt;&gt;&gt; print(f\"Trainable params: {results['model_info']['trainable_params']}\")\n        ```\n    \"\"\"\n    if rank_zero_only.rank == 0:\n        print_parameters(model, is_human_readable=True)\n\n        if self.model_save_path is not None:\n            with timeit_context(f\"Saving the model to {self.model_save_path}\"):\n                separate_save(model, self.model_save_path)\n\n    return get_model_summary(model)\n</code></pre>"},{"location":"api/fusion_bench.tasks/","title":"fusion_bench.tasks","text":""},{"location":"api/fusion_bench.tasks/#image-classification-tasks","title":"Image Classification Tasks","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.clip_classification","title":"<code>fusion_bench.tasks.clip_classification</code>","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.clip_classification.CLIPTemplateFactory","title":"<code>CLIPTemplateFactory</code>","text":"<p>A factory class for creating CLIP dataset templates.</p> <p>This class provides methods to retrieve class names and templates for various datasets, register new datasets, and get a list of all available datasets. It uses a mapping from dataset names to their respective module paths or detailed information, facilitating dynamic import and usage of dataset-specific class names and templates.</p> <p>Attributes:</p> <ul> <li> <code>_dataset_mapping</code>               (<code>dict</code>)           \u2013            <p>A mapping from dataset names to their respective module paths</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_classnames_and_templates</code>             \u2013              <p>str): Retrieves class names and templates for the specified dataset.</p> </li> <li> <code>register_dataset</code>             \u2013              <p>str, dataset_info: Dict[str, Any] = None, classnames: List[str] = None, templates: List[Callable] = None): Registers a new dataset with its associated information.</p> </li> <li> <code>get_available_datasets</code>             \u2013              <p>Returns a list of all available dataset names.</p> </li> </ul> Source code in <code>fusion_bench/tasks/clip_classification/__init__.py</code> <pre><code>class CLIPTemplateFactory:\n    \"\"\"\n    A factory class for creating CLIP dataset templates.\n\n    This class provides methods to retrieve class names and templates for various datasets,\n    register new datasets, and get a list of all available datasets. It uses a mapping\n    from dataset names to their respective module paths or detailed information, facilitating\n    dynamic import and usage of dataset-specific class names and templates.\n\n    Attributes:\n        _dataset_mapping (dict): A mapping from dataset names to their respective module paths\n        or detailed information including module path, class names, and templates.\n\n    Methods:\n        get_classnames_and_templates(dataset_name: str): Retrieves class names and templates for the specified dataset.\n        register_dataset(dataset_name: str, dataset_info: Dict[str, Any] = None, classnames: List[str] = None, templates: List[Callable] = None): Registers a new dataset with its associated information.\n        get_available_datasets(): Returns a list of all available dataset names.\n    \"\"\"\n\n    _dataset_mapping = {\n        \"mnist\": \".mnist\",\n        \"stanford-cars\": \".stanford_cars\",\n        \"stanford_cars\": \".stanford_cars\",\n        \"tanganke/stanford_cars\": \".stanford_cars\",\n        \"gtsrb\": \".gtsrb\",\n        \"tanganke/gtsrb\": \".gtsrb\",\n        \"resisc45\": \".resisc45\",\n        \"tanganke/resisc45\": \".resisc45\",\n        \"dtd\": \".dtd\",\n        \"tanganke/dtd\": \".dtd\",\n        \"eurosat\": \".eurosat\",\n        \"tanganke/eurosat\": \".eurosat\",\n        \"sun397\": \".sun397\",\n        \"tanganke/sun397\": \".sun397\",\n        \"cifar10\": \".cifar10\",\n        \"svhn\": \".svhn\",\n        \"cifar100\": {\n            \"module\": \".cifar100\",\n            \"classnames\": \"fine_label\",\n            \"templates\": \"templates\",\n        },\n        \"nateraw/rendered-sst2\": \".rendered_sst2\",\n        \"rendered-sst2\": \".rendered_sst2\",\n        \"tanganke/stl10\": \".stl10\",\n        \"stl10\": \".stl10\",\n        \"dpdl-benchmark/oxford_flowers102\": \".flower102\",\n        \"oxford_flowers102\": \".flower102\",\n        \"timm/oxford-iiit-pet\": \".oxford_iiit_pet\",\n        \"oxford-iiit-pet\": \".oxford_iiit_pet\",\n        \"imagenet\": \".imagenet\",\n        \"tiny-imagenet\": \".tiny_imagenet\",\n        \"pcam\": \".pcam\",\n        \"fer2013\": \".fer2013\",\n        \"emnist_mnist\": \".emnist_mnist\",\n        \"emnist_letters\": \".emnist_letters\",\n        \"kmnist\": \".kmnist\",\n        \"food101\": \".food101\",\n        \"fashion_mnist\": \".fashion_mnist\",\n        \"cub-200-2011\": \".cub_200_2011\",\n        \"mango-leaf-disease\": \".mango_leaf_disease\",\n    }\n\n    @staticmethod\n    def get_classnames_and_templates(\n        dataset_name: str,\n    ) -&gt; Tuple[List[str], List[Callable]]:\n        \"\"\"\n        Retrieves class names and templates for the specified dataset.\n\n        This method looks up the dataset information in the internal mapping and dynamically imports\n        the class names and templates from the specified module. It supports both simple string mappings\n        and detailed dictionary mappings for datasets.\n\n        Args:\n            dataset_name (str): The name of the dataset for which to retrieve class names and templates.\n\n        Returns:\n            Tuple[List[str], List[Callable]]: A tuple containing a list of class names and a list of template callables.\n\n        Raises:\n            ValueError: If the dataset_name is not found in the internal mapping.\n        \"\"\"\n        if dataset_name not in CLIPTemplateFactory._dataset_mapping:\n            raise ValueError(\n                f\"Unknown dataset {dataset_name}, available datasets: {CLIPTemplateFactory._dataset_mapping.keys()}. You can register a new dataset using `CLIPTemplateFactory.register_dataset()` method.\"\n            )\n\n        dataset_info = CLIPTemplateFactory._dataset_mapping[dataset_name]\n        # convert dataset_info to dict format: { 'module': str, 'classnames': str, 'templates': str }\n        if isinstance(dataset_info, str):\n            dataset_info = _check_module_name(dataset_info)\n            dataset_info = {\n                \"module\": dataset_info,\n                \"classnames\": \"classnames\",\n                \"templates\": \"templates\",\n            }\n        elif isinstance(dataset_info, dict):\n            if \"module\" in dataset_info:\n                dataset_info[\"module\"] = _check_module_name(dataset_info[\"module\"])\n\n        # import classnames and templates from the specified module\n        # convert to dict format: { 'labels': List[str], 'templates': List[Callable] }\n        if \"module\" in dataset_info:\n            module = importlib.import_module(dataset_info[\"module\"])\n            classnames = getattr(module, dataset_info[\"classnames\"])\n            templates = getattr(module, dataset_info[\"templates\"])\n        else:\n            classnames = dataset_info[\"classnames\"]\n            templates = dataset_info[\"templates\"]\n\n        return classnames, templates\n\n    @staticmethod\n    def register_dataset(\n        dataset_name: str,\n        *,\n        dataset_info: Dict[str, Any] = None,\n        classnames: List[str] = None,\n        templates: List[Callable] = None,\n    ):\n        \"\"\"\n        Registers a new dataset with its associated information.\n\n        This method allows for the dynamic addition of datasets to the internal mapping. It supports\n        registration through either a detailed dictionary (`dataset_info`) or separate lists of class names\n        and templates. If a dataset with the same name already exists, it will be overwritten.\n\n        The expected format and contents of `dataset_info` can vary depending on the needs of the dataset being registered, but typically, it includes the following keys:\n\n        - \"module\": A string specifying the module path where the dataset's related classes and functions are located. This is used for dynamic import of the dataset's class names and templates.\n        - \"classnames\": This key is expected to hold the name of the attribute or variable in the specified module that contains a list of class names relevant to the dataset. These class names are used to label data points in the dataset.\n        - \"templates\": Similar to \"classnames\", this key should specify the name of the attribute or variable in the module that contains a list of template callables. These templates are functions or methods that define how data points should be processed or transformed.\n\n        Args:\n            dataset_name (str): The name of the dataset to register.\n            dataset_info (Dict[str, Any], optional): A dictionary containing the dataset information, including module path, class names, and templates. Defaults to None.\n            classnames (List[str], optional): A list of class names for the dataset. Required if `dataset_info` is not provided. Defaults to None.\n            templates (List[Callable], optional): A list of template callables for the dataset. Required if `dataset_info` is not provided. Defaults to None.\n\n        Raises:\n            AssertionError: If neither `dataset_info` nor both `classnames` and `templates` are provided.\n        \"\"\"\n        assert dataset_info is None or (\n            classnames is not None and templates is not None\n        ), \"You must provide either `dataset_info` or both `classnames` and `templates`.\"\n\n        if dataset_name in CLIPTemplateFactory._dataset_mapping:\n            warnings.warn(\n                f\"Dataset {dataset_name} is already registered, overwriting the existing dataset information.\"\n            )\n        if dataset_info is None:\n            dataset_info = {\"classnames\": classnames, \"temolates\": templates}\n        CLIPTemplateFactory._dataset_mapping[dataset_name] = dataset_info\n\n    @staticmethod\n    def get_available_datasets() -&gt; List[str]:\n        \"\"\"\n        Get a list of all available dataset names.\n\n        Returns:\n            List[str]: A list of dataset names.\n        \"\"\"\n        return list(CLIPTemplateFactory._dataset_mapping.keys())\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.clip_classification.CLIPTemplateFactory.get_available_datasets","title":"<code>get_available_datasets()</code>  <code>staticmethod</code>","text":"<p>Get a list of all available dataset names.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of dataset names.</p> </li> </ul> Source code in <code>fusion_bench/tasks/clip_classification/__init__.py</code> <pre><code>@staticmethod\ndef get_available_datasets() -&gt; List[str]:\n    \"\"\"\n    Get a list of all available dataset names.\n\n    Returns:\n        List[str]: A list of dataset names.\n    \"\"\"\n    return list(CLIPTemplateFactory._dataset_mapping.keys())\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.clip_classification.CLIPTemplateFactory.get_classnames_and_templates","title":"<code>get_classnames_and_templates(dataset_name)</code>  <code>staticmethod</code>","text":"<p>Retrieves class names and templates for the specified dataset.</p> <p>This method looks up the dataset information in the internal mapping and dynamically imports the class names and templates from the specified module. It supports both simple string mappings and detailed dictionary mappings for datasets.</p> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>The name of the dataset for which to retrieve class names and templates.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple[List[str], List[Callable]]</code>           \u2013            <p>Tuple[List[str], List[Callable]]: A tuple containing a list of class names and a list of template callables.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the dataset_name is not found in the internal mapping.</p> </li> </ul> Source code in <code>fusion_bench/tasks/clip_classification/__init__.py</code> <pre><code>@staticmethod\ndef get_classnames_and_templates(\n    dataset_name: str,\n) -&gt; Tuple[List[str], List[Callable]]:\n    \"\"\"\n    Retrieves class names and templates for the specified dataset.\n\n    This method looks up the dataset information in the internal mapping and dynamically imports\n    the class names and templates from the specified module. It supports both simple string mappings\n    and detailed dictionary mappings for datasets.\n\n    Args:\n        dataset_name (str): The name of the dataset for which to retrieve class names and templates.\n\n    Returns:\n        Tuple[List[str], List[Callable]]: A tuple containing a list of class names and a list of template callables.\n\n    Raises:\n        ValueError: If the dataset_name is not found in the internal mapping.\n    \"\"\"\n    if dataset_name not in CLIPTemplateFactory._dataset_mapping:\n        raise ValueError(\n            f\"Unknown dataset {dataset_name}, available datasets: {CLIPTemplateFactory._dataset_mapping.keys()}. You can register a new dataset using `CLIPTemplateFactory.register_dataset()` method.\"\n        )\n\n    dataset_info = CLIPTemplateFactory._dataset_mapping[dataset_name]\n    # convert dataset_info to dict format: { 'module': str, 'classnames': str, 'templates': str }\n    if isinstance(dataset_info, str):\n        dataset_info = _check_module_name(dataset_info)\n        dataset_info = {\n            \"module\": dataset_info,\n            \"classnames\": \"classnames\",\n            \"templates\": \"templates\",\n        }\n    elif isinstance(dataset_info, dict):\n        if \"module\" in dataset_info:\n            dataset_info[\"module\"] = _check_module_name(dataset_info[\"module\"])\n\n    # import classnames and templates from the specified module\n    # convert to dict format: { 'labels': List[str], 'templates': List[Callable] }\n    if \"module\" in dataset_info:\n        module = importlib.import_module(dataset_info[\"module\"])\n        classnames = getattr(module, dataset_info[\"classnames\"])\n        templates = getattr(module, dataset_info[\"templates\"])\n    else:\n        classnames = dataset_info[\"classnames\"]\n        templates = dataset_info[\"templates\"]\n\n    return classnames, templates\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.clip_classification.CLIPTemplateFactory.register_dataset","title":"<code>register_dataset(dataset_name, *, dataset_info=None, classnames=None, templates=None)</code>  <code>staticmethod</code>","text":"<p>Registers a new dataset with its associated information.</p> <p>This method allows for the dynamic addition of datasets to the internal mapping. It supports registration through either a detailed dictionary (<code>dataset_info</code>) or separate lists of class names and templates. If a dataset with the same name already exists, it will be overwritten.</p> <p>The expected format and contents of <code>dataset_info</code> can vary depending on the needs of the dataset being registered, but typically, it includes the following keys:</p> <ul> <li>\"module\": A string specifying the module path where the dataset's related classes and functions are located. This is used for dynamic import of the dataset's class names and templates.</li> <li>\"classnames\": This key is expected to hold the name of the attribute or variable in the specified module that contains a list of class names relevant to the dataset. These class names are used to label data points in the dataset.</li> <li>\"templates\": Similar to \"classnames\", this key should specify the name of the attribute or variable in the module that contains a list of template callables. These templates are functions or methods that define how data points should be processed or transformed.</li> </ul> <p>Parameters:</p> <ul> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>The name of the dataset to register.</p> </li> <li> <code>dataset_info</code>               (<code>Dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary containing the dataset information, including module path, class names, and templates. Defaults to None.</p> </li> <li> <code>classnames</code>               (<code>List[str]</code>, default:                   <code>None</code> )           \u2013            <p>A list of class names for the dataset. Required if <code>dataset_info</code> is not provided. Defaults to None.</p> </li> <li> <code>templates</code>               (<code>List[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>A list of template callables for the dataset. Required if <code>dataset_info</code> is not provided. Defaults to None.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AssertionError</code>             \u2013            <p>If neither <code>dataset_info</code> nor both <code>classnames</code> and <code>templates</code> are provided.</p> </li> </ul> Source code in <code>fusion_bench/tasks/clip_classification/__init__.py</code> <pre><code>@staticmethod\ndef register_dataset(\n    dataset_name: str,\n    *,\n    dataset_info: Dict[str, Any] = None,\n    classnames: List[str] = None,\n    templates: List[Callable] = None,\n):\n    \"\"\"\n    Registers a new dataset with its associated information.\n\n    This method allows for the dynamic addition of datasets to the internal mapping. It supports\n    registration through either a detailed dictionary (`dataset_info`) or separate lists of class names\n    and templates. If a dataset with the same name already exists, it will be overwritten.\n\n    The expected format and contents of `dataset_info` can vary depending on the needs of the dataset being registered, but typically, it includes the following keys:\n\n    - \"module\": A string specifying the module path where the dataset's related classes and functions are located. This is used for dynamic import of the dataset's class names and templates.\n    - \"classnames\": This key is expected to hold the name of the attribute or variable in the specified module that contains a list of class names relevant to the dataset. These class names are used to label data points in the dataset.\n    - \"templates\": Similar to \"classnames\", this key should specify the name of the attribute or variable in the module that contains a list of template callables. These templates are functions or methods that define how data points should be processed or transformed.\n\n    Args:\n        dataset_name (str): The name of the dataset to register.\n        dataset_info (Dict[str, Any], optional): A dictionary containing the dataset information, including module path, class names, and templates. Defaults to None.\n        classnames (List[str], optional): A list of class names for the dataset. Required if `dataset_info` is not provided. Defaults to None.\n        templates (List[Callable], optional): A list of template callables for the dataset. Required if `dataset_info` is not provided. Defaults to None.\n\n    Raises:\n        AssertionError: If neither `dataset_info` nor both `classnames` and `templates` are provided.\n    \"\"\"\n    assert dataset_info is None or (\n        classnames is not None and templates is not None\n    ), \"You must provide either `dataset_info` or both `classnames` and `templates`.\"\n\n    if dataset_name in CLIPTemplateFactory._dataset_mapping:\n        warnings.warn(\n            f\"Dataset {dataset_name} is already registered, overwriting the existing dataset information.\"\n        )\n    if dataset_info is None:\n        dataset_info = {\"classnames\": classnames, \"temolates\": templates}\n    CLIPTemplateFactory._dataset_mapping[dataset_name] = dataset_info\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.clip_classification.get_classnames_and_templates","title":"<code>get_classnames_and_templates(dataset_name)</code>","text":"Source code in <code>fusion_bench/tasks/clip_classification/__init__.py</code> <pre><code>def get_classnames_and_templates(dataset_name: str) -&gt; Tuple[List[str], List[Callable]]:\n    return CLIPTemplateFactory.get_classnames_and_templates(dataset_name)\n</code></pre>"},{"location":"api/fusion_bench.tasks/#flan-t5-text-generation-tasks","title":"Flan-T5 Text Generation Tasks","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors","title":"<code>fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors</code>","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.CoLA_Preprocessor","title":"<code>CoLA_Preprocessor</code>","text":"<p>               Bases: <code>DatasetPreprocessor</code></p> <p>dataset URL: https://huggingface.co/datasets/glue/viewer/cola</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>class CoLA_Preprocessor(DatasetPreprocessor):\n    \"\"\"\n    dataset URL: https://huggingface.co/datasets/glue/viewer/cola\n    \"\"\"\n\n    def preprocess(self, sentence: str, label: int):\n        assert isinstance(sentence, str)\n        assert isinstance(label, int)\n        input_text = self.template[\"input_text\"].format(sentence=sentence)\n        if label in [0, 1]:\n            target_text = self.template[\"target_text\"][str(label)]\n        else:\n            target_text = \"\"\n        return input_text, target_text\n\n    def __call__(self, example: Dict[str, Any]):\n        \"\"\"\n        Preprocess the CoLA dataset into a text-to-text format.\n        \"\"\"\n        if isinstance(example[\"sentence\"], str):\n            # not batched\n            input_text, target_text = self.preprocess(\n                example[\"sentence\"], example[\"label\"]\n            )\n        else:\n            # batched\n            input_text, target_text = [], []\n            for sentence, label in zip(example[\"sentence\"], example[\"label\"]):\n                _input_text, _target_text = self.preprocess(sentence, label)\n                input_text.append(_input_text)\n                target_text.append(_target_text)\n\n        return preprocess(\n            tokenizer=self.tokenizer,\n            input_text=input_text,\n            target_text=target_text,\n            tokenizer_kwawgs=self.tokenizer_kwargs,\n        )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.CoLA_Preprocessor.__call__","title":"<code>__call__(example)</code>","text":"<p>Preprocess the CoLA dataset into a text-to-text format.</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>def __call__(self, example: Dict[str, Any]):\n    \"\"\"\n    Preprocess the CoLA dataset into a text-to-text format.\n    \"\"\"\n    if isinstance(example[\"sentence\"], str):\n        # not batched\n        input_text, target_text = self.preprocess(\n            example[\"sentence\"], example[\"label\"]\n        )\n    else:\n        # batched\n        input_text, target_text = [], []\n        for sentence, label in zip(example[\"sentence\"], example[\"label\"]):\n            _input_text, _target_text = self.preprocess(sentence, label)\n            input_text.append(_input_text)\n            target_text.append(_target_text)\n\n    return preprocess(\n        tokenizer=self.tokenizer,\n        input_text=input_text,\n        target_text=target_text,\n        tokenizer_kwawgs=self.tokenizer_kwargs,\n    )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.MNLI_Preprocessor","title":"<code>MNLI_Preprocessor</code>","text":"<p>               Bases: <code>DatasetPreprocessor</code></p> <p>dataset URL: https://huggingface.co/datasets/glue/viewer/mnli/</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>class MNLI_Preprocessor(DatasetPreprocessor):\n    \"\"\"\n    dataset URL: https://huggingface.co/datasets/glue/viewer/mnli/\n    \"\"\"\n\n    def preprocess(self, hypothesis, premise, label):\n        assert isinstance(hypothesis, str)\n        assert isinstance(premise, str)\n        assert isinstance(label, int)\n        input_text = self.template[\"input_text\"].format(\n            hypothesis=hypothesis, premise=premise\n        )\n        if label in [0, 1, 2]:\n            target_text = self.template[\"target_text\"][str(label)]\n        else:\n            target_text = \"\"\n        return input_text, target_text\n\n    def __call__(self, example):\n        \"\"\"\n        Preprocess the MNLI dataset into a text-to-text format.\n        \"\"\"\n        if isinstance(example[\"hypothesis\"], str):\n            # not batched\n            input_text, target_text = self.preprocess(\n                example[\"hypothesis\"], example[\"premise\"], example[\"label\"]\n            )\n        else:\n            # batched\n            input_text, target_text = [], []\n            for hypothesis, premise, label in zip(\n                example[\"hypothesis\"], example[\"premise\"], example[\"label\"]\n            ):\n                _input_text, _target_text = self.preprocess(hypothesis, premise, label)\n                input_text.append(_input_text)\n                target_text.append(_target_text)\n\n        return preprocess(\n            tokenizer=self.tokenizer,\n            input_text=input_text,\n            target_text=target_text,\n            tokenizer_kwawgs=self.tokenizer_kwargs,\n        )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.MNLI_Preprocessor.__call__","title":"<code>__call__(example)</code>","text":"<p>Preprocess the MNLI dataset into a text-to-text format.</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>def __call__(self, example):\n    \"\"\"\n    Preprocess the MNLI dataset into a text-to-text format.\n    \"\"\"\n    if isinstance(example[\"hypothesis\"], str):\n        # not batched\n        input_text, target_text = self.preprocess(\n            example[\"hypothesis\"], example[\"premise\"], example[\"label\"]\n        )\n    else:\n        # batched\n        input_text, target_text = [], []\n        for hypothesis, premise, label in zip(\n            example[\"hypothesis\"], example[\"premise\"], example[\"label\"]\n        ):\n            _input_text, _target_text = self.preprocess(hypothesis, premise, label)\n            input_text.append(_input_text)\n            target_text.append(_target_text)\n\n    return preprocess(\n        tokenizer=self.tokenizer,\n        input_text=input_text,\n        target_text=target_text,\n        tokenizer_kwawgs=self.tokenizer_kwargs,\n    )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.MRPC_Preprocessor","title":"<code>MRPC_Preprocessor</code>","text":"<p>               Bases: <code>DatasetPreprocessor</code></p> <p>dataset URL: https://huggingface.co/datasets/glue/viewer/mrpc</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>class MRPC_Preprocessor(DatasetPreprocessor):\n    \"\"\"\n    dataset URL: https://huggingface.co/datasets/glue/viewer/mrpc\n    \"\"\"\n\n    def preprocess(self, sentence1: str, sentence2: str, label: int):\n        assert isinstance(sentence1, str)\n        assert isinstance(sentence2, str)\n        assert isinstance(label, int)\n        input_text = self.template[\"input_text\"].format(\n            sentence1=sentence1, sentence2=sentence2\n        )\n        if label in [0, 1]:\n            target_text = self.template[\"target_text\"][str(label)]\n        else:\n            target_text = \"\"\n        return input_text, target_text\n\n    def __call__(self, example):\n        \"\"\"\n        Preprocess the MRPC dataset into a text-to-text format.\n        \"\"\"\n        if isinstance(example[\"sentence1\"], str):\n            # not batched\n            input_text, target_text = self.preprocess(\n                example[\"sentence1\"], example[\"sentence2\"], example[\"label\"]\n            )\n        else:\n            # batched\n            input_text, target_text = [], []\n            for sentence1, sentence2, label in zip(\n                example[\"sentence1\"], example[\"sentence2\"], example[\"label\"]\n            ):\n                _input_text, _target_text = self.preprocess(sentence1, sentence2, label)\n                input_text.append(_input_text)\n                target_text.append(_target_text)\n\n        return preprocess(\n            tokenizer=self.tokenizer,\n            input_text=input_text,\n            target_text=target_text,\n            tokenizer_kwawgs=self.tokenizer_kwargs,\n        )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.MRPC_Preprocessor.__call__","title":"<code>__call__(example)</code>","text":"<p>Preprocess the MRPC dataset into a text-to-text format.</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>def __call__(self, example):\n    \"\"\"\n    Preprocess the MRPC dataset into a text-to-text format.\n    \"\"\"\n    if isinstance(example[\"sentence1\"], str):\n        # not batched\n        input_text, target_text = self.preprocess(\n            example[\"sentence1\"], example[\"sentence2\"], example[\"label\"]\n        )\n    else:\n        # batched\n        input_text, target_text = [], []\n        for sentence1, sentence2, label in zip(\n            example[\"sentence1\"], example[\"sentence2\"], example[\"label\"]\n        ):\n            _input_text, _target_text = self.preprocess(sentence1, sentence2, label)\n            input_text.append(_input_text)\n            target_text.append(_target_text)\n\n    return preprocess(\n        tokenizer=self.tokenizer,\n        input_text=input_text,\n        target_text=target_text,\n        tokenizer_kwawgs=self.tokenizer_kwargs,\n    )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.QNLI_Preprocessor","title":"<code>QNLI_Preprocessor</code>","text":"<p>               Bases: <code>DatasetPreprocessor</code></p> <p>dataset URL: https://huggingface.co/datasets/glue/viewer/qnli</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>class QNLI_Preprocessor(DatasetPreprocessor):\n    \"\"\"\n    dataset URL: https://huggingface.co/datasets/glue/viewer/qnli\n    \"\"\"\n\n    def preprocess(self, question: str, sentence: str, label: int):\n        assert isinstance(question, str)\n        assert isinstance(sentence, str)\n        assert isinstance(label, int)\n        input_text = self.template[\"input_text\"].format(\n            question=question, sentence=sentence\n        )\n        if label in [0, 1]:\n            target_text = self.template[\"target_text\"][str(label)]\n        else:\n            target_text = \"\"\n        return input_text, target_text\n\n    def __call__(self, example):\n        \"\"\"\n        Preprocess the QNLI dataset into a text-to-text format.\n        \"\"\"\n        if isinstance(example[\"question\"], str):\n            # not batched\n            input_text, target_text = self.preprocess(\n                example[\"question\"], example[\"sentence\"], example[\"label\"]\n            )\n        else:\n            # batched\n            input_text, target_text = [], []\n            for question, sentence, label in zip(\n                example[\"question\"], example[\"sentence\"], example[\"label\"]\n            ):\n                _input_text, _target_text = self.preprocess(question, sentence, label)\n                input_text.append(_input_text)\n                target_text.append(_target_text)\n\n        return preprocess(\n            tokenizer=self.tokenizer,\n            input_text=input_text,\n            target_text=target_text,\n            tokenizer_kwawgs=self.tokenizer_kwargs,\n        )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.QNLI_Preprocessor.__call__","title":"<code>__call__(example)</code>","text":"<p>Preprocess the QNLI dataset into a text-to-text format.</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>def __call__(self, example):\n    \"\"\"\n    Preprocess the QNLI dataset into a text-to-text format.\n    \"\"\"\n    if isinstance(example[\"question\"], str):\n        # not batched\n        input_text, target_text = self.preprocess(\n            example[\"question\"], example[\"sentence\"], example[\"label\"]\n        )\n    else:\n        # batched\n        input_text, target_text = [], []\n        for question, sentence, label in zip(\n            example[\"question\"], example[\"sentence\"], example[\"label\"]\n        ):\n            _input_text, _target_text = self.preprocess(question, sentence, label)\n            input_text.append(_input_text)\n            target_text.append(_target_text)\n\n    return preprocess(\n        tokenizer=self.tokenizer,\n        input_text=input_text,\n        target_text=target_text,\n        tokenizer_kwawgs=self.tokenizer_kwargs,\n    )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.QQP_Preprocessor","title":"<code>QQP_Preprocessor</code>","text":"<p>               Bases: <code>DatasetPreprocessor</code></p> <p>dataset URL: https://huggingface.co/datasets/glue/viewer/qqp</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>class QQP_Preprocessor(DatasetPreprocessor):\n    \"\"\"\n    dataset URL: https://huggingface.co/datasets/glue/viewer/qqp\n    \"\"\"\n\n    def preprocess(self, question1, question2, label):\n        assert isinstance(\n            question1, str\n        ), f\"question1 must be a string, got {type(question1)}, question1={question1}\"\n        assert isinstance(\n            question2, str\n        ), f\"question2 must be a string, got {type(question2)}, question2={question2}\"\n        assert isinstance(\n            label, int\n        ), f\"label must be an int, got {type(label)}, label={label}\"\n        input_text: str = self.template[\"input_text\"].format(\n            question1=question1, question2=question2\n        )\n        if label in [0, 1]:\n            target_text: str = self.template[\"target_text\"][str(label)]\n        else:\n            target_text = \"\"\n        return input_text, target_text\n\n    def __call__(self, example):\n        \"\"\"\n        Preprocess the QQP dataset into a text-to-text format.\n        \"\"\"\n        if isinstance(example[\"question1\"], str):\n            # batched\n            input_text, target_text = self.preprocess(\n                example[\"question1\"], example[\"question2\"], example[\"label\"]\n            )\n        else:\n            # not batched\n            input_text, target_text = [], []\n            for question1, question2, label in zip(\n                example[\"question1\"], example[\"question2\"], example[\"label\"]\n            ):\n                _input_text, _target_text = self.preprocess(question1, question2, label)\n                input_text.append(_input_text)\n                target_text.append(_target_text)\n\n        return preprocess(\n            tokenizer=self.tokenizer,\n            input_text=input_text,\n            target_text=target_text,\n            tokenizer_kwawgs=self.tokenizer_kwargs,\n        )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.QQP_Preprocessor.__call__","title":"<code>__call__(example)</code>","text":"<p>Preprocess the QQP dataset into a text-to-text format.</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>def __call__(self, example):\n    \"\"\"\n    Preprocess the QQP dataset into a text-to-text format.\n    \"\"\"\n    if isinstance(example[\"question1\"], str):\n        # batched\n        input_text, target_text = self.preprocess(\n            example[\"question1\"], example[\"question2\"], example[\"label\"]\n        )\n    else:\n        # not batched\n        input_text, target_text = [], []\n        for question1, question2, label in zip(\n            example[\"question1\"], example[\"question2\"], example[\"label\"]\n        ):\n            _input_text, _target_text = self.preprocess(question1, question2, label)\n            input_text.append(_input_text)\n            target_text.append(_target_text)\n\n    return preprocess(\n        tokenizer=self.tokenizer,\n        input_text=input_text,\n        target_text=target_text,\n        tokenizer_kwawgs=self.tokenizer_kwargs,\n    )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.RTE_Preprocessor","title":"<code>RTE_Preprocessor</code>","text":"<p>               Bases: <code>DatasetPreprocessor</code></p> <p>dataset URL: https://huggingface.co/datasets/glue/viewer/rte</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>class RTE_Preprocessor(DatasetPreprocessor):\n    \"\"\"\n    dataset URL: https://huggingface.co/datasets/glue/viewer/rte\n    \"\"\"\n\n    def preprocess(self, sentence1, sentence2, label):\n        assert isinstance(sentence1, str)\n        assert isinstance(sentence2, str)\n        assert isinstance(label, int)\n\n        input_text: str = self.template[\"input_text\"].format(\n            sentence1=sentence1, sentence2=sentence2\n        )\n        if label in [0, 1]:\n            target_text: str = self.template[\"target_text\"][str(label)]\n        else:\n            target_text = \"\"\n        return input_text, target_text\n\n    def __call__(self, example):\n        \"\"\"\n        Preprocess the RTE dataset into a text-to-text format.\n        \"\"\"\n        if isinstance(example[\"sentence1\"], str):\n            # not batched\n            input_text, target_text = self.preprocess(\n                example[\"sentence1\"], example[\"sentence2\"], example[\"label\"]\n            )\n        else:\n            # batched\n            input_text, target_text = [], []\n            for sentence1, sentence2, label in zip(\n                example[\"sentence1\"], example[\"sentence2\"], example[\"label\"]\n            ):\n                _input_text, _target_text = self.preprocess(sentence1, sentence2, label)\n                input_text.append(_input_text)\n                target_text.append(_target_text)\n\n        return preprocess(\n            tokenizer=self.tokenizer,\n            input_text=input_text,\n            target_text=target_text,\n            tokenizer_kwawgs=self.tokenizer_kwargs,\n        )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.RTE_Preprocessor.__call__","title":"<code>__call__(example)</code>","text":"<p>Preprocess the RTE dataset into a text-to-text format.</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>def __call__(self, example):\n    \"\"\"\n    Preprocess the RTE dataset into a text-to-text format.\n    \"\"\"\n    if isinstance(example[\"sentence1\"], str):\n        # not batched\n        input_text, target_text = self.preprocess(\n            example[\"sentence1\"], example[\"sentence2\"], example[\"label\"]\n        )\n    else:\n        # batched\n        input_text, target_text = [], []\n        for sentence1, sentence2, label in zip(\n            example[\"sentence1\"], example[\"sentence2\"], example[\"label\"]\n        ):\n            _input_text, _target_text = self.preprocess(sentence1, sentence2, label)\n            input_text.append(_input_text)\n            target_text.append(_target_text)\n\n    return preprocess(\n        tokenizer=self.tokenizer,\n        input_text=input_text,\n        target_text=target_text,\n        tokenizer_kwawgs=self.tokenizer_kwargs,\n    )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.SST2_Preprocessor","title":"<code>SST2_Preprocessor</code>","text":"<p>               Bases: <code>DatasetPreprocessor</code></p> <p>dataset URL: https://huggingface.co/datasets/glue/viewer/sst2</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>class SST2_Preprocessor(DatasetPreprocessor):\n    \"\"\"\n    dataset URL: https://huggingface.co/datasets/glue/viewer/sst2\n    \"\"\"\n\n    def preprocess(self, sentence: str, label: int):\n        assert isinstance(\n            sentence, str\n        ), f\"sentence must be a string, got {type(sentence)}, sentence={sentence}\"\n        assert isinstance(\n            label, int\n        ), f\"label must be an integer, got {type(label)}, label={label}\"\n        input_text = self.template[\"input_text\"].format(sentence=sentence)\n        if label in [0, 1]:\n            target_text = self.template[\"target_text\"][str(label)]\n        else:\n            target_text = \"\"\n        return input_text, target_text\n\n    def __call__(self, example):\n        \"\"\"\n        Preprocess the SST2 dataset into a text-to-text format.\n        \"\"\"\n        if isinstance(example[\"sentence\"], str):\n            # not batched\n            input_text, target_text = self.preprocess(\n                example[\"sentence\"], example[\"label\"]\n            )\n        else:\n            # batched\n            input_text, target_text = [], []\n            for sentence, label in zip(example[\"sentence\"], example[\"label\"]):\n                _input_text, _target_text = self.preprocess(sentence, label)\n                input_text.append(_input_text)\n                target_text.append(_target_text)\n\n        return preprocess(\n            tokenizer=self.tokenizer,\n            input_text=input_text,\n            target_text=target_text,\n            tokenizer_kwawgs=self.tokenizer_kwargs,\n        )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.SST2_Preprocessor.__call__","title":"<code>__call__(example)</code>","text":"<p>Preprocess the SST2 dataset into a text-to-text format.</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>def __call__(self, example):\n    \"\"\"\n    Preprocess the SST2 dataset into a text-to-text format.\n    \"\"\"\n    if isinstance(example[\"sentence\"], str):\n        # not batched\n        input_text, target_text = self.preprocess(\n            example[\"sentence\"], example[\"label\"]\n        )\n    else:\n        # batched\n        input_text, target_text = [], []\n        for sentence, label in zip(example[\"sentence\"], example[\"label\"]):\n            _input_text, _target_text = self.preprocess(sentence, label)\n            input_text.append(_input_text)\n            target_text.append(_target_text)\n\n    return preprocess(\n        tokenizer=self.tokenizer,\n        input_text=input_text,\n        target_text=target_text,\n        tokenizer_kwawgs=self.tokenizer_kwargs,\n    )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.STSB_Preprocessor","title":"<code>STSB_Preprocessor</code>","text":"<p>               Bases: <code>DatasetPreprocessor</code></p> <p>dataset URL: https://huggingface.co/datasets/glue/viewer/stsb</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>class STSB_Preprocessor(DatasetPreprocessor):\n    \"\"\"\n    dataset URL: https://huggingface.co/datasets/glue/viewer/stsb\n    \"\"\"\n\n    def preprocess(self, sentence1, sentence2, label):\n        assert isinstance(\n            sentence1, str\n        ), f\"sentence1 must be a string, got {type(sentence1)}, sentence1={sentence1}\"\n        assert isinstance(\n            sentence2, str\n        ), f\"sentence2 must be a string, got {type(sentence2)}, sentence2={sentence2}\"\n        assert isinstance(\n            label, (float, int)\n        ), f\"label must be a float or an integer, got {type(label)}, label={label}\"\n        input_text = self.template[\"input_text\"].format(\n            sentence1=sentence1, sentence2=sentence2\n        )\n        target_text = self.template[\"target_text\"].format(label)\n        return input_text, target_text\n\n    def __call__(self, example):\n        \"\"\"\n        Preprocess the STSB dataset into a text-to-text format.\n        \"\"\"\n        if isinstance(example[\"sentence1\"], str):\n            # not batched\n            input_text, target_text = self.preprocess(\n                example[\"sentence1\"], example[\"sentence2\"], example[\"label\"]\n            )\n        else:\n            # batched\n            input_text, target_text = [], []\n            for sentence1, sentence2, label in zip(\n                example[\"sentence1\"], example[\"sentence2\"], example[\"label\"]\n            ):\n                _input_text, _target_text = self.preprocess(sentence1, sentence2, label)\n                input_text.append(_input_text)\n                target_text.append(_target_text)\n\n        return preprocess(\n            tokenizer=self.tokenizer,\n            input_text=input_text,\n            target_text=target_text,\n            tokenizer_kwawgs=self.tokenizer_kwargs,\n        )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_preprocessors.STSB_Preprocessor.__call__","title":"<code>__call__(example)</code>","text":"<p>Preprocess the STSB dataset into a text-to-text format.</p> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_preprocessors.py</code> <pre><code>def __call__(self, example):\n    \"\"\"\n    Preprocess the STSB dataset into a text-to-text format.\n    \"\"\"\n    if isinstance(example[\"sentence1\"], str):\n        # not batched\n        input_text, target_text = self.preprocess(\n            example[\"sentence1\"], example[\"sentence2\"], example[\"label\"]\n        )\n    else:\n        # batched\n        input_text, target_text = [], []\n        for sentence1, sentence2, label in zip(\n            example[\"sentence1\"], example[\"sentence2\"], example[\"label\"]\n        ):\n            _input_text, _target_text = self.preprocess(sentence1, sentence2, label)\n            input_text.append(_input_text)\n            target_text.append(_target_text)\n\n    return preprocess(\n        tokenizer=self.tokenizer,\n        input_text=input_text,\n        target_text=target_text,\n        tokenizer_kwawgs=self.tokenizer_kwargs,\n    )\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_load_dataset","title":"<code>fusion_bench.tasks.flan_t5_text_generation.glue_load_dataset</code>","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_load_dataset.load_glue_dataset","title":"<code>load_glue_dataset(name, tokenizer, cache_dir='outputs/cache', split=None)</code>","text":"Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_load_dataset.py</code> <pre><code>def load_glue_dataset(\n    name,\n    tokenizer,\n    cache_dir: Optional[str] = \"outputs/cache\",\n    split: Optional[str] = None,\n):\n    with timeit_context(f\"Loading {name} dataset\"):\n        if cache_dir is not None:\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n            cache_path = os.path.join(\n                cache_dir, \"flan-t5\", f\"_load_{name}_dataset_cached\"\n            )\n            if os.path.exists(cache_path):\n                dataset = load_from_disk(cache_path)\n            else:\n                dataset = _load_glue_dataset(name, tokenizer)\n                log.info(f\"Saving {name} dataset to {cache_path}\")\n                dataset.save_to_disk(cache_path)\n        else:\n            dataset = _load_glue_dataset(name, tokenizer)\n\n    if split is not None:\n        return dataset[split]\n    else:\n        return dataset\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_evaluation","title":"<code>fusion_bench.tasks.flan_t5_text_generation.glue_evaluation</code>","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_evaluation.evaluate_accuracy","title":"<code>evaluate_accuracy(model, val_loader, tokenizer)</code>","text":"<p>This function evaluates the accuracy of a language model on a validation set.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The language model to be evaluated.</p> </li> <li> <code>val_loader</code>               (<code>DataLoader</code>)           \u2013            <p>The DataLoader object containing the validation data.</p> </li> <li> <code>tokenizer</code>               (<code>Tokenizer</code>)           \u2013            <p>The tokenizer object used for tokenizing text.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>          \u2013            <p>The accuracy of the model on the validation set.</p> </li> </ul> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_evaluation.py</code> <pre><code>def evaluate_accuracy(model, val_loader: DataLoader, tokenizer):\n    \"\"\"\n    This function evaluates the accuracy of a language model on a validation set.\n\n    Parameters:\n        model (nn.Module): The language model to be evaluated.\n        val_loader (DataLoader): The DataLoader object containing the validation data.\n        tokenizer (Tokenizer): The tokenizer object used for tokenizing text.\n\n    Returns:\n        float: The accuracy of the model on the validation set.\n    \"\"\"\n    from tqdm import tqdm\n\n    correct = 0\n    total = 0\n\n    model = model.eval()\n    for batch_idx, batch in enumerate(\n        tqdm(\n            val_loader, desc=\"Evaluate Exact Accuracy\", leave=False, dynamic_ncols=True\n        )\n    ):\n        with torch.no_grad():\n            outputs = model.generate(batch[\"input_ids\"], max_length=10)\n            output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n            labels = [\n                remove_special_tokens(tokenizer, label_token)\n                for label_token in batch[\"labels\"]\n            ]\n            labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n            # compare output_text and labels\n            for i, j in zip(output_text, labels):\n                if i == j:\n                    correct += 1\n                total += 1\n\n    # return accuracy\n    return correct / total\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_evaluation.evaluate_spearman_rho","title":"<code>evaluate_spearman_rho(model, val_loader, tokenizer)</code>","text":"<p>This function evaluates the Spearman's rank correlation coefficient (rho) between the model's predictions and the actual labels on a validation set.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The language model to be evaluated.</p> </li> <li> <code>val_loader</code>               (<code>DataLoader</code>)           \u2013            <p>The DataLoader object containing the validation data.</p> </li> <li> <code>tokenizer</code>               (<code>Tokenizer</code>)           \u2013            <p>The tokenizer object used for tokenizing text.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>          \u2013            <p>The Spearman's rho between the model's predictions and the actual labels.</p> </li> </ul> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_evaluation.py</code> <pre><code>def evaluate_spearman_rho(model, val_loader: DataLoader, tokenizer):\n    \"\"\"\n    This function evaluates the Spearman's rank correlation coefficient (rho) between the model's predictions and the actual labels on a validation set.\n\n    Parameters:\n        model (nn.Module): The language model to be evaluated.\n        val_loader (DataLoader): The DataLoader object containing the validation data.\n        tokenizer (Tokenizer): The tokenizer object used for tokenizing text.\n\n    Returns:\n        float: The Spearman's rho between the model's predictions and the actual labels.\n    \"\"\"\n    from tqdm import tqdm\n\n    model = model.eval()\n    all_preds: List[str] = []\n    all_labels: List[str] = []\n    for batch_idx, batch in enumerate(\n        tqdm(val_loader, desc=\"Evaluate Spearman Rho\", leave=False, dynamic_ncols=True)\n    ):\n        with torch.no_grad():\n            outputs = model.generate(batch[\"input_ids\"], max_length=10)\n            output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n            labels = [\n                remove_special_tokens(tokenizer, label_token)\n                for label_token in batch[\"labels\"]\n            ]\n            labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n            all_preds.extend(output_text)\n            all_labels.extend(labels)\n\n    # save `all_preds` and `all_labels`\n    # with open(\"temp/all_preds.txt\", \"w\") as f:\n    #     for preds in all_preds:\n    #         for pred in preds:\n    #             f.write(pred + \"\\n\")\n    # with open(\"temp/all_labels.txt\", \"w\") as f:\n    #     for labels in all_labels:\n    #         for label in labels:\n    #             f.write(label + \"\\n\")\n\n    # calculate spearman's rho\n    # 1. convert string list `all_preds` and `all_labels` to numpy array\n    # 2. compute spearman's rho\n    from scipy.stats import spearmanr\n\n    def parse_flost(s: str):\n        try:\n            return float(s)\n        except Exception:\n            return 0.0\n\n    all_preds = np.array([parse_flost(pred) for pred in all_preds])\n    all_labels = np.array([parse_flost(label) for label in all_labels])\n    rho = spearmanr(all_preds, all_labels)[0]\n    return rho\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_evaluation.remove_special_tokens","title":"<code>remove_special_tokens(tokenizer, token_list)</code>","text":"<p>This function removes special tokens from a list of tokens. It also stops processing when it encounters a token with a value of -100.</p> <p>Parameters:</p> <ul> <li> <code>tokenizer</code>               (<code>Tokenizer</code>)           \u2013            <p>The tokenizer object used for tokenizing text.</p> </li> <li> <code>token_list</code>               (<code>list</code>)           \u2013            <p>The list of tokens to be processed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code>          \u2013            <p>The list of tokens after removing special tokens.</p> </li> </ul> Source code in <code>fusion_bench/tasks/flan_t5_text_generation/glue_evaluation.py</code> <pre><code>def remove_special_tokens(tokenizer, token_list: list):\n    \"\"\"\n    This function removes special tokens from a list of tokens. It also stops processing\n    when it encounters a token with a value of -100.\n\n    Parameters:\n        tokenizer (Tokenizer): The tokenizer object used for tokenizing text.\n        token_list (list): The list of tokens to be processed.\n\n    Returns:\n        list: The list of tokens after removing special tokens.\n    \"\"\"\n    ret = []\n    for token in token_list:\n        if token not in tokenizer.all_special_ids and token &gt; 0:\n            ret.append(token)\n        if token == -100:\n            break\n    return ret\n</code></pre>"},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_prompt_templates","title":"<code>fusion_bench.tasks.flan_t5_text_generation.glue_prompt_templates</code>","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_prompt_templates.glue_prompt_templates","title":"<code>glue_prompt_templates = {'cola': cola, 'mnli': mnli, 'mrpc': mrpc, 'qnli': qnli, 'qqp': qqp, 'rte': rte, 'stsb': stsb, 'sst2': sst2}</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_prompt_templates.cola","title":"<code>cola = {'description': 'template used by GLUE-CoLA', 'input_text': 'Indicate if the following sentence is grammatically correct or not: \"{sentence}\". Answere \\'acceptable\\' or \\'unacceptable\\'.', 'target_text': {'0': 'unacceptable', '1': 'acceptable'}}</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_prompt_templates.mnli","title":"<code>mnli = {'input_text': \"Does the premise: '{premise}' logically imply, contradict, or is neutral to the hypothesis: '{hypothesis}'? Answere with 'entailment', 'contradiction', or 'neutral'.\", 'target_text': {'0': 'entailment', '1': 'neutral', '2': 'contradiction'}}</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_prompt_templates.mrpc","title":"<code>mrpc = {'input_text': \"Are the following sentences '{sentence1}' and '{sentence2}' conveying the same meaning? Answere with 'yes' or 'no'.\", 'target_text': {'0': 'no', '1': 'yes'}}</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_prompt_templates.qnli","title":"<code>qnli = {'input_text': \"Given the context: '{sentence}', does the question '{question}' have an answer based on the information provided? Answer with 'yes' or 'no'.\", 'target_text': {'0': 'yes', '1': 'no'}}</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_prompt_templates.qqp","title":"<code>qqp = {'input_text': \"Do the questions '{question1}' and '{question2}' have the same intent? Answere with 'yes' or 'no'.\", 'target_text': {'0': 'no', '1': 'yes'}}</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_prompt_templates.rte","title":"<code>rte = {'description': 'Template used by GLUE-RTE', 'input_text': \"Does the text: '{sentence1}' entail that '{sentence2}' is true? Provide 'yes' or 'no'.\", 'target_text': {'0': 'yes', '1': 'no'}}</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_prompt_templates.stsb","title":"<code>stsb = {'input_text': \"Consider the sentences '{sentence1}' and '{sentence2}'. On a scale from 1 (completely different) to 5 (completely similar), rate the similarity.\", 'target_text': '{:.1f}'}</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.tasks/#fusion_bench.tasks.flan_t5_text_generation.glue_prompt_templates.sst2","title":"<code>sst2 = {'input_text': \"Given the sentence '{sentence}', determine the sentiment. Is it positive or negative?\", 'target_text': {'0': 'negative', '1': 'positive'}}</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.method/","title":"fusion_bench.method","text":""},{"location":"api/fusion_bench.method/#base-class","title":"Base Class","text":"<ul> <li>BaseAlgorithm: The base class for all fusion algorithms in FusionBench.</li> </ul>"},{"location":"api/fusion_bench.method/#fusion_bench.method.BaseAlgorithm","title":"<code>BaseAlgorithm</code>","text":"<p>               Bases: <code>BaseYAMLSerializable</code></p> <p>Base class for model fusion algorithms.</p> <p>This abstract class provides a standardized interface for implementing model fusion algorithms. It inherits from BaseYAMLSerializable to support configuration loading from YAML files.</p> <p>The class follows a template method pattern where subclasses must implement the core fusion logic in the <code>run</code> method, while optional lifecycle hooks allow for setup and cleanup operations.</p> <p>Attributes:</p> <ul> <li> <code>_program</code>           \u2013            <p>Optional program reference for algorithm execution context.</p> </li> <li> <code>_config_key</code>               (<code>str</code>)           \u2013            <p>Configuration key used for YAML serialization, defaults to \"method\".</p> </li> </ul> <p>Examples:</p> <p>Creating a simple averaging algorithm:</p> <pre><code>&gt;&gt;&gt; class SimpleAverageAlgorithm(BaseAlgorithm):\n...     def run(self, modelpool: BaseModelPool):\n...         # Implementation of model averaging logic\n...         return averaged_model\n...\n&gt;&gt;&gt; algorithm = SimpleAverageAlgorithm()\n&gt;&gt;&gt; merged_model = algorithm.run(modelpool)\n</code></pre> <p>Loading algorithm from YAML configuration:</p> <pre><code>&gt;&gt;&gt; algorithm = BaseAlgorithm.from_yaml(\"config.yaml\")\n&gt;&gt;&gt; result = algorithm.run(modelpool)\n</code></pre> Note <p>Subclasses must implement the abstract <code>run</code> method to define the specific fusion strategy (e.g., simple averaging, task arithmetic, etc.).</p> Source code in <code>fusion_bench/method/base_algorithm.py</code> <pre><code>class BaseAlgorithm(BaseYAMLSerializable):\n    \"\"\"\n    Base class for model fusion algorithms.\n\n    This abstract class provides a standardized interface for implementing model fusion\n    algorithms. It inherits from BaseYAMLSerializable to support configuration loading\n    from YAML files.\n\n    The class follows a template method pattern where subclasses must implement the\n    core fusion logic in the `run` method, while optional lifecycle hooks allow for\n    setup and cleanup operations.\n\n    Attributes:\n        _program: Optional program reference for algorithm execution context.\n        _config_key (str): Configuration key used for YAML serialization, defaults to \"method\".\n\n    Examples:\n        Creating a simple averaging algorithm:\n\n        &gt;&gt;&gt; class SimpleAverageAlgorithm(BaseAlgorithm):\n        ...     def run(self, modelpool: BaseModelPool):\n        ...         # Implementation of model averaging logic\n        ...         return averaged_model\n        ...\n        &gt;&gt;&gt; algorithm = SimpleAverageAlgorithm()\n        &gt;&gt;&gt; merged_model = algorithm.run(modelpool)\n\n        Loading algorithm from YAML configuration:\n\n        &gt;&gt;&gt; algorithm = BaseAlgorithm.from_yaml(\"config.yaml\")\n        &gt;&gt;&gt; result = algorithm.run(modelpool)\n\n    Note:\n        Subclasses must implement the abstract `run` method to define the specific\n        fusion strategy (e.g., simple averaging, task arithmetic, etc.).\n    \"\"\"\n\n    _program = None\n    _config_key = \"method\"\n\n    def on_run_start(self):\n        \"\"\"\n        Lifecycle hook called at the beginning of algorithm execution.\n\n        This method is invoked before the main `run` method executes, providing\n        an opportunity for subclasses to perform initialization tasks such as:\n\n        - Setting up logging or monitoring\n        - Initializing algorithm-specific state\n        - Validating prerequisites\n        - Preparing computational resources\n\n        The default implementation does nothing, allowing subclasses to override\n        as needed for their specific requirements.\n\n        Examples:\n            &gt;&gt;&gt; class MyAlgorithm(BaseAlgorithm):\n            ...     def on_run_start(self):\n            ...         super().on_run_start()\n            ...         print(\"Starting model fusion...\")\n            ...         self.start_time = time.time()\n        \"\"\"\n        pass\n\n    def on_run_end(self):\n        \"\"\"\n        Lifecycle hook called at the end of algorithm execution.\n\n        This method is invoked after the main `run` method completes, providing\n        an opportunity for subclasses to perform cleanup and finalization tasks such as:\n\n        - Logging execution statistics or results\n        - Cleaning up temporary resources\n        - Saving intermediate results or metrics\n        - Releasing computational resources\n\n        The method is called regardless of whether the `run` method succeeded or failed,\n        making it suitable for cleanup operations that should always occur.\n\n        The default implementation does nothing, allowing subclasses to override\n        as needed for their specific requirements.\n\n        Examples:\n            &gt;&gt;&gt; class MyAlgorithm(BaseAlgorithm):\n            ...     def on_run_end(self):\n            ...         super().on_run_end()\n            ...         elapsed = time.time() - self.start_time\n            ...         print(f\"Fusion completed in {elapsed:.2f}s\")\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def run(self, modelpool: BaseModelPool):\n        \"\"\"\n        Execute the model fusion algorithm on the provided model pool.\n\n        This is the core method that must be implemented by all subclasses to define\n        their specific fusion strategy. The method takes a pool of models and produces\n        a fused result according to the algorithm's logic.\n\n        Args:\n            modelpool (BaseModelPool): A collection of models to be fused. The modelpool\n                provides access to individual models and their metadata, allowing the\n                algorithm to iterate over models, access their parameters, and perform\n                fusion operations.\n\n        Returns:\n            The type of return value depends on the specific algorithm implementation.\n                Common return types include:\n\n                - A single fused model (torch.nn.Module)\n                - A dictionary of fused models for multi-task scenarios\n                - Fusion results with additional metadata\n                - Custom data structures specific to the algorithm\n\n        Raises:\n            NotImplementedError: If called on the base class without implementation.\n            ValueError: If the modelpool is invalid or incompatible with the algorithm.\n            RuntimeError: If fusion fails due to model incompatibilities or other issues.\n\n        Examples:\n            Simple averaging implementation:\n\n            &gt;&gt;&gt; def run(self, modelpool: BaseModelPool):\n            ...     models = [model for model in modelpool]\n            ...     averaged_params = {}\n            ...     for name in models[0].state_dict():\n            ...         averaged_params[name] = torch.stack([\n            ...             model.state_dict()[name] for model in models\n            ...         ]).mean(dim=0)\n            ...     fused_model = copy.deepcopy(models[0])\n            ...     fused_model.load_state_dict(averaged_params)\n            ...     return fused_model\n\n            Task arithmetic implementation:\n\n            &gt;&gt;&gt; def run(self, modelpool: BaseModelPool):\n            ...     pretrained = modelpool.get_model('pretrained')\n            ...     task_vectors = []\n            ...     for model_name in modelpool.model_names:\n            ...         if model_name != 'pretrained':\n            ...             task_vector = self.compute_task_vector(\n            ...                 modelpool.get_model(model_name), pretrained\n            ...             )\n            ...             task_vectors.append(task_vector)\n            ...     return self.merge_task_vectors(pretrained, task_vectors)\n\n        Note:\n            - The modelpool iteration order may affect results for non-commutative operations\n            - Ensure model compatibility (architecture, parameter shapes) before fusion\n            - Consider memory constraints when loading multiple large models\n            - Use appropriate device placement for GPU/CPU computation\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/fusion_bench.method/#fusion_bench.method.BaseAlgorithm.on_run_end","title":"<code>on_run_end()</code>","text":"<p>Lifecycle hook called at the end of algorithm execution.</p> <p>This method is invoked after the main <code>run</code> method completes, providing an opportunity for subclasses to perform cleanup and finalization tasks such as:</p> <ul> <li>Logging execution statistics or results</li> <li>Cleaning up temporary resources</li> <li>Saving intermediate results or metrics</li> <li>Releasing computational resources</li> </ul> <p>The method is called regardless of whether the <code>run</code> method succeeded or failed, making it suitable for cleanup operations that should always occur.</p> <p>The default implementation does nothing, allowing subclasses to override as needed for their specific requirements.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyAlgorithm(BaseAlgorithm):\n...     def on_run_end(self):\n...         super().on_run_end()\n...         elapsed = time.time() - self.start_time\n...         print(f\"Fusion completed in {elapsed:.2f}s\")\n</code></pre> Source code in <code>fusion_bench/method/base_algorithm.py</code> <pre><code>def on_run_end(self):\n    \"\"\"\n    Lifecycle hook called at the end of algorithm execution.\n\n    This method is invoked after the main `run` method completes, providing\n    an opportunity for subclasses to perform cleanup and finalization tasks such as:\n\n    - Logging execution statistics or results\n    - Cleaning up temporary resources\n    - Saving intermediate results or metrics\n    - Releasing computational resources\n\n    The method is called regardless of whether the `run` method succeeded or failed,\n    making it suitable for cleanup operations that should always occur.\n\n    The default implementation does nothing, allowing subclasses to override\n    as needed for their specific requirements.\n\n    Examples:\n        &gt;&gt;&gt; class MyAlgorithm(BaseAlgorithm):\n        ...     def on_run_end(self):\n        ...         super().on_run_end()\n        ...         elapsed = time.time() - self.start_time\n        ...         print(f\"Fusion completed in {elapsed:.2f}s\")\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/#fusion_bench.method.BaseAlgorithm.on_run_start","title":"<code>on_run_start()</code>","text":"<p>Lifecycle hook called at the beginning of algorithm execution.</p> <p>This method is invoked before the main <code>run</code> method executes, providing an opportunity for subclasses to perform initialization tasks such as:</p> <ul> <li>Setting up logging or monitoring</li> <li>Initializing algorithm-specific state</li> <li>Validating prerequisites</li> <li>Preparing computational resources</li> </ul> <p>The default implementation does nothing, allowing subclasses to override as needed for their specific requirements.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyAlgorithm(BaseAlgorithm):\n...     def on_run_start(self):\n...         super().on_run_start()\n...         print(\"Starting model fusion...\")\n...         self.start_time = time.time()\n</code></pre> Source code in <code>fusion_bench/method/base_algorithm.py</code> <pre><code>def on_run_start(self):\n    \"\"\"\n    Lifecycle hook called at the beginning of algorithm execution.\n\n    This method is invoked before the main `run` method executes, providing\n    an opportunity for subclasses to perform initialization tasks such as:\n\n    - Setting up logging or monitoring\n    - Initializing algorithm-specific state\n    - Validating prerequisites\n    - Preparing computational resources\n\n    The default implementation does nothing, allowing subclasses to override\n    as needed for their specific requirements.\n\n    Examples:\n        &gt;&gt;&gt; class MyAlgorithm(BaseAlgorithm):\n        ...     def on_run_start(self):\n        ...         super().on_run_start()\n        ...         print(\"Starting model fusion...\")\n        ...         self.start_time = time.time()\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/#fusion_bench.method.BaseAlgorithm.run","title":"<code>run(modelpool)</code>  <code>abstractmethod</code>","text":"<p>Execute the model fusion algorithm on the provided model pool.</p> <p>This is the core method that must be implemented by all subclasses to define their specific fusion strategy. The method takes a pool of models and produces a fused result according to the algorithm's logic.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool</code>)           \u2013            <p>A collection of models to be fused. The modelpool provides access to individual models and their metadata, allowing the algorithm to iterate over models, access their parameters, and perform fusion operations.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>The type of return value depends on the specific algorithm implementation. Common return types include:</p> <ul> <li>A single fused model (torch.nn.Module)</li> <li>A dictionary of fused models for multi-task scenarios</li> <li>Fusion results with additional metadata</li> <li>Custom data structures specific to the algorithm</li> </ul> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If called on the base class without implementation.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the modelpool is invalid or incompatible with the algorithm.</p> </li> <li> <code>RuntimeError</code>             \u2013            <p>If fusion fails due to model incompatibilities or other issues.</p> </li> </ul> <p>Examples:</p> <p>Simple averaging implementation:</p> <pre><code>&gt;&gt;&gt; def run(self, modelpool: BaseModelPool):\n...     models = [model for model in modelpool]\n...     averaged_params = {}\n...     for name in models[0].state_dict():\n...         averaged_params[name] = torch.stack([\n...             model.state_dict()[name] for model in models\n...         ]).mean(dim=0)\n...     fused_model = copy.deepcopy(models[0])\n...     fused_model.load_state_dict(averaged_params)\n...     return fused_model\n</code></pre> <p>Task arithmetic implementation:</p> <pre><code>&gt;&gt;&gt; def run(self, modelpool: BaseModelPool):\n...     pretrained = modelpool.get_model('pretrained')\n...     task_vectors = []\n...     for model_name in modelpool.model_names:\n...         if model_name != 'pretrained':\n...             task_vector = self.compute_task_vector(\n...                 modelpool.get_model(model_name), pretrained\n...             )\n...             task_vectors.append(task_vector)\n...     return self.merge_task_vectors(pretrained, task_vectors)\n</code></pre> Note <ul> <li>The modelpool iteration order may affect results for non-commutative operations</li> <li>Ensure model compatibility (architecture, parameter shapes) before fusion</li> <li>Consider memory constraints when loading multiple large models</li> <li>Use appropriate device placement for GPU/CPU computation</li> </ul> Source code in <code>fusion_bench/method/base_algorithm.py</code> <pre><code>@abstractmethod\ndef run(self, modelpool: BaseModelPool):\n    \"\"\"\n    Execute the model fusion algorithm on the provided model pool.\n\n    This is the core method that must be implemented by all subclasses to define\n    their specific fusion strategy. The method takes a pool of models and produces\n    a fused result according to the algorithm's logic.\n\n    Args:\n        modelpool (BaseModelPool): A collection of models to be fused. The modelpool\n            provides access to individual models and their metadata, allowing the\n            algorithm to iterate over models, access their parameters, and perform\n            fusion operations.\n\n    Returns:\n        The type of return value depends on the specific algorithm implementation.\n            Common return types include:\n\n            - A single fused model (torch.nn.Module)\n            - A dictionary of fused models for multi-task scenarios\n            - Fusion results with additional metadata\n            - Custom data structures specific to the algorithm\n\n    Raises:\n        NotImplementedError: If called on the base class without implementation.\n        ValueError: If the modelpool is invalid or incompatible with the algorithm.\n        RuntimeError: If fusion fails due to model incompatibilities or other issues.\n\n    Examples:\n        Simple averaging implementation:\n\n        &gt;&gt;&gt; def run(self, modelpool: BaseModelPool):\n        ...     models = [model for model in modelpool]\n        ...     averaged_params = {}\n        ...     for name in models[0].state_dict():\n        ...         averaged_params[name] = torch.stack([\n        ...             model.state_dict()[name] for model in models\n        ...         ]).mean(dim=0)\n        ...     fused_model = copy.deepcopy(models[0])\n        ...     fused_model.load_state_dict(averaged_params)\n        ...     return fused_model\n\n        Task arithmetic implementation:\n\n        &gt;&gt;&gt; def run(self, modelpool: BaseModelPool):\n        ...     pretrained = modelpool.get_model('pretrained')\n        ...     task_vectors = []\n        ...     for model_name in modelpool.model_names:\n        ...         if model_name != 'pretrained':\n        ...             task_vector = self.compute_task_vector(\n        ...                 modelpool.get_model(model_name), pretrained\n        ...             )\n        ...             task_vectors.append(task_vector)\n        ...     return self.merge_task_vectors(pretrained, task_vectors)\n\n    Note:\n        - The modelpool iteration order may affect results for non-commutative operations\n        - Ensure model compatibility (architecture, parameter shapes) before fusion\n        - Consider memory constraints when loading multiple large models\n        - Use appropriate device placement for GPU/CPU computation\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/#fusion_bench.BaseModelFusionAlgorithm","title":"<code>BaseModelFusionAlgorithm = BaseAlgorithm</code>  <code>module-attribute</code>","text":"<p>Alias for BaseAlgorithm class.</p> <p>This alias is provided for backward compatibility and semantic clarity. Some users may prefer the more explicit name 'BaseModelFusionAlgorithm' to emphasize that this class is specifically designed for model fusion tasks, while others may prefer the shorter 'BaseAlgorithm' name.</p> <p>Both names refer to the exact same class and can be used interchangeably.</p> <p>Examples:</p> <p>Using the original name:</p> <pre><code>&gt;&gt;&gt; class MyAlgorithm(BaseAlgorithm):\n...     def run(self, modelpool): pass\n</code></pre> <p>Using the alias:</p> <pre><code>&gt;&gt;&gt; class MyAlgorithm(BaseModelFusionAlgorithm):\n...     def run(self, modelpool): pass\n</code></pre> Note <p>The alias is maintained for compatibility but BaseAlgorithm is the preferred name for new implementations.</p>"},{"location":"api/fusion_bench.method/#implemented-algorithms","title":"Implemented Algorithms","text":"<p>In FusionBench, we categorize deep model fusion methods into three categories: Model Ensemble, Model Merging, and Model Mixing. Learn more at here</p> <ul> <li> Model Ensemble Algorithms</li> <li> Model Merging Algorithms</li> <li> Model Mixing Algorithms</li> <li> Model Compression Algorithms</li> <li> Model Training/Fine-tuning Algorithms</li> <li> Utility Classes</li> </ul>"},{"location":"api/fusion_bench.method/compression/","title":"Model Compression","text":""},{"location":"api/fusion_bench.method/compression/#task-vector-compression","title":"Task Vector Compression","text":""},{"location":"api/fusion_bench.method/compression/#bitdelta","title":"BitDelta","text":""},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.BitDeltaAlgorithm","title":"<code>BitDeltaAlgorithm</code>","text":"<p>               Bases: <code>LightningFabricMixin</code>, <code>SimpleProfilerMixin</code>, <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/bitdelta/bitdelta.py</code> <pre><code>@auto_register_config\nclass BitDeltaAlgorithm(\n    LightningFabricMixin,\n    SimpleProfilerMixin,\n    BaseAlgorithm,\n):\n    def __init__(\n        self,\n        save_dir: str,\n        save_full_model: bool = False,\n        lr: float = 1e-4,\n        batch_size: int = 4,\n        num_steps: int = 100,\n        dataset_name: str = \"c4\",\n        subset: str = \"en\",\n        split: str = \"train\",\n        max_length: int = 128,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n    def run(self, modelpool: CausalLMPool):\n        if self.save_dir is None:\n            log.info(\n                f\"save_dir not set, using log_dir instead. log_dir: {self.log_dir}\"\n            )\n            self.save_dir = self.log_dir\n\n        with self.profile(\"model loading\"):\n            tokenizer = modelpool.load_tokenizer()\n            base_model = modelpool.load_pretrained_model()\n            finetuned_model = modelpool.load_model(modelpool.model_names[0])\n            finetuned_compressed_model = modelpool.load_model(modelpool.model_names[0])\n\n        with self.profile(\"model compression\"):\n            print(f\"compressing diff...\")\n            compress_diff(base_model, finetuned_model, finetuned_compressed_model)\n\n        # save untrained delta\n        save_diff(\n            finetuned_compressed_model, os.path.join(self.save_dir, \"diff_untrained.pt\")\n        )\n\n        optimizer = torch.optim.AdamW(\n            finetuned_compressed_model.parameters(), lr=self.lr\n        )\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, self.num_steps\n        )\n\n        train_num_samples = self.batch_size * self.num_steps\n        train_dataset = get_dataset(\n            self.dataset_name,\n            self.subset,\n            \"train\",\n            size=train_num_samples,\n        )\n        train_dataloader = get_dataloader(\n            train_dataset,\n            tokenizer,\n            self.batch_size,\n            num_workers=4,\n            max_length=self.max_length,\n        )\n\n        bar = tqdm(train_dataloader)\n\n        train_loss_list = []\n\n        # Train loop\n        for step, batch in enumerate(bar):\n            batch1 = {k: v.to(finetuned_model.device) for k, v in batch.items()}\n            with torch.inference_mode():\n                finetuned_outputs = finetuned_model(**batch1)\n\n            batch2 = {\n                k: v.to(finetuned_compressed_model.device) for k, v in batch.items()\n            }\n            finetuned_compressed_outputs = finetuned_compressed_model(**batch2)\n\n            loss = F.mse_loss(\n                finetuned_outputs.logits.clone().to(\n                    finetuned_compressed_outputs.logits.device\n                ),\n                finetuned_compressed_outputs.logits,\n            )\n\n            train_loss_list.append(loss.item())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            bar.set_description(f\"train loss: {loss.item()}\")\n\n        # save trained delta\n        save_diff(finetuned_compressed_model, os.path.join(self.save_dir, \"diff.pt\"))\n\n        if self.save_full_model:\n            print(\"saving uncalibrated model\")\n            save_full_model(\n                base_model,\n                tokenizer,\n                os.path.join(self.save_dir, \"diff_untrained.pt\"),\n                os.path.join(self.save_dir, \"uncalibrated_model\"),\n                device=\"cpu\",\n            )\n            print(\"saving calibrated model\")\n            save_full_model(\n                base_model,\n                tokenizer,\n                os.path.join(self.save_dir, \"diff.pt\"),\n                os.path.join(self.save_dir, \"calibrated_model\"),\n                device=\"cpu\",\n            )\n\n        del base_model, finetuned_model, finetuned_compressed_model\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#parameter-pruning","title":"Parameter Pruning","text":""},{"location":"api/fusion_bench.method/compression/#random-pruning","title":"Random Pruning","text":""},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.RandomPruningForLlama","title":"<code>RandomPruningForLlama</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code></p> <p>A class to perform random pruning for Llama models.</p> <p>Attributes:</p> <ul> <li> <code>prune_type</code>               (<code>PruningType</code>)           \u2013            <p>The type of pruning to be performed.</p> </li> <li> <code>sparsity_ratio</code>               (<code>float</code>)           \u2013            <p>The ratio of weights to be pruned.</p> </li> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>The number of weights to be pruned in each group (for semistructured pruning).</p> </li> <li> <code>m</code>               (<code>int</code>)           \u2013            <p>The total number of weights in each group (for semistructured pruning).</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/llama_random_prune.py</code> <pre><code>class RandomPruningForLlama(BaseAlgorithm, SimpleProfilerMixin):\n    \"\"\"\n    A class to perform random pruning for Llama models.\n\n    Attributes:\n        prune_type (PruningType): The type of pruning to be performed.\n        sparsity_ratio (float): The ratio of weights to be pruned.\n        n (int): The number of weights to be pruned in each group (for semistructured pruning).\n        m (int): The total number of weights in each group (for semistructured pruning).\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"prune_type\": \"prune_type\",\n        \"sparsity_ratio\": \"sparsity_ratio\",\n        \"n\": \"n\",\n        \"m\": \"m\",\n    }\n\n    def __init__(\n        self,\n        *,\n        prune_type: PruningType,\n        sparsity_ratio: float,\n        n: int,\n        m: int,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the RandomPruningForLlama class.\n\n        Args:\n            prune_type (PruningType): The type of pruning to be performed.\n            sparsity_ratio (float): The ratio of weights to be pruned.\n            n (int): The number of weights to be pruned in each group (for semistructured pruning).\n            m (int): The total number of weights in each group (for semistructured pruning).\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        self.prune_type = prune_type\n        self.sparsity_ratio = sparsity_ratio\n        self.n = n\n        self.m = m\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(self, modelpool: CausalLMPool):\n        \"\"\"\n        Run the pruning algorithm on the first model from the given model pool.\n\n        Args:\n            modelpool (CausalLMPool): The pool of models to be pruned.\n\n        Returns:\n            The pruned model.\n        \"\"\"\n        # load pre-trained model or the first model in the pool\n        base_model = modelpool.load_pretrained_or_first_model()\n\n        if self.prune_type == PruningType.UNSTRUCTURED:\n            unstructured_magnitude_prune_(base_model, self.sparsity_ratio)\n        elif self.prune_type == PruningType.SEMISTRUCTURED:\n            semistructured_magnitude_prune_(base_model, self.n, self.m)\n        else:\n            raise ValueError(\n                f\"Invalid pruning type: {self.prune_type}\"\n                \"Choose from 'unstructured' or 'semistructured'\"\n            )\n\n        return base_model\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.RandomPruningForLlama.__init__","title":"<code>__init__(*, prune_type, sparsity_ratio, n, m, **kwargs)</code>","text":"<p>Initialize the RandomPruningForLlama class.</p> <p>Parameters:</p> <ul> <li> <code>prune_type</code>               (<code>PruningType</code>)           \u2013            <p>The type of pruning to be performed.</p> </li> <li> <code>sparsity_ratio</code>               (<code>float</code>)           \u2013            <p>The ratio of weights to be pruned.</p> </li> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>The number of weights to be pruned in each group (for semistructured pruning).</p> </li> <li> <code>m</code>               (<code>int</code>)           \u2013            <p>The total number of weights in each group (for semistructured pruning).</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/llama_random_prune.py</code> <pre><code>def __init__(\n    self,\n    *,\n    prune_type: PruningType,\n    sparsity_ratio: float,\n    n: int,\n    m: int,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the RandomPruningForLlama class.\n\n    Args:\n        prune_type (PruningType): The type of pruning to be performed.\n        sparsity_ratio (float): The ratio of weights to be pruned.\n        n (int): The number of weights to be pruned in each group (for semistructured pruning).\n        m (int): The total number of weights in each group (for semistructured pruning).\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    self.prune_type = prune_type\n    self.sparsity_ratio = sparsity_ratio\n    self.n = n\n    self.m = m\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.RandomPruningForLlama.run","title":"<code>run(modelpool)</code>","text":"<p>Run the pruning algorithm on the first model from the given model pool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>CausalLMPool</code>)           \u2013            <p>The pool of models to be pruned.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>The pruned model.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/llama_random_prune.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: CausalLMPool):\n    \"\"\"\n    Run the pruning algorithm on the first model from the given model pool.\n\n    Args:\n        modelpool (CausalLMPool): The pool of models to be pruned.\n\n    Returns:\n        The pruned model.\n    \"\"\"\n    # load pre-trained model or the first model in the pool\n    base_model = modelpool.load_pretrained_or_first_model()\n\n    if self.prune_type == PruningType.UNSTRUCTURED:\n        unstructured_magnitude_prune_(base_model, self.sparsity_ratio)\n    elif self.prune_type == PruningType.SEMISTRUCTURED:\n        semistructured_magnitude_prune_(base_model, self.n, self.m)\n    else:\n        raise ValueError(\n            f\"Invalid pruning type: {self.prune_type}\"\n            \"Choose from 'unstructured' or 'semistructured'\"\n        )\n\n    return base_model\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#magnitude-based-pruning","title":"Magnitude-based Pruning","text":""},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.MagnitudeDiffPruningAlgorithm","title":"<code>MagnitudeDiffPruningAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code></p> <p>Implements magnitude-based pruning on the difference between pretrained and fine-tuned model parameters.</p> <p>This class supports pruning the difference between the pretrained and fine-tuned model parameters based on their magnitude. It allows specifying the ratio of weights to prune and the names of parameters to extract for pruning.</p> <p>Methods:</p> <ul> <li> <code>run</code>             \u2013              <p>BaseModelPool) -&gt; nn.Module: Executes the pruning process on the model pool and returns the pruned model.</p> </li> <li> <code>magnitude_prune</code>             \u2013              <p>nn.Module, finetuned_model: nn.Module, in_place: bool = True) -&gt; nn.Module: Prunes the difference between the pretrained and fine-tuned model parameters.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/magnitude_diff_pruning.py</code> <pre><code>class MagnitudeDiffPruningAlgorithm(\n    BaseAlgorithm,\n    SimpleProfilerMixin,\n):\n    \"\"\"\n    Implements magnitude-based pruning on the difference between pretrained and fine-tuned model parameters.\n\n    This class supports pruning the difference between the pretrained and fine-tuned model parameters\n    based on their magnitude. It allows specifying the ratio of weights to prune and the names of\n    parameters to extract for pruning.\n\n    Methods:\n        run(modelpool: BaseModelPool) -&gt; nn.Module:\n            Executes the pruning process on the model pool and returns the pruned model.\n        magnitude_prune(pretrained_model: nn.Module, finetuned_model: nn.Module, in_place: bool = True) -&gt; nn.Module:\n            Prunes the difference between the pretrained and fine-tuned model parameters.\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"prune_ratio\": \"prune_ratio\",\n        \"extract_names\": \"extract_names\",\n    }\n\n    def __init__(\n        self,\n        prune_ratio: float,\n        rescale: Optional[Union[bool, float]] = None,\n        extract_names: List[str] = None,\n        prune_type: Literal[\"minor\", \"major\"] = \"minor\",\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the MagnitudeDiffPruningAlgorithm with the given configuration.\n\n        Args:\n            prune_ratio (float): The ratio of weights to prune.\n            extract_names (List[str], optional): List of regular expressions to match the parameter names for pruning. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        self.prune_ratio = prune_ratio\n        self.rescale = rescale\n        self.extract_names = extract_names\n        self.prune_type = prune_type\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool):\n        \"\"\"\n        Execute the pruning process on the model pool.\n\n        This method loads the pretrained and fine-tuned models from the model pool,\n        prunes the difference between their parameters, and returns the pruned model.\n\n        Args:\n            modelpool (BaseModelPool): The model pool containing the models to prune.\n\n        Returns:\n            nn.Module: The pruned model.\n        \"\"\"\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n\n        assert (\n            len(modelpool.model_names) == 1\n        ), \"Only one fine-tuned model is allowed in the model pool.\"\n        with self.profile(\"load pretrained model\"):\n            pretrained_model = modelpool.load_model(\"_pretrained_\")\n        with self.profile(\"load fine-tuned model\"):\n            finetuned_model = modelpool.load_model(modelpool.model_names[0])\n\n        with self.profile(\"prune model\"):\n            model = self.magnitude_prune(pretrained_model, finetuned_model)\n\n        self.print_profile_summary()\n        return model\n\n    @torch.no_grad()\n    def magnitude_prune(\n        self,\n        pretrained_model: nn.Module,\n        finetuned_model: nn.Module,\n        in_place: bool = True,\n    ):\n        \"\"\"\n        Prune the difference between the pretrained and fine-tuned model parameters.\n\n        This method calculates the difference between the pretrained and fine-tuned model parameters,\n        prunes the difference based on their magnitude, and updates the pretrained model parameters\n        with the pruned difference.\n\n        Args:\n            pretrained_model (nn.Module): The pretrained model.\n            finetuned_model (nn.Module): The fine-tuned model.\n            in_place (bool, optional): Whether to perform the pruning in place. Defaults to True.\n\n        Returns:\n            nn.Module: The pruned model.\n        \"\"\"\n        if in_place:\n            model = pretrained_model\n        else:\n            model = deepcopy(pretrained_model)\n\n        if self.extract_names is not None:\n            extract_names: List[str] = (\n                self.extract_names\n            )  # regular expressions for the names of the parameters\n        else:\n            # extract the weight matrix of each linear layer\n            extract_names = []\n            for name, module in model.named_modules():\n                if isinstance(module, nn.Linear):\n                    extract_names.append(f\"{name}.weight\")\n\n        ft_state_dict = finetuned_model.state_dict()\n        for name, param in tqdm(\n            model.named_parameters(),\n            \"Magnitude Pruning On Parameter Difference\",\n            total=len(tuple(model.named_parameters())),\n        ):\n            if not param.requires_grad:\n                continue\n\n            # Prune the diff parameter if its name matches\n            if _is_name_matched(name, extract_names):\n                w_diff = ft_state_dict[name] - param\n                w_diff = unstructured_magnitude_prune_(\n                    w_diff,\n                    (\n                        torch.abs\n                        if self.prune_type == \"minor\"\n                        else lambda x: -torch.abs(x)\n                    ),\n                    sparsity_ratio=self.prune_ratio,\n                )\n                if self.rescale is not None:\n                    rescale = (\n                        1 / self.prune_ratio if self.rescale == True else self.rescale\n                    )\n                    w_diff = w_diff * rescale\n                param.data = param + w_diff\n\n        return model\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.MagnitudeDiffPruningAlgorithm.__init__","title":"<code>__init__(prune_ratio, rescale=None, extract_names=None, prune_type='minor', **kwargs)</code>","text":"<p>Initialize the MagnitudeDiffPruningAlgorithm with the given configuration.</p> <p>Parameters:</p> <ul> <li> <code>prune_ratio</code>               (<code>float</code>)           \u2013            <p>The ratio of weights to prune.</p> </li> <li> <code>extract_names</code>               (<code>List[str]</code>, default:                   <code>None</code> )           \u2013            <p>List of regular expressions to match the parameter names for pruning. Defaults to None.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/magnitude_diff_pruning.py</code> <pre><code>def __init__(\n    self,\n    prune_ratio: float,\n    rescale: Optional[Union[bool, float]] = None,\n    extract_names: List[str] = None,\n    prune_type: Literal[\"minor\", \"major\"] = \"minor\",\n    **kwargs,\n):\n    \"\"\"\n    Initialize the MagnitudeDiffPruningAlgorithm with the given configuration.\n\n    Args:\n        prune_ratio (float): The ratio of weights to prune.\n        extract_names (List[str], optional): List of regular expressions to match the parameter names for pruning. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    self.prune_ratio = prune_ratio\n    self.rescale = rescale\n    self.extract_names = extract_names\n    self.prune_type = prune_type\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.MagnitudeDiffPruningAlgorithm.magnitude_prune","title":"<code>magnitude_prune(pretrained_model, finetuned_model, in_place=True)</code>","text":"<p>Prune the difference between the pretrained and fine-tuned model parameters.</p> <p>This method calculates the difference between the pretrained and fine-tuned model parameters, prunes the difference based on their magnitude, and updates the pretrained model parameters with the pruned difference.</p> <p>Parameters:</p> <ul> <li> <code>pretrained_model</code>               (<code>Module</code>)           \u2013            <p>The pretrained model.</p> </li> <li> <code>finetuned_model</code>               (<code>Module</code>)           \u2013            <p>The fine-tuned model.</p> </li> <li> <code>in_place</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to perform the pruning in place. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The pruned model.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/magnitude_diff_pruning.py</code> <pre><code>@torch.no_grad()\ndef magnitude_prune(\n    self,\n    pretrained_model: nn.Module,\n    finetuned_model: nn.Module,\n    in_place: bool = True,\n):\n    \"\"\"\n    Prune the difference between the pretrained and fine-tuned model parameters.\n\n    This method calculates the difference between the pretrained and fine-tuned model parameters,\n    prunes the difference based on their magnitude, and updates the pretrained model parameters\n    with the pruned difference.\n\n    Args:\n        pretrained_model (nn.Module): The pretrained model.\n        finetuned_model (nn.Module): The fine-tuned model.\n        in_place (bool, optional): Whether to perform the pruning in place. Defaults to True.\n\n    Returns:\n        nn.Module: The pruned model.\n    \"\"\"\n    if in_place:\n        model = pretrained_model\n    else:\n        model = deepcopy(pretrained_model)\n\n    if self.extract_names is not None:\n        extract_names: List[str] = (\n            self.extract_names\n        )  # regular expressions for the names of the parameters\n    else:\n        # extract the weight matrix of each linear layer\n        extract_names = []\n        for name, module in model.named_modules():\n            if isinstance(module, nn.Linear):\n                extract_names.append(f\"{name}.weight\")\n\n    ft_state_dict = finetuned_model.state_dict()\n    for name, param in tqdm(\n        model.named_parameters(),\n        \"Magnitude Pruning On Parameter Difference\",\n        total=len(tuple(model.named_parameters())),\n    ):\n        if not param.requires_grad:\n            continue\n\n        # Prune the diff parameter if its name matches\n        if _is_name_matched(name, extract_names):\n            w_diff = ft_state_dict[name] - param\n            w_diff = unstructured_magnitude_prune_(\n                w_diff,\n                (\n                    torch.abs\n                    if self.prune_type == \"minor\"\n                    else lambda x: -torch.abs(x)\n                ),\n                sparsity_ratio=self.prune_ratio,\n            )\n            if self.rescale is not None:\n                rescale = (\n                    1 / self.prune_ratio if self.rescale == True else self.rescale\n                )\n                w_diff = w_diff * rescale\n            param.data = param + w_diff\n\n    return model\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.MagnitudeDiffPruningAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Execute the pruning process on the model pool.</p> <p>This method loads the pretrained and fine-tuned models from the model pool, prunes the difference between their parameters, and returns the pruned model.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool</code>)           \u2013            <p>The model pool containing the models to prune.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The pruned model.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/magnitude_diff_pruning.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool):\n    \"\"\"\n    Execute the pruning process on the model pool.\n\n    This method loads the pretrained and fine-tuned models from the model pool,\n    prunes the difference between their parameters, and returns the pruned model.\n\n    Args:\n        modelpool (BaseModelPool): The model pool containing the models to prune.\n\n    Returns:\n        nn.Module: The pruned model.\n    \"\"\"\n    if not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(modelpool)\n\n    assert (\n        len(modelpool.model_names) == 1\n    ), \"Only one fine-tuned model is allowed in the model pool.\"\n    with self.profile(\"load pretrained model\"):\n        pretrained_model = modelpool.load_model(\"_pretrained_\")\n    with self.profile(\"load fine-tuned model\"):\n        finetuned_model = modelpool.load_model(modelpool.model_names[0])\n\n    with self.profile(\"prune model\"):\n        model = self.magnitude_prune(pretrained_model, finetuned_model)\n\n    self.print_profile_summary()\n    return model\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.MagnitudePruningForLlama","title":"<code>MagnitudePruningForLlama</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code></p> <p>Implements magnitude-based pruning for LLama models.</p> <p>This class supports both unstructured and semistructured pruning methods. It loads a pre-trained model or the first model in the pool and applies the specified pruning technique.</p> <p>Methods:</p> <ul> <li> <code>run</code>             \u2013              <p>LLamaForCausalLMPool) -&gt; nn.Module: Executes the pruning process on the model pool and returns the pruned model.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/llama_magnitude_prune.py</code> <pre><code>class MagnitudePruningForLlama(BaseAlgorithm, SimpleProfilerMixin):\n    \"\"\"\n    Implements magnitude-based pruning for LLama models.\n\n    This class supports both unstructured and semistructured pruning methods.\n    It loads a pre-trained model or the first model in the pool and applies the specified pruning technique.\n\n    Methods:\n        run(modelpool: LLamaForCausalLMPool) -&gt; nn.Module:\n            Executes the pruning process on the model pool and returns the pruned model.\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"prune_type\": \"prune_type\",\n        \"device\": \"device\",\n        \"dtype\": \"dtype\",\n        \"sparsity_ratio\": \"sparsity_ratio\",\n        \"n\": \"n\",\n        \"m\": \"m\",\n    }\n\n    def __init__(\n        self,\n        *,\n        prune_type: Literal[\"unstructured\", \"semistructured\"],\n        device: str,\n        dtype: Optional[str],\n        sparsity_ratio: float,\n        n: int,\n        m: int,\n        **kwargs,\n    ):\n        self.prune_type = prune_type\n        self.device = device\n        self.dtype = dtype\n        self.sparsity_ratio = sparsity_ratio\n        self.n = n\n        self.m = m\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(self, modelpool: CausalLMPool) -&gt; LlamaForCausalLM:\n        \"\"\"\n        Execute the pruning process on the first model from the given model pool.\n\n        Args:\n            modelpool (CausalLMPool): The model pool containing the models to prune.\n\n        Returns:\n            nn.Module: The pruned model.\n        \"\"\"\n        config = self.config\n\n        # load pre-trained model or the first model in the pool\n        base_model = modelpool.load_pretrained_or_first_model()\n\n        dtype = parse_dtype(config.dtype)\n        device = torch.device(config.device)\n\n        if config.prune_type == \"unstructured\":\n            unstructured_magnitude_prune_(\n                base_model, config.sparsity_ratio, dtype=dtype, device=device\n            )\n        elif config.prune_type == \"semistructured\":\n            semistructured_magnitude_prune_(\n                base_model, config.n, config.m, dtype=dtype, device=device\n            )\n        else:\n            raise ValueError(\n                f\"Invalid pruning type: {config.prune_type}\"\n                \"Choose from 'unstructured' or 'semistructured'\"\n            )\n\n        return base_model\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.MagnitudePruningForLlama.run","title":"<code>run(modelpool)</code>","text":"<p>Execute the pruning process on the first model from the given model pool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>CausalLMPool</code>)           \u2013            <p>The model pool containing the models to prune.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LlamaForCausalLM</code>           \u2013            <p>nn.Module: The pruned model.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/llama_magnitude_prune.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: CausalLMPool) -&gt; LlamaForCausalLM:\n    \"\"\"\n    Execute the pruning process on the first model from the given model pool.\n\n    Args:\n        modelpool (CausalLMPool): The model pool containing the models to prune.\n\n    Returns:\n        nn.Module: The pruned model.\n    \"\"\"\n    config = self.config\n\n    # load pre-trained model or the first model in the pool\n    base_model = modelpool.load_pretrained_or_first_model()\n\n    dtype = parse_dtype(config.dtype)\n    device = torch.device(config.device)\n\n    if config.prune_type == \"unstructured\":\n        unstructured_magnitude_prune_(\n            base_model, config.sparsity_ratio, dtype=dtype, device=device\n        )\n    elif config.prune_type == \"semistructured\":\n        semistructured_magnitude_prune_(\n            base_model, config.n, config.m, dtype=dtype, device=device\n        )\n    else:\n        raise ValueError(\n            f\"Invalid pruning type: {config.prune_type}\"\n            \"Choose from 'unstructured' or 'semistructured'\"\n        )\n\n    return base_model\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#wanda","title":"Wanda","text":""},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.WandaPruningForLlama","title":"<code>WandaPruningForLlama</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code></p> <p>Class for Wanda pruning for Llama models.</p> Source code in <code>fusion_bench/method/pruning/llama_wanda_prune.py</code> <pre><code>class WandaPruningForLlama(BaseAlgorithm, SimpleProfilerMixin):\n    \"\"\"\n    Class for Wanda pruning for Llama models.\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"nsamples\": \"nsamples\",\n        \"seed\": \"seed\",\n        \"use_variant\": \"use_variant\",\n        \"prune_type\": \"prune_type\",\n        \"device\": \"device\",\n        \"dtype\": \"dtype\",\n        \"sparsity_ratio\": \"sparsity_ratio\",\n        \"n\": \"n\",\n        \"m\": \"m\",\n    }\n\n    def __init__(\n        self,\n        *,\n        nsamples: int,\n        seed: int,\n        use_variant: bool,\n        prune_type: PruningType,\n        device: str,\n        dtype: str,\n        sparsity_ratio: float,\n        n: int,\n        m: int,\n        model_save_path: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the WandaPruningForLlama class.\n\n        Args:\n            nsamples (int): Number of samples for calibration.\n            seed (int): Random seed.\n            use_variant (bool): Whether to use a variant of the pruning method.\n            prune_type (PruningType): Type of pruning to perform.\n            device (str): Device to use for computation.\n            dtype (str): Data type to use for computation.\n            sparsity_ratio (float): Sparsity ratio for pruning.\n            n (int): Number of elements to keep in semi-structured pruning.\n            m (int): Number of elements in a group for semi-structured pruning.\n            model_save_path (Optional[str]): Path to save the pruned model.\n            **kwargs: Additional arguments.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.nsamples = nsamples\n        self.seed = seed\n        self.use_variant = use_variant\n        self.prune_type = prune_type\n        self.device = device\n        self.dtype = dtype\n        self.sparsity_ratio = sparsity_ratio\n        self.n = n\n        self.m = m\n        self.model_save_path = model_save_path\n\n    def run(self, modelpool: CausalLMPool):\n        \"\"\"\n        Run the pruning algorithm on the model pool.\n\n        Args:\n            modelpool (CausalLMPool): Pool of causal language models.\n\n        Returns:\n            LlamaForCausalLM: Pruned model.\n        \"\"\"\n\n        # load pre-trained model or the first model in the pool\n        with self.profile(\"load_model\"):\n            model = modelpool.load_pretrained_or_first_model()\n            model.seqlen = model.config.max_position_embeddings\n            tokenizer = modelpool.load_tokenizer(use_fast=False)\n\n        if not isinstance(model, (LlamaForCausalLM,)):\n            log.warning(f\"Model type {type(model)} may not supported.\")\n\n        inps, outs, attention_mask, position_ids = self.prepare_calibration_data(\n            model, tokenizer\n        )\n\n        self.prune_using_calibration_data_(\n            model,\n            inps=inps,\n            outs=outs,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n        )\n\n        if self.model_save_path is not None:\n            with timeit_context(f\"Saving pruned model to {self.model_save_path}\"):\n                tokenizer.save_pretrained(self.model_save_path)\n                model.save_pretrained(self.model_save_path)\n        return model\n\n    def _prepare_calibration_data(self, model, tokenizer):\n        \"\"\"\n        Prepare calibration data for pruning.\n\n        Args:\n            model (LlamaForCausalLM): Model to be pruned.\n            tokenizer: Tokenizer for the model.\n\n        Returns:\n            Tuple: Calibration data (inputs, outputs, attention mask, position IDs).\n        \"\"\"\n        with timeit_context(\"loading calibration data\"):\n            dataloader, _ = get_loaders(\n                \"c4\",\n                nsamples=self.nsamples,\n                seed=self.seed,\n                seqlen=model.seqlen,\n                tokenizer=tokenizer,\n            )\n\n        with torch.no_grad():\n            # collect input to the first layer\n            inps, outs, attention_mask, position_ids = prepare_calibration_input(\n                model, dataloader, self.device\n            )\n        return inps, outs, attention_mask, position_ids\n\n    def prepare_calibration_data(self, model: LlamaForCausalLM, tokenizer):\n        \"\"\"\n        Prepare calibration data for pruning with caching.\n\n        Args:\n            model (LlamaForCausalLM): Model to be pruned.\n            tokenizer: Tokenizer for the model.\n\n        Returns:\n            Tuple: Calibration data (inputs, outputs, attention mask, position IDs).\n        \"\"\"\n\n        @cache_to_disk(\n            f\"outputs/cache/{model.config.name_or_path.split('/')[-1]}/calibration_data.pkl\"\n        )\n        def _prepare_calibration_data(model, tokenizer):\n            return self._prepare_calibration_data(model, tokenizer)\n\n        return _prepare_calibration_data(model, tokenizer)\n\n    def prune_using_calibration_data_(\n        self,\n        model: LlamaForCausalLM,\n        *,\n        inps,\n        outs,\n        attention_mask,\n        position_ids,\n    ):\n        \"\"\"\n        Prune the model using calibration data.\n\n        Args:\n            model (LlamaForCausalLM): Model to be pruned.\n            inps: Calibration inputs.\n            outs: Calibration outputs.\n            attention_mask: Attention mask for calibration data.\n            position_ids: Position IDs for calibration data.\n        \"\"\"\n        layers = model.model.layers\n        for layer_idx, layer in tqdm(\n            enumerate(layers),\n            \"Pruning Layers\",\n            total=len(layers),\n            dynamic_ncols=True,\n        ):\n            if (\n                hasattr(model, \"hf_device_map\")\n                and f\"model.layers.{layer_idx}\" in model.hf_device_map\n            ):\n                # handle the case for llama-30B and llama-65B, when the device map has multiple GPUs;\n                dev = model.hf_device_map[f\"model.layers.{layer_idx}\"]\n                inps, outs, attention_mask, position_ids = (\n                    inps.to(dev),\n                    outs.to(dev),\n                    attention_mask.to(dev) if attention_mask is not None else None,\n                    position_ids.to(dev) if position_ids is not None else None,\n                )\n\n            # collect the importance scores\n            linear_layers = cast(\n                Dict[str, nn.Linear],\n                find_linear_layers(layer, layers=[nn.Linear]),\n            )\n\n            # register hooks to collect the importance scores\n            def get_hook_fn(linear: nn.Linear):\n                hook_fn = WandaHookFn(linear)\n                return hook_fn\n\n            hooks = {}\n            handles: List[torch.utils.hooks.RemovableHandle] = []\n            for name, linear in linear_layers.items():\n                hook_fn = get_hook_fn(linear)\n                hooks[name] = hook_fn\n                handles.append(linear.register_forward_hook(hook_fn))\n\n            with torch.no_grad():\n                for j in range(self.nsamples):\n                    outs[j] = layer(\n                        inps[j].unsqueeze(0),\n                        attention_mask=attention_mask,\n                        position_ids=position_ids,\n                    )[0]\n\n            # compute the importance scores and remove the hooks\n            metrics = {}\n            for name, hook in hooks.items():\n                metrics[name] = hook.compute()\n            for h in handles:\n                h.remove()\n\n            # prune the weights based on the importance scores\n            if self.prune_type == PruningType.UNSTRUCTURED:\n                for name, linear in linear_layers.items():\n                    log.info(f\"Pruning {name}\")\n                    unstructured_magnitude_prune_(\n                        linear.weight.data,\n                        metrics[name],\n                        sparsity_ratio=self.sparsity_ratio,\n                    )\n                    self.check_sparsity(linear.weight)\n            elif self.prune_type == PruningType.SEMISTRUCTURED:\n                for name, linear in linear_layers.items():\n                    log.info(f\"Pruning {name}\")\n                    semistructured_magnitude_prune_(\n                        linear.weight.data,\n                        metrics[name],\n                        n=self.n,\n                        m=self.m,\n                    )\n                    self.check_sparsity(linear.weight)\n            else:\n                raise ValueError(f\"Invalid pruning type: {self.prune_type}\")\n\n            # compute the input to the next layer\n            with torch.no_grad():\n                for j in range(self.nsamples):\n                    outs[j] = layer(\n                        inps[j].unsqueeze(0),\n                        attention_mask=attention_mask,\n                        position_ids=position_ids,\n                    )[0]\n            inps, outs = outs, inps\n\n    @torch.no_grad()\n    def check_sparsity(self, weight: Tensor, tol: float = 0.01):\n        \"\"\"\n        Check the sparsity of the weight tensor.\n\n        Args:\n            weight (Tensor): Weight tensor.\n            tol (float): Tolerance for sparsity check.\n\n        Raises:\n            ValueError: If the pruning type is invalid.\n        \"\"\"\n        if self.prune_type == PruningType.UNSTRUCTURED:\n            assert (compute_sparsity(weight) - self.sparsity_ratio).abs() &lt; tol\n        elif self.prune_type == PruningType.SEMISTRUCTURED:\n            assert (compute_sparsity(weight) - self.n / self.m).abs() &lt; tol\n        else:\n            raise ValueError(f\"Invalid pruning type: {self.prune_type}\")\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.WandaPruningForLlama.__init__","title":"<code>__init__(*, nsamples, seed, use_variant, prune_type, device, dtype, sparsity_ratio, n, m, model_save_path=None, **kwargs)</code>","text":"<p>Initialize the WandaPruningForLlama class.</p> <p>Parameters:</p> <ul> <li> <code>nsamples</code>               (<code>int</code>)           \u2013            <p>Number of samples for calibration.</p> </li> <li> <code>seed</code>               (<code>int</code>)           \u2013            <p>Random seed.</p> </li> <li> <code>use_variant</code>               (<code>bool</code>)           \u2013            <p>Whether to use a variant of the pruning method.</p> </li> <li> <code>prune_type</code>               (<code>PruningType</code>)           \u2013            <p>Type of pruning to perform.</p> </li> <li> <code>device</code>               (<code>str</code>)           \u2013            <p>Device to use for computation.</p> </li> <li> <code>dtype</code>               (<code>str</code>)           \u2013            <p>Data type to use for computation.</p> </li> <li> <code>sparsity_ratio</code>               (<code>float</code>)           \u2013            <p>Sparsity ratio for pruning.</p> </li> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>Number of elements to keep in semi-structured pruning.</p> </li> <li> <code>m</code>               (<code>int</code>)           \u2013            <p>Number of elements in a group for semi-structured pruning.</p> </li> <li> <code>model_save_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Path to save the pruned model.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/llama_wanda_prune.py</code> <pre><code>def __init__(\n    self,\n    *,\n    nsamples: int,\n    seed: int,\n    use_variant: bool,\n    prune_type: PruningType,\n    device: str,\n    dtype: str,\n    sparsity_ratio: float,\n    n: int,\n    m: int,\n    model_save_path: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the WandaPruningForLlama class.\n\n    Args:\n        nsamples (int): Number of samples for calibration.\n        seed (int): Random seed.\n        use_variant (bool): Whether to use a variant of the pruning method.\n        prune_type (PruningType): Type of pruning to perform.\n        device (str): Device to use for computation.\n        dtype (str): Data type to use for computation.\n        sparsity_ratio (float): Sparsity ratio for pruning.\n        n (int): Number of elements to keep in semi-structured pruning.\n        m (int): Number of elements in a group for semi-structured pruning.\n        model_save_path (Optional[str]): Path to save the pruned model.\n        **kwargs: Additional arguments.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.nsamples = nsamples\n    self.seed = seed\n    self.use_variant = use_variant\n    self.prune_type = prune_type\n    self.device = device\n    self.dtype = dtype\n    self.sparsity_ratio = sparsity_ratio\n    self.n = n\n    self.m = m\n    self.model_save_path = model_save_path\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.WandaPruningForLlama.check_sparsity","title":"<code>check_sparsity(weight, tol=0.01)</code>","text":"<p>Check the sparsity of the weight tensor.</p> <p>Parameters:</p> <ul> <li> <code>weight</code>               (<code>Tensor</code>)           \u2013            <p>Weight tensor.</p> </li> <li> <code>tol</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Tolerance for sparsity check.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the pruning type is invalid.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/llama_wanda_prune.py</code> <pre><code>@torch.no_grad()\ndef check_sparsity(self, weight: Tensor, tol: float = 0.01):\n    \"\"\"\n    Check the sparsity of the weight tensor.\n\n    Args:\n        weight (Tensor): Weight tensor.\n        tol (float): Tolerance for sparsity check.\n\n    Raises:\n        ValueError: If the pruning type is invalid.\n    \"\"\"\n    if self.prune_type == PruningType.UNSTRUCTURED:\n        assert (compute_sparsity(weight) - self.sparsity_ratio).abs() &lt; tol\n    elif self.prune_type == PruningType.SEMISTRUCTURED:\n        assert (compute_sparsity(weight) - self.n / self.m).abs() &lt; tol\n    else:\n        raise ValueError(f\"Invalid pruning type: {self.prune_type}\")\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.WandaPruningForLlama.prepare_calibration_data","title":"<code>prepare_calibration_data(model, tokenizer)</code>","text":"<p>Prepare calibration data for pruning with caching.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>LlamaForCausalLM</code>)           \u2013            <p>Model to be pruned.</p> </li> <li> <code>tokenizer</code>           \u2013            <p>Tokenizer for the model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple</code>          \u2013            <p>Calibration data (inputs, outputs, attention mask, position IDs).</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/llama_wanda_prune.py</code> <pre><code>def prepare_calibration_data(self, model: LlamaForCausalLM, tokenizer):\n    \"\"\"\n    Prepare calibration data for pruning with caching.\n\n    Args:\n        model (LlamaForCausalLM): Model to be pruned.\n        tokenizer: Tokenizer for the model.\n\n    Returns:\n        Tuple: Calibration data (inputs, outputs, attention mask, position IDs).\n    \"\"\"\n\n    @cache_to_disk(\n        f\"outputs/cache/{model.config.name_or_path.split('/')[-1]}/calibration_data.pkl\"\n    )\n    def _prepare_calibration_data(model, tokenizer):\n        return self._prepare_calibration_data(model, tokenizer)\n\n    return _prepare_calibration_data(model, tokenizer)\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.WandaPruningForLlama.prune_using_calibration_data_","title":"<code>prune_using_calibration_data_(model, *, inps, outs, attention_mask, position_ids)</code>","text":"<p>Prune the model using calibration data.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>LlamaForCausalLM</code>)           \u2013            <p>Model to be pruned.</p> </li> <li> <code>inps</code>           \u2013            <p>Calibration inputs.</p> </li> <li> <code>outs</code>           \u2013            <p>Calibration outputs.</p> </li> <li> <code>attention_mask</code>           \u2013            <p>Attention mask for calibration data.</p> </li> <li> <code>position_ids</code>           \u2013            <p>Position IDs for calibration data.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/llama_wanda_prune.py</code> <pre><code>def prune_using_calibration_data_(\n    self,\n    model: LlamaForCausalLM,\n    *,\n    inps,\n    outs,\n    attention_mask,\n    position_ids,\n):\n    \"\"\"\n    Prune the model using calibration data.\n\n    Args:\n        model (LlamaForCausalLM): Model to be pruned.\n        inps: Calibration inputs.\n        outs: Calibration outputs.\n        attention_mask: Attention mask for calibration data.\n        position_ids: Position IDs for calibration data.\n    \"\"\"\n    layers = model.model.layers\n    for layer_idx, layer in tqdm(\n        enumerate(layers),\n        \"Pruning Layers\",\n        total=len(layers),\n        dynamic_ncols=True,\n    ):\n        if (\n            hasattr(model, \"hf_device_map\")\n            and f\"model.layers.{layer_idx}\" in model.hf_device_map\n        ):\n            # handle the case for llama-30B and llama-65B, when the device map has multiple GPUs;\n            dev = model.hf_device_map[f\"model.layers.{layer_idx}\"]\n            inps, outs, attention_mask, position_ids = (\n                inps.to(dev),\n                outs.to(dev),\n                attention_mask.to(dev) if attention_mask is not None else None,\n                position_ids.to(dev) if position_ids is not None else None,\n            )\n\n        # collect the importance scores\n        linear_layers = cast(\n            Dict[str, nn.Linear],\n            find_linear_layers(layer, layers=[nn.Linear]),\n        )\n\n        # register hooks to collect the importance scores\n        def get_hook_fn(linear: nn.Linear):\n            hook_fn = WandaHookFn(linear)\n            return hook_fn\n\n        hooks = {}\n        handles: List[torch.utils.hooks.RemovableHandle] = []\n        for name, linear in linear_layers.items():\n            hook_fn = get_hook_fn(linear)\n            hooks[name] = hook_fn\n            handles.append(linear.register_forward_hook(hook_fn))\n\n        with torch.no_grad():\n            for j in range(self.nsamples):\n                outs[j] = layer(\n                    inps[j].unsqueeze(0),\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                )[0]\n\n        # compute the importance scores and remove the hooks\n        metrics = {}\n        for name, hook in hooks.items():\n            metrics[name] = hook.compute()\n        for h in handles:\n            h.remove()\n\n        # prune the weights based on the importance scores\n        if self.prune_type == PruningType.UNSTRUCTURED:\n            for name, linear in linear_layers.items():\n                log.info(f\"Pruning {name}\")\n                unstructured_magnitude_prune_(\n                    linear.weight.data,\n                    metrics[name],\n                    sparsity_ratio=self.sparsity_ratio,\n                )\n                self.check_sparsity(linear.weight)\n        elif self.prune_type == PruningType.SEMISTRUCTURED:\n            for name, linear in linear_layers.items():\n                log.info(f\"Pruning {name}\")\n                semistructured_magnitude_prune_(\n                    linear.weight.data,\n                    metrics[name],\n                    n=self.n,\n                    m=self.m,\n                )\n                self.check_sparsity(linear.weight)\n        else:\n            raise ValueError(f\"Invalid pruning type: {self.prune_type}\")\n\n        # compute the input to the next layer\n        with torch.no_grad():\n            for j in range(self.nsamples):\n                outs[j] = layer(\n                    inps[j].unsqueeze(0),\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                )[0]\n        inps, outs = outs, inps\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.WandaPruningForLlama.run","title":"<code>run(modelpool)</code>","text":"<p>Run the pruning algorithm on the model pool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>CausalLMPool</code>)           \u2013            <p>Pool of causal language models.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LlamaForCausalLM</code>          \u2013            <p>Pruned model.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/llama_wanda_prune.py</code> <pre><code>def run(self, modelpool: CausalLMPool):\n    \"\"\"\n    Run the pruning algorithm on the model pool.\n\n    Args:\n        modelpool (CausalLMPool): Pool of causal language models.\n\n    Returns:\n        LlamaForCausalLM: Pruned model.\n    \"\"\"\n\n    # load pre-trained model or the first model in the pool\n    with self.profile(\"load_model\"):\n        model = modelpool.load_pretrained_or_first_model()\n        model.seqlen = model.config.max_position_embeddings\n        tokenizer = modelpool.load_tokenizer(use_fast=False)\n\n    if not isinstance(model, (LlamaForCausalLM,)):\n        log.warning(f\"Model type {type(model)} may not supported.\")\n\n    inps, outs, attention_mask, position_ids = self.prepare_calibration_data(\n        model, tokenizer\n    )\n\n    self.prune_using_calibration_data_(\n        model,\n        inps=inps,\n        outs=outs,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n\n    if self.model_save_path is not None:\n        with timeit_context(f\"Saving pruned model to {self.model_save_path}\"):\n            tokenizer.save_pretrained(self.model_save_path)\n            model.save_pretrained(self.model_save_path)\n    return model\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#sparsegpt","title":"SparseGPT","text":""},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.SparseGPTPruningForLlama","title":"<code>SparseGPTPruningForLlama</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code></p> Source code in <code>fusion_bench/method/pruning/llama_sparsegpt_prune.py</code> <pre><code>class SparseGPTPruningForLlama(BaseAlgorithm, SimpleProfilerMixin):\n    def __init__(\n        self,\n        *,\n        nsamples: int,\n        seed: int,\n        use_variant: bool,\n        prune_type: PruningType,\n        device: str,\n        dtype: str,\n        sparsity_ratio: float,\n        n: int,\n        m: int,\n        model_save_path: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the SparseGPTPruningForLlama class.\n\n        Args:\n            nsamples (int): Number of samples for calibration.\n            seed (int): Random seed.\n            use_variant (bool): Whether to use a variant of the pruning method.\n            prune_type (PruningType): Type of pruning to perform.\n            device (str): Device to use for computation.\n            dtype (str): Data type to use for computation.\n            sparsity_ratio (float): Sparsity ratio for pruning.\n            n (int): Number of elements to keep in semi-structured pruning.\n            m (int): Number of elements in a group for semi-structured pruning.\n            model_save_path (Optional[str]): Path to save the pruned model.\n            **kwargs: Additional arguments.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.nsamples = nsamples\n        self.seed = seed\n        self.use_variant = use_variant\n        self.prune_type = prune_type\n        self.device = device\n        self.dtype = dtype\n        self.sparsity_ratio = sparsity_ratio\n        self.n = n\n        self.m = m\n        self.model_save_path = model_save_path\n\n    def run(self, modelpool: CausalLMPool):\n        # load pre-trained model or the first model in the pool\n        with self.profile(\"load_model\"):\n            model = modelpool.load_pretrained_or_first_model()\n            model.seqlen = model.config.max_position_embeddings\n            tokenizer = modelpool.load_tokenizer(use_fast=False)\n\n        if not isinstance(model, (LlamaForCausalLM,)):\n            log.warning(f\"Model type {type(model)} may not supported.\")\n\n        inps, outs, attention_mask, position_ids = self.prepare_calibration_data(\n            model, tokenizer\n        )\n\n        self.prune_using_calibration_data_(\n            model,\n            inps=inps,\n            outs=outs,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n        )\n\n        if self.model_save_path is not None:\n            with timeit_context(f\"Saving pruned model to {self.model_save_path}\"):\n                tokenizer.save_pretrained(self.model_save_path)\n                model.save_pretrained(self.model_save_path)\n        return model\n\n    def _prepare_calibration_data(self, model, tokenizer):\n        \"\"\"\n        Prepare calibration data for pruning.\n\n        Args:\n            model (LlamaForCausalLM): Model to be pruned.\n            tokenizer: Tokenizer for the model.\n\n        Returns:\n            Tuple: Calibration data (inputs, outputs, attention mask, position IDs).\n        \"\"\"\n        with timeit_context(\"loading calibration data\"):\n            dataloader, _ = get_loaders(\n                \"c4\",\n                nsamples=self.nsamples,\n                seed=self.seed,\n                seqlen=model.seqlen,\n                tokenizer=tokenizer,\n            )\n\n        with torch.no_grad():\n            # collect input to the first layer\n            inps, outs, attention_mask, position_ids = prepare_calibration_input(\n                model, dataloader, self.device\n            )\n        return inps, outs, attention_mask, position_ids\n\n    def prepare_calibration_data(self, model: LlamaForCausalLM, tokenizer):\n        \"\"\"\n        Prepare calibration data for pruning with caching.\n\n        Args:\n            model (LlamaForCausalLM): Model to be pruned.\n            tokenizer: Tokenizer for the model.\n\n        Returns:\n            Tuple: Calibration data (inputs, outputs, attention mask, position IDs).\n        \"\"\"\n\n        @cache_to_disk(\n            f\"outputs/cache/{model.config.name_or_path.split('/')[-1]}/calibration_data.pkl\"\n        )\n        def _prepare_calibration_data(model, tokenizer):\n            return self._prepare_calibration_data(model, tokenizer)\n\n        return _prepare_calibration_data(model, tokenizer)\n\n    @torch.no_grad()\n    def prune_using_calibration_data_(\n        self,\n        model: LlamaForCausalLM,\n        *,\n        inps,\n        outs,\n        attention_mask,\n        position_ids,\n    ):\n        layers = model.model.layers\n        for layer_indx, layer in tqdm(\n            enumerate(layers),\n            \"Pruning Layers\",\n            total=len(layers),\n            dynamic_ncols=True,\n        ):\n            layer = layers[layer_indx]\n            if f\"model.layers.{layer_indx}\" in model.hf_device_map:\n                dev = model.hf_device_map[f\"model.layers.{layer_indx}\"]\n                print(f\"layer {layer_indx} device {dev}\")\n                inps, outs, attention_mask, position_ids = (\n                    inps.to(dev),\n                    outs.to(dev),\n                    attention_mask.to(dev),\n                    position_ids.to(dev),\n                )\n\n            subset = find_linear_layers(layer, layers=[nn.Linear])\n\n            gpts: Dict[str, SparseGPT] = {}\n            for name in subset:\n                gpts[name] = SparseGPT(subset[name])\n\n            def add_batch(name):\n                def tmp(_, inp, out):\n                    gpts[name].add_batch(inp[0].data, out.data)\n\n                return tmp\n\n            handles = []\n            for name in gpts:\n                handles.append(subset[name].register_forward_hook(add_batch(name)))\n\n            for j in range(self.nsamples):\n                outs[j] = layer(\n                    inps[j].unsqueeze(0),\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                )[0]\n            for h in handles:\n                h.remove()\n\n            for name in gpts:\n                print(layer_indx, name)\n                print(\"Pruning ...\")\n\n                gpts[name].fasterprune(\n                    self.sparsity_ratio,\n                    prune_n=self.n,\n                    prune_m=self.m,\n                    percdamp=0.01,\n                    blocksize=128,\n                )\n                gpts[name].free()\n\n            for j in range(self.nsamples):\n                outs[j] = layer(\n                    inps[j].unsqueeze(0),\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                )[0]\n\n            layers[layer_indx] = layer\n            torch.cuda.empty_cache()\n\n            inps, outs = outs, inps\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.SparseGPTPruningForLlama.__init__","title":"<code>__init__(*, nsamples, seed, use_variant, prune_type, device, dtype, sparsity_ratio, n, m, model_save_path=None, **kwargs)</code>","text":"<p>Initialize the SparseGPTPruningForLlama class.</p> <p>Parameters:</p> <ul> <li> <code>nsamples</code>               (<code>int</code>)           \u2013            <p>Number of samples for calibration.</p> </li> <li> <code>seed</code>               (<code>int</code>)           \u2013            <p>Random seed.</p> </li> <li> <code>use_variant</code>               (<code>bool</code>)           \u2013            <p>Whether to use a variant of the pruning method.</p> </li> <li> <code>prune_type</code>               (<code>PruningType</code>)           \u2013            <p>Type of pruning to perform.</p> </li> <li> <code>device</code>               (<code>str</code>)           \u2013            <p>Device to use for computation.</p> </li> <li> <code>dtype</code>               (<code>str</code>)           \u2013            <p>Data type to use for computation.</p> </li> <li> <code>sparsity_ratio</code>               (<code>float</code>)           \u2013            <p>Sparsity ratio for pruning.</p> </li> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>Number of elements to keep in semi-structured pruning.</p> </li> <li> <code>m</code>               (<code>int</code>)           \u2013            <p>Number of elements in a group for semi-structured pruning.</p> </li> <li> <code>model_save_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Path to save the pruned model.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/llama_sparsegpt_prune.py</code> <pre><code>def __init__(\n    self,\n    *,\n    nsamples: int,\n    seed: int,\n    use_variant: bool,\n    prune_type: PruningType,\n    device: str,\n    dtype: str,\n    sparsity_ratio: float,\n    n: int,\n    m: int,\n    model_save_path: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the SparseGPTPruningForLlama class.\n\n    Args:\n        nsamples (int): Number of samples for calibration.\n        seed (int): Random seed.\n        use_variant (bool): Whether to use a variant of the pruning method.\n        prune_type (PruningType): Type of pruning to perform.\n        device (str): Device to use for computation.\n        dtype (str): Data type to use for computation.\n        sparsity_ratio (float): Sparsity ratio for pruning.\n        n (int): Number of elements to keep in semi-structured pruning.\n        m (int): Number of elements in a group for semi-structured pruning.\n        model_save_path (Optional[str]): Path to save the pruned model.\n        **kwargs: Additional arguments.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.nsamples = nsamples\n    self.seed = seed\n    self.use_variant = use_variant\n    self.prune_type = prune_type\n    self.device = device\n    self.dtype = dtype\n    self.sparsity_ratio = sparsity_ratio\n    self.n = n\n    self.m = m\n    self.model_save_path = model_save_path\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.SparseGPTPruningForLlama.prepare_calibration_data","title":"<code>prepare_calibration_data(model, tokenizer)</code>","text":"<p>Prepare calibration data for pruning with caching.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>LlamaForCausalLM</code>)           \u2013            <p>Model to be pruned.</p> </li> <li> <code>tokenizer</code>           \u2013            <p>Tokenizer for the model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple</code>          \u2013            <p>Calibration data (inputs, outputs, attention mask, position IDs).</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/llama_sparsegpt_prune.py</code> <pre><code>def prepare_calibration_data(self, model: LlamaForCausalLM, tokenizer):\n    \"\"\"\n    Prepare calibration data for pruning with caching.\n\n    Args:\n        model (LlamaForCausalLM): Model to be pruned.\n        tokenizer: Tokenizer for the model.\n\n    Returns:\n        Tuple: Calibration data (inputs, outputs, attention mask, position IDs).\n    \"\"\"\n\n    @cache_to_disk(\n        f\"outputs/cache/{model.config.name_or_path.split('/')[-1]}/calibration_data.pkl\"\n    )\n    def _prepare_calibration_data(model, tokenizer):\n        return self._prepare_calibration_data(model, tokenizer)\n\n    return _prepare_calibration_data(model, tokenizer)\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#pruning-with-low-rank-refinement","title":"Pruning with Low-Rank Refinement","text":""},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.SparseLoForLlama","title":"<code>SparseLoForLlama</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code></p> <p>Zero-Shot SVD Algorithm</p> Source code in <code>fusion_bench/method/sparselo/sparselo.py</code> <pre><code>class SparseLoForLlama(BaseAlgorithm, SimpleProfilerMixin):\n    \"Zero-Shot SVD Algorithm\"\n\n    _variants_requires_calibration_data = [\"wanda\"]\n    _variants_hook_mapping = {\"wanda\": WandaHookFn}\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"nsamples\": \"nsamples\",\n        \"seed\": \"seed\",\n        \"rank\": \"rank\",\n        \"sparsity_ratio\": \"sparsity_ratio\",\n        \"prune_type\": \"prune_type\",\n        \"n\": \"n\",\n        \"m\": \"m\",\n        \"device\": \"device\",\n        \"variant\": \"variant\",\n    }\n\n    def __init__(\n        self,\n        *,\n        nsamples: int,\n        variant: Literal[\"dense\", \"random\", \"wanda\", \"lowrank-only\", \"magnitude\"],\n        seed: int,\n        rank: int,\n        sparsity_ratio: float,\n        prune_type: PruningType,\n        n: int,\n        m: int,\n        device: Optional[str] = None,\n        model_save_path: Optional[str] = None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.nsamples = nsamples\n        self.variant = variant\n        self.seed = seed\n        self.rank = rank\n        self.sparsity_ratio = sparsity_ratio\n        self.prune_type = prune_type\n        self.device = device\n        self.model_save_path = model_save_path\n        self.n = n\n        self.m = m\n\n    @override\n    def run(self, modelpool: CausalLMPool):\n        self.modelpool = modelpool\n        if self.seed is not None:\n            L.seed_everything(self.seed)\n\n        # load pre-trained model or the first model in the pool\n        with self.profile(\"load_model\"):\n            model = modelpool.load_pretrained_or_first_model()\n            model.seqlen = model.config.max_position_embeddings\n            tokenizer = modelpool.load_tokenizer(use_fast=False)\n\n        if not isinstance(model, (LlamaForCausalLM,)):\n            log.warning(f\"Model type {type(model)} may not supported.\")\n\n        if self.variant in self._variants_requires_calibration_data:\n            inps, outs, attention_mask, position_ids = self.prepare_calibration_data(\n                model, tokenizer\n            )\n\n        model = convert_to_losparse_llama(model, rank=self.rank)\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        for linear in find_linear_layers(model, layers=[LoSparseLinear]).values():\n            linear = cast(LoSparseLinear, linear)\n            linear.lo_A.data.zero_()\n            linear.lo_B.data.zero_()\n            linear.skip_lowrank = True\n\n        match self.variant:\n            case \"dense\":\n                # this variant is a no-op, just for debug the conversion\n                pass\n            case \"lowrank-only\":\n                self.extract_low_rank_parts_(model)\n                self.set_weights_to_zeros_(model)\n            case \"random\":\n                self.random_prune_(model)\n            case \"magnitude\":\n                self.magnitude_prune_(model)\n            case variant if variant in self._variants_requires_calibration_data:\n                self.prune_using_calibration_data_(\n                    model,\n                    inps=inps,\n                    outs=outs,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                )\n            case _:\n                raise ValueError(f\"Invalid variant: {self.variant}\")\n\n        if self.model_save_path is not None:\n            with timeit_context(f\"Saving the model to {self.model_save_path}\"):\n                tokenizer.save_pretrained(self.model_save_path)\n                model.save_pretrained(self.model_save_path)\n\n        return model\n\n    def set_weights_to_zeros_(self, model):\n        layers: nn.ModuleList = model.model.layers\n        for layer in tqdm(\n            list(layers),\n            \"Pruning Layers\",\n            dynamic_ncols=True,\n        ):\n            for name, losparse_linear in layer.named_modules():\n                if isinstance(losparse_linear, LoSparseLinear):\n                    log.info(f\"Pruning {name}, set weights to zeros\")\n                    losparse_linear.weight.data.zero_()\n\n    @torch.no_grad()\n    def extract_low_rank_parts_(self, model):\n        for layer in tqdm(\n            list(model.model.layers),\n            \"Extract Low-Rank Parts (Layers)\",\n            dynamic_ncols=True,\n        ):\n            for losparse_linear in layer.modules():\n                if isinstance(losparse_linear, LoSparseLinear):\n                    if self.device is not None:\n                        original_device = get_device(losparse_linear)\n                        losparse_linear.to(self.device)\n                    extract_low_rank_part_(losparse_linear, self.rank)\n                    if self.device is not None:\n                        losparse_linear.to(original_device)\n\n    def _prepare_calibration_data(self, model, tokenizer):\n        with timeit_context(\"loading calibration data\"):\n            dataloader, _ = get_loaders(\n                \"c4\",\n                nsamples=self.nsamples,\n                seed=self.seed,\n                seqlen=model.seqlen,\n                tokenizer=tokenizer,\n            )\n\n        with torch.no_grad():\n            # collect input to the first layer\n            inps, outs, attention_mask, position_ids = prepare_calibration_input(\n                model, dataloader, self.device\n            )\n        return inps, outs, attention_mask, position_ids\n\n    def prepare_calibration_data(self, model: LlamaForCausalLM, tokenizer):\n\n        @cache_to_disk(\n            f\"outputs/cache/{model.config.name_or_path.split('/')[-1]}/calibration_data.pkl\"\n        )\n        def _prepare_calibration_data(model, tokenizer):\n            return self._prepare_calibration_data(model, tokenizer)\n\n        return _prepare_calibration_data(model, tokenizer)\n\n    def random_prune_(self, model):\n        layers: nn.ModuleList = model.model.layers\n        for layer in tqdm(\n            list(layers),\n            \"Pruning Layers\",\n            dynamic_ncols=True,\n        ):\n            for name, losparse_linear in layer.named_modules():\n                if isinstance(losparse_linear, LoSparseLinear):\n                    log.info(f\"Pruning {name}, set weights to zeros\")\n                    if self.prune_type == PruningType.UNSTRUCTURED:\n                        _, pruned_weights = unstructured_magnitude_prune_(\n                            losparse_linear.weight.data,\n                            metric_function_or_scores=torch.rand_like,\n                            sparsity_ratio=self.sparsity_ratio,\n                            return_pruned_weight=True,\n                        )\n                    elif self.prune_type == PruningType.SEMISTRUCTURED:\n                        _, pruned_weights = semistructured_magnitude_prune_(\n                            losparse_linear.weight.data,\n                            metric_function_or_scores=torch.rand_like,\n                            n=self.n,\n                            m=self.m,\n                            return_pruned_weight=True,\n                        )\n                    else:\n                        raise ValueError(f\"Invalid pruning type: {self.prune_type}\")\n                    self.check_sparsity(losparse_linear.weight)\n                    self.extract_low_rank_part_using_pruned_(\n                        losparse_linear, pruned_weights\n                    )\n\n    def magnitude_prune_(self, model):\n        layers: nn.ModuleList = model.model.layers\n        for layer_idx, layer in tqdm(\n            enumerate(layers), \"Pruning Layers\", total=len(layers), dynamic_ncols=True\n        ):\n            for name, losparse_linear in layer.named_modules():\n                if isinstance(losparse_linear, LoSparseLinear):\n                    log.info(f\"Magnitude Pruning {name}\")\n                    if self.prune_type == PruningType.UNSTRUCTURED:\n                        _, pruned_weights = unstructured_magnitude_prune_(\n                            losparse_linear.weight.data,\n                            metric_function_or_scores=torch.abs,\n                            sparsity_ratio=self.sparsity_ratio,\n                            return_pruned_weight=True,\n                        )\n                    elif self.prune_type == PruningType.SEMISTRUCTURED:\n                        _, pruned_weights = semistructured_magnitude_prune_(\n                            losparse_linear.weight.data,\n                            metric_function_or_scores=torch.abs,\n                            n=self.n,\n                            m=self.m,\n                            return_pruned_weight=True,\n                        )\n                    else:\n                        raise ValueError(f\"Invalid pruning type: {self.prune_type}\")\n                    self.check_sparsity(losparse_linear.weight)\n                    self.extract_low_rank_part_using_pruned_(\n                        losparse_linear, pruned_weights\n                    )\n\n    def prune_using_calibration_data_(\n        self,\n        model: LoSparseLlamaForCausalLM,\n        *,\n        inps: Tensor,\n        outs: Tensor,\n        attention_mask: Optional[Tensor],\n        position_ids: Optional[Tensor],\n    ):\n        layers = model.model.layers\n        for layer_idx, layer in tqdm(\n            enumerate(layers),\n            \"Pruning Layers\",\n            total=len(layers),\n            dynamic_ncols=True,\n        ):\n            if (\n                hasattr(model, \"hf_device_map\")\n                and f\"model.layers.{layer_idx}\" in model.hf_device_map\n            ):  ## handle the case for llama-30B and llama-65B, when the device map has multiple GPUs;\n                dev = model.hf_device_map[f\"model.layers.{layer_idx}\"]\n                inps, outs, attention_mask, position_ids = (\n                    inps.to(dev),\n                    outs.to(dev),\n                    attention_mask.to(dev) if attention_mask is not None else None,\n                    position_ids.to(dev) if position_ids is not None else None,\n                )\n\n            # collect the importance scores\n            linear_layers = cast(\n                Dict[str, LoSparseLinear],\n                find_linear_layers(layer, layers=[LoSparseLinear]),\n            )\n\n            # register hooks to collect the importance scores\n            def get_hook_fn(linear: LoSparseLinear):\n                hook_fn = self._variants_hook_mapping[self.variant](linear)\n                return hook_fn\n\n            hooks = {}\n            handles: List[torch.utils.hooks.RemovableHandle] = []\n            for name, linear in linear_layers.items():\n                hook_fn = get_hook_fn(linear)\n                hooks[name] = hook_fn\n                handles.append(linear.register_forward_hook(hook_fn))\n\n            with torch.no_grad():\n                for j in range(self.nsamples):\n                    outs[j] = layer(\n                        inps[j].unsqueeze(0),\n                        attention_mask=attention_mask,\n                        position_ids=position_ids,\n                    )[0]\n\n            # compute the importance scores and remove the hooks\n            metrics = {}\n            for name, hook in hooks.items():\n                metrics[name] = hook.compute()\n            for h in handles:\n                h.remove()\n\n            # prune the weights based on the importance scores\n            pruned_weights_dict = {}\n            for name, linear in linear_layers.items():\n                log.info(f\"Pruning {name}\")\n                if self.prune_type == PruningType.UNSTRUCTURED:\n                    _, pruned_weights = unstructured_magnitude_prune_(\n                        linear.weight.data,\n                        metrics[name],\n                        sparsity_ratio=self.sparsity_ratio,\n                        return_pruned_weight=True,\n                    )\n                elif self.prune_type == PruningType.SEMISTRUCTURED:\n                    _, pruned_weights = semistructured_magnitude_prune_(\n                        linear.weight.data,\n                        metrics[name],\n                        n=self.n,\n                        m=self.m,\n                        return_pruned_weight=True,\n                    )\n                else:\n                    raise ValueError(f\"Invalid pruning type: {self.prune_type}\")\n                self.check_sparsity(linear.weight)\n                pruned_weights_dict[name] = pruned_weights\n\n            # compute the input to the next layer\n            with torch.no_grad():\n                for j in range(self.nsamples):\n                    outs[j] = layer(\n                        inps[j].unsqueeze(0),\n                        attention_mask=attention_mask,\n                        position_ids=position_ids,\n                    )[0]\n            inps, outs = outs, inps\n\n            # extract the low-rank parts\n            for name, linear in linear_layers.items():\n                log.info(f\"Extracting low-rank part for {name}\")\n                self.extract_low_rank_part_using_pruned_(\n                    linear, pruned_weights_dict[name]\n                )\n                linear.skip_lowrank = False\n\n    @torch.no_grad()\n    def extract_low_rank_part_using_pruned_(\n        self, linear: LoSparseLinear, pruned_weight: Tensor\n    ):\n        assert isinstance(\n            linear, LoSparseLinear\n        ), f\"Expected LoSparseLinear, got {type(linear)}\"\n\n        u, s, vh = cast(\n            Tuple[Tensor, Tensor, Tensor],\n            torch.linalg.svd(pruned_weight.float(), full_matrices=False),\n        )\n        v = vh.T\n        uk = u[:, : self.rank]\n        sk = s[: self.rank]\n        vk = v[:, : self.rank]\n        linear.lo_A.data = vk.T.to(linear.lo_A.dtype).contiguous()\n        linear.lo_B.data = (uk * sk).to(linear.lo_B.dtype).contiguous()\n        return linear\n\n    @torch.no_grad()\n    def check_sparsity(self, weight: Tensor, tol: float = 0.01):\n        if self.prune_type == PruningType.UNSTRUCTURED:\n            assert (compute_sparsity(weight) - self.sparsity_ratio).abs() &lt; tol\n        elif self.prune_type == PruningType.SEMISTRUCTURED:\n            assert (compute_sparsity(weight) - self.n / self.m).abs() &lt; tol\n        else:\n            raise ValueError(f\"Invalid pruning type: {self.prune_type}\")\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.PCPSparseLoForLlama","title":"<code>PCPSparseLoForLlama</code>","text":"<p>               Bases: <code>SparseLoForLlama</code></p> <p>PCP with mask</p> Source code in <code>fusion_bench/method/sparselo/sparselo.py</code> <pre><code>class PCPSparseLoForLlama(SparseLoForLlama):\n    \"PCP with mask\"\n\n    _config_mapping = SparseLoForLlama._config_mapping | {\n        \"num_iterations\": \"num_iterations\",\n    }\n\n    def __init__(self, num_iterations: int, **kwargs):\n        super().__init__(**kwargs)\n        self.num_iterations = num_iterations\n\n    @override\n    def run(self, modelpool):\n        if self.seed is not None:\n            L.seed_everything(self.seed)\n\n        # load pre-trained model or the first model in the pool\n        with self.profile(\"load_model\"):\n            model = modelpool.load_pretrained_or_first_model()\n            model.seqlen = model.config.max_position_embeddings\n            tokenizer = modelpool.load_tokenizer(use_fast=False)\n\n        if not isinstance(model, (LlamaForCausalLM,)):\n            log.warning(f\"Model type {type(model)} may not supported.\")\n\n        if self.variant in self._variants_requires_calibration_data:\n            inps, outs, attention_mask, position_ids = self.prepare_calibration_data(\n                model, tokenizer\n            )\n\n        model = convert_to_losparse_llama(model, rank=self.rank)\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        for linear in find_linear_layers(model, layers=[LoSparseLinear]).values():\n            linear = cast(LoSparseLinear, linear)\n            linear.lo_A.data.zero_()\n            linear.lo_B.data.zero_()\n            linear.skip_lowrank = True\n\n        match self.variant:\n            case \"dense\":\n                # this variant is a no-op, just for debug the conversion\n                pass\n            case \"lowrank-only\":\n                self.extract_low_rank_parts_(model)\n                self.set_weights_to_zeros_(model)\n            case \"random\":\n                self.pcp_random_prune_(model)\n            case \"magnitude\":\n                self.pcp_magnitude_prune_(model)\n            case variant if variant in self._variants_requires_calibration_data:\n                self.pcp_prune_using_calibration_data_(\n                    model,\n                    inps=inps,\n                    outs=outs,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                )\n            case _:\n                raise ValueError(f\"Invalid variant: {self.variant}\")\n\n        if self.model_save_path is not None:\n            with timeit_context(f\"Saving the model to {self.model_save_path}\"):\n                tokenizer.save_pretrained(self.model_save_path)\n                model.save_pretrained(self.model_save_path)\n\n        return model\n\n    @torch.no_grad()\n    def pcp_random_prune_(self, model):\n        layers: nn.ModuleList = model.model.layers\n        for layer_idx, layer in tqdm(\n            list(enumerate(layers)),\n            \"Pruning Layers\",\n            dynamic_ncols=True,\n        ):\n            for name, linear in layer.named_modules():\n                if isinstance(linear, LoSparseLinear):\n                    log.info(f\"Pruning {name}, set weights to zeros\")\n                    W = linear.weight.data.clone()\n                    if self.prune_type == PruningType.UNSTRUCTURED:\n                        unstructured_magnitude_prune_(\n                            linear.weight.data,\n                            metric_function_or_scores=torch.rand_like,\n                            sparsity_ratio=self.sparsity_ratio,\n                        )\n                    elif self.prune_type == PruningType.SEMISTRUCTURED:\n                        semistructured_magnitude_prune_(\n                            linear.weight.data,\n                            metric_function_or_scores=torch.rand_like,\n                            n=self.n,\n                            m=self.m,\n                        )\n                    else:\n                        raise ValueError(f\"Invalid pruning type: {self.prune_type}\")\n                    self.check_sparsity(linear.weight)\n                    mask = linear.weight != 0\n                    linear.weight.data = PCP_search_with_mask(\n                        W, mask, T_max=self.num_iterations\n                    )\n                    self.extract_low_rank_part_using_pruned_(linear, W - linear.weight)\n\n    def pcp_magnitude_prune_(self, model):\n        layers: nn.ModuleList = model.model.layers\n        for layer_idx, layer in tqdm(\n            enumerate(layers), \"Pruning Layers\", total=len(layers), dynamic_ncols=True\n        ):\n            for name, linear in layer.named_modules():\n                if isinstance(linear, LoSparseLinear):\n                    log.info(f\"Magnitude Pruning {name}\")\n                    W = linear.weight.data.clone()\n                    if self.prune_type == PruningType.UNSTRUCTURED:\n                        unstructured_magnitude_prune_(\n                            linear.weight.data,\n                            metric_function_or_scores=torch.abs,\n                            sparsity_ratio=self.sparsity_ratio,\n                        )\n                    elif self.prune_type == PruningType.SEMISTRUCTURED:\n                        semistructured_magnitude_prune_(\n                            linear.weight.data,\n                            metric_function_or_scores=torch.abs,\n                            n=self.n,\n                            m=self.m,\n                        )\n                    else:\n                        raise ValueError(f\"Invalid pruning type: {self.prune_type}\")\n                    self.check_sparsity(linear.weight)\n                    mask = linear.weight != 0\n                    linear.weight.data = PCP_search_with_mask(\n                        W, mask, T_max=self.num_iterations\n                    )\n                    self.extract_low_rank_part_using_pruned_(linear, W - linear.weight)\n\n    def pcp_prune_using_calibration_data_(\n        self,\n        model: LoSparseLlamaForCausalLM,\n        *,\n        inps: Tensor,\n        outs: Tensor,\n        attention_mask: Optional[Tensor],\n        position_ids: Optional[Tensor],\n    ):\n        layers = model.model.layers\n        for layer_idx, layer in tqdm(\n            enumerate(layers),\n            \"Pruning Layers\",\n            total=len(layers),\n            dynamic_ncols=True,\n        ):\n            if (\n                hasattr(model, \"hf_device_map\")\n                and f\"model.layers.{layer_idx}\" in model.hf_device_map\n            ):  ## handle the case for llama-30B and llama-65B, when the device map has multiple GPUs;\n                dev = model.hf_device_map[f\"model.layers.{layer_idx}\"]\n                inps, outs, attention_mask, position_ids = (\n                    inps.to(dev),\n                    outs.to(dev),\n                    attention_mask.to(dev) if attention_mask is not None else None,\n                    position_ids.to(dev) if position_ids is not None else None,\n                )\n\n            # collect the importance scores\n            linear_layers = cast(\n                Dict[str, LoSparseLinear],\n                find_linear_layers(layer, layers=[LoSparseLinear]),\n            )\n\n            # register hooks to collect the importance scores\n            def get_hook_fn(linear: LoSparseLinear):\n                hook_fn = self._variants_hook_mapping[self.variant](linear)\n                return hook_fn\n\n            hooks = {}\n            handles: List[torch.utils.hooks.RemovableHandle] = []\n            for name, linear in linear_layers.items():\n                hook_fn = get_hook_fn(linear)\n                hooks[name] = hook_fn\n                handles.append(linear.register_forward_hook(hook_fn))\n\n            with torch.no_grad():\n                for j in range(self.nsamples):\n                    outs[j] = layer(\n                        inps[j].unsqueeze(0),\n                        attention_mask=attention_mask,\n                        position_ids=position_ids,\n                    )[0]\n\n            # compute the importance scores and remove the hooks\n            metrics = {}\n            for name, hook in hooks.items():\n                metrics[name] = hook.compute()\n            for h in handles:\n                h.remove()\n\n            # prune the weights based on the importance scores\n            for name, linear in linear_layers.items():\n                log.info(f\"Pruning {name}\")\n                W = linear.weight.data.clone()\n                if self.prune_type == PruningType.UNSTRUCTURED:\n                    _, pruned_weights = unstructured_magnitude_prune_(\n                        linear.weight.data,\n                        metrics[name],\n                        sparsity_ratio=self.sparsity_ratio,\n                        return_pruned_weight=True,\n                    )\n                elif self.prune_type == PruningType.SEMISTRUCTURED:\n                    _, pruned_weights = semistructured_magnitude_prune_(\n                        linear.weight.data,\n                        metrics[name],\n                        n=self.n,\n                        m=self.m,\n                        return_pruned_weight=True,\n                    )\n                else:\n                    raise ValueError(f\"Invalid pruning type: {self.prune_type}\")\n                self.check_sparsity(linear.weight)\n                mask = linear.weight != 0\n                linear.weight.data = PCP_search_with_mask(\n                    W, mask, T_max=self.num_iterations\n                )\n                self.extract_low_rank_part_using_pruned_(linear, W - linear.weight)\n                linear.skip_lowrank = False\n\n            # compute the input to the next layer\n            with torch.no_grad():\n                for j in range(self.nsamples):\n                    outs[j] = layer(\n                        inps[j].unsqueeze(0),\n                        attention_mask=attention_mask,\n                        position_ids=position_ids,\n                    )[0]\n            inps, outs = outs, inps\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.IterativeSparseLoForLlama","title":"<code>IterativeSparseLoForLlama</code>","text":"<p>               Bases: <code>SparseLoForLlama</code></p> <p>Iterative Weight Update</p> Source code in <code>fusion_bench/method/sparselo/sparselo.py</code> <pre><code>class IterativeSparseLoForLlama(SparseLoForLlama):\n    \"Iterative Weight Update\"\n\n    _config_mapping = SparseLoForLlama._config_mapping | {\n        \"num_iterations\": \"num_iterations\",\n    }\n\n    def __init__(\n        self, num_iterations: int, use_reference_model: bool = False, **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.num_iterations = num_iterations\n        self.use_reference_model = use_reference_model\n\n    @override\n    def run(self, modelpool):\n        self.modelpool = modelpool\n        if self.seed is not None:\n            L.seed_everything(self.seed)\n\n        # load pre-trained model or the first model in the pool\n        with self.profile(\"load_model\"):\n            model = modelpool.load_pretrained_or_first_model()\n            model.seqlen = model.config.max_position_embeddings\n            tokenizer = modelpool.load_tokenizer(use_fast=False)\n\n        if not isinstance(model, (LlamaForCausalLM,)):\n            log.warning(f\"Model type {type(model)} may not supported.\")\n\n        if self.variant in self._variants_requires_calibration_data:\n            inps, outs, attention_mask, position_ids = self.prepare_calibration_data(\n                model, tokenizer\n            )\n\n        model = convert_to_losparse_llama(model, rank=self.rank)\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        for linear in find_linear_layers(model, layers=[LoSparseLinear]).values():\n            linear = cast(LoSparseLinear, linear)\n            linear.lo_A.data.zero_()\n            linear.lo_B.data.zero_()\n            linear.skip_lowrank = True\n\n        match self.variant:\n            case \"dense\":\n                # this variant is a no-op, just for debug the conversion\n                pass\n            case \"lowrank-only\":\n                self.extract_low_rank_parts_(model)\n                self.set_weights_to_zeros_(model)\n            case \"random\":\n                self.iterative_random_prune_(model)\n            case \"magnitude\":\n                self.iterative_magnitude_prune_(model)\n            case variant if variant in self._variants_requires_calibration_data:\n                self.iterative_prune_using_calibration_data_(\n                    model,\n                    inps=inps,\n                    outs=outs,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                )\n            case _:\n                raise ValueError(f\"Invalid variant: {self.variant}\")\n\n        if self.model_save_path is not None:\n            with timeit_context(f\"Saving the model to {self.model_save_path}\"):\n                tokenizer.save_pretrained(self.model_save_path)\n                model.save_pretrained(self.model_save_path)\n\n        return model\n\n    @torch.no_grad()\n    def iterative_random_prune_(self, model):\n        layers: nn.ModuleList = model.model.layers\n        for layer_idx, layer in tqdm(\n            list(enumerate(layers)),\n            \"Pruning Layers\",\n            dynamic_ncols=True,\n        ):\n            for name, linear in layer.named_modules():\n                if isinstance(linear, LoSparseLinear):\n                    log.info(f\"Pruning {name}, set weights to zeros\")\n                    W = linear.weight.data.clone()\n                    if self.prune_type == PruningType.UNSTRUCTURED:\n                        unstructured_magnitude_prune_(\n                            linear.weight.data,\n                            metric_function_or_scores=torch.rand_like,\n                            sparsity_ratio=self.sparsity_ratio,\n                        )\n                    elif self.prune_type == PruningType.SEMISTRUCTURED:\n                        semistructured_magnitude_prune_(\n                            linear.weight.data,\n                            metric_function_or_scores=torch.rand_like,\n                            n=self.n,\n                            m=self.m,\n                        )\n                    else:\n                        raise ValueError(f\"Invalid pruning type: {self.prune_type}\")\n                    self.check_sparsity(linear.weight)\n                    mask = linear.weight != 0\n                    for rank in tqdm(\n                        np.linspace(1, self.rank, self.num_iterations, dtype=np.int64),\n                        \"Iterative Pruning\",\n                        leave=False,\n                        dynamic_ncols=True,\n                    ):\n                        linear.weight.data, specturm_ratio = iterative_weight_update(\n                            W,\n                            linear.weight,\n                            mask,\n                            rank=rank,\n                        )\n                        if specturm_ratio &gt; 0.99:\n                            break\n                    self.extract_low_rank_part_using_pruned_(linear, W - linear.weight)\n\n    @torch.no_grad()\n    def iterative_magnitude_prune_(self, model):\n        layers: nn.ModuleList = model.model.layers\n        if self.use_reference_model:\n            reference_model = self.modelpool.load_model(\n                \"reference_model\", torch_dtype=\"float16\"\n            )\n            reference_layers: nn.ModuleList = reference_model.model.layers\n        for layer_idx, layer in tqdm(\n            enumerate(layers), \"Pruning Layers\", total=len(layers), dynamic_ncols=True\n        ):\n            for name, linear in layer.named_modules():\n                if isinstance(linear, LoSparseLinear):\n                    log.info(f\"Magnitude Pruning {name}\")\n                    W = (\n                        linear.weight.data.clone()\n                        if not self.use_reference_model\n                        else reference_layers[layer_idx]\n                        .get_submodule(name)\n                        .weight.data.clone()\n                        .to(linear.weight.data.device)\n                    )\n                    if self.prune_type == PruningType.UNSTRUCTURED:\n                        unstructured_magnitude_prune_(\n                            linear.weight.data,\n                            metric_function_or_scores=torch.abs,\n                            sparsity_ratio=self.sparsity_ratio,\n                        )\n                    elif self.prune_type == PruningType.SEMISTRUCTURED:\n                        semistructured_magnitude_prune_(\n                            linear.weight.data,\n                            metric_function_or_scores=torch.abs,\n                            n=self.n,\n                            m=self.m,\n                        )\n                    else:\n                        raise ValueError(f\"Invalid pruning type: {self.prune_type}\")\n                    self.check_sparsity(linear.weight)\n                    mask = linear.weight != 0\n                    for rank in tqdm(\n                        np.linspace(1, self.rank, self.num_iterations, dtype=np.int64),\n                        \"Iterative Pruning\",\n                        leave=False,\n                        dynamic_ncols=True,\n                    ):\n                        linear.weight.data, specturm_ratio = iterative_weight_update(\n                            W,\n                            linear.weight,\n                            mask,\n                            rank=rank,\n                        )\n                        if specturm_ratio &gt; 0.99:\n                            break\n                    self.extract_low_rank_part_using_pruned_(linear, W - linear.weight)\n\n    @torch.no_grad()\n    def iterative_prune_using_calibration_data_(\n        self,\n        model: LoSparseLlamaForCausalLM,\n        *,\n        inps: Tensor,\n        outs: Tensor,\n        attention_mask: Optional[Tensor],\n        position_ids: Optional[Tensor],\n    ):\n        layers = model.model.layers\n        for layer_idx, layer in tqdm(\n            enumerate(layers),\n            \"Pruning Layers\",\n            total=len(layers),\n            dynamic_ncols=True,\n        ):\n            if (\n                hasattr(model, \"hf_device_map\")\n                and f\"model.layers.{layer_idx}\" in model.hf_device_map\n            ):  ## handle the case for llama-30B and llama-65B, when the device map has multiple GPUs;\n                dev = model.hf_device_map[f\"model.layers.{layer_idx}\"]\n                inps, outs, attention_mask, position_ids = (\n                    inps.to(dev),\n                    outs.to(dev),\n                    attention_mask.to(dev) if attention_mask is not None else None,\n                    position_ids.to(dev) if position_ids is not None else None,\n                )\n\n            # collect the importance scores\n            linear_layers = cast(\n                Dict[str, LoSparseLinear],\n                find_linear_layers(layer, layers=[LoSparseLinear]),\n            )\n\n            # register hooks to collect the importance scores\n            def get_hook_fn(linear: LoSparseLinear):\n                hook_fn = self._variants_hook_mapping[self.variant](linear)\n                return hook_fn\n\n            hooks = {}\n            handles: List[torch.utils.hooks.RemovableHandle] = []\n            for name, linear in linear_layers.items():\n                hook_fn = get_hook_fn(linear)\n                hooks[name] = hook_fn\n                handles.append(linear.register_forward_hook(hook_fn))\n\n            with torch.no_grad():\n                for j in range(self.nsamples):\n                    outs[j] = layer(\n                        inps[j].unsqueeze(0),\n                        attention_mask=attention_mask,\n                        position_ids=position_ids,\n                    )[0]\n\n            # compute the importance scores and remove the hooks\n            metrics = {}\n            for name, hook in hooks.items():\n                metrics[name] = hook.compute()\n            for h in handles:\n                h.remove()\n\n            # prune the weights based on the importance scores\n            for name, linear in linear_layers.items():\n                log.info(f\"Pruning {name}\")\n                W = linear.weight.data.clone()\n                if self.prune_type == PruningType.UNSTRUCTURED:\n                    _, pruned_weights = unstructured_magnitude_prune_(\n                        linear.weight.data,\n                        metrics[name],\n                        sparsity_ratio=self.sparsity_ratio,\n                        return_pruned_weight=True,\n                    )\n                elif self.prune_type == PruningType.SEMISTRUCTURED:\n                    _, pruned_weights = semistructured_magnitude_prune_(\n                        linear.weight.data,\n                        metrics[name],\n                        n=self.n,\n                        m=self.m,\n                        return_pruned_weight=True,\n                    )\n                else:\n                    raise ValueError(f\"Invalid pruning type: {self.prune_type}\")\n                self.check_sparsity(linear.weight)\n                mask = linear.weight != 0\n                for rank in tqdm(\n                    np.linspace(1, self.rank, self.num_iterations, dtype=np.int64),\n                    \"Iterative Pruning\",\n                    leave=False,\n                    dynamic_ncols=True,\n                ):\n                    linear.weight.data, specturm_ratio = iterative_weight_update(\n                        W,\n                        linear.weight,\n                        mask,\n                        rank=rank,\n                    )\n                    if specturm_ratio &gt; 0.99:\n                        break\n                self.extract_low_rank_part_using_pruned_(linear, W - linear.weight)\n                linear.skip_lowrank = False\n\n            # compute the input to the next layer\n            with torch.no_grad():\n                for j in range(self.nsamples):\n                    outs[j] = layer(\n                        inps[j].unsqueeze(0),\n                        attention_mask=attention_mask,\n                        position_ids=position_ids,\n                    )[0]\n            inps, outs = outs, inps\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#moe-expert-pruning","title":"MoE Expert Pruning","text":""},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.DynamicSkippingPruningForMixtral","title":"<code>DynamicSkippingPruningForMixtral</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>LightningFabricMixin</code>, <code>SimpleProfilerMixin</code></p> Source code in <code>fusion_bench/method/expert_sparsity/mixtral/dynamic_skipping.py</code> <pre><code>@auto_register_config\nclass DynamicSkippingPruningForMixtral(\n    fb.BaseAlgorithm,\n    fb.mixins.LightningFabricMixin,\n    fb.mixins.SimpleProfilerMixin,\n):\n    modelpool: fb.modelpool.CausalLMPool\n\n    def __init__(\n        self,\n        calib_set: str,\n        max_block_size: int,\n        n_blocks_for_stat: int,\n        batch_size: int,\n        num_workers: int,\n        num_preserved_experts: int,\n        seed: int = 42,\n        model_save_path: str = R\"{log_dir}/pruned_model\",\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.model_save_path = model_save_path\n        self.calib_set = calib_set\n        self.max_block_size = max_block_size\n        self.n_blocks_for_stat = n_blocks_for_stat\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.seed = seed\n        self.num_preserved_experts = num_preserved_experts\n\n    def run(self, modelpool: fb.modelpool.CausalLMPool):\n        \"\"\"\n        Args:\n            modelpool (fb.modelpool.CausalLMPool): The model pool to run the algorithm on.\n                Example Config: config/modelpool/CausalLMPool/mixtral-8x7b.yaml\n        \"\"\"\n        self.modelpool = modelpool\n        # set random seed\n        if self.seed is not None:\n            L.seed_everything(self.seed)\n        # parse model_save_path\n        self.model_save_path = self.model_save_path.format(log_dir=self.log_dir)\n\n        with self.profile(\"load model\"):\n            model = modelpool.load_pretrained_or_first_model()\n            tokenizer = modelpool.load_tokenizer()\n\n        # Load the calibration data\n        with self.profile(\"load calibration data\"):\n            calib_loader = build_calib_loader(\n                self.calib_set,\n                tokenizer=tokenizer,\n                max_block_size=self.max_block_size,\n                n_blocks_for_stat=self.n_blocks_for_stat,\n                batch_size=self.batch_size,\n                num_workers=self.num_workers,\n                seed=self.seed,\n            )\n\n        with self.profile(\"prune model\"):\n            model, info = dynamic_skipping(\n                model,\n                calib_loader,\n                batch_size=self.batch_size,\n            )\n\n        if self.model_save_path is not None:\n            with self.profile(\"save model\"):\n                modelpool.save_model(\n                    model,\n                    path=self.model_save_path,\n                    tokenizer=tokenizer,\n                )\n                torch.save(info, os.path.join(self.log_dir, \"pruning_info.pt\"))\n\n        self.print_profile_summary()\n        return model\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.DynamicSkippingPruningForMixtral.run","title":"<code>run(modelpool)</code>","text":"<p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>CausalLMPool</code>)           \u2013            <p>The model pool to run the algorithm on. Example Config: config/modelpool/CausalLMPool/mixtral-8x7b.yaml</p> </li> </ul> Source code in <code>fusion_bench/method/expert_sparsity/mixtral/dynamic_skipping.py</code> <pre><code>def run(self, modelpool: fb.modelpool.CausalLMPool):\n    \"\"\"\n    Args:\n        modelpool (fb.modelpool.CausalLMPool): The model pool to run the algorithm on.\n            Example Config: config/modelpool/CausalLMPool/mixtral-8x7b.yaml\n    \"\"\"\n    self.modelpool = modelpool\n    # set random seed\n    if self.seed is not None:\n        L.seed_everything(self.seed)\n    # parse model_save_path\n    self.model_save_path = self.model_save_path.format(log_dir=self.log_dir)\n\n    with self.profile(\"load model\"):\n        model = modelpool.load_pretrained_or_first_model()\n        tokenizer = modelpool.load_tokenizer()\n\n    # Load the calibration data\n    with self.profile(\"load calibration data\"):\n        calib_loader = build_calib_loader(\n            self.calib_set,\n            tokenizer=tokenizer,\n            max_block_size=self.max_block_size,\n            n_blocks_for_stat=self.n_blocks_for_stat,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            seed=self.seed,\n        )\n\n    with self.profile(\"prune model\"):\n        model, info = dynamic_skipping(\n            model,\n            calib_loader,\n            batch_size=self.batch_size,\n        )\n\n    if self.model_save_path is not None:\n        with self.profile(\"save model\"):\n            modelpool.save_model(\n                model,\n                path=self.model_save_path,\n                tokenizer=tokenizer,\n            )\n            torch.save(info, os.path.join(self.log_dir, \"pruning_info.pt\"))\n\n    self.print_profile_summary()\n    return model\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.ProgressivePruningForMixtral","title":"<code>ProgressivePruningForMixtral</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>LightningFabricMixin</code>, <code>SimpleProfilerMixin</code></p> Source code in <code>fusion_bench/method/expert_sparsity/mixtral/progressive_pruning.py</code> <pre><code>@auto_register_config\nclass ProgressivePruningForMixtral(\n    fb.BaseAlgorithm,\n    fb.mixins.LightningFabricMixin,\n    fb.mixins.SimpleProfilerMixin,\n):\n    modelpool: fb.modelpool.CausalLMPool\n\n    def __init__(\n        self,\n        calib_set: str,\n        max_block_size: int,\n        n_blocks_for_stat: int,\n        batch_size: int,\n        num_workers: int,\n        num_preserved_experts: int,\n        seed: int = 42,\n        model_save_path: str = R\"{log_dir}/pruned_model\",\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.model_save_path = model_save_path\n        self.calib_set = calib_set\n        self.max_block_size = max_block_size\n        self.n_blocks_for_stat = n_blocks_for_stat\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.seed = seed\n        self.num_preserved_experts = num_preserved_experts\n\n    def run(self, modelpool: fb.modelpool.CausalLMPool):\n        \"\"\"\n        Args:\n            modelpool (fb.modelpool.CausalLMPool): The model pool to run the algorithm on.\n                Example Config: config/modelpool/CausalLMPool/mixtral-8x7b.yaml\n        \"\"\"\n        self.modelpool = modelpool\n        # set random seed\n        if self.seed is not None:\n            L.seed_everything(self.seed)\n        # parse model_save_path\n        self.model_save_path = self.model_save_path.format(log_dir=self.log_dir)\n\n        with self.profile(\"load model\"):\n            model = modelpool.load_pretrained_or_first_model()\n            tokenizer = modelpool.load_tokenizer()\n\n        # Load the calibration data\n        with self.profile(\"load calibration data\"):\n            calib_loader = build_calib_loader(\n                self.calib_set,\n                tokenizer=tokenizer,\n                max_block_size=self.max_block_size,\n                n_blocks_for_stat=self.n_blocks_for_stat,\n                batch_size=self.batch_size,\n                num_workers=self.num_workers,\n                seed=self.seed,\n            )\n\n        with self.profile(\"prune model\"):\n            model, info = progressive_pruning(\n                model,\n                calib_loader,\n                r=self.num_preserved_experts,\n            )\n\n        if self.model_save_path is not None:\n            with self.profile(\"save model\"):\n                modelpool.save_model(\n                    model,\n                    path=self.model_save_path,\n                    tokenizer=tokenizer,\n                )\n                torch.save(info, os.path.join(self.log_dir, \"pruning_info.pt\"))\n\n        self.print_profile_summary()\n        return model\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.ProgressivePruningForMixtral.run","title":"<code>run(modelpool)</code>","text":"<p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>CausalLMPool</code>)           \u2013            <p>The model pool to run the algorithm on. Example Config: config/modelpool/CausalLMPool/mixtral-8x7b.yaml</p> </li> </ul> Source code in <code>fusion_bench/method/expert_sparsity/mixtral/progressive_pruning.py</code> <pre><code>def run(self, modelpool: fb.modelpool.CausalLMPool):\n    \"\"\"\n    Args:\n        modelpool (fb.modelpool.CausalLMPool): The model pool to run the algorithm on.\n            Example Config: config/modelpool/CausalLMPool/mixtral-8x7b.yaml\n    \"\"\"\n    self.modelpool = modelpool\n    # set random seed\n    if self.seed is not None:\n        L.seed_everything(self.seed)\n    # parse model_save_path\n    self.model_save_path = self.model_save_path.format(log_dir=self.log_dir)\n\n    with self.profile(\"load model\"):\n        model = modelpool.load_pretrained_or_first_model()\n        tokenizer = modelpool.load_tokenizer()\n\n    # Load the calibration data\n    with self.profile(\"load calibration data\"):\n        calib_loader = build_calib_loader(\n            self.calib_set,\n            tokenizer=tokenizer,\n            max_block_size=self.max_block_size,\n            n_blocks_for_stat=self.n_blocks_for_stat,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            seed=self.seed,\n        )\n\n    with self.profile(\"prune model\"):\n        model, info = progressive_pruning(\n            model,\n            calib_loader,\n            r=self.num_preserved_experts,\n        )\n\n    if self.model_save_path is not None:\n        with self.profile(\"save model\"):\n            modelpool.save_model(\n                model,\n                path=self.model_save_path,\n                tokenizer=tokenizer,\n            )\n            torch.save(info, os.path.join(self.log_dir, \"pruning_info.pt\"))\n\n    self.print_profile_summary()\n    return model\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.LayerWisePruningForMixtral","title":"<code>LayerWisePruningForMixtral</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>LightningFabricMixin</code>, <code>SimpleProfilerMixin</code></p> Source code in <code>fusion_bench/method/expert_sparsity/mixtral/layer_wise_pruning.py</code> <pre><code>@auto_register_config\nclass LayerWisePruningForMixtral(\n    fb.BaseAlgorithm,\n    fb.mixins.LightningFabricMixin,\n    fb.mixins.SimpleProfilerMixin,\n):\n    modelpool: fb.modelpool.CausalLMPool\n\n    def __init__(\n        self,\n        calib_set: str,\n        max_block_size: int,\n        n_blocks_for_stat: int,\n        batch_size: int,\n        num_workers: int,\n        num_preserved_experts: int,\n        seed: int = 42,\n        model_save_path: str = R\"{log_dir}/pruned_model\",\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.model_save_path = model_save_path\n        self.calib_set = calib_set\n        self.max_block_size = max_block_size\n        self.n_blocks_for_stat = n_blocks_for_stat\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.seed = seed\n        self.num_preserved_experts = num_preserved_experts\n\n    def run(self, modelpool: fb.modelpool.CausalLMPool):\n        \"\"\"\n        Args:\n            modelpool (fb.modelpool.CausalLMPool): The model pool to run the algorithm on.\n                Example Config: config/modelpool/CausalLMPool/mixtral-8x7b.yaml\n        \"\"\"\n        self.modelpool = modelpool\n        # set random seed\n        if self.seed is not None:\n            L.seed_everything(self.seed)\n        # parse model_save_path\n        self.model_save_path = self.model_save_path.format(log_dir=self.log_dir)\n\n        with self.profile(\"load model\"):\n            model = modelpool.load_pretrained_or_first_model()\n            tokenizer = modelpool.load_tokenizer()\n\n        # Load the calibration data\n        with self.profile(\"load calibration data\"):\n            calib_loader = build_calib_loader(\n                self.calib_set,\n                tokenizer=tokenizer,\n                max_block_size=self.max_block_size,\n                n_blocks_for_stat=self.n_blocks_for_stat,\n                batch_size=self.batch_size,\n                num_workers=self.num_workers,\n                seed=self.seed,\n            )\n\n        with self.profile(\"prune model\"):\n            model, info = layerwise_pruning(\n                model,\n                calib_loader,\n                r=self.num_preserved_experts,\n            )\n\n        if self.model_save_path is not None:\n            with self.profile(\"save model\"):\n                modelpool.save_model(\n                    model,\n                    path=self.model_save_path,\n                    tokenizer=tokenizer,\n                )\n                torch.save(info, os.path.join(self.log_dir, \"pruning_info.pt\"))\n\n        self.print_profile_summary()\n        return model\n</code></pre>"},{"location":"api/fusion_bench.method/compression/#fusion_bench.method.LayerWisePruningForMixtral.run","title":"<code>run(modelpool)</code>","text":"<p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>CausalLMPool</code>)           \u2013            <p>The model pool to run the algorithm on. Example Config: config/modelpool/CausalLMPool/mixtral-8x7b.yaml</p> </li> </ul> Source code in <code>fusion_bench/method/expert_sparsity/mixtral/layer_wise_pruning.py</code> <pre><code>def run(self, modelpool: fb.modelpool.CausalLMPool):\n    \"\"\"\n    Args:\n        modelpool (fb.modelpool.CausalLMPool): The model pool to run the algorithm on.\n            Example Config: config/modelpool/CausalLMPool/mixtral-8x7b.yaml\n    \"\"\"\n    self.modelpool = modelpool\n    # set random seed\n    if self.seed is not None:\n        L.seed_everything(self.seed)\n    # parse model_save_path\n    self.model_save_path = self.model_save_path.format(log_dir=self.log_dir)\n\n    with self.profile(\"load model\"):\n        model = modelpool.load_pretrained_or_first_model()\n        tokenizer = modelpool.load_tokenizer()\n\n    # Load the calibration data\n    with self.profile(\"load calibration data\"):\n        calib_loader = build_calib_loader(\n            self.calib_set,\n            tokenizer=tokenizer,\n            max_block_size=self.max_block_size,\n            n_blocks_for_stat=self.n_blocks_for_stat,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            seed=self.seed,\n        )\n\n    with self.profile(\"prune model\"):\n        model, info = layerwise_pruning(\n            model,\n            calib_loader,\n            r=self.num_preserved_experts,\n        )\n\n    if self.model_save_path is not None:\n        with self.profile(\"save model\"):\n            modelpool.save_model(\n                model,\n                path=self.model_save_path,\n                tokenizer=tokenizer,\n            )\n            torch.save(info, os.path.join(self.log_dir, \"pruning_info.pt\"))\n\n    self.print_profile_summary()\n    return model\n</code></pre>"},{"location":"api/fusion_bench.method/ensemble/","title":"Model Ensemble","text":""},{"location":"api/fusion_bench.method/ensemble/#fusion_bench.method.SimpleEnsembleAlgorithm","title":"<code>SimpleEnsembleAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/ensemble.py</code> <pre><code>@auto_register_config\nclass SimpleEnsembleAlgorithm(BaseAlgorithm):\n    def __init__(\n        self,\n        device_map: Optional[Mapping[int, Union[str, torch.device]]] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the SimpleEnsembleAlgorithm with an optional device map.\n\n        Args:\n            device_map (Optional[Mapping[int, Union[str, torch.device]]], optional): A mapping from model index to device. Defaults to None.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool | List[nn.Module]) -&gt; EnsembleModule:\n        \"\"\"\n        Run the simple ensemble algorithm on the given model pool.\n\n        Args:\n            modelpool (BaseModelPool | List[nn.Module]): The pool of models to ensemble.\n\n        Returns:\n            EnsembleModule: The ensembled model.\n        \"\"\"\n        log.info(f\"Running ensemble algorithm with {len(modelpool)} models\")\n        models = [modelpool.load_model(m) for m in modelpool.model_names]\n\n        log.info(\"creating ensemble module\")\n        ensemble = EnsembleModule(models=models, device_map=self.device_map)\n        return ensemble\n</code></pre>"},{"location":"api/fusion_bench.method/ensemble/#fusion_bench.method.SimpleEnsembleAlgorithm.__init__","title":"<code>__init__(device_map=None, **kwargs)</code>","text":"<p>Initializes the SimpleEnsembleAlgorithm with an optional device map.</p> <p>Parameters:</p> <ul> <li> <code>device_map</code>               (<code>Optional[Mapping[int, Union[str, device]]]</code>, default:                   <code>None</code> )           \u2013            <p>A mapping from model index to device. Defaults to None.</p> </li> </ul> Source code in <code>fusion_bench/method/ensemble.py</code> <pre><code>def __init__(\n    self,\n    device_map: Optional[Mapping[int, Union[str, torch.device]]] = None,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the SimpleEnsembleAlgorithm with an optional device map.\n\n    Args:\n        device_map (Optional[Mapping[int, Union[str, torch.device]]], optional): A mapping from model index to device. Defaults to None.\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/ensemble/#fusion_bench.method.SimpleEnsembleAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Run the simple ensemble algorithm on the given model pool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool | List[Module]</code>)           \u2013            <p>The pool of models to ensemble.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>EnsembleModule</code> (              <code>EnsembleModule</code> )          \u2013            <p>The ensembled model.</p> </li> </ul> Source code in <code>fusion_bench/method/ensemble.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool | List[nn.Module]) -&gt; EnsembleModule:\n    \"\"\"\n    Run the simple ensemble algorithm on the given model pool.\n\n    Args:\n        modelpool (BaseModelPool | List[nn.Module]): The pool of models to ensemble.\n\n    Returns:\n        EnsembleModule: The ensembled model.\n    \"\"\"\n    log.info(f\"Running ensemble algorithm with {len(modelpool)} models\")\n    models = [modelpool.load_model(m) for m in modelpool.model_names]\n\n    log.info(\"creating ensemble module\")\n    ensemble = EnsembleModule(models=models, device_map=self.device_map)\n    return ensemble\n</code></pre>"},{"location":"api/fusion_bench.method/ensemble/#fusion_bench.method.WeightedEnsembleAlgorithm","title":"<code>WeightedEnsembleAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/ensemble.py</code> <pre><code>@auto_register_config\nclass WeightedEnsembleAlgorithm(BaseAlgorithm):\n\n    def __init__(\n        self,\n        normalize: bool = True,\n        weights: Optional[List[float]] = None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool | List[nn.Module]) -&gt; WeightedEnsembleModule:\n        \"\"\"\n        Run the weighted ensemble algorithm on the given model pool.\n\n        Args:\n            modelpool (BaseModelPool | List[nn.Module]): The pool of models to ensemble.\n\n        Returns:\n            WeightedEnsembleModule: The weighted ensembled model.\n        \"\"\"\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(models=modelpool)\n\n        log.info(f\"Running weighted ensemble algorithm with {len(modelpool)} models\")\n\n        models = [modelpool.load_model(m) for m in modelpool.model_names]\n        if self.weights is None:\n            weights = np.ones(len(models)) / len(models)\n        else:\n            weights = self.weights\n        ensemble = WeightedEnsembleModule(\n            models,\n            weights=weights,\n            normalize=self.config.get(\"normalize\", True),\n        )\n        return ensemble\n</code></pre>"},{"location":"api/fusion_bench.method/ensemble/#fusion_bench.method.WeightedEnsembleAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Run the weighted ensemble algorithm on the given model pool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool | List[Module]</code>)           \u2013            <p>The pool of models to ensemble.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>WeightedEnsembleModule</code> (              <code>WeightedEnsembleModule</code> )          \u2013            <p>The weighted ensembled model.</p> </li> </ul> Source code in <code>fusion_bench/method/ensemble.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool | List[nn.Module]) -&gt; WeightedEnsembleModule:\n    \"\"\"\n    Run the weighted ensemble algorithm on the given model pool.\n\n    Args:\n        modelpool (BaseModelPool | List[nn.Module]): The pool of models to ensemble.\n\n    Returns:\n        WeightedEnsembleModule: The weighted ensembled model.\n    \"\"\"\n    if not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(models=modelpool)\n\n    log.info(f\"Running weighted ensemble algorithm with {len(modelpool)} models\")\n\n    models = [modelpool.load_model(m) for m in modelpool.model_names]\n    if self.weights is None:\n        weights = np.ones(len(models)) / len(models)\n    else:\n        weights = self.weights\n    ensemble = WeightedEnsembleModule(\n        models,\n        weights=weights,\n        normalize=self.config.get(\"normalize\", True),\n    )\n    return ensemble\n</code></pre>"},{"location":"api/fusion_bench.method/ensemble/#fusion_bench.method.MaxModelPredictorAlgorithm","title":"<code>MaxModelPredictorAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/ensemble.py</code> <pre><code>class MaxModelPredictorAlgorithm(BaseAlgorithm):\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool | List[nn.Module]) -&gt; MaxModelPredictor:\n        \"\"\"\n        Run the max model predictor algorithm on the given model pool.\n\n        Args:\n            modelpool (BaseModelPool | List[nn.Module]): The pool of models to ensemble.\n\n        Returns:\n            MaxModelPredictor: The max model predictor ensembled model.\n        \"\"\"\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(models=modelpool)\n\n        log.info(f\"Running max predictor algorithm with {len(modelpool)} models\")\n\n        models = [modelpool.load_model(m) for m in modelpool.model_names]\n        ensemble = MaxModelPredictor(models=models)\n        return ensemble\n</code></pre>"},{"location":"api/fusion_bench.method/ensemble/#fusion_bench.method.MaxModelPredictorAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Run the max model predictor algorithm on the given model pool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool | List[Module]</code>)           \u2013            <p>The pool of models to ensemble.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MaxModelPredictor</code> (              <code>MaxModelPredictor</code> )          \u2013            <p>The max model predictor ensembled model.</p> </li> </ul> Source code in <code>fusion_bench/method/ensemble.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool | List[nn.Module]) -&gt; MaxModelPredictor:\n    \"\"\"\n    Run the max model predictor algorithm on the given model pool.\n\n    Args:\n        modelpool (BaseModelPool | List[nn.Module]): The pool of models to ensemble.\n\n    Returns:\n        MaxModelPredictor: The max model predictor ensembled model.\n    \"\"\"\n    if not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(models=modelpool)\n\n    log.info(f\"Running max predictor algorithm with {len(modelpool)} models\")\n\n    models = [modelpool.load_model(m) for m in modelpool.model_names]\n    ensemble = MaxModelPredictor(models=models)\n    return ensemble\n</code></pre>"},{"location":"api/fusion_bench.method/merging/","title":"Model Merging","text":""},{"location":"api/fusion_bench.method/merging/#linear-interpolation","title":"Linear Interpolation","text":""},{"location":"api/fusion_bench.method/merging/#simple-average","title":"Simple Average","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.simple_average.simple_average","title":"<code>simple_average(modules, base_module=None)</code>","text":"<p>Averages the parameters of a list of PyTorch modules or state dictionaries.</p> <p>This function takes a list of PyTorch modules or state dictionaries and returns a new module with the averaged parameters, or a new state dictionary with the averaged parameters.</p> <p>Parameters:</p> <ul> <li> <code>modules</code>               (<code>List[Union[Module, StateDictType]]</code>)           \u2013            <p>A list of PyTorch modules or state dictionaries.</p> </li> <li> <code>base_module</code>               (<code>Optional[Module]</code>, default:                   <code>None</code> )           \u2013            <p>A base module to use for the new module. If provided, the averaged parameters will be loaded into this module. If not provided, a new module will be created by copying the first module in the list.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>module_or_state_dict</code> (              <code>Union[Module, StateDictType]</code> )          \u2013            <p>A new PyTorch module with the averaged parameters, or a new state dictionary with the averaged parameters.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; model1 = nn.Linear(10, 10)\n&gt;&gt;&gt; model2 = nn.Linear(10, 10)\n&gt;&gt;&gt; averaged_model = simple_average([model1, model2])\n</code></pre> <pre><code>&gt;&gt;&gt; state_dict1 = model1.state_dict()\n&gt;&gt;&gt; state_dict2 = model2.state_dict()\n&gt;&gt;&gt; averaged_state_dict = simple_average([state_dict1, state_dict2])\n</code></pre> Source code in <code>fusion_bench/method/simple_average.py</code> <pre><code>def simple_average(\n    modules: List[Union[nn.Module, StateDictType]],\n    base_module: Optional[nn.Module] = None,\n):\n    R\"\"\"\n    Averages the parameters of a list of PyTorch modules or state dictionaries.\n\n    This function takes a list of PyTorch modules or state dictionaries and returns a new module with the averaged parameters, or a new state dictionary with the averaged parameters.\n\n    Args:\n        modules (List[Union[nn.Module, StateDictType]]): A list of PyTorch modules or state dictionaries.\n        base_module (Optional[nn.Module]): A base module to use for the new module. If provided, the averaged parameters will be loaded into this module. If not provided, a new module will be created by copying the first module in the list.\n\n    Returns:\n        module_or_state_dict (Union[nn.Module, StateDictType]): A new PyTorch module with the averaged parameters, or a new state dictionary with the averaged parameters.\n\n    Examples:\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt; model1 = nn.Linear(10, 10)\n        &gt;&gt;&gt; model2 = nn.Linear(10, 10)\n        &gt;&gt;&gt; averaged_model = simple_average([model1, model2])\n\n        &gt;&gt;&gt; state_dict1 = model1.state_dict()\n        &gt;&gt;&gt; state_dict2 = model2.state_dict()\n        &gt;&gt;&gt; averaged_state_dict = simple_average([state_dict1, state_dict2])\n    \"\"\"\n    assert len(modules) &gt; 0, \"modules must be a non-empty list\"\n    if isinstance(modules[0], nn.Module):\n        if base_module is None:\n            new_module = deepcopy(modules[0])\n        else:\n            new_module = base_module\n        state_dict = state_dict_avg([module.state_dict() for module in modules])\n        new_module.load_state_dict(state_dict)\n        return new_module\n    elif isinstance(modules[0], Mapping):\n        return state_dict_avg(modules)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.SimpleAverageForLlama","title":"<code>SimpleAverageForLlama = SimpleAverageForCausalLM</code>  <code>module-attribute</code>","text":"<p>Alias for SimpleAverageForCausalLM</p>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.SimpleAverageAlgorithm","title":"<code>SimpleAverageAlgorithm</code>","text":"<p>               Bases: <code>SimpleProfilerMixin</code>, <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/simple_average.py</code> <pre><code>@auto_register_config\nclass SimpleAverageAlgorithm(\n    SimpleProfilerMixin,\n    BaseAlgorithm,\n):\n    def __init__(self, show_pbar: bool = False, **kwargs):\n        \"\"\"\n        Args:\n            show_pbar (bool): If True, shows a progress bar during model loading and merging. Default is False.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(self, modelpool: Union[BaseModelPool, Dict[str, nn.Module]]) -&gt; nn.Module:\n        \"\"\"\n        Fuse the models in the given model pool using simple averaging.\n\n        This method iterates over the names of the models in the model pool, loads each model, and appends it to a list.\n        It then returns the simple average of the models in the list.\n\n        Args:\n            modelpool: The pool of models to fuse.\n\n        Returns:\n            The fused model obtained by simple averaging.\n        \"\"\"\n        if isinstance(modelpool, dict):\n            modelpool = BaseModelPool(modelpool)\n\n        log.info(\n            f\"Fusing models using simple average on {len(modelpool.model_names)} models. \"\n            f\"models: {modelpool.model_names}\"\n        )\n        sd: Optional[StateDictType] = None\n        forward_model = None\n        merged_model_names = []\n\n        for model_name in modelpool.model_names:\n            with self.profile(\"load model\"):\n                model = modelpool.load_model(model_name)\n                merged_model_names.append(model_name)\n                print(f\"load model of type: {type(model).__name__}\")\n            with self.profile(\"merge weights\"):\n                if sd is None:\n                    # Initialize the state dictionary with the first model's state dictionary\n                    sd = model.state_dict(keep_vars=True)\n                    forward_model = model\n                else:\n                    # Add the current model's state dictionary to the accumulated state dictionary\n                    sd = state_dict_add(\n                        sd, model.state_dict(keep_vars=True), show_pbar=self.show_pbar\n                    )\n        with self.profile(\"merge weights\"):\n            # Divide the accumulated state dictionary by the number of models to get the average\n            sd = state_dict_div(\n                sd, len(modelpool.model_names), show_pbar=self.show_pbar\n            )\n\n        if isinstance(forward_model, LazyStateDict):\n            # if the model is a LazyStateDict, convert it to an empty module\n            forward_model = deepcopy(forward_model.meta_module).to_empty(\n                device=forward_model._device\n            )\n        result = forward_model.load_state_dict(sd, strict=False)\n        if result.unexpected_keys:\n            raise ValueError(f\"Unexpected keys in state dict: {result.unexpected_keys}\")\n        if result.missing_keys:\n            log.warning(f\"Missing keys in state dict: {result.missing_keys}\")\n        # print profile report and log the merged models\n        self.print_profile_summary()\n        log.info(f\"merged {len(merged_model_names)} models:\")\n        for model_name in merged_model_names:\n            log.info(f\"  - {model_name}\")\n        return forward_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.SimpleAverageAlgorithm.__init__","title":"<code>__init__(show_pbar=False, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>show_pbar</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, shows a progress bar during model loading and merging. Default is False.</p> </li> </ul> Source code in <code>fusion_bench/method/simple_average.py</code> <pre><code>def __init__(self, show_pbar: bool = False, **kwargs):\n    \"\"\"\n    Args:\n        show_pbar (bool): If True, shows a progress bar during model loading and merging. Default is False.\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.SimpleAverageAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Fuse the models in the given model pool using simple averaging.</p> <p>This method iterates over the names of the models in the model pool, loads each model, and appends it to a list. It then returns the simple average of the models in the list.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>Union[BaseModelPool, Dict[str, Module]]</code>)           \u2013            <p>The pool of models to fuse.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The fused model obtained by simple averaging.</p> </li> </ul> Source code in <code>fusion_bench/method/simple_average.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: Union[BaseModelPool, Dict[str, nn.Module]]) -&gt; nn.Module:\n    \"\"\"\n    Fuse the models in the given model pool using simple averaging.\n\n    This method iterates over the names of the models in the model pool, loads each model, and appends it to a list.\n    It then returns the simple average of the models in the list.\n\n    Args:\n        modelpool: The pool of models to fuse.\n\n    Returns:\n        The fused model obtained by simple averaging.\n    \"\"\"\n    if isinstance(modelpool, dict):\n        modelpool = BaseModelPool(modelpool)\n\n    log.info(\n        f\"Fusing models using simple average on {len(modelpool.model_names)} models. \"\n        f\"models: {modelpool.model_names}\"\n    )\n    sd: Optional[StateDictType] = None\n    forward_model = None\n    merged_model_names = []\n\n    for model_name in modelpool.model_names:\n        with self.profile(\"load model\"):\n            model = modelpool.load_model(model_name)\n            merged_model_names.append(model_name)\n            print(f\"load model of type: {type(model).__name__}\")\n        with self.profile(\"merge weights\"):\n            if sd is None:\n                # Initialize the state dictionary with the first model's state dictionary\n                sd = model.state_dict(keep_vars=True)\n                forward_model = model\n            else:\n                # Add the current model's state dictionary to the accumulated state dictionary\n                sd = state_dict_add(\n                    sd, model.state_dict(keep_vars=True), show_pbar=self.show_pbar\n                )\n    with self.profile(\"merge weights\"):\n        # Divide the accumulated state dictionary by the number of models to get the average\n        sd = state_dict_div(\n            sd, len(modelpool.model_names), show_pbar=self.show_pbar\n        )\n\n    if isinstance(forward_model, LazyStateDict):\n        # if the model is a LazyStateDict, convert it to an empty module\n        forward_model = deepcopy(forward_model.meta_module).to_empty(\n            device=forward_model._device\n        )\n    result = forward_model.load_state_dict(sd, strict=False)\n    if result.unexpected_keys:\n        raise ValueError(f\"Unexpected keys in state dict: {result.unexpected_keys}\")\n    if result.missing_keys:\n        log.warning(f\"Missing keys in state dict: {result.missing_keys}\")\n    # print profile report and log the merged models\n    self.print_profile_summary()\n    log.info(f\"merged {len(merged_model_names)} models:\")\n    for model_name in merged_model_names:\n        log.info(f\"  - {model_name}\")\n    return forward_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#weighted-average","title":"Weighted Average","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.LinearInterpolationAlgorithm","title":"<code>LinearInterpolationAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p><code>LinearInterpolationAlgorithm</code> performs linear interpolation between two models. Returns a model with the state dict that is a linear interpolation of the state dicts of the two models. \\(\\theta = (1-t) \\theta_1 + t \\theta_2\\)</p> Source code in <code>fusion_bench/method/linear/linear_interpolation.py</code> <pre><code>class LinearInterpolationAlgorithm(BaseAlgorithm):\n    R\"\"\"\n    `LinearInterpolationAlgorithm` performs linear interpolation between two models.\n    Returns a model with the state dict that is a linear interpolation of the state dicts of the two models.\n    $\\theta = (1-t) \\theta_1 + t \\theta_2$\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"t\": \"t\",\n    }\n\n    def __init__(self, t: float, **kwargs: Any):\n        \"\"\"\n        Initialize the `LinearInterpolationAlgorithm` with the given interpolation parameter.\n\n        Args:\n            t (float): The interpolation parameter, should be in the range [0, 1].\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        assert 0 &lt;= t &lt;= 1, \"t should be in the range [0, 1]\"\n        self.t = t\n        super().__init__(**kwargs)\n\n    def run(self, modelpool: BaseModelPool) -&gt; nn.Module:\n        \"\"\"\n        Run the linear interpolation algorithm on the given model pool.\n\n        This method performs linear interpolation between two models in the model pool\n        and returns a model with the interpolated state dict.\n\n        Args:\n            modelpool (BaseModelPool): The pool of models to interpolate. Must contain exactly two models.\n\n        Returns:\n            nn.Module: The model with the interpolated state dict.\n        \"\"\"\n        assert (\n            modelpool.all_model_names == 2\n        ), \"linear interpolation expect exactly 2 models\"\n        primary_model = modelpool.load_model(modelpool.all_model_names[0])\n        secondary_model = modelpool.load_model(modelpool.all_model_names[1])\n\n        with torch.no_grad():\n            primary_state_dict = primary_model.state_dict()\n            secondary_state_dict = secondary_model.state_dict()\n            state_dict = state_dict_weighted_sum(\n                [primary_state_dict, secondary_state_dict], [1 - self.t, self.t]\n            )\n        primary_model.load_state_dict(state_dict)\n        return primary_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.LinearInterpolationAlgorithm.__init__","title":"<code>__init__(t, **kwargs)</code>","text":"<p>Initialize the <code>LinearInterpolationAlgorithm</code> with the given interpolation parameter.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>float</code>)           \u2013            <p>The interpolation parameter, should be in the range [0, 1].</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments.</p> </li> </ul> Source code in <code>fusion_bench/method/linear/linear_interpolation.py</code> <pre><code>def __init__(self, t: float, **kwargs: Any):\n    \"\"\"\n    Initialize the `LinearInterpolationAlgorithm` with the given interpolation parameter.\n\n    Args:\n        t (float): The interpolation parameter, should be in the range [0, 1].\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    assert 0 &lt;= t &lt;= 1, \"t should be in the range [0, 1]\"\n    self.t = t\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.LinearInterpolationAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Run the linear interpolation algorithm on the given model pool.</p> <p>This method performs linear interpolation between two models in the model pool and returns a model with the interpolated state dict.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool</code>)           \u2013            <p>The pool of models to interpolate. Must contain exactly two models.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: The model with the interpolated state dict.</p> </li> </ul> Source code in <code>fusion_bench/method/linear/linear_interpolation.py</code> <pre><code>def run(self, modelpool: BaseModelPool) -&gt; nn.Module:\n    \"\"\"\n    Run the linear interpolation algorithm on the given model pool.\n\n    This method performs linear interpolation between two models in the model pool\n    and returns a model with the interpolated state dict.\n\n    Args:\n        modelpool (BaseModelPool): The pool of models to interpolate. Must contain exactly two models.\n\n    Returns:\n        nn.Module: The model with the interpolated state dict.\n    \"\"\"\n    assert (\n        modelpool.all_model_names == 2\n    ), \"linear interpolation expect exactly 2 models\"\n    primary_model = modelpool.load_model(modelpool.all_model_names[0])\n    secondary_model = modelpool.load_model(modelpool.all_model_names[1])\n\n    with torch.no_grad():\n        primary_state_dict = primary_model.state_dict()\n        secondary_state_dict = secondary_model.state_dict()\n        state_dict = state_dict_weighted_sum(\n            [primary_state_dict, secondary_state_dict], [1 - self.t, self.t]\n        )\n    primary_model.load_state_dict(state_dict)\n    return primary_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.WeightedAverageAlgorithm","title":"<code>WeightedAverageAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code></p> Source code in <code>fusion_bench/method/weighted_average/weighted_average.py</code> <pre><code>class WeightedAverageAlgorithm(BaseAlgorithm, SimpleProfilerMixin):\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"normalize\": \"normalize\",\n        \"weights\": \"weights\",\n    }\n\n    def __init__(\n        self,\n        normalize: bool,\n        weights: List[float],\n        verbose: bool = True,\n        **kwargs,\n    ):\n        self.normalize = normalize\n        self.weights = weights\n        self.verbose = verbose\n        log.disabled = not self.verbose\n        super().__init__(**kwargs)\n\n    @override\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool):\n        \"\"\"\n        Fuses the models in the model pool using a weighted average approach.\n\n        Parameters\n            modelpool (ModelPool): The pool of models to be fused.\n\n        Raises\n            ValueError: If the number of weights does not match the number of models in the model pool.\n\n        Returns\n            forward_model (torch.nn.Module): The resulting model after fusion.\n        \"\"\"\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n\n        log.info(\"Fusing models using weighted average.\")\n        weights = np.asarray(self.weights)\n        if len(weights) != len(modelpool.model_names):\n            raise ValueError(\n                \"Number of weights must match the number of models.,\"\n                f\"but got {len(weights)} weights and {len(modelpool.model_names)} models.\"\n                f\"weights: {weights}, models: {modelpool.model_names}\"\n            )\n        if self.normalize:\n            weights = weights / np.sum(weights)\n        if self.verbose:\n            print(f\"weights: {weights}, normalized: {self.normalize}\")\n\n        sd: Optional[StateDictType] = None\n        forward_model = None\n\n        for model_name, weight in zip(modelpool.model_names, weights):\n            with self.profile(\"load_model\"):\n                model = modelpool.load_model(model_name)\n            with self.profile(\"merge weights\"):\n                if sd is None:\n                    sd = state_dict_mul(model.state_dict(keep_vars=True), weight)\n                    forward_model = model\n                else:\n                    sd = state_dict_add(\n                        sd, state_dict_mul(model.state_dict(keep_vars=True), weight)\n                    )\n\n        forward_model.load_state_dict(sd)\n        if self.verbose:\n            self.print_profile_summary()\n        return forward_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.WeightedAverageAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Fuses the models in the model pool using a weighted average approach.</p> <p>Parameters     modelpool (ModelPool): The pool of models to be fused.</p> <p>Raises     ValueError: If the number of weights does not match the number of models in the model pool.</p> <p>Returns     forward_model (torch.nn.Module): The resulting model after fusion.</p> Source code in <code>fusion_bench/method/weighted_average/weighted_average.py</code> <pre><code>@override\n@torch.no_grad()\ndef run(self, modelpool: BaseModelPool):\n    \"\"\"\n    Fuses the models in the model pool using a weighted average approach.\n\n    Parameters\n        modelpool (ModelPool): The pool of models to be fused.\n\n    Raises\n        ValueError: If the number of weights does not match the number of models in the model pool.\n\n    Returns\n        forward_model (torch.nn.Module): The resulting model after fusion.\n    \"\"\"\n    if not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(modelpool)\n\n    log.info(\"Fusing models using weighted average.\")\n    weights = np.asarray(self.weights)\n    if len(weights) != len(modelpool.model_names):\n        raise ValueError(\n            \"Number of weights must match the number of models.,\"\n            f\"but got {len(weights)} weights and {len(modelpool.model_names)} models.\"\n            f\"weights: {weights}, models: {modelpool.model_names}\"\n        )\n    if self.normalize:\n        weights = weights / np.sum(weights)\n    if self.verbose:\n        print(f\"weights: {weights}, normalized: {self.normalize}\")\n\n    sd: Optional[StateDictType] = None\n    forward_model = None\n\n    for model_name, weight in zip(modelpool.model_names, weights):\n        with self.profile(\"load_model\"):\n            model = modelpool.load_model(model_name)\n        with self.profile(\"merge weights\"):\n            if sd is None:\n                sd = state_dict_mul(model.state_dict(keep_vars=True), weight)\n                forward_model = model\n            else:\n                sd = state_dict_add(\n                    sd, state_dict_mul(model.state_dict(keep_vars=True), weight)\n                )\n\n    forward_model.load_state_dict(sd)\n    if self.verbose:\n        self.print_profile_summary()\n    return forward_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.WeightedAverageForLLama","title":"<code>WeightedAverageForLLama</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>A class to perform weighted averaging of LlaMa/Mistral models.</p> Source code in <code>fusion_bench/method/weighted_average/llama.py</code> <pre><code>@auto_register_config\nclass WeightedAverageForLLama(BaseAlgorithm):\n    \"\"\"\n    A class to perform weighted averaging of LlaMa/Mistral models.\n    \"\"\"\n\n    def __init__(\n        self,\n        normalize: bool,\n        weights: List[float],\n        backbone_only: bool,\n        merged_model_save_path: str,\n        save_tokenizer: bool,\n        push_to_hub: bool,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the WeightedAverageForLLama class with the given parameters.\n\n        Args:\n            normalize (bool): Whether to normalize the weights.\n            weights (List[float]): The weights for averaging the models.\n            backbone_only (bool): Whether to use only the backbone of the models.\n            merged_model_save_path (str): The path to save the merged model.\n            save_tokenizer (bool): Whether to save the tokenizer.\n            push_to_hub (bool): Whether to push the model to the hub.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    @override\n    @torch.no_grad()\n    def run(self, modelpool: CausalLMPool) -&gt; PreTrainedModel:\n        \"\"\"\n        Executes the weighted averaging of models in the provided model pool.\n\n        Args:\n            modelpool (LLamaForCausalLMPoolThe):  pool of models to be averaged.\n\n        Returns:\n            base_model: The base model after merging the state dictionaries of the models in the pool.\n\n        Raises:\n            ValueError: If the number of weights does not match the number of models in the pool.\n        \"\"\"\n        if modelpool.has_pretrained:\n            base_model = modelpool.load_model(\"_pretrained_\")\n        else:\n            base_model = modelpool.load_model(modelpool.model_names[0])\n\n        weights = self.weights\n        if len(weights) != len(modelpool.model_names):\n            raise ValueError(\n                \"Number of weights must match the number of models.,\"\n                f\"but got {len(weights)} weights and {len(modelpool.model_names)} models.\"\n                f\"weights: {weights}, models: {modelpool.model_names}\"\n            )\n        if self.normalize:\n            weights = np.asarray(weights)\n            weights = weights / np.sum(weights)\n\n        merged_state_dict: StateDictType = None\n        for model_name, weight in zip(modelpool.model_names, weights):\n            model = modelpool.load_model(model_name, backbone_only=self.backbone_only)\n            sd = state_dict_mul(model.state_dict(), weight)\n            if merged_state_dict is None:\n                merged_state_dict = sd\n            else:\n                merged_state_dict = state_dict_add(merged_state_dict, sd)\n\n        base_model.load_state_dict(\n            merged_state_dict, strict=False if self.backbone_only else True\n        )\n        if self.merged_model_save_path is not None:\n            with timeit_context(\n                f\"Saving the merged model to {self.merged_model_save_path}\"\n            ):\n                modelpool.save_model(\n                    base_model,\n                    path=self.merged_model_save_path,\n                    save_tokenizer=self.save_tokenizer,\n                    push_to_hub=self.push_to_hub,\n                )\n        return base_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.WeightedAverageForLLama.__init__","title":"<code>__init__(normalize, weights, backbone_only, merged_model_save_path, save_tokenizer, push_to_hub, **kwargs)</code>","text":"<p>Initialize the WeightedAverageForLLama class with the given parameters.</p> <p>Parameters:</p> <ul> <li> <code>normalize</code>               (<code>bool</code>)           \u2013            <p>Whether to normalize the weights.</p> </li> <li> <code>weights</code>               (<code>List[float]</code>)           \u2013            <p>The weights for averaging the models.</p> </li> <li> <code>backbone_only</code>               (<code>bool</code>)           \u2013            <p>Whether to use only the backbone of the models.</p> </li> <li> <code>merged_model_save_path</code>               (<code>str</code>)           \u2013            <p>The path to save the merged model.</p> </li> <li> <code>save_tokenizer</code>               (<code>bool</code>)           \u2013            <p>Whether to save the tokenizer.</p> </li> <li> <code>push_to_hub</code>               (<code>bool</code>)           \u2013            <p>Whether to push the model to the hub.</p> </li> </ul> Source code in <code>fusion_bench/method/weighted_average/llama.py</code> <pre><code>def __init__(\n    self,\n    normalize: bool,\n    weights: List[float],\n    backbone_only: bool,\n    merged_model_save_path: str,\n    save_tokenizer: bool,\n    push_to_hub: bool,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the WeightedAverageForLLama class with the given parameters.\n\n    Args:\n        normalize (bool): Whether to normalize the weights.\n        weights (List[float]): The weights for averaging the models.\n        backbone_only (bool): Whether to use only the backbone of the models.\n        merged_model_save_path (str): The path to save the merged model.\n        save_tokenizer (bool): Whether to save the tokenizer.\n        push_to_hub (bool): Whether to push the model to the hub.\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.WeightedAverageForLLama.run","title":"<code>run(modelpool)</code>","text":"<p>Executes the weighted averaging of models in the provided model pool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>LLamaForCausalLMPoolThe</code>)           \u2013            <p>pool of models to be averaged.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>base_model</code> (              <code>PreTrainedModel</code> )          \u2013            <p>The base model after merging the state dictionaries of the models in the pool.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the number of weights does not match the number of models in the pool.</p> </li> </ul> Source code in <code>fusion_bench/method/weighted_average/llama.py</code> <pre><code>@override\n@torch.no_grad()\ndef run(self, modelpool: CausalLMPool) -&gt; PreTrainedModel:\n    \"\"\"\n    Executes the weighted averaging of models in the provided model pool.\n\n    Args:\n        modelpool (LLamaForCausalLMPoolThe):  pool of models to be averaged.\n\n    Returns:\n        base_model: The base model after merging the state dictionaries of the models in the pool.\n\n    Raises:\n        ValueError: If the number of weights does not match the number of models in the pool.\n    \"\"\"\n    if modelpool.has_pretrained:\n        base_model = modelpool.load_model(\"_pretrained_\")\n    else:\n        base_model = modelpool.load_model(modelpool.model_names[0])\n\n    weights = self.weights\n    if len(weights) != len(modelpool.model_names):\n        raise ValueError(\n            \"Number of weights must match the number of models.,\"\n            f\"but got {len(weights)} weights and {len(modelpool.model_names)} models.\"\n            f\"weights: {weights}, models: {modelpool.model_names}\"\n        )\n    if self.normalize:\n        weights = np.asarray(weights)\n        weights = weights / np.sum(weights)\n\n    merged_state_dict: StateDictType = None\n    for model_name, weight in zip(modelpool.model_names, weights):\n        model = modelpool.load_model(model_name, backbone_only=self.backbone_only)\n        sd = state_dict_mul(model.state_dict(), weight)\n        if merged_state_dict is None:\n            merged_state_dict = sd\n        else:\n            merged_state_dict = state_dict_add(merged_state_dict, sd)\n\n    base_model.load_state_dict(\n        merged_state_dict, strict=False if self.backbone_only else True\n    )\n    if self.merged_model_save_path is not None:\n        with timeit_context(\n            f\"Saving the merged model to {self.merged_model_save_path}\"\n        ):\n            modelpool.save_model(\n                base_model,\n                path=self.merged_model_save_path,\n                save_tokenizer=self.save_tokenizer,\n                push_to_hub=self.push_to_hub,\n            )\n    return base_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#spherical-linear-interpolation-slerp","title":"Spherical Linear Interpolation (SLERP)","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.SlerpMergeAlgorithm","title":"<code>SlerpMergeAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>General purpose implementation of Slerp (Spherical Linear Interpolation) for PyTorch models.</p> Source code in <code>fusion_bench/method/slerp/slerp.py</code> <pre><code>@auto_register_config\nclass SlerpMergeAlgorithm(BaseAlgorithm):\n    \"\"\"\n    General purpose implementation of Slerp (Spherical Linear Interpolation) for PyTorch models.\n    \"\"\"\n\n    def __init__(\n        self,\n        t: float,\n        DOT_THRESHOLD: float = 0.9995,\n        epsilon: float = 1e-8,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the SlerpMergeAlgorithm.\n\n        Args:\n            t (float): The interpolation parameter. Must be in the range [0, 1].\n            DOT_THRESHOLD (float, optional): The threshold for the dot product of the two vectors. Defaults to 0.9995.\n            epsilon (float, optional): The epsilon value for numerical stability. Defaults to 1e-8.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    @override\n    def run(self, modelpool: BaseModelPool) -&gt; nn.Module:\n        \"\"\"\n        Run the SlerpMergeAlgorithm on the given model pool.\n\n        Args:\n            modelpool (BaseModelPool): The pool of models to fuse.\n\n        Returns:\n            nn.Module: The fused model.\n        \"\"\"\n        assert len(modelpool.all_model_names) == 2, \"Slerp expect exactly 2 models\"\n        primary_model = modelpool.load_model(modelpool.all_model_names[0])\n        secondary_model = modelpool.load_model(modelpool.all_model_names[1])\n\n        with torch.no_grad():\n            primary_state_dict = primary_model.state_dict()\n            secondary_state_dict = secondary_model.state_dict()\n            state_dict = slerp_on_state_dicts(\n                self.t,\n                primary_state_dict,\n                secondary_state_dict,\n                DOT_THRESHOLD=self.DOT_THRESHOLD,\n                epsilon=self.epsilon,\n            )\n\n        primary_model.load_state_dict(state_dict)\n        return primary_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.SlerpMergeAlgorithm.__init__","title":"<code>__init__(t, DOT_THRESHOLD=0.9995, epsilon=1e-08, **kwargs)</code>","text":"<p>Initialize the SlerpMergeAlgorithm.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>float</code>)           \u2013            <p>The interpolation parameter. Must be in the range [0, 1].</p> </li> <li> <code>DOT_THRESHOLD</code>               (<code>float</code>, default:                   <code>0.9995</code> )           \u2013            <p>The threshold for the dot product of the two vectors. Defaults to 0.9995.</p> </li> <li> <code>epsilon</code>               (<code>float</code>, default:                   <code>1e-08</code> )           \u2013            <p>The epsilon value for numerical stability. Defaults to 1e-8.</p> </li> </ul> Source code in <code>fusion_bench/method/slerp/slerp.py</code> <pre><code>def __init__(\n    self,\n    t: float,\n    DOT_THRESHOLD: float = 0.9995,\n    epsilon: float = 1e-8,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the SlerpMergeAlgorithm.\n\n    Args:\n        t (float): The interpolation parameter. Must be in the range [0, 1].\n        DOT_THRESHOLD (float, optional): The threshold for the dot product of the two vectors. Defaults to 0.9995.\n        epsilon (float, optional): The epsilon value for numerical stability. Defaults to 1e-8.\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.SlerpMergeAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Run the SlerpMergeAlgorithm on the given model pool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool</code>)           \u2013            <p>The pool of models to fuse.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: The fused model.</p> </li> </ul> Source code in <code>fusion_bench/method/slerp/slerp.py</code> <pre><code>@override\ndef run(self, modelpool: BaseModelPool) -&gt; nn.Module:\n    \"\"\"\n    Run the SlerpMergeAlgorithm on the given model pool.\n\n    Args:\n        modelpool (BaseModelPool): The pool of models to fuse.\n\n    Returns:\n        nn.Module: The fused model.\n    \"\"\"\n    assert len(modelpool.all_model_names) == 2, \"Slerp expect exactly 2 models\"\n    primary_model = modelpool.load_model(modelpool.all_model_names[0])\n    secondary_model = modelpool.load_model(modelpool.all_model_names[1])\n\n    with torch.no_grad():\n        primary_state_dict = primary_model.state_dict()\n        secondary_state_dict = secondary_model.state_dict()\n        state_dict = slerp_on_state_dicts(\n            self.t,\n            primary_state_dict,\n            secondary_state_dict,\n            DOT_THRESHOLD=self.DOT_THRESHOLD,\n            epsilon=self.epsilon,\n        )\n\n    primary_model.load_state_dict(state_dict)\n    return primary_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.SlerpForCausalLM","title":"<code>SlerpForCausalLM</code>","text":"<p>               Bases: <code>SimpleProfilerMixin</code>, <code>BaseAlgorithm</code></p> <p>Slerp (Spherical Linear Interpolation) for Causal Language Models.</p> Source code in <code>fusion_bench/method/slerp/slerp.py</code> <pre><code>@auto_register_config\nclass SlerpForCausalLM(\n    SimpleProfilerMixin,\n    BaseAlgorithm,\n):\n    \"\"\"\n    Slerp (Spherical Linear Interpolation) for Causal Language Models.\n    \"\"\"\n\n    def __init__(\n        self,\n        t: float,\n        DOT_THRESHOLD: float = 0.9995,\n        epsilon: float = 1e-8,\n        model_save_path: Optional[str] = None,\n        show_pbar: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the SlerpForCausalLM algorithm.\n\n        Args:\n            t (float): The interpolation parameter. Must be in the range [0, 1].\n                      t=0 returns the first model, t=1 returns the second model,\n                      t=0.5 provides balanced interpolation.\n            DOT_THRESHOLD (float, optional): The threshold for the dot product of normalized vectors.\n                                           When the absolute dot product exceeds this threshold,\n                                           vectors are considered nearly collinear and linear\n                                           interpolation (LERP) is used instead of SLERP for\n                                           numerical stability. Defaults to 0.9995.\n            epsilon (float, optional): Small value used for numerical stability to avoid\n                                     division by zero during vector normalization.\n                                     Defaults to 1e-8.\n            model_save_path (Optional[str], optional): Path where the merged model should be saved.\n                                                     If None, the model is not saved to disk.\n                                                     Defaults to None.\n            show_pbar (bool, optional): Whether to display a progress bar during the interpolation\n                                      process. Useful for debugging or monitoring progress with\n                                      large models. Defaults to False.\n            **kwargs: Additional keyword arguments passed to the parent BaseAlgorithm class.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    @override\n    def run(self, modelpool: CausalLMPool):\n        assert len(modelpool.all_model_names) == 2, \"Slerp expect exactly 2 models\"\n        primary_model = modelpool.load_model(modelpool.all_model_names[0])\n        secondary_model = modelpool.load_model(modelpool.all_model_names[1])\n\n        with torch.no_grad():\n            primary_state_dict = primary_model.state_dict()\n            secondary_state_dict = secondary_model.state_dict()\n            state_dict = slerp_on_state_dicts(\n                self.t,\n                primary_state_dict,\n                secondary_state_dict,\n                DOT_THRESHOLD=self.DOT_THRESHOLD,\n                epsilon=self.epsilon,\n            )\n\n        if isinstance(primary_model, nn.Module):\n            model = primary_model\n            model.load_state_dict(state_dict)\n        elif isinstance(primary_model, LazyStateDict):\n            model: \"PreTrainedModel\" = deepcopy(primary_model.meta_module)\n            model.to(device=primary_model._device)\n            model.load_state_dict(state_dict)\n        else:\n            raise TypeError(\n                f\"Unsupported model type: {type(primary_model)}. \"\n                \"Expected nn.Module or LazyStateDict.\"\n            )\n        if self.model_save_path is not None:\n            with timeit_context(f\"Saving the model to {self.model_save_path}\"):\n                tokenizer = modelpool.load_tokenizer()\n                tokenizer.save_pretrained(self.model_save_path)\n                model.save_pretrained(self.model_save_path)\n                model_card_str = create_default_model_card(\n                    models=[modelpool.get_model_path(m) for m in modelpool.model_names],\n                    description=\"Merged model using Slerp.\",\n                    algorithm_config=self.config,\n                    modelpool_config=modelpool.config,\n                )\n                with open(os.path.join(self.model_save_path, \"README.md\"), \"w\") as f:\n                    f.write(model_card_str)\n        return model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.SlerpForCausalLM.__init__","title":"<code>__init__(t, DOT_THRESHOLD=0.9995, epsilon=1e-08, model_save_path=None, show_pbar=False, **kwargs)</code>","text":"<p>Initialize the SlerpForCausalLM algorithm.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>float</code>)           \u2013            <p>The interpolation parameter. Must be in the range [0, 1].       t=0 returns the first model, t=1 returns the second model,       t=0.5 provides balanced interpolation.</p> </li> <li> <code>DOT_THRESHOLD</code>               (<code>float</code>, default:                   <code>0.9995</code> )           \u2013            <p>The threshold for the dot product of normalized vectors.                            When the absolute dot product exceeds this threshold,                            vectors are considered nearly collinear and linear                            interpolation (LERP) is used instead of SLERP for                            numerical stability. Defaults to 0.9995.</p> </li> <li> <code>epsilon</code>               (<code>float</code>, default:                   <code>1e-08</code> )           \u2013            <p>Small value used for numerical stability to avoid                      division by zero during vector normalization.                      Defaults to 1e-8.</p> </li> <li> <code>model_save_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Path where the merged model should be saved.                                      If None, the model is not saved to disk.                                      Defaults to None.</p> </li> <li> <code>show_pbar</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to display a progress bar during the interpolation                       process. Useful for debugging or monitoring progress with                       large models. Defaults to False.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments passed to the parent BaseAlgorithm class.</p> </li> </ul> Source code in <code>fusion_bench/method/slerp/slerp.py</code> <pre><code>def __init__(\n    self,\n    t: float,\n    DOT_THRESHOLD: float = 0.9995,\n    epsilon: float = 1e-8,\n    model_save_path: Optional[str] = None,\n    show_pbar: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the SlerpForCausalLM algorithm.\n\n    Args:\n        t (float): The interpolation parameter. Must be in the range [0, 1].\n                  t=0 returns the first model, t=1 returns the second model,\n                  t=0.5 provides balanced interpolation.\n        DOT_THRESHOLD (float, optional): The threshold for the dot product of normalized vectors.\n                                       When the absolute dot product exceeds this threshold,\n                                       vectors are considered nearly collinear and linear\n                                       interpolation (LERP) is used instead of SLERP for\n                                       numerical stability. Defaults to 0.9995.\n        epsilon (float, optional): Small value used for numerical stability to avoid\n                                 division by zero during vector normalization.\n                                 Defaults to 1e-8.\n        model_save_path (Optional[str], optional): Path where the merged model should be saved.\n                                                 If None, the model is not saved to disk.\n                                                 Defaults to None.\n        show_pbar (bool, optional): Whether to display a progress bar during the interpolation\n                                  process. Useful for debugging or monitoring progress with\n                                  large models. Defaults to False.\n        **kwargs: Additional keyword arguments passed to the parent BaseAlgorithm class.\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#task-arithmetic","title":"Task Arithmetic","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.TaskArithmeticForLlama","title":"<code>TaskArithmeticForLlama = TaskArithmeticForCausalLM</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.TaskArithmeticAlgorithm","title":"<code>TaskArithmeticAlgorithm</code>","text":"<p>               Bases: <code>SimpleProfilerMixin</code>, <code>BaseAlgorithm</code></p> <p>Task Arithmetic Algorithm for model fusion.</p> <p>This class implements the Task Arithmetic method for fusing models. It inherits from BaseModelFusionAlgorithm and SimpleProfilerMixin to provide the necessary functionality for model fusion and profiling.</p> <p>Attributes:</p> <ul> <li> <code>scaling_factor</code>               (<code>int</code>)           \u2013            <p>The factor by which the task vectors will be scaled before merging.</p> </li> </ul> Source code in <code>fusion_bench/method/task_arithmetic/task_arithmetic.py</code> <pre><code>@auto_register_config\nclass TaskArithmeticAlgorithm(\n    SimpleProfilerMixin,\n    BaseAlgorithm,\n):\n    \"\"\"\n    Task Arithmetic Algorithm for model fusion.\n\n    This class implements the Task Arithmetic method for fusing models. It inherits from\n    BaseModelFusionAlgorithm and SimpleProfilerMixin to provide the necessary functionality\n    for model fusion and profiling.\n\n    Attributes:\n        scaling_factor (int): The factor by which the task vectors will be scaled before merging.\n    \"\"\"\n\n    def __init__(self, scaling_factor: int, **kwargs):\n        \"\"\"\n        Initializes the TaskArithmeticAlgorithm with the given scaling factor.\n\n        Args:\n            scaling_factor (int): The factor by which the task vectors will be scaled before merging.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(self, modelpool: Union[BaseModelPool, Dict[str, nn.Module]]) -&gt; nn.Module:\n        \"\"\"\n        Runs the Task Arithmetic Algorithm to fuse models in the given model pool.\n\n        Args:\n            modelpool (Union[BaseModelPool, Dict[str, nn.Module]]): The pool of models to fuse.\n\n        Returns:\n            nn.Module: The pre-trained model with the merged task vectors.\n        \"\"\"\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n\n        log.info(\"Fusing models using task arithmetic.\")\n        task_vector = None\n        with self.profile(\"load model\"):\n            pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n        # Calculate the total task vector\n        for model_name in modelpool.model_names:\n            with self.profile(\"load model\"):\n                model = modelpool.load_model(model_name)\n            with self.profile(\"merge weights\"):\n                if task_vector is None:\n                    task_vector = state_dict_sub(\n                        model.state_dict(),\n                        pretrained_model.state_dict(),\n                    )\n                else:\n                    task_vector = state_dict_add(\n                        task_vector,\n                        state_dict_sub(\n                            model.state_dict(),\n                            pretrained_model.state_dict(),\n                        ),\n                    )\n        with self.profile(\"merge weights\"):\n            # scale the task vector\n            task_vector = state_dict_mul(task_vector, self.config.scaling_factor)\n            # add the task vector to the pretrained model\n            state_dict = state_dict_add(pretrained_model.state_dict(), task_vector)\n\n        self.print_profile_summary()\n\n        # apply state dict to model\n        if isinstance(pretrained_model, nn.Module):\n            model = pretrained_model\n            model.load_state_dict(state_dict)\n        elif isinstance(pretrained_model, LazyStateDict):\n            model = deepcopy(pretrained_model.meta_module)\n            model = model.to_empty(device=pretrained_model._device)\n            result = model.load_state_dict(state_dict, strict=False)\n            if result.unexpected_keys:\n                raise ValueError(\n                    f\"Unexpected keys in state dict: {result.unexpected_keys}\"\n                )\n            if result.missing_keys:\n                log.warning(f\"Missing keys in state dict: {result.missing_keys}\")\n        else:\n            raise TypeError(f\"Unsupported model type: {type(pretrained_model)}\")\n        return model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.TaskArithmeticAlgorithm.__init__","title":"<code>__init__(scaling_factor, **kwargs)</code>","text":"<p>Initializes the TaskArithmeticAlgorithm with the given scaling factor.</p> <p>Parameters:</p> <ul> <li> <code>scaling_factor</code>               (<code>int</code>)           \u2013            <p>The factor by which the task vectors will be scaled before merging.</p> </li> </ul> Source code in <code>fusion_bench/method/task_arithmetic/task_arithmetic.py</code> <pre><code>def __init__(self, scaling_factor: int, **kwargs):\n    \"\"\"\n    Initializes the TaskArithmeticAlgorithm with the given scaling factor.\n\n    Args:\n        scaling_factor (int): The factor by which the task vectors will be scaled before merging.\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.TaskArithmeticAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Runs the Task Arithmetic Algorithm to fuse models in the given model pool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>Union[BaseModelPool, Dict[str, Module]]</code>)           \u2013            <p>The pool of models to fuse.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: The pre-trained model with the merged task vectors.</p> </li> </ul> Source code in <code>fusion_bench/method/task_arithmetic/task_arithmetic.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: Union[BaseModelPool, Dict[str, nn.Module]]) -&gt; nn.Module:\n    \"\"\"\n    Runs the Task Arithmetic Algorithm to fuse models in the given model pool.\n\n    Args:\n        modelpool (Union[BaseModelPool, Dict[str, nn.Module]]): The pool of models to fuse.\n\n    Returns:\n        nn.Module: The pre-trained model with the merged task vectors.\n    \"\"\"\n    if not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(modelpool)\n\n    log.info(\"Fusing models using task arithmetic.\")\n    task_vector = None\n    with self.profile(\"load model\"):\n        pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n    # Calculate the total task vector\n    for model_name in modelpool.model_names:\n        with self.profile(\"load model\"):\n            model = modelpool.load_model(model_name)\n        with self.profile(\"merge weights\"):\n            if task_vector is None:\n                task_vector = state_dict_sub(\n                    model.state_dict(),\n                    pretrained_model.state_dict(),\n                )\n            else:\n                task_vector = state_dict_add(\n                    task_vector,\n                    state_dict_sub(\n                        model.state_dict(),\n                        pretrained_model.state_dict(),\n                    ),\n                )\n    with self.profile(\"merge weights\"):\n        # scale the task vector\n        task_vector = state_dict_mul(task_vector, self.config.scaling_factor)\n        # add the task vector to the pretrained model\n        state_dict = state_dict_add(pretrained_model.state_dict(), task_vector)\n\n    self.print_profile_summary()\n\n    # apply state dict to model\n    if isinstance(pretrained_model, nn.Module):\n        model = pretrained_model\n        model.load_state_dict(state_dict)\n    elif isinstance(pretrained_model, LazyStateDict):\n        model = deepcopy(pretrained_model.meta_module)\n        model = model.to_empty(device=pretrained_model._device)\n        result = model.load_state_dict(state_dict, strict=False)\n        if result.unexpected_keys:\n            raise ValueError(\n                f\"Unexpected keys in state dict: {result.unexpected_keys}\"\n            )\n        if result.missing_keys:\n            log.warning(f\"Missing keys in state dict: {result.missing_keys}\")\n    else:\n        raise TypeError(f\"Unsupported model type: {type(pretrained_model)}\")\n    return model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#ties-merging","title":"Ties-Merging","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.TiesMergingAlgorithm","title":"<code>TiesMergingAlgorithm</code>","text":"<p>               Bases: <code>SimpleProfilerMixin</code>, <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/ties_merging/ties_merging.py</code> <pre><code>@auto_register_config\nclass TiesMergingAlgorithm(\n    SimpleProfilerMixin,\n    BaseAlgorithm,\n):\n    def __init__(\n        self,\n        scaling_factor: float,\n        threshold: float,\n        remove_keys: List[str],\n        merge_func: Literal[\"sum\", \"mean\", \"max\"],\n        **kwargs: Any,\n    ):\n        \"\"\"\n        TiesMergingAlgorithm is a class for fusing multiple models using the TIES merging technique.\n\n        Initialize the TiesMergingAlgorithm with the given parameters.\n\n        Args:\n            scaling_factor (float): The scaling factor to apply to the merged task vector.\n            threshold (float): The threshold for resetting values in the task vector.\n            remove_keys (List[str]): List of keys to remove from the state dictionary.\n            merge_func (Literal[\"sum\", \"mean\", \"max\"]): The merge function to use for disjoint merging.\n            **kwargs: Additional keyword arguments for the base class.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(\n        self, modelpool: BaseModelPool | Dict[str, nn.Module], **kwargs: Any\n    ) -&gt; nn.Module:\n        \"\"\"\n        Run the TIES merging algorithm to fuse models in the model pool.\n\n        Args:\n            modelpool (BaseModelPool | Dict[str, nn.Module]): The model pool containing the models to fuse.\n\n        Returns:\n            nn.Module: The fused model.\n        \"\"\"\n        log.info(\"Fusing models using ties merging.\")\n        modelpool = to_modelpool(modelpool)\n        remove_keys = self.config.get(\"remove_keys\", [])\n        merge_func = self.config.get(\"merge_func\", \"sum\")\n        scaling_factor = self.scaling_factor\n        threshold = self.threshold\n\n        with self.profile(\"loading models\"):\n            # Load the pretrained model\n            pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n            # Load the state dicts of the models\n            ft_checks: List[StateDictType] = [\n                modelpool.load_model(model_name).state_dict(keep_vars=True)\n                for model_name in modelpool.model_names\n            ]\n            ptm_check: StateDictType = pretrained_model.state_dict(keep_vars=True)\n\n        with self.profile(\"merging models\"):\n            # Compute the task vectors\n            flat_ft: Tensor = torch.vstack(\n                [state_dict_to_vector(check, remove_keys) for check in ft_checks]\n            )\n            flat_ptm: Tensor = state_dict_to_vector(ptm_check, remove_keys)\n            tv_flat_checks = flat_ft - flat_ptm\n\n            # Perform TIES Merging\n            merged_tv = ties_merging(\n                tv_flat_checks,\n                reset_thresh=threshold,\n                merge_func=merge_func,\n            )\n            merged_check = flat_ptm + scaling_factor * merged_tv\n            state_dict = vector_to_state_dict(\n                merged_check, ptm_check, remove_keys=remove_keys\n            )\n        self.print_profile_summary()\n\n        # apply state dict to model\n        if isinstance(pretrained_model, nn.Module):\n            model = pretrained_model\n            model.load_state_dict(state_dict)\n        elif isinstance(pretrained_model, LazyStateDict):\n            model = deepcopy(pretrained_model.meta_module)\n            model = model.to_empty(device=pretrained_model._device)\n            result = model.load_state_dict(state_dict, strict=False)\n            if result.unexpected_keys:\n                raise ValueError(\n                    f\"Unexpected keys in state dict: {result.unexpected_keys}\"\n                )\n            if result.missing_keys:\n                log.warning(f\"Missing keys in state dict: {result.missing_keys}\")\n        else:\n            raise TypeError(f\"Unsupported model type: {type(pretrained_model)}\")\n        return model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.TiesMergingAlgorithm.__init__","title":"<code>__init__(scaling_factor, threshold, remove_keys, merge_func, **kwargs)</code>","text":"<p>TiesMergingAlgorithm is a class for fusing multiple models using the TIES merging technique.</p> <p>Initialize the TiesMergingAlgorithm with the given parameters.</p> <p>Parameters:</p> <ul> <li> <code>scaling_factor</code>               (<code>float</code>)           \u2013            <p>The scaling factor to apply to the merged task vector.</p> </li> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>The threshold for resetting values in the task vector.</p> </li> <li> <code>remove_keys</code>               (<code>List[str]</code>)           \u2013            <p>List of keys to remove from the state dictionary.</p> </li> <li> <code>merge_func</code>               (<code>Literal['sum', 'mean', 'max']</code>)           \u2013            <p>The merge function to use for disjoint merging.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the base class.</p> </li> </ul> Source code in <code>fusion_bench/method/ties_merging/ties_merging.py</code> <pre><code>def __init__(\n    self,\n    scaling_factor: float,\n    threshold: float,\n    remove_keys: List[str],\n    merge_func: Literal[\"sum\", \"mean\", \"max\"],\n    **kwargs: Any,\n):\n    \"\"\"\n    TiesMergingAlgorithm is a class for fusing multiple models using the TIES merging technique.\n\n    Initialize the TiesMergingAlgorithm with the given parameters.\n\n    Args:\n        scaling_factor (float): The scaling factor to apply to the merged task vector.\n        threshold (float): The threshold for resetting values in the task vector.\n        remove_keys (List[str]): List of keys to remove from the state dictionary.\n        merge_func (Literal[\"sum\", \"mean\", \"max\"]): The merge function to use for disjoint merging.\n        **kwargs: Additional keyword arguments for the base class.\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.TiesMergingAlgorithm.run","title":"<code>run(modelpool, **kwargs)</code>","text":"<p>Run the TIES merging algorithm to fuse models in the model pool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool | Dict[str, Module]</code>)           \u2013            <p>The model pool containing the models to fuse.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: The fused model.</p> </li> </ul> Source code in <code>fusion_bench/method/ties_merging/ties_merging.py</code> <pre><code>@torch.no_grad()\ndef run(\n    self, modelpool: BaseModelPool | Dict[str, nn.Module], **kwargs: Any\n) -&gt; nn.Module:\n    \"\"\"\n    Run the TIES merging algorithm to fuse models in the model pool.\n\n    Args:\n        modelpool (BaseModelPool | Dict[str, nn.Module]): The model pool containing the models to fuse.\n\n    Returns:\n        nn.Module: The fused model.\n    \"\"\"\n    log.info(\"Fusing models using ties merging.\")\n    modelpool = to_modelpool(modelpool)\n    remove_keys = self.config.get(\"remove_keys\", [])\n    merge_func = self.config.get(\"merge_func\", \"sum\")\n    scaling_factor = self.scaling_factor\n    threshold = self.threshold\n\n    with self.profile(\"loading models\"):\n        # Load the pretrained model\n        pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n        # Load the state dicts of the models\n        ft_checks: List[StateDictType] = [\n            modelpool.load_model(model_name).state_dict(keep_vars=True)\n            for model_name in modelpool.model_names\n        ]\n        ptm_check: StateDictType = pretrained_model.state_dict(keep_vars=True)\n\n    with self.profile(\"merging models\"):\n        # Compute the task vectors\n        flat_ft: Tensor = torch.vstack(\n            [state_dict_to_vector(check, remove_keys) for check in ft_checks]\n        )\n        flat_ptm: Tensor = state_dict_to_vector(ptm_check, remove_keys)\n        tv_flat_checks = flat_ft - flat_ptm\n\n        # Perform TIES Merging\n        merged_tv = ties_merging(\n            tv_flat_checks,\n            reset_thresh=threshold,\n            merge_func=merge_func,\n        )\n        merged_check = flat_ptm + scaling_factor * merged_tv\n        state_dict = vector_to_state_dict(\n            merged_check, ptm_check, remove_keys=remove_keys\n        )\n    self.print_profile_summary()\n\n    # apply state dict to model\n    if isinstance(pretrained_model, nn.Module):\n        model = pretrained_model\n        model.load_state_dict(state_dict)\n    elif isinstance(pretrained_model, LazyStateDict):\n        model = deepcopy(pretrained_model.meta_module)\n        model = model.to_empty(device=pretrained_model._device)\n        result = model.load_state_dict(state_dict, strict=False)\n        if result.unexpected_keys:\n            raise ValueError(\n                f\"Unexpected keys in state dict: {result.unexpected_keys}\"\n            )\n        if result.missing_keys:\n            log.warning(f\"Missing keys in state dict: {result.missing_keys}\")\n    else:\n        raise TypeError(f\"Unsupported model type: {type(pretrained_model)}\")\n    return model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fisher-merging","title":"Fisher Merging","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FisherMergingAlgorithm","title":"<code>FisherMergingAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code></p> <p>Implements the Fisher Merging Algorithm.</p> <p>This class extends the BaseModelFusionAlgorithm to handle merging of models using Fisher weights. It supports excluding certain parameters, normalizing Fisher weights, and setting a minimal value for Fisher weights.</p> <p>Methods:</p> <ul> <li> <code>run</code>             \u2013              <p>BaseModelPool) -&gt; nn.Module: Executes the Fisher merging process on the model pool and returns the merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/fisher_merging/fisher_merging.py</code> <pre><code>@auto_register_config\nclass FisherMergingAlgorithm(BaseAlgorithm, SimpleProfilerMixin):\n    \"\"\"\n    Implements the Fisher Merging Algorithm.\n\n    This class extends the BaseModelFusionAlgorithm to handle merging of models using Fisher weights.\n    It supports excluding certain parameters, normalizing Fisher weights, and setting a minimal value for Fisher weights.\n\n    Methods:\n        run(modelpool: BaseModelPool) -&gt; nn.Module:\n            Executes the Fisher merging process on the model pool and returns the merged model.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        exclude_param_names_regex: list,\n        normalize_fisher_weight: bool,\n        minimal_fisher_weight: float,\n        num_fisher_examples: int,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n    def run(self, modelpool: BaseModelPool) -&gt; nn.Module:\n        \"\"\"\n        Run the Fisher Merging Algorithm.\n\n        This method constructs the wrapped model and performs test-time adaptation if necessary.\n\n        Args:\n            modelpool (BaseModelPool): The model pool containing the pretrained and fine-tuned models.\n\n        Returns:\n            nn.Module: The merged model after test-time adaptation.\n        \"\"\"\n        log.info(\"Running Fisher Merging Algorithm\")\n        if isinstance(modelpool, (dict, list, tuple)):\n            modelpool = BaseModelPool(modelpool)\n\n        assert len(modelpool) &gt; 0, \"model pool is empty\"\n        assert (\n            modelpool.has_pretrained\n        ), \"no pretrained model (base model) in the model pool\"\n\n        self.modelpool = modelpool\n        self.on_fisher_merging_start()\n\n        # dictionary of list, where key is the parameter name,\n        # value is a list of the corresponding parameters of all the models that need to be merged\n        models_to_merge_param_dict = defaultdict(list)\n\n        # list of dictionaries with length len(models_to_merge),\n        # each dictionary records the fisher weights (matrix or vector) of parameters for each model that needs to be merged\n        models_to_merge_fisher_weights_list = []\n\n        param_names_to_merge = None\n\n        for name, model in modelpool.named_models():\n            param_dict = model.state_dict()\n            if param_names_to_merge is None:\n                param_names_to_merge = get_param_names_to_merge(\n                    input_param_names=list(param_dict.keys()),\n                    exclude_param_names_regex=self.config.get(\n                        \"exclude_param_names_regex\", []\n                    ),\n                )\n\n            for param_name in param_names_to_merge:\n                models_to_merge_param_dict[param_name].append(param_dict[param_name])\n\n            with (\n                self.profile(\"merging models\"),\n                self.profile(\"computing fisher weights\"),\n            ):\n                model_to_merge_fisher_weights = self.get_fisher_weights(\n                    model_name=name,\n                    model=model,\n                    train_dataset=modelpool.load_train_dataset(name),\n                    param_names_to_merge=param_names_to_merge,\n                )\n\n                models_to_merge_fisher_weights_list.append(\n                    model_to_merge_fisher_weights\n                )\n\n        with self.profile(\"merging models\"):\n            merged_params = merging_with_fisher_weights(\n                models_to_merge_param_dict=models_to_merge_param_dict,\n                models_to_merge_fisher_weights_list=models_to_merge_fisher_weights_list,\n                fisher_scaling_coefficients=torch.ones(len(modelpool)) / len(modelpool),\n                normalize_fisher_weight=self.config.get(\n                    \"normalize_fisher_weight\", True\n                ),\n                minimal_fisher_weight=self.config.get(\"minimal_fisher_weight\", 1e-6),\n            )\n\n            merged_model = modelpool.load_model(\"_pretrained_\")\n            merged_model.load_state_dict(merged_params, strict=False)\n\n        self.print_profile_summary()\n        return merged_model\n\n    def get_fisher_weights(\n        self,\n        model_name: str,\n        model: nn.Module,\n        train_dataset: Any,\n        param_names_to_merge: List[str],\n    ) -&gt; Dict[str, Tensor]:\n        \"\"\"\n        Compute the Fisher weights for the given model and training dataset.\n\n        Args:\n            model_name (str): The name of the model.\n            model (nn.Module): The model module.\n            train_dataset: The training dataset.\n            param_names_to_merge (List[str]): List of parameter names to merge.\n\n        Returns:\n            Dict[str, Tensor]: The computed Fisher weights for each parameter.\n        \"\"\"\n        # this function is used to compute fisher weights for a model\n        # it should be implemented in the subclass\n        raise NotImplementedError\n\n    def on_fisher_merging_start(self):\n        \"\"\"\n        Setup the zero-shot classification head before starting the Fisher merging process.\n        \"\"\"\n        # this function is used to initialize some variables before running fisher merging\n        pass\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FisherMergingAlgorithm.get_fisher_weights","title":"<code>get_fisher_weights(model_name, model, train_dataset, param_names_to_merge)</code>","text":"<p>Compute the Fisher weights for the given model and training dataset.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model.</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model module.</p> </li> <li> <code>train_dataset</code>               (<code>Any</code>)           \u2013            <p>The training dataset.</p> </li> <li> <code>param_names_to_merge</code>               (<code>List[str]</code>)           \u2013            <p>List of parameter names to merge.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Tensor]</code>           \u2013            <p>Dict[str, Tensor]: The computed Fisher weights for each parameter.</p> </li> </ul> Source code in <code>fusion_bench/method/fisher_merging/fisher_merging.py</code> <pre><code>def get_fisher_weights(\n    self,\n    model_name: str,\n    model: nn.Module,\n    train_dataset: Any,\n    param_names_to_merge: List[str],\n) -&gt; Dict[str, Tensor]:\n    \"\"\"\n    Compute the Fisher weights for the given model and training dataset.\n\n    Args:\n        model_name (str): The name of the model.\n        model (nn.Module): The model module.\n        train_dataset: The training dataset.\n        param_names_to_merge (List[str]): List of parameter names to merge.\n\n    Returns:\n        Dict[str, Tensor]: The computed Fisher weights for each parameter.\n    \"\"\"\n    # this function is used to compute fisher weights for a model\n    # it should be implemented in the subclass\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FisherMergingAlgorithm.on_fisher_merging_start","title":"<code>on_fisher_merging_start()</code>","text":"<p>Setup the zero-shot classification head before starting the Fisher merging process.</p> Source code in <code>fusion_bench/method/fisher_merging/fisher_merging.py</code> <pre><code>def on_fisher_merging_start(self):\n    \"\"\"\n    Setup the zero-shot classification head before starting the Fisher merging process.\n    \"\"\"\n    # this function is used to initialize some variables before running fisher merging\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FisherMergingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Run the Fisher Merging Algorithm.</p> <p>This method constructs the wrapped model and performs test-time adaptation if necessary.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool</code>)           \u2013            <p>The model pool containing the pretrained and fine-tuned models.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: The merged model after test-time adaptation.</p> </li> </ul> Source code in <code>fusion_bench/method/fisher_merging/fisher_merging.py</code> <pre><code>def run(self, modelpool: BaseModelPool) -&gt; nn.Module:\n    \"\"\"\n    Run the Fisher Merging Algorithm.\n\n    This method constructs the wrapped model and performs test-time adaptation if necessary.\n\n    Args:\n        modelpool (BaseModelPool): The model pool containing the pretrained and fine-tuned models.\n\n    Returns:\n        nn.Module: The merged model after test-time adaptation.\n    \"\"\"\n    log.info(\"Running Fisher Merging Algorithm\")\n    if isinstance(modelpool, (dict, list, tuple)):\n        modelpool = BaseModelPool(modelpool)\n\n    assert len(modelpool) &gt; 0, \"model pool is empty\"\n    assert (\n        modelpool.has_pretrained\n    ), \"no pretrained model (base model) in the model pool\"\n\n    self.modelpool = modelpool\n    self.on_fisher_merging_start()\n\n    # dictionary of list, where key is the parameter name,\n    # value is a list of the corresponding parameters of all the models that need to be merged\n    models_to_merge_param_dict = defaultdict(list)\n\n    # list of dictionaries with length len(models_to_merge),\n    # each dictionary records the fisher weights (matrix or vector) of parameters for each model that needs to be merged\n    models_to_merge_fisher_weights_list = []\n\n    param_names_to_merge = None\n\n    for name, model in modelpool.named_models():\n        param_dict = model.state_dict()\n        if param_names_to_merge is None:\n            param_names_to_merge = get_param_names_to_merge(\n                input_param_names=list(param_dict.keys()),\n                exclude_param_names_regex=self.config.get(\n                    \"exclude_param_names_regex\", []\n                ),\n            )\n\n        for param_name in param_names_to_merge:\n            models_to_merge_param_dict[param_name].append(param_dict[param_name])\n\n        with (\n            self.profile(\"merging models\"),\n            self.profile(\"computing fisher weights\"),\n        ):\n            model_to_merge_fisher_weights = self.get_fisher_weights(\n                model_name=name,\n                model=model,\n                train_dataset=modelpool.load_train_dataset(name),\n                param_names_to_merge=param_names_to_merge,\n            )\n\n            models_to_merge_fisher_weights_list.append(\n                model_to_merge_fisher_weights\n            )\n\n    with self.profile(\"merging models\"):\n        merged_params = merging_with_fisher_weights(\n            models_to_merge_param_dict=models_to_merge_param_dict,\n            models_to_merge_fisher_weights_list=models_to_merge_fisher_weights_list,\n            fisher_scaling_coefficients=torch.ones(len(modelpool)) / len(modelpool),\n            normalize_fisher_weight=self.config.get(\n                \"normalize_fisher_weight\", True\n            ),\n            minimal_fisher_weight=self.config.get(\"minimal_fisher_weight\", 1e-6),\n        )\n\n        merged_model = modelpool.load_model(\"_pretrained_\")\n        merged_model.load_state_dict(merged_params, strict=False)\n\n    self.print_profile_summary()\n    return merged_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FisherMergingForCLIPVisionModel","title":"<code>FisherMergingForCLIPVisionModel</code>","text":"<p>               Bases: <code>CLIPClassificationMixin</code>, <code>FisherMergingAlgorithm</code></p> <p>Implements Fisher Merging for CLIP Vision Models.</p> <p>This class extends the FisherMergingAlgorithm and CLIPClassificationMixin to handle the specifics of merging CLIP Vision models using Fisher weights.</p> Source code in <code>fusion_bench/method/fisher_merging/clip_fisher_merging.py</code> <pre><code>class FisherMergingForCLIPVisionModel(\n    CLIPClassificationMixin,\n    FisherMergingAlgorithm,\n):\n    \"\"\"\n    Implements Fisher Merging for CLIP Vision Models.\n\n    This class extends the FisherMergingAlgorithm and CLIPClassificationMixin to handle\n    the specifics of merging CLIP Vision models using Fisher weights.\n    \"\"\"\n\n    _clip_processor: CLIPProcessor = None\n    zeroshot_weights = {}\n\n    _config_mapping = FisherMergingAlgorithm._config_mapping | {\n        \"_dataloader_kwargs\": \"dataloader_kwargs\",\n    }\n\n    def __init__(\n        self,\n        *,\n        exclude_param_names_regex,\n        normalize_fisher_weight,\n        minimal_fisher_weight,\n        num_fisher_examples,\n        dataloader_kwargs: DictConfig,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the FisherMergingForCLIPVisionModel with the given configuration.\n\n        Args:\n            exclude_param_names_regex (list): List of regex patterns to exclude certain parameter names.\n            normalize_fisher_weight (bool): Whether to normalize Fisher weights.\n            minimal_fisher_weight (float): Minimal value for Fisher weights to avoid numerical issues.\n            num_fisher_examples (int): Number of examples to compute Fisher weights.\n            dataloader_kwargs (DictConfig): Configuration for the dataloader.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            exclude_param_names_regex=exclude_param_names_regex,\n            normalize_fisher_weight=normalize_fisher_weight,\n            minimal_fisher_weight=minimal_fisher_weight,\n            num_fisher_examples=num_fisher_examples,\n        )\n        self.dataloader_kwargs = dataloader_kwargs\n        for key, value in kwargs.items():\n            log.warning(f\"Unused argument: {key}={value}\")\n            setattr(self, key, value)\n\n    def on_fisher_merging_start(self):\n        \"\"\"\n        Setup the zero-shot classification head before starting the Fisher merging process.\n        \"\"\"\n        self.setup_zero_shot_classification_head()\n\n    def compute_logits(self, module, batch, task: str) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for the given images and task.\n\n        Args:\n            module (Module): The model module.\n            batch (tuple): A batch of data containing images and labels.\n            task (str): The name of the task.\n\n        Returns:\n            Tensor: The computed logits.\n        \"\"\"\n        images, _ = batch\n        text_embeds = self.zeroshot_weights[task]\n\n        image_embeds = module(images)[1]\n        image_embeds = self.visual_projection(image_embeds)\n\n        # normalize embeddings\n        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n        # cosine similarity\n        logits_per_text = (\n            torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n        )\n        logits_per_image = logits_per_text.t()\n\n        return logits_per_image\n\n    def get_fisher_weights(\n        self,\n        model_name: str,\n        model: Module,\n        train_dataset,\n        param_names_to_merge: List[str],\n    ) -&gt; Dict[str, Tensor]:\n        \"\"\"\n        Compute the Fisher weights for the given model and training dataset.\n\n        Args:\n            model_name (str): The name of the model.\n            model (Module): The model module.\n            train_dataset: The training dataset.\n            param_names_to_merge (List[str]): List of parameter names to merge.\n\n        Returns:\n            Dict[str, Tensor]: The computed Fisher weights for each parameter.\n        \"\"\"\n        # setup dataloader\n        train_dataset = CLIPDataset(train_dataset, self.clip_processor)\n        train_dataloader = DataLoader(train_dataset, **self.dataloader_kwargs)\n        if self.fabric is not None:\n            train_dataloader = self.fabric.setup_dataloaders(train_dataloader)\n            model = self.fabric.setup(model)\n        num_fisher_examples = self.config.num_fisher_examples\n        if num_fisher_examples % train_dataloader.batch_size != 0:\n            print(\n                \"warning: the number of examples for computing fisher cannot be fully divided by the batch size for model, \"\n                \"which may lead to a slightly different number of the actually used examples.\"\n            )\n        num_computed_examples = 0\n        batches_fisher_weights_list = []\n        for step, batch in tqdm(\n            enumerate(train_dataloader),\n            desc=\"computing fisher weights\",\n            total=num_fisher_examples // train_dataloader.batch_size,\n        ):\n            if num_computed_examples &gt;= num_fisher_examples:\n                break\n            logits = self.compute_logits(model, batch, model_name)\n            # Tensor, shape (batch_size, num_label_classes)\n\n            # compute fisher weights for classification task\n            # use detach() to detach from the computation graph\n            # Tensor, shape (batch_size, num_label_classes)\n            labels_probabilities = torch.softmax(logits, dim=-1).detach()\n            labels_log_probabilities = torch.log_softmax(logits, dim=-1)\n            # sqrt labels_probabilities, since torch.sqrt(labels_probabilities) would be squared in the following squared gradients\n            labels_expectations = (\n                torch.sqrt(labels_probabilities) * labels_log_probabilities\n            )\n            # sum over label classes and batch dimension\n            sum_labels_expectations = labels_expectations.sum(dim=-1).sum(dim=0)\n            model.zero_grad()\n            sum_labels_expectations.backward()\n            # dict, fisher weights of a batch\n            batch_fisher_weights = get_param_squared_gradients(\n                model=model, param_names_to_merge=param_names_to_merge\n            )\n\n            # move fisher weights to cpu to save GPU memory\n            for key, weights in batch_fisher_weights.items():\n                batch_fisher_weights[key] = weights.detach().cpu()\n\n            batches_fisher_weights_list.append(batch_fisher_weights)\n            num_computed_examples += batch[0].size(0)\n\n        model_to_merge_fisher_weights = {}\n        for batch_fisher_weights in batches_fisher_weights_list:\n            for key in batch_fisher_weights:\n                if key not in model_to_merge_fisher_weights:\n                    model_to_merge_fisher_weights[key] = batch_fisher_weights[key]\n                else:\n                    model_to_merge_fisher_weights[key] += batch_fisher_weights[key]\n\n        # mean over batches\n        for key in model_to_merge_fisher_weights:\n            model_to_merge_fisher_weights[key] /= num_computed_examples\n            model_to_merge_fisher_weights[key] = (\n                model_to_merge_fisher_weights[key].detach().cpu()\n            )\n        return model_to_merge_fisher_weights\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FisherMergingForCLIPVisionModel.__init__","title":"<code>__init__(*, exclude_param_names_regex, normalize_fisher_weight, minimal_fisher_weight, num_fisher_examples, dataloader_kwargs, **kwargs)</code>","text":"<p>Initialize the FisherMergingForCLIPVisionModel with the given configuration.</p> <p>Parameters:</p> <ul> <li> <code>exclude_param_names_regex</code>               (<code>list</code>)           \u2013            <p>List of regex patterns to exclude certain parameter names.</p> </li> <li> <code>normalize_fisher_weight</code>               (<code>bool</code>)           \u2013            <p>Whether to normalize Fisher weights.</p> </li> <li> <code>minimal_fisher_weight</code>               (<code>float</code>)           \u2013            <p>Minimal value for Fisher weights to avoid numerical issues.</p> </li> <li> <code>num_fisher_examples</code>               (<code>int</code>)           \u2013            <p>Number of examples to compute Fisher weights.</p> </li> <li> <code>dataloader_kwargs</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration for the dataloader.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments.</p> </li> </ul> Source code in <code>fusion_bench/method/fisher_merging/clip_fisher_merging.py</code> <pre><code>def __init__(\n    self,\n    *,\n    exclude_param_names_regex,\n    normalize_fisher_weight,\n    minimal_fisher_weight,\n    num_fisher_examples,\n    dataloader_kwargs: DictConfig,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the FisherMergingForCLIPVisionModel with the given configuration.\n\n    Args:\n        exclude_param_names_regex (list): List of regex patterns to exclude certain parameter names.\n        normalize_fisher_weight (bool): Whether to normalize Fisher weights.\n        minimal_fisher_weight (float): Minimal value for Fisher weights to avoid numerical issues.\n        num_fisher_examples (int): Number of examples to compute Fisher weights.\n        dataloader_kwargs (DictConfig): Configuration for the dataloader.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        exclude_param_names_regex=exclude_param_names_regex,\n        normalize_fisher_weight=normalize_fisher_weight,\n        minimal_fisher_weight=minimal_fisher_weight,\n        num_fisher_examples=num_fisher_examples,\n    )\n    self.dataloader_kwargs = dataloader_kwargs\n    for key, value in kwargs.items():\n        log.warning(f\"Unused argument: {key}={value}\")\n        setattr(self, key, value)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FisherMergingForCLIPVisionModel.compute_logits","title":"<code>compute_logits(module, batch, task)</code>","text":"<p>Compute the logits for the given images and task.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The model module.</p> </li> <li> <code>batch</code>               (<code>tuple</code>)           \u2013            <p>A batch of data containing images and labels.</p> </li> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The computed logits.</p> </li> </ul> Source code in <code>fusion_bench/method/fisher_merging/clip_fisher_merging.py</code> <pre><code>def compute_logits(self, module, batch, task: str) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for the given images and task.\n\n    Args:\n        module (Module): The model module.\n        batch (tuple): A batch of data containing images and labels.\n        task (str): The name of the task.\n\n    Returns:\n        Tensor: The computed logits.\n    \"\"\"\n    images, _ = batch\n    text_embeds = self.zeroshot_weights[task]\n\n    image_embeds = module(images)[1]\n    image_embeds = self.visual_projection(image_embeds)\n\n    # normalize embeddings\n    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n    # cosine similarity\n    logits_per_text = (\n        torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n    )\n    logits_per_image = logits_per_text.t()\n\n    return logits_per_image\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FisherMergingForCLIPVisionModel.get_fisher_weights","title":"<code>get_fisher_weights(model_name, model, train_dataset, param_names_to_merge)</code>","text":"<p>Compute the Fisher weights for the given model and training dataset.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model.</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model module.</p> </li> <li> <code>train_dataset</code>           \u2013            <p>The training dataset.</p> </li> <li> <code>param_names_to_merge</code>               (<code>List[str]</code>)           \u2013            <p>List of parameter names to merge.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Tensor]</code>           \u2013            <p>Dict[str, Tensor]: The computed Fisher weights for each parameter.</p> </li> </ul> Source code in <code>fusion_bench/method/fisher_merging/clip_fisher_merging.py</code> <pre><code>def get_fisher_weights(\n    self,\n    model_name: str,\n    model: Module,\n    train_dataset,\n    param_names_to_merge: List[str],\n) -&gt; Dict[str, Tensor]:\n    \"\"\"\n    Compute the Fisher weights for the given model and training dataset.\n\n    Args:\n        model_name (str): The name of the model.\n        model (Module): The model module.\n        train_dataset: The training dataset.\n        param_names_to_merge (List[str]): List of parameter names to merge.\n\n    Returns:\n        Dict[str, Tensor]: The computed Fisher weights for each parameter.\n    \"\"\"\n    # setup dataloader\n    train_dataset = CLIPDataset(train_dataset, self.clip_processor)\n    train_dataloader = DataLoader(train_dataset, **self.dataloader_kwargs)\n    if self.fabric is not None:\n        train_dataloader = self.fabric.setup_dataloaders(train_dataloader)\n        model = self.fabric.setup(model)\n    num_fisher_examples = self.config.num_fisher_examples\n    if num_fisher_examples % train_dataloader.batch_size != 0:\n        print(\n            \"warning: the number of examples for computing fisher cannot be fully divided by the batch size for model, \"\n            \"which may lead to a slightly different number of the actually used examples.\"\n        )\n    num_computed_examples = 0\n    batches_fisher_weights_list = []\n    for step, batch in tqdm(\n        enumerate(train_dataloader),\n        desc=\"computing fisher weights\",\n        total=num_fisher_examples // train_dataloader.batch_size,\n    ):\n        if num_computed_examples &gt;= num_fisher_examples:\n            break\n        logits = self.compute_logits(model, batch, model_name)\n        # Tensor, shape (batch_size, num_label_classes)\n\n        # compute fisher weights for classification task\n        # use detach() to detach from the computation graph\n        # Tensor, shape (batch_size, num_label_classes)\n        labels_probabilities = torch.softmax(logits, dim=-1).detach()\n        labels_log_probabilities = torch.log_softmax(logits, dim=-1)\n        # sqrt labels_probabilities, since torch.sqrt(labels_probabilities) would be squared in the following squared gradients\n        labels_expectations = (\n            torch.sqrt(labels_probabilities) * labels_log_probabilities\n        )\n        # sum over label classes and batch dimension\n        sum_labels_expectations = labels_expectations.sum(dim=-1).sum(dim=0)\n        model.zero_grad()\n        sum_labels_expectations.backward()\n        # dict, fisher weights of a batch\n        batch_fisher_weights = get_param_squared_gradients(\n            model=model, param_names_to_merge=param_names_to_merge\n        )\n\n        # move fisher weights to cpu to save GPU memory\n        for key, weights in batch_fisher_weights.items():\n            batch_fisher_weights[key] = weights.detach().cpu()\n\n        batches_fisher_weights_list.append(batch_fisher_weights)\n        num_computed_examples += batch[0].size(0)\n\n    model_to_merge_fisher_weights = {}\n    for batch_fisher_weights in batches_fisher_weights_list:\n        for key in batch_fisher_weights:\n            if key not in model_to_merge_fisher_weights:\n                model_to_merge_fisher_weights[key] = batch_fisher_weights[key]\n            else:\n                model_to_merge_fisher_weights[key] += batch_fisher_weights[key]\n\n    # mean over batches\n    for key in model_to_merge_fisher_weights:\n        model_to_merge_fisher_weights[key] /= num_computed_examples\n        model_to_merge_fisher_weights[key] = (\n            model_to_merge_fisher_weights[key].detach().cpu()\n        )\n    return model_to_merge_fisher_weights\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FisherMergingForCLIPVisionModel.on_fisher_merging_start","title":"<code>on_fisher_merging_start()</code>","text":"<p>Setup the zero-shot classification head before starting the Fisher merging process.</p> Source code in <code>fusion_bench/method/fisher_merging/clip_fisher_merging.py</code> <pre><code>def on_fisher_merging_start(self):\n    \"\"\"\n    Setup the zero-shot classification head before starting the Fisher merging process.\n    \"\"\"\n    self.setup_zero_shot_classification_head()\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FisherMergingAlgorithmForGPT2","title":"<code>FisherMergingAlgorithmForGPT2</code>","text":"<p>               Bases: <code>LightningFabricMixin</code>, <code>FisherMergingAlgorithm</code></p> <p>Implements the Fisher Merging Algorithm for GPT-2 models on text classification tasks.</p> <p>This class extends the FisherMergingAlgorithm to handle GPT-2 models specifically. It supports caching, batch processing, and multi-worker data loading.</p> <p>Attributes:</p> <ul> <li> <code>classifiers</code>               (<code>dict</code>)           \u2013            <p>A dictionary to store classifiers for each model.</p> </li> <li> <code>modelpool</code>               (<code>HuggingFaceGPT2ClassificationPool</code>)           \u2013            <p>The model pool containing the GPT-2 models.</p> </li> <li> <code>cache_dir</code>               (<code>str</code>)           \u2013            <p>Directory to cache data.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for data loading.</p> </li> <li> <code>num_workers</code>               (<code>int</code>)           \u2013            <p>Number of workers for data loading.</p> </li> </ul> Source code in <code>fusion_bench/method/fisher_merging/gpt2_fisher_merging.py</code> <pre><code>@auto_register_config\nclass FisherMergingAlgorithmForGPT2(\n    LightningFabricMixin,\n    FisherMergingAlgorithm,\n):\n    \"\"\"\n    Implements the Fisher Merging Algorithm for GPT-2 models on text classification tasks.\n\n    This class extends the FisherMergingAlgorithm to handle GPT-2 models specifically.\n    It supports caching, batch processing, and multi-worker data loading.\n\n    Attributes:\n        classifiers (dict): A dictionary to store classifiers for each model.\n        modelpool (HuggingFaceGPT2ClassificationPool): The model pool containing the GPT-2 models.\n        cache_dir (str): Directory to cache data.\n        batch_size (int): Batch size for data loading.\n        num_workers (int): Number of workers for data loading.\n    \"\"\"\n\n    classifiers = {}\n    modelpool: GPT2ForSequenceClassificationPool = None\n\n    def __init__(\n        self,\n        cache_dir: str,\n        batch_size: int,\n        num_workers: int,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the FisherMergingAlgorithmForGPT2 with the given configuration.\n\n        Args:\n            cache_dir (str): Directory to cache data.\n            batch_size (int): Batch size for data loading.\n            num_workers (int): Number of workers for data loading.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    def on_fisher_merging_start(self):\n        \"\"\"\n        Setup the classifiers for each model in the model pool before starting the Fisher merging process.\n        \"\"\"\n        for model_name in self.modelpool.model_names:\n            classifier = cast(\n                GPT2ForSequenceClassification,\n                self.modelpool.load_classifier(model_name),\n            ).requires_grad_(False)\n            classifier.transformer = None\n            classifier = classifier.to(self.fabric.device)\n            self.classifiers[model_name] = classifier\n\n    def compute_logits(self, module: GPT2Model, batch, task: str) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for the given batch and task.\n\n        Args:\n            module (GPT2Model): The GPT-2 model module.\n            batch (dict): The input batch.\n            task (str): The name of the task.\n\n        Returns:\n            Tensor: The computed logits.\n        \"\"\"\n        self.classifiers[task].transformer = module\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n\n        outputs = self.classifiers[task](input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        assert logits.dim() == 2\n        return logits\n\n    def get_fisher_weights(\n        self,\n        model_name: str,\n        model: Module,\n        train_dataset,\n        param_names_to_merge: List[str],\n    ) -&gt; Dict[str, Tensor]:\n        \"\"\"\n        Compute the Fisher weights for the given model and training dataset.\n\n        Args:\n            model_name (str): The name of the model.\n            model (Module): The model module.\n            train_dataset: The training dataset.\n            param_names_to_merge (List[str]): List of parameter names to merge.\n\n        Returns:\n            Dict[str, Tensor]: The computed Fisher weights for each parameter.\n        \"\"\"\n        # setup dataloader\n        train_dataloader = DataLoader(\n            train_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            collate_fn=default_data_collator,\n            num_workers=self.config.num_workers,\n            pin_memory=True,\n        )\n        train_dataloader = self.fabric.setup_dataloaders(train_dataloader)\n        model = self.fabric.setup(model)\n        num_fisher_examples = self.config.num_fisher_examples\n        if num_fisher_examples % train_dataloader.batch_size != 0:\n            print(\n                f\"warning: the number of examples for computing fisher cannot be fully divided by the batch size for model, \"\n                \"which may lead to a slightly different number of the actually used examples.\"\n            )\n        num_computed_examples = 0\n        batches_fisher_weights_list = []\n        for step, batch in tqdm(\n            enumerate(train_dataloader),\n            desc=f\"computing fisher weights\",\n            total=num_fisher_examples // train_dataloader.batch_size,\n        ):\n            if num_computed_examples &gt;= num_fisher_examples:\n                break\n            logits = self.compute_logits(model, batch, model_name)\n            # Tensor, shape (batch_size, num_label_classes)\n\n            # compute fisher weights for classifxication task\n            # use detach() to detach from the computation graph\n            # Tensor, shape (batch_size, num_label_classes)\n            labels_probabilities = torch.softmax(logits, dim=-1).detach()\n            labels_log_probabilities = torch.log_softmax(logits, dim=-1)\n            # sqrt labels_probabilities, since torch.sqrt(labels_probabilities) would be squared in the following squared gradients\n            labels_expectations = (\n                torch.sqrt(labels_probabilities) * labels_log_probabilities\n            )\n            # sum over label classes and batch dimension\n            sum_labels_expectations = labels_expectations.sum(dim=-1).sum(dim=0)\n            model.zero_grad()\n            sum_labels_expectations.backward()\n            # dict, fisher weights of a batch\n            batch_fisher_weights = get_param_squared_gradients(\n                model=model, param_names_to_merge=param_names_to_merge\n            )\n\n            # move fisher weights to cpu to save GPU memory\n            for key, weights in batch_fisher_weights.items():\n                batch_fisher_weights[key] = weights.detach().cpu()\n\n            batches_fisher_weights_list.append(batch_fisher_weights)\n            num_computed_examples += batch[\"input_ids\"].size(0)\n\n        model_to_merge_fisher_weights = {}\n        for batch_fisher_weights in batches_fisher_weights_list:\n            for key in batch_fisher_weights:\n                if key not in model_to_merge_fisher_weights:\n                    model_to_merge_fisher_weights[key] = batch_fisher_weights[key]\n                else:\n                    model_to_merge_fisher_weights[key] += batch_fisher_weights[key]\n\n        # mean over batches\n        for key in model_to_merge_fisher_weights:\n            model_to_merge_fisher_weights[key] /= num_computed_examples\n            model_to_merge_fisher_weights[key] = (\n                model_to_merge_fisher_weights[key].detach().cpu()\n            )\n        return model_to_merge_fisher_weights\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FisherMergingAlgorithmForGPT2.__init__","title":"<code>__init__(cache_dir, batch_size, num_workers, **kwargs)</code>","text":"<p>Initialize the FisherMergingAlgorithmForGPT2 with the given configuration.</p> <p>Parameters:</p> <ul> <li> <code>cache_dir</code>               (<code>str</code>)           \u2013            <p>Directory to cache data.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for data loading.</p> </li> <li> <code>num_workers</code>               (<code>int</code>)           \u2013            <p>Number of workers for data loading.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments.</p> </li> </ul> Source code in <code>fusion_bench/method/fisher_merging/gpt2_fisher_merging.py</code> <pre><code>def __init__(\n    self,\n    cache_dir: str,\n    batch_size: int,\n    num_workers: int,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the FisherMergingAlgorithmForGPT2 with the given configuration.\n\n    Args:\n        cache_dir (str): Directory to cache data.\n        batch_size (int): Batch size for data loading.\n        num_workers (int): Number of workers for data loading.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FisherMergingAlgorithmForGPT2.compute_logits","title":"<code>compute_logits(module, batch, task)</code>","text":"<p>Compute the logits for the given batch and task.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>GPT2Model</code>)           \u2013            <p>The GPT-2 model module.</p> </li> <li> <code>batch</code>               (<code>dict</code>)           \u2013            <p>The input batch.</p> </li> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The computed logits.</p> </li> </ul> Source code in <code>fusion_bench/method/fisher_merging/gpt2_fisher_merging.py</code> <pre><code>def compute_logits(self, module: GPT2Model, batch, task: str) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for the given batch and task.\n\n    Args:\n        module (GPT2Model): The GPT-2 model module.\n        batch (dict): The input batch.\n        task (str): The name of the task.\n\n    Returns:\n        Tensor: The computed logits.\n    \"\"\"\n    self.classifiers[task].transformer = module\n    input_ids = batch[\"input_ids\"]\n    attention_mask = batch[\"attention_mask\"]\n\n    outputs = self.classifiers[task](input_ids, attention_mask=attention_mask)\n    logits = outputs.logits\n    assert logits.dim() == 2\n    return logits\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FisherMergingAlgorithmForGPT2.get_fisher_weights","title":"<code>get_fisher_weights(model_name, model, train_dataset, param_names_to_merge)</code>","text":"<p>Compute the Fisher weights for the given model and training dataset.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model.</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model module.</p> </li> <li> <code>train_dataset</code>           \u2013            <p>The training dataset.</p> </li> <li> <code>param_names_to_merge</code>               (<code>List[str]</code>)           \u2013            <p>List of parameter names to merge.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Tensor]</code>           \u2013            <p>Dict[str, Tensor]: The computed Fisher weights for each parameter.</p> </li> </ul> Source code in <code>fusion_bench/method/fisher_merging/gpt2_fisher_merging.py</code> <pre><code>def get_fisher_weights(\n    self,\n    model_name: str,\n    model: Module,\n    train_dataset,\n    param_names_to_merge: List[str],\n) -&gt; Dict[str, Tensor]:\n    \"\"\"\n    Compute the Fisher weights for the given model and training dataset.\n\n    Args:\n        model_name (str): The name of the model.\n        model (Module): The model module.\n        train_dataset: The training dataset.\n        param_names_to_merge (List[str]): List of parameter names to merge.\n\n    Returns:\n        Dict[str, Tensor]: The computed Fisher weights for each parameter.\n    \"\"\"\n    # setup dataloader\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=self.config.batch_size,\n        shuffle=True,\n        collate_fn=default_data_collator,\n        num_workers=self.config.num_workers,\n        pin_memory=True,\n    )\n    train_dataloader = self.fabric.setup_dataloaders(train_dataloader)\n    model = self.fabric.setup(model)\n    num_fisher_examples = self.config.num_fisher_examples\n    if num_fisher_examples % train_dataloader.batch_size != 0:\n        print(\n            f\"warning: the number of examples for computing fisher cannot be fully divided by the batch size for model, \"\n            \"which may lead to a slightly different number of the actually used examples.\"\n        )\n    num_computed_examples = 0\n    batches_fisher_weights_list = []\n    for step, batch in tqdm(\n        enumerate(train_dataloader),\n        desc=f\"computing fisher weights\",\n        total=num_fisher_examples // train_dataloader.batch_size,\n    ):\n        if num_computed_examples &gt;= num_fisher_examples:\n            break\n        logits = self.compute_logits(model, batch, model_name)\n        # Tensor, shape (batch_size, num_label_classes)\n\n        # compute fisher weights for classifxication task\n        # use detach() to detach from the computation graph\n        # Tensor, shape (batch_size, num_label_classes)\n        labels_probabilities = torch.softmax(logits, dim=-1).detach()\n        labels_log_probabilities = torch.log_softmax(logits, dim=-1)\n        # sqrt labels_probabilities, since torch.sqrt(labels_probabilities) would be squared in the following squared gradients\n        labels_expectations = (\n            torch.sqrt(labels_probabilities) * labels_log_probabilities\n        )\n        # sum over label classes and batch dimension\n        sum_labels_expectations = labels_expectations.sum(dim=-1).sum(dim=0)\n        model.zero_grad()\n        sum_labels_expectations.backward()\n        # dict, fisher weights of a batch\n        batch_fisher_weights = get_param_squared_gradients(\n            model=model, param_names_to_merge=param_names_to_merge\n        )\n\n        # move fisher weights to cpu to save GPU memory\n        for key, weights in batch_fisher_weights.items():\n            batch_fisher_weights[key] = weights.detach().cpu()\n\n        batches_fisher_weights_list.append(batch_fisher_weights)\n        num_computed_examples += batch[\"input_ids\"].size(0)\n\n    model_to_merge_fisher_weights = {}\n    for batch_fisher_weights in batches_fisher_weights_list:\n        for key in batch_fisher_weights:\n            if key not in model_to_merge_fisher_weights:\n                model_to_merge_fisher_weights[key] = batch_fisher_weights[key]\n            else:\n                model_to_merge_fisher_weights[key] += batch_fisher_weights[key]\n\n    # mean over batches\n    for key in model_to_merge_fisher_weights:\n        model_to_merge_fisher_weights[key] /= num_computed_examples\n        model_to_merge_fisher_weights[key] = (\n            model_to_merge_fisher_weights[key].detach().cpu()\n        )\n    return model_to_merge_fisher_weights\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FisherMergingAlgorithmForGPT2.on_fisher_merging_start","title":"<code>on_fisher_merging_start()</code>","text":"<p>Setup the classifiers for each model in the model pool before starting the Fisher merging process.</p> Source code in <code>fusion_bench/method/fisher_merging/gpt2_fisher_merging.py</code> <pre><code>def on_fisher_merging_start(self):\n    \"\"\"\n    Setup the classifiers for each model in the model pool before starting the Fisher merging process.\n    \"\"\"\n    for model_name in self.modelpool.model_names:\n        classifier = cast(\n            GPT2ForSequenceClassification,\n            self.modelpool.load_classifier(model_name),\n        ).requires_grad_(False)\n        classifier.transformer = None\n        classifier = classifier.to(self.fabric.device)\n        self.classifiers[model_name] = classifier\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#drop-and-rescale-dare","title":"Drop And REscale (DARE)","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.DareSimpleAverage","title":"<code>DareSimpleAverage</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/dare/simple_average.py</code> <pre><code>class DareSimpleAverage(BaseAlgorithm):\n\n    def __init__(\n        self,\n        sparsity_ratio: float,\n        only_on_linear_weights: bool,\n        rescale: bool = True,\n        **kwargs,\n    ):\n        self.sparsity_ratio = sparsity_ratio\n        self.only_on_linear_weight = only_on_linear_weights\n        self.rescale = rescale\n        super().__init__(**kwargs)\n\n    def run(self, modelpool: BaseModelPool):\n        return DareTaskArithmetic(\n            scaling_factor=1 / len(modelpool),\n            sparsity_ratio=self.sparsity_ratio,\n            only_on_linear_weights=self.only_on_linear_weight,\n            rescale=self.rescale,\n        ).run(modelpool)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.DareTaskArithmetic","title":"<code>DareTaskArithmetic</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>Implementation of Task Arithmetic w/ DARE.</p> <ul> <li>Yu et al. Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch. 2023. http://arxiv.org/abs/2311.03099</li> </ul> Source code in <code>fusion_bench/method/dare/task_arithmetic.py</code> <pre><code>class DareTaskArithmetic(BaseAlgorithm):\n    \"\"\"\n    Implementation of Task Arithmetic w/ DARE.\n\n    - Yu et al. Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch. 2023. http://arxiv.org/abs/2311.03099\n    \"\"\"\n\n    def __init__(\n        self,\n        scaling_factor: float,\n        sparsity_ratio: float,\n        only_on_linear_weights: bool,\n        rescale: bool = True,\n        **kwargs,\n    ):\n        self.scaling_factor = scaling_factor\n        self.sparsity_ratio = sparsity_ratio\n        self.only_on_linear_weights = only_on_linear_weights\n        self.rescale = rescale\n        super().__init__(**kwargs)\n\n    def _load_task_vector(\n        self,\n        modelpool: BaseModelPool,\n        model_name: str,\n        pretrained_model: nn.Module,\n    ):\n        finetuned_model = modelpool.load_model(model_name)\n        task_vector = module_sub_(finetuned_model, pretrained_model)\n        return task_vector\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool):\n        assert (\n            self.sparsity_ratio &gt;= 0 and self.sparsity_ratio &lt;= 1\n        ), \"Sparsity ratio must be between 0 and 1\"\n        pretrained_model = modelpool.load_pretrained_model()\n\n        # load task vectors\n        task_vectors = {\n            model_name: self._load_task_vector(modelpool, model_name, pretrained_model)\n            for model_name in modelpool.model_names\n        }\n\n        # drop and rescale task vectors\n        for model_name, tv in task_vectors.items():\n            if self.only_on_linear_weights:\n                for module_name, module in tv.named_modules():\n                    if isinstance(module, nn.Linear):\n                        print(f\"pruning model: `{model_name}`, layer: {module_name}.\")\n                        param_random_drop_(\n                            module.weight, self.sparsity_ratio, rescale=self.rescale\n                        )\n            else:\n                print(f\"pruning model: `{model_name}`\")\n                module_random_drop_(tv, self.sparsity_ratio, rescale=self.rescale)\n\n        # merge task vectors\n        task_vector_sum = state_dict_sum(\n            [trainable_state_dict(tv) for tv in task_vectors.values()]\n        )\n\n        # scale the task vector and add it to the pretrained model\n        for name, delta in task_vector_sum.items():\n            delta = delta * self.scaling_factor\n            pretrained_model.get_parameter(name).data.add_(delta)\n\n        return pretrained_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.DareTiesMerging","title":"<code>DareTiesMerging</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/dare/ties_merging.py</code> <pre><code>class DareTiesMerging(BaseAlgorithm):\n    def __init__(\n        self,\n        # DARE parameters\n        sparsity_ratio: float,\n        only_on_linear_weights: bool,\n        rescale: bool,\n        # Ties merging parameters\n        scaling_factor: float,\n        threshold: int,\n        remove_keys: list[str],\n        merge_func: Literal[\"sum\", \"mean\", \"max\"],\n        **kwargs,\n    ):\n        self.sparsity_ratio = sparsity_ratio\n        self.only_on_linear_weights = only_on_linear_weights\n        self.rescale = rescale\n        self.scaling_factor = scaling_factor\n        self.threshold = threshold\n        self.remove_keys = remove_keys\n        self.merge_func = merge_func\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def _load_task_vector(\n        self,\n        modelpool: BaseModelPool,\n        model_name: str,\n        pretrained_model: nn.Module,\n    ):\n        finetuned_model = modelpool.load_model(model_name)\n        task_vector = module_sub_(finetuned_model, pretrained_model)\n        return task_vector\n\n    def run(self, modelpool: BaseModelPool):\n        assert (\n            self.sparsity_ratio &gt;= 0 and self.sparsity_ratio &lt;= 1\n        ), \"Sparsity ratio must be between 0 and 1\"\n        pretrained_model = modelpool.load_pretrained_model()\n\n        # load task vectors\n        task_vectors = {\n            model_name: self._load_task_vector(modelpool, model_name, pretrained_model)\n            for model_name in modelpool.model_names\n        }\n\n        # drop and rescale task vectors\n        for model_name, tv in task_vectors.items():\n            if self.only_on_linear_weights:\n                for module_name, module in tv.named_modules():\n                    if isinstance(module, nn.Linear):\n                        print(f\"pruning model: `{model_name}`, layer: {module_name}.\")\n                        param_random_drop_(\n                            module.weight, self.sparsity_ratio, rescale=self.rescale\n                        )\n            else:\n                print(f\"pruning model: `{model_name}`\")\n                module_random_drop_(tv, self.sparsity_ratio, rescale=self.rescale)\n\n        ptm_check = pretrained_model.state_dict()\n        flat_ptm = state_dict_to_vector(ptm_check, self.remove_keys)\n        tv_flat_checks = torch.vstack(\n            [\n                state_dict_to_vector(check.state_dict(), self.remove_keys)\n                for check in task_vectors.values()\n            ]\n        )\n        del task_vectors\n\n        # Perform TIES Merging\n        merged_tv = ties_merging(\n            tv_flat_checks,\n            reset_thresh=self.threshold,\n            merge_func=self.merge_func,\n        )\n        merged_check = flat_ptm + self.scaling_factor * merged_tv\n        merged_state_dict = vector_to_state_dict(\n            merged_check, ptm_check, remove_keys=self.remove_keys\n        )\n\n        pretrained_model.load_state_dict(merged_state_dict)\n        return pretrained_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#model-extrapolation-expo","title":"Model Extrapolation (ExPO)","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.ExPOAlgorithm","title":"<code>ExPOAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>ExPO merge algorithm.</p> <p>This algorithm merges a pretrained model with a finetuned model.</p> \\[\\theta_{merged} = \\theta_{sft} + \\alpha (\\theta_{rlhf} - \\theta_{sft})\\] <p>where \\(\\theta_{merged}\\) is the merged model, \\(\\theta_{rlhf}\\) is the finetuned model (medium-aligned model), \\(\\theta_{sft}\\) is the pretrained model (base model), and \\(\\alpha\\) is the extrapolation factor.</p> <p>In the configuration, the SFT model should have name <code>_pretrained_</code> and the rlhf name can be set arbitarily.</p> Source code in <code>fusion_bench/method/linear/expo.py</code> <pre><code>class ExPOAlgorithm(BaseAlgorithm):\n    R\"\"\"\n    ExPO merge algorithm.\n\n    This algorithm merges a pretrained model with a finetuned model.\n\n    $$\\theta_{merged} = \\theta_{sft} + \\alpha (\\theta_{rlhf} - \\theta_{sft})$$\n\n    where $\\theta_{merged}$ is the merged model, $\\theta_{rlhf}$ is the finetuned model (medium-aligned model),\n    $\\theta_{sft}$ is the pretrained model (base model), and $\\alpha$ is the extrapolation factor.\n\n    In the configuration, the SFT model should have name `_pretrained_` and the rlhf name can be set arbitarily.\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"extrapolation_factor\": \"extrapolation_factor\"\n    }\n\n    def __init__(self, extrapolation_factor: float, **kwargs):\n        self.extrapolation_factor = extrapolation_factor\n        super().__init__(**kwargs)\n\n    def run(self, modelpool: Union[BaseModelPool, list]) -&gt; nn.Module:\n        \"\"\"\n        Run the ExPO merge algorithm.\n\n        Args:\n            modelpool (BaseModelPool): The pool of models to merge.\n\n        Returns:\n            nn.Module: The merged model.\n        \"\"\"\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n\n        assert len(modelpool.model_names) &gt;= 1, \"ExPO requires at least one model.\"\n        assert modelpool.has_pretrained, \"ExPO requires pretrained models (base model).\"\n\n        sft_model = modelpool.load_pretrained_model()\n        if len(modelpool) == 1:\n            rlhf_model = modelpool.load_model(modelpool.model_names[0])\n        else:\n            # if there are multiple RLHF models, use simple average to merge them before running ExPO\n            log.info(\n                f\"There are {len(modelpool)} models in the model pool, averaging them first...\"\n            )\n            rlhf_model = SimpleAverageAlgorithm().run(modelpool)\n\n        # merge the pretrained model and the finetuned model\n        delta_parameters = state_dict_sub(\n            rlhf_model.state_dict(), sft_model.state_dict()\n        )\n        merged_sd = state_dict_add(\n            rlhf_model.state_dict(),\n            state_dict_mul(delta_parameters, scalar=self.extrapolation_factor),\n        )\n\n        rlhf_model.load_state_dict(merged_sd)\n        return rlhf_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.ExPOAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Run the ExPO merge algorithm.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool</code>)           \u2013            <p>The pool of models to merge.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: The merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/linear/expo.py</code> <pre><code>def run(self, modelpool: Union[BaseModelPool, list]) -&gt; nn.Module:\n    \"\"\"\n    Run the ExPO merge algorithm.\n\n    Args:\n        modelpool (BaseModelPool): The pool of models to merge.\n\n    Returns:\n        nn.Module: The merged model.\n    \"\"\"\n    if not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(modelpool)\n\n    assert len(modelpool.model_names) &gt;= 1, \"ExPO requires at least one model.\"\n    assert modelpool.has_pretrained, \"ExPO requires pretrained models (base model).\"\n\n    sft_model = modelpool.load_pretrained_model()\n    if len(modelpool) == 1:\n        rlhf_model = modelpool.load_model(modelpool.model_names[0])\n    else:\n        # if there are multiple RLHF models, use simple average to merge them before running ExPO\n        log.info(\n            f\"There are {len(modelpool)} models in the model pool, averaging them first...\"\n        )\n        rlhf_model = SimpleAverageAlgorithm().run(modelpool)\n\n    # merge the pretrained model and the finetuned model\n    delta_parameters = state_dict_sub(\n        rlhf_model.state_dict(), sft_model.state_dict()\n    )\n    merged_sd = state_dict_add(\n        rlhf_model.state_dict(),\n        state_dict_mul(delta_parameters, scalar=self.extrapolation_factor),\n    )\n\n    rlhf_model.load_state_dict(merged_sd)\n    return rlhf_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.ExPOAlgorithmForLlama","title":"<code>ExPOAlgorithmForLlama</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/linear/llama_expo.py</code> <pre><code>class ExPOAlgorithmForLlama(BaseAlgorithm):\n\n    def __init__(\n        self,\n        extrapolation_factor: float,\n        attention_scaling_factor: float = 0.5,\n        only_on_backbone: bool = True,\n        on_linear_weights: bool = True,\n        on_linear_bias: bool = False,\n        on_embedding: bool = False,\n        fix_last_n_layers: int = 0,\n        fix_first_n_layers: int = 0,\n        magnitude_sparsity_ratio: Optional[float] = None,\n        **kwargs,\n    ):\n        self.extrapolation_factor = extrapolation_factor\n        self.attention_scaling_factor = attention_scaling_factor\n        self.only_on_backbone = only_on_backbone\n        self.on_linear_weights = on_linear_weights\n        self.on_linear_bias = on_linear_bias\n        self.on_embedding = on_embedding\n        self.fix_last_n_layers = fix_last_n_layers\n        self.fix_first_n_layers = fix_first_n_layers\n        self.magnitude_sparsity_ratio = magnitude_sparsity_ratio\n        super().__init__(**kwargs)\n\n    def load_models(self, modelpool: BaseModelPool):\n        sft_model: LlamaForCausalLM = modelpool.load_pretrained_model()\n        if len(modelpool) == 1:\n            rlhf_model = modelpool.load_model(modelpool.model_names[0])\n        else:\n            # if there are multiple RLHF models, use simple average to merge them before running ExPO\n            log.info(\n                f\"There are {len(modelpool)} models in the model pool, averaging them first...\"\n            )\n            rlhf_model = SimpleAverageAlgorithm().run(modelpool)\n        rlhf_model = cast(LlamaForCausalLM, rlhf_model)\n        return sft_model, rlhf_model\n\n    def run(self, modelpool: BaseModelPool):\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n\n        assert len(modelpool.model_names) &gt;= 1, \"ExPO requires at least one model.\"\n        assert modelpool.has_pretrained, \"ExPO requires pretrained models (base model).\"\n\n        sft_model, rlhf_model = self.load_models(modelpool)\n\n        if not self.on_linear_bias:\n            for name, module in sft_model.named_modules():\n                if isinstance(module, nn.Linear):\n                    module.bias = rlhf_model.get_submodule(name).bias\n        if not self.on_linear_weights:\n            for name, module in sft_model.named_modules():\n                if isinstance(module, nn.Linear):\n                    module.weight = rlhf_model.get_submodule(name).weight\n\n        if not self.only_on_backbone:\n            expo_(sft_model.lm_head, rlhf_model.lm_head, self.extrapolation_factor)\n\n        # expo on the backbone\n        self._expo_lm_model_(\n            sft_model.model, rlhf_model.model, self.extrapolation_factor\n        )\n        return rlhf_model\n\n    def _expo_lm_model_(\n        self,\n        sft_model: LlamaModel,\n        rlhf_model: LlamaModel,\n        extrapolation_factor: float,\n    ):\n        if self.on_embedding:\n            expo_(sft_model.embed_tokens, rlhf_model.embed_tokens, extrapolation_factor)\n\n        if self.fix_first_n_layers == \"half\":\n            self.fix_first_n_layers = len(sft_model.layers) // 2\n        if self.fix_last_n_layers == \"half\":\n            self.fix_last_n_layers = len(sft_model.layers) // 2\n\n        for layer_idx in range(\n            self.fix_first_n_layers, len(sft_model.layers) - self.fix_last_n_layers\n        ):\n            sft_layer = sft_model.layers[layer_idx]\n            expo_linear_modules_(\n                sft_layer.self_attn,\n                rlhf_model.layers[layer_idx].self_attn,\n                extrapolation_factor=extrapolation_factor\n                * self.attention_scaling_factor,\n                merge_dtype=torch.float32,\n                magnitude_sparsity_ratio=self.magnitude_sparsity_ratio,\n            )\n            expo_linear_modules_(\n                sft_layer.mlp,\n                rlhf_model.layers[layer_idx].mlp,\n                extrapolation_factor=extrapolation_factor,\n                merge_dtype=torch.float32,\n                magnitude_sparsity_ratio=self.magnitude_sparsity_ratio,\n            )\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#doge","title":"DOGE","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.DOGE_TA_Algorithm","title":"<code>DOGE_TA_Algorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code>, <code>LightningFabricMixin</code></p> <p>Task Arithmetic Algorithm for model fusion with learnable delta.</p> <p>This class extends the Task Arithmetic method to include a learnable delta for task vectors, optimized to maximize cosine similarity among the task vectors.</p> <p>Attributes:</p> <ul> <li> <code>scaling_factor</code>               (<code>int</code>)           \u2013            <p>The factor by which the task vectors will be scaled before merging.</p> </li> <li> <code>delta</code>               (<code>StateDictType</code>)           \u2013            <p>A learnable parameter to adjust task vectors, initialized as zeros.</p> </li> </ul> Source code in <code>fusion_bench/method/doge_ta/doge_ta.py</code> <pre><code>class DOGE_TA_Algorithm(\n    BaseAlgorithm,\n    SimpleProfilerMixin,\n    LightningFabricMixin,\n):\n    \"\"\"\n    Task Arithmetic Algorithm for model fusion with learnable delta.\n\n    This class extends the Task Arithmetic method to include a learnable delta\n    for task vectors, optimized to maximize cosine similarity among the task vectors.\n\n    Attributes:\n        scaling_factor (int): The factor by which the task vectors will be scaled before merging.\n        delta (StateDictType): A learnable parameter to adjust task vectors, initialized as zeros.\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"subspace\": \"subspace\",\n        \"K\": \"K\",\n        \"lamda\": \"lamda\",\n    }\n\n    def __init__(self, subspace, K, lamda):\n        self.delta = None  # Initialize delta as None; will be set during run\n        self.subspace = subspace\n        self.K = K\n        self.lamda = lamda\n        super().__init__()\n\n    @property\n    def device(self) -&gt; torch.device:\n        return self.fabric.device\n\n    @torch.no_grad()\n    def compute_task_vectors(\n        self, modelpool: BaseModelPool, pretrained_model: nn.Module\n    ) -&gt; List[StateDictType]:\n        \"\"\"\n        Computes task vectors for each model in the model pool relative to the pretrained model.\n        \"\"\"\n        task_vectors = []\n        pretrained_sd = pretrained_model.state_dict(keep_vars=True)\n        filtered_keys = [\n            k\n            for k in pretrained_sd.keys()\n            if (\"encoder\" in k and \"layer_norm\" not in k and \"weight\" in k)\n        ]  # Flan T5: \"layer_norm\" not in k and (\"q.weight\" in k or \"v.weight\" in k)\n\n        for model_name in modelpool.model_names:\n            model = modelpool.load_model(model_name)\n            model_sd = model.state_dict(keep_vars=True)\n\n            filtered_task_vector = {\n                k: (model_sd[k] - pretrained_sd[k]) for k in filtered_keys\n            }\n            task_vectors.append(filtered_task_vector)\n\n        return task_vectors\n\n    def taskvector_loss(self, layer_vectors, layer_delta, layer_lamdas) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the loss based on delta and task vectors for a specific layer.\n        \"\"\"\n        total_loss = 0.0\n\n        layer_vectors_scale = layer_vectors * layer_lamdas.view(-1, 1, 1)\n        sum_over_num_vectors = layer_vectors_scale.sum(dim=0)\n\n        layer_delta_scale = layer_delta.unsqueeze(0) * layer_lamdas.view(-1, 1, 1)\n        sum_over_delta = layer_delta_scale.sum(dim=0)\n\n        # Iterate through each vector and calculate the loss one by one\n        for v_j in layer_vectors:\n            part1 = -v_j * sum_over_num_vectors\n            part2 = -v_j * sum_over_delta\n            part3 = v_j * v_j\n\n            expression = part1 + part2 + part3\n            layer_loss = expression.sum(dim=1).pow(2).sum()\n\n            # Cumulative total loss\n            total_loss += layer_loss\n        return total_loss\n\n    @torch.enable_grad()\n    def optimize_delta(self, task_vectors: List[StateDictType]) -&gt; None:\n        \"\"\"\n        Optimizes the delta based on the loss of task vectors.\n        \"\"\"\n        if self.delta is None:\n            self.delta = {\n                k: nn.Parameter(torch.zeros_like(v, device=self.device).detach())\n                for k, v in task_vectors[0].items()\n            }\n\n        optimizer = torch.optim.Adam(self.delta.values(), lr=1e-4)\n        initial_mem = torch.cuda.memory_allocated()\n        start_time = time.time()\n        for layer_name in task_vectors[0].keys():\n            layer_vectors = torch.stack([vec[layer_name] for vec in task_vectors]).to(\n                self.device\n            )\n            layer_lamdas = torch.stack(\n                [lamdas[layer_name] for lamdas in self.lamdas]\n            ).to(self.device)\n            for _ in range(400):\n                optimizer.zero_grad()\n                loss = self.taskvector_loss(\n                    layer_vectors, self.delta[layer_name], layer_lamdas\n                )\n                self.fabric.backward(loss)\n                grad_proj = (\n                    self.projection[layer_name] @ self.delta[layer_name].grad.detach()\n                )\n                self.delta[layer_name].grad.data = self.delta[\n                    layer_name\n                ].grad.data.sub_(grad_proj)\n                optimizer.step()\n                self.delta[layer_name].grad = None\n        end_time = time.time()\n        print(f\"Running time: {end_time - start_time} s\")\n        final_mem = torch.cuda.memory_allocated()\n        print(f\"Memory usage: {(final_mem - initial_mem) / (1024 ** 2)} MB\")\n        print(\"Optimization completed.\")\n\n    @torch.no_grad()\n    def run(self, modelpool: Union[BaseModelPool, Dict[str, nn.Module]]):\n        \"\"\"\n        Runs the Algorithm with learnable delta to fuse models in the given model pool.\n\n        Args:\n            modelpool (Union[BaseModelPool, Dict[str, nn.Module]]): The pool of models to fuse.\n\n        Returns:\n            nn.Module: The pre-trained model with the merged task vectors after optimizing delta.\n        \"\"\"\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n\n        log.info(\"Fusing models using DOGE_TA with learnable delta.\")\n        with self.profile(\"load model\"):\n            pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n        task_vectors = self.compute_task_vectors(modelpool, pretrained_model)\n\n        self.lamdas = self.compute_layer_lamdas(task_vectors)\n        self.projection = {}\n        for layer_name in task_vectors[0].keys():\n            for i, vector in enumerate(task_vectors):\n                layer_vector = vector[layer_name].to(self.device)\n                u, s, v = torch.linalg.svd(layer_vector, full_matrices=False)\n                if i == 0:\n                    print(f\"Computed SVD for {layer_name}...\")\n                    sum_u = torch.zeros_like(u, device=layer_vector.device)\n                    sum_s = torch.zeros_like(s, device=layer_vector.device)\n                    sum_v = torch.zeros_like(v, device=layer_vector.device)\n\n                reduced_index_s = int(s.shape[0] / len(task_vectors))\n\n                # select only the first reduced_index_s columns of u and place them\n                sum_u[:, i * reduced_index_s : (i + 1) * reduced_index_s] = u[\n                    :, :reduced_index_s\n                ]\n                sum_s[i * reduced_index_s : (i + 1) * reduced_index_s] = s[\n                    :reduced_index_s\n                ]\n                # select only the first reduced_index_s rows of v and place them\n                sum_v[i * reduced_index_s : (i + 1) * reduced_index_s, :] = v[\n                    :reduced_index_s, :\n                ]\n            u_u, s_u, v_u = torch.linalg.svd(sum_u, full_matrices=False)\n            layer_proj = torch.matmul(\n                u_u[:, : int(s.shape[0] / self.config.subspace)],\n                u_u[:, : int(s.shape[0] / self.config.subspace)].T,\n            )\n            self.projection[layer_name] = layer_proj\n\n        self.optimize_delta(task_vectors)\n\n        del self.projection\n        self.delta = {key: param.detach().cpu() for key, param in self.delta.items()}\n        self.lamdas = [\n            {key: param.cpu() for key, param in lamdas.items()}\n            for lamdas in self.lamdas\n        ]\n        task_vectors = [\n            {k: v.cpu() for k, v in task_vector.items()} for task_vector in task_vectors\n        ]\n        flat_vectors = []\n        vector_masks = []\n        for idx, task_vector in enumerate(task_vectors):\n            flat_vector = self.state_dict_to_vector(task_vector)\n            vector_mask = self.topk_values_mask(flat_vector, K=self.config.K)\n            flat_vectors.append(flat_vector)\n            vector_masks.append(vector_mask)\n        flat_delta = self.state_dict_to_vector(self.delta)\n\n        adjusted_vectors = [\n            self.vector_to_state_dict(\n                (flat_vector + flat_delta) * vector_mask, self.delta\n            )\n            for flat_vector, vector_mask in zip(flat_vectors, vector_masks)\n        ]\n\n        for layer_name in adjusted_vectors[0].keys():\n            layer_vectors = torch.stack(\n                [vec[layer_name] for vec in adjusted_vectors], dim=0\n            )\n            layer_lamdas = torch.stack(\n                [lamdas[layer_name] for lamdas in self.lamdas], dim=0\n            )\n            layer_vectors_scale = layer_vectors * layer_lamdas.view(-1, 1, 1)\n            task_vectors[0][layer_name] = layer_vectors_scale.sum(dim=0)\n\n        final_state_dict = {}\n        pretrained_sd = pretrained_model.state_dict(keep_vars=True)\n        for k, v in pretrained_sd.items():\n            if k in task_vectors[0]:\n                final_state_dict[k] = v + task_vectors[0][k]\n            else:\n                final_state_dict[k] = v\n\n        pretrained_model.load_state_dict(final_state_dict)\n\n        self.print_profile_summary()\n        return pretrained_model\n\n    def compute_lamdas(self, vectors: List[StateDictType]) -&gt; torch.Tensor:\n        lamdas = []\n        for vec in vectors:\n            norm_vec = torch.norm(\n                torch.cat([param.flatten() for param in vec.values()])\n            )\n            # norm_vec = sum([torch.norm(param) for param in vec.values()])\n            lamdas.append(self.config.lamda / norm_vec)\n        print(lamdas)\n        return lamdas\n\n    def compute_layer_lamdas(self, vectors: List[StateDictType]) -&gt; torch.Tensor:\n        lamdas = []\n        for vec in vectors:\n            tmp = {}\n            for layer_name in vec.keys():\n                norm_vec = torch.norm(vec[layer_name])\n                tmp[layer_name] = self.config.lamda / norm_vec\n            lamdas.append(tmp)\n        return lamdas\n\n    def topk_values_mask(self, M, K):\n        if K &gt; 1:\n            K /= 100\n\n        original_shape = M.shape\n        if M.dim() == 1:\n            M = M.unsqueeze(0)\n\n        n, d = M.shape\n        k = int(d * K)\n        k = d - k  # Keep top k elements instead of bottom k elements\n\n        # Find the k-th smallest element by magnitude for each row\n        kth_values, _ = M.abs().kthvalue(k, dim=1, keepdim=True)\n        # Create a mask tensor with True for the top k elements in each row\n        mask = M.abs() &gt;= kth_values\n        final_mask = mask.squeeze() if original_shape == M.squeeze().shape else mask\n\n        return final_mask\n\n    def state_dict_to_vector(self, state_dict, remove_keys=[]):\n        \"\"\"\n        Convert a state dictionary to a vector, removing specified keys.\n\n        Args:\n            state_dict (dict): The state dictionary to convert.\n            remove_keys (list): List of keys to remove from the state dictionary.\n\n        Returns:\n            Tensor: A vector representation of the state dictionary.\n        \"\"\"\n        shared_state_dict = copy.deepcopy(state_dict)\n        for key in remove_keys:\n            if key in shared_state_dict:\n                del shared_state_dict[key]\n        sorted_shared_state_dict = OrderedDict(sorted(shared_state_dict.items()))\n        return nn.utils.parameters_to_vector(\n            [value.reshape(-1) for key, value in sorted_shared_state_dict.items()]\n        )\n\n    def vector_to_state_dict(self, vector, state_dict, remove_keys=[]):\n        \"\"\"\n        Convert a vector back to a state dictionary, removing specified keys.\n\n        Args:\n            vector (Tensor): The vector to convert.\n            state_dict (dict): The reference state dictionary.\n            remove_keys (list): List of keys to remove from the state dictionary.\n\n        Returns:\n            dict: A state dictionary representation of the vector.\n        \"\"\"\n        # create a reference dict to define the order of the vector\n        reference_dict = copy.deepcopy(state_dict)\n        for key in remove_keys:\n            if key in reference_dict:\n                del reference_dict[key]\n        sorted_reference_dict = OrderedDict(sorted(reference_dict.items()))\n\n        # create a shared state dict using the reference dict\n        nn.utils.vector_to_parameters(vector, sorted_reference_dict.values())\n\n        # add back the encoder and decoder embedding weights.\n        if \"transformer.shared.weight\" in sorted_reference_dict:\n            for key in remove_keys:\n                sorted_reference_dict[key] = sorted_reference_dict[\n                    \"transformer.shared.weight\"\n                ]\n        return sorted_reference_dict\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.DOGE_TA_Algorithm.compute_task_vectors","title":"<code>compute_task_vectors(modelpool, pretrained_model)</code>","text":"<p>Computes task vectors for each model in the model pool relative to the pretrained model.</p> Source code in <code>fusion_bench/method/doge_ta/doge_ta.py</code> <pre><code>@torch.no_grad()\ndef compute_task_vectors(\n    self, modelpool: BaseModelPool, pretrained_model: nn.Module\n) -&gt; List[StateDictType]:\n    \"\"\"\n    Computes task vectors for each model in the model pool relative to the pretrained model.\n    \"\"\"\n    task_vectors = []\n    pretrained_sd = pretrained_model.state_dict(keep_vars=True)\n    filtered_keys = [\n        k\n        for k in pretrained_sd.keys()\n        if (\"encoder\" in k and \"layer_norm\" not in k and \"weight\" in k)\n    ]  # Flan T5: \"layer_norm\" not in k and (\"q.weight\" in k or \"v.weight\" in k)\n\n    for model_name in modelpool.model_names:\n        model = modelpool.load_model(model_name)\n        model_sd = model.state_dict(keep_vars=True)\n\n        filtered_task_vector = {\n            k: (model_sd[k] - pretrained_sd[k]) for k in filtered_keys\n        }\n        task_vectors.append(filtered_task_vector)\n\n    return task_vectors\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.DOGE_TA_Algorithm.optimize_delta","title":"<code>optimize_delta(task_vectors)</code>","text":"<p>Optimizes the delta based on the loss of task vectors.</p> Source code in <code>fusion_bench/method/doge_ta/doge_ta.py</code> <pre><code>@torch.enable_grad()\ndef optimize_delta(self, task_vectors: List[StateDictType]) -&gt; None:\n    \"\"\"\n    Optimizes the delta based on the loss of task vectors.\n    \"\"\"\n    if self.delta is None:\n        self.delta = {\n            k: nn.Parameter(torch.zeros_like(v, device=self.device).detach())\n            for k, v in task_vectors[0].items()\n        }\n\n    optimizer = torch.optim.Adam(self.delta.values(), lr=1e-4)\n    initial_mem = torch.cuda.memory_allocated()\n    start_time = time.time()\n    for layer_name in task_vectors[0].keys():\n        layer_vectors = torch.stack([vec[layer_name] for vec in task_vectors]).to(\n            self.device\n        )\n        layer_lamdas = torch.stack(\n            [lamdas[layer_name] for lamdas in self.lamdas]\n        ).to(self.device)\n        for _ in range(400):\n            optimizer.zero_grad()\n            loss = self.taskvector_loss(\n                layer_vectors, self.delta[layer_name], layer_lamdas\n            )\n            self.fabric.backward(loss)\n            grad_proj = (\n                self.projection[layer_name] @ self.delta[layer_name].grad.detach()\n            )\n            self.delta[layer_name].grad.data = self.delta[\n                layer_name\n            ].grad.data.sub_(grad_proj)\n            optimizer.step()\n            self.delta[layer_name].grad = None\n    end_time = time.time()\n    print(f\"Running time: {end_time - start_time} s\")\n    final_mem = torch.cuda.memory_allocated()\n    print(f\"Memory usage: {(final_mem - initial_mem) / (1024 ** 2)} MB\")\n    print(\"Optimization completed.\")\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.DOGE_TA_Algorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Runs the Algorithm with learnable delta to fuse models in the given model pool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>Union[BaseModelPool, Dict[str, Module]]</code>)           \u2013            <p>The pool of models to fuse.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The pre-trained model with the merged task vectors after optimizing delta.</p> </li> </ul> Source code in <code>fusion_bench/method/doge_ta/doge_ta.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: Union[BaseModelPool, Dict[str, nn.Module]]):\n    \"\"\"\n    Runs the Algorithm with learnable delta to fuse models in the given model pool.\n\n    Args:\n        modelpool (Union[BaseModelPool, Dict[str, nn.Module]]): The pool of models to fuse.\n\n    Returns:\n        nn.Module: The pre-trained model with the merged task vectors after optimizing delta.\n    \"\"\"\n    if not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(modelpool)\n\n    log.info(\"Fusing models using DOGE_TA with learnable delta.\")\n    with self.profile(\"load model\"):\n        pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n    task_vectors = self.compute_task_vectors(modelpool, pretrained_model)\n\n    self.lamdas = self.compute_layer_lamdas(task_vectors)\n    self.projection = {}\n    for layer_name in task_vectors[0].keys():\n        for i, vector in enumerate(task_vectors):\n            layer_vector = vector[layer_name].to(self.device)\n            u, s, v = torch.linalg.svd(layer_vector, full_matrices=False)\n            if i == 0:\n                print(f\"Computed SVD for {layer_name}...\")\n                sum_u = torch.zeros_like(u, device=layer_vector.device)\n                sum_s = torch.zeros_like(s, device=layer_vector.device)\n                sum_v = torch.zeros_like(v, device=layer_vector.device)\n\n            reduced_index_s = int(s.shape[0] / len(task_vectors))\n\n            # select only the first reduced_index_s columns of u and place them\n            sum_u[:, i * reduced_index_s : (i + 1) * reduced_index_s] = u[\n                :, :reduced_index_s\n            ]\n            sum_s[i * reduced_index_s : (i + 1) * reduced_index_s] = s[\n                :reduced_index_s\n            ]\n            # select only the first reduced_index_s rows of v and place them\n            sum_v[i * reduced_index_s : (i + 1) * reduced_index_s, :] = v[\n                :reduced_index_s, :\n            ]\n        u_u, s_u, v_u = torch.linalg.svd(sum_u, full_matrices=False)\n        layer_proj = torch.matmul(\n            u_u[:, : int(s.shape[0] / self.config.subspace)],\n            u_u[:, : int(s.shape[0] / self.config.subspace)].T,\n        )\n        self.projection[layer_name] = layer_proj\n\n    self.optimize_delta(task_vectors)\n\n    del self.projection\n    self.delta = {key: param.detach().cpu() for key, param in self.delta.items()}\n    self.lamdas = [\n        {key: param.cpu() for key, param in lamdas.items()}\n        for lamdas in self.lamdas\n    ]\n    task_vectors = [\n        {k: v.cpu() for k, v in task_vector.items()} for task_vector in task_vectors\n    ]\n    flat_vectors = []\n    vector_masks = []\n    for idx, task_vector in enumerate(task_vectors):\n        flat_vector = self.state_dict_to_vector(task_vector)\n        vector_mask = self.topk_values_mask(flat_vector, K=self.config.K)\n        flat_vectors.append(flat_vector)\n        vector_masks.append(vector_mask)\n    flat_delta = self.state_dict_to_vector(self.delta)\n\n    adjusted_vectors = [\n        self.vector_to_state_dict(\n            (flat_vector + flat_delta) * vector_mask, self.delta\n        )\n        for flat_vector, vector_mask in zip(flat_vectors, vector_masks)\n    ]\n\n    for layer_name in adjusted_vectors[0].keys():\n        layer_vectors = torch.stack(\n            [vec[layer_name] for vec in adjusted_vectors], dim=0\n        )\n        layer_lamdas = torch.stack(\n            [lamdas[layer_name] for lamdas in self.lamdas], dim=0\n        )\n        layer_vectors_scale = layer_vectors * layer_lamdas.view(-1, 1, 1)\n        task_vectors[0][layer_name] = layer_vectors_scale.sum(dim=0)\n\n    final_state_dict = {}\n    pretrained_sd = pretrained_model.state_dict(keep_vars=True)\n    for k, v in pretrained_sd.items():\n        if k in task_vectors[0]:\n            final_state_dict[k] = v + task_vectors[0][k]\n        else:\n            final_state_dict[k] = v\n\n    pretrained_model.load_state_dict(final_state_dict)\n\n    self.print_profile_summary()\n    return pretrained_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.DOGE_TA_Algorithm.state_dict_to_vector","title":"<code>state_dict_to_vector(state_dict, remove_keys=[])</code>","text":"<p>Convert a state dictionary to a vector, removing specified keys.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>dict</code>)           \u2013            <p>The state dictionary to convert.</p> </li> <li> <code>remove_keys</code>               (<code>list</code>, default:                   <code>[]</code> )           \u2013            <p>List of keys to remove from the state dictionary.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>          \u2013            <p>A vector representation of the state dictionary.</p> </li> </ul> Source code in <code>fusion_bench/method/doge_ta/doge_ta.py</code> <pre><code>def state_dict_to_vector(self, state_dict, remove_keys=[]):\n    \"\"\"\n    Convert a state dictionary to a vector, removing specified keys.\n\n    Args:\n        state_dict (dict): The state dictionary to convert.\n        remove_keys (list): List of keys to remove from the state dictionary.\n\n    Returns:\n        Tensor: A vector representation of the state dictionary.\n    \"\"\"\n    shared_state_dict = copy.deepcopy(state_dict)\n    for key in remove_keys:\n        if key in shared_state_dict:\n            del shared_state_dict[key]\n    sorted_shared_state_dict = OrderedDict(sorted(shared_state_dict.items()))\n    return nn.utils.parameters_to_vector(\n        [value.reshape(-1) for key, value in sorted_shared_state_dict.items()]\n    )\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.DOGE_TA_Algorithm.taskvector_loss","title":"<code>taskvector_loss(layer_vectors, layer_delta, layer_lamdas)</code>","text":"<p>Computes the loss based on delta and task vectors for a specific layer.</p> Source code in <code>fusion_bench/method/doge_ta/doge_ta.py</code> <pre><code>def taskvector_loss(self, layer_vectors, layer_delta, layer_lamdas) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the loss based on delta and task vectors for a specific layer.\n    \"\"\"\n    total_loss = 0.0\n\n    layer_vectors_scale = layer_vectors * layer_lamdas.view(-1, 1, 1)\n    sum_over_num_vectors = layer_vectors_scale.sum(dim=0)\n\n    layer_delta_scale = layer_delta.unsqueeze(0) * layer_lamdas.view(-1, 1, 1)\n    sum_over_delta = layer_delta_scale.sum(dim=0)\n\n    # Iterate through each vector and calculate the loss one by one\n    for v_j in layer_vectors:\n        part1 = -v_j * sum_over_num_vectors\n        part2 = -v_j * sum_over_delta\n        part3 = v_j * v_j\n\n        expression = part1 + part2 + part3\n        layer_loss = expression.sum(dim=1).pow(2).sum()\n\n        # Cumulative total loss\n        total_loss += layer_loss\n    return total_loss\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.DOGE_TA_Algorithm.vector_to_state_dict","title":"<code>vector_to_state_dict(vector, state_dict, remove_keys=[])</code>","text":"<p>Convert a vector back to a state dictionary, removing specified keys.</p> <p>Parameters:</p> <ul> <li> <code>vector</code>               (<code>Tensor</code>)           \u2013            <p>The vector to convert.</p> </li> <li> <code>state_dict</code>               (<code>dict</code>)           \u2013            <p>The reference state dictionary.</p> </li> <li> <code>remove_keys</code>               (<code>list</code>, default:                   <code>[]</code> )           \u2013            <p>List of keys to remove from the state dictionary.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>A state dictionary representation of the vector.</p> </li> </ul> Source code in <code>fusion_bench/method/doge_ta/doge_ta.py</code> <pre><code>def vector_to_state_dict(self, vector, state_dict, remove_keys=[]):\n    \"\"\"\n    Convert a vector back to a state dictionary, removing specified keys.\n\n    Args:\n        vector (Tensor): The vector to convert.\n        state_dict (dict): The reference state dictionary.\n        remove_keys (list): List of keys to remove from the state dictionary.\n\n    Returns:\n        dict: A state dictionary representation of the vector.\n    \"\"\"\n    # create a reference dict to define the order of the vector\n    reference_dict = copy.deepcopy(state_dict)\n    for key in remove_keys:\n        if key in reference_dict:\n            del reference_dict[key]\n    sorted_reference_dict = OrderedDict(sorted(reference_dict.items()))\n\n    # create a shared state dict using the reference dict\n    nn.utils.vector_to_parameters(vector, sorted_reference_dict.values())\n\n    # add back the encoder and decoder embedding weights.\n    if \"transformer.shared.weight\" in sorted_reference_dict:\n        for key in remove_keys:\n            sorted_reference_dict[key] = sorted_reference_dict[\n                \"transformer.shared.weight\"\n            ]\n    return sorted_reference_dict\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#adamerging","title":"AdaMerging","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.CLIPTaskWiseAdaMergingAlgorithm","title":"<code>CLIPTaskWiseAdaMergingAlgorithm</code>","text":"<p>               Bases: <code>TaskWiseAdaMergingAlgorithm</code></p> <p>A class for task-wise adaptive merging of CLIP models.</p> <p>This class extends the TaskWiseAdaMergingAlgorithm to provide specific functionality for CLIP models, including loading datasets, constructing zero-shot classification heads, and computing logits.</p> <p>Attributes:</p> <ul> <li> <code>modelpool</code>               (<code>CLIPVisionModelPool</code>)           \u2013            <p>The model pool containing CLIP models.</p> </li> <li> <code>_clip_processor</code>               (<code>CLIPProcessor</code>)           \u2013            <p>The CLIP processor for preparing inputs.</p> </li> <li> <code>zeroshot_weights</code>               (<code>dict</code>)           \u2013            <p>A dictionary to store zero-shot weights for each task.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/clip_task_wise_adamerging.py</code> <pre><code>class CLIPTaskWiseAdaMergingAlgorithm(TaskWiseAdaMergingAlgorithm):\n    \"\"\"\n    A class for task-wise adaptive merging of CLIP models.\n\n    This class extends the TaskWiseAdaMergingAlgorithm to provide specific\n    functionality for CLIP models, including loading datasets, constructing\n    zero-shot classification heads, and computing logits.\n\n    Attributes:\n        modelpool (CLIPVisionModelPool): The model pool containing CLIP models.\n        _clip_processor (CLIPProcessor): The CLIP processor for preparing inputs.\n        zeroshot_weights (dict): A dictionary to store zero-shot weights for each task.\n    \"\"\"\n\n    modelpool: CLIPVisionModelPool = None\n    _clip_processor: CLIPProcessor = None\n    zeroshot_weights = {}\n\n    def __init__(self, algorithm_config: DictConfig):\n        super().__init__(algorithm_config)\n\n    @functools.cache\n    def get_test_dataset(self, task: str) -&gt; CLIPDataset:\n        \"\"\"\n        Load the test dataset for the task.\n        This method is cached, so the dataset is loaded only once.\n\n        Args:\n            task (str): The name of the task.\n\n        Returns:\n            CLIPDataset: The test dataset for the task.\n        \"\"\"\n        log.info(f\"Loading test dataset: {task}\")\n        dataset = self.modelpool.load_test_dataset(task)\n        dataset = CLIPDataset(dataset, self._clip_processor)\n        return dataset\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(self, task: str) -&gt; Iterator:\n        \"\"\"\n        Get an iterator over the shuffled test DataLoader for the task.\n\n        Args:\n            task (str): The name of the task.\n\n        Returns:\n            iterator: An iterator over the shuffled test DataLoader.\n        \"\"\"\n        loader = DataLoader(\n            self.get_test_dataset(task),\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            num_workers=self.config.num_workers,\n            pin_memory=True,\n        )\n        if self._fabric is not None:\n            loader = self._fabric.setup_dataloaders(loader)\n        return iter(InfiniteDataLoader(loader))\n\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Prepare for test-time adaptation.\n\n        This method loads the CLIP processor and constructs the zero-shot\n        classification head for each task.\n        \"\"\"\n        clip_model_config = self.modelpool.get_model_config(\"_pretrained_\")\n        if isinstance(clip_model_config, str):\n            pretrained_path = clip_model_config\n        else:\n            pretrained_path = (\n                clip_model_config.pretrained_model_name_or_path\n                if hasattr(clip_model_config, \"pretrained_model_name_or_path\")\n                else clip_model_config.path\n            )\n\n        with timeit_context(\"Loading CLIP processor and pretrained CLIP model.\"):\n            self._clip_processor = CLIPProcessor.from_pretrained(pretrained_path)\n            clip_model: CLIPModel = CLIPModel.from_pretrained(pretrained_path)\n\n            clip_classifier = HFCLIPClassifier(clip_model, self._clip_processor)\n            self.visual_projection = clip_model.visual_projection.requires_grad_(False)\n            self.logit_scale_exp = clip_model.logit_scale.exp()\n            if self._fabric is not None:\n                self.visual_projection = self._fabric.to_device(self.visual_projection)\n                self.logit_scale_exp = self._fabric.to_device(self.logit_scale_exp)\n\n        for task in self.modelpool.model_names:\n            cache_file = os.path.join(\n                self.config.cache_dir,\n                f\"{os.path.basename(pretrained_path)}_{task}_zeroshot_weights.pt\",\n            )\n            if os.path.exists(cache_file):\n                log.info(f\"Loading cached zeroshot weights for task: {task}\")\n                zeroshot_weights = torch.load(cache_file, map_location=\"cpu\")\n            else:\n                log.info(f\"Construct zero shot classification head for task: {task}\")\n                classnames, templates = get_classnames_and_templates(task)\n                clip_classifier.set_classification_task(classnames, templates)\n                zeroshot_weights = clip_classifier.zeroshot_weights\n                log.info(f\"save zeroshot weights to {cache_file}\")\n                torch.save(zeroshot_weights, cache_file)\n            self.zeroshot_weights[task] = zeroshot_weights\n            if self._fabric is not None:\n                self.zeroshot_weights[task] = self._fabric.to_device(\n                    self.zeroshot_weights[task]\n                )\n\n    def compute_logits(self, module, batch, task: str) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for the given batch and task.\n\n        This method computes the image embeddings, normalizes them, and calculates\n        the cosine similarity with the text embeddings to produce classification logits.\n\n        Args:\n            module (nn.Module): The model module.\n            batch (tuple): A batch of input data.\n            task (str): The name of the task.\n\n        Returns:\n            Tensor: The classification logits for the batch.\n        \"\"\"\n        images, _ = batch\n        text_embeds = self.zeroshot_weights[task]\n\n        image_embeds = module(images)[1]\n        image_embeds = self.visual_projection(image_embeds)\n\n        # normalize embeddings\n        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n        # cosine similarity\n        logits_per_text = (\n            torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n        )\n        logits_per_image = logits_per_text.t()\n\n        return logits_per_image\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.CLIPTaskWiseAdaMergingAlgorithm.compute_logits","title":"<code>compute_logits(module, batch, task)</code>","text":"<p>Compute the logits for the given batch and task.</p> <p>This method computes the image embeddings, normalizes them, and calculates the cosine similarity with the text embeddings to produce classification logits.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The model module.</p> </li> <li> <code>batch</code>               (<code>tuple</code>)           \u2013            <p>A batch of input data.</p> </li> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The classification logits for the batch.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/clip_task_wise_adamerging.py</code> <pre><code>def compute_logits(self, module, batch, task: str) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for the given batch and task.\n\n    This method computes the image embeddings, normalizes them, and calculates\n    the cosine similarity with the text embeddings to produce classification logits.\n\n    Args:\n        module (nn.Module): The model module.\n        batch (tuple): A batch of input data.\n        task (str): The name of the task.\n\n    Returns:\n        Tensor: The classification logits for the batch.\n    \"\"\"\n    images, _ = batch\n    text_embeds = self.zeroshot_weights[task]\n\n    image_embeds = module(images)[1]\n    image_embeds = self.visual_projection(image_embeds)\n\n    # normalize embeddings\n    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n    # cosine similarity\n    logits_per_text = (\n        torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n    )\n    logits_per_image = logits_per_text.t()\n\n    return logits_per_image\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.CLIPTaskWiseAdaMergingAlgorithm.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(task)</code>  <code>cached</code>","text":"<p>Get an iterator over the shuffled test DataLoader for the task.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>iterator</code> (              <code>Iterator</code> )          \u2013            <p>An iterator over the shuffled test DataLoader.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/clip_task_wise_adamerging.py</code> <pre><code>@functools.cache\ndef get_shuffled_test_loader_iter(self, task: str) -&gt; Iterator:\n    \"\"\"\n    Get an iterator over the shuffled test DataLoader for the task.\n\n    Args:\n        task (str): The name of the task.\n\n    Returns:\n        iterator: An iterator over the shuffled test DataLoader.\n    \"\"\"\n    loader = DataLoader(\n        self.get_test_dataset(task),\n        batch_size=self.config.batch_size,\n        shuffle=True,\n        num_workers=self.config.num_workers,\n        pin_memory=True,\n    )\n    if self._fabric is not None:\n        loader = self._fabric.setup_dataloaders(loader)\n    return iter(InfiniteDataLoader(loader))\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.CLIPTaskWiseAdaMergingAlgorithm.get_test_dataset","title":"<code>get_test_dataset(task)</code>  <code>cached</code>","text":"<p>Load the test dataset for the task. This method is cached, so the dataset is loaded only once.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>CLIPDataset</code> (              <code>CLIPDataset</code> )          \u2013            <p>The test dataset for the task.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/clip_task_wise_adamerging.py</code> <pre><code>@functools.cache\ndef get_test_dataset(self, task: str) -&gt; CLIPDataset:\n    \"\"\"\n    Load the test dataset for the task.\n    This method is cached, so the dataset is loaded only once.\n\n    Args:\n        task (str): The name of the task.\n\n    Returns:\n        CLIPDataset: The test dataset for the task.\n    \"\"\"\n    log.info(f\"Loading test dataset: {task}\")\n    dataset = self.modelpool.load_test_dataset(task)\n    dataset = CLIPDataset(dataset, self._clip_processor)\n    return dataset\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.CLIPTaskWiseAdaMergingAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Prepare for test-time adaptation.</p> <p>This method loads the CLIP processor and constructs the zero-shot classification head for each task.</p> Source code in <code>fusion_bench/method/adamerging/clip_task_wise_adamerging.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Prepare for test-time adaptation.\n\n    This method loads the CLIP processor and constructs the zero-shot\n    classification head for each task.\n    \"\"\"\n    clip_model_config = self.modelpool.get_model_config(\"_pretrained_\")\n    if isinstance(clip_model_config, str):\n        pretrained_path = clip_model_config\n    else:\n        pretrained_path = (\n            clip_model_config.pretrained_model_name_or_path\n            if hasattr(clip_model_config, \"pretrained_model_name_or_path\")\n            else clip_model_config.path\n        )\n\n    with timeit_context(\"Loading CLIP processor and pretrained CLIP model.\"):\n        self._clip_processor = CLIPProcessor.from_pretrained(pretrained_path)\n        clip_model: CLIPModel = CLIPModel.from_pretrained(pretrained_path)\n\n        clip_classifier = HFCLIPClassifier(clip_model, self._clip_processor)\n        self.visual_projection = clip_model.visual_projection.requires_grad_(False)\n        self.logit_scale_exp = clip_model.logit_scale.exp()\n        if self._fabric is not None:\n            self.visual_projection = self._fabric.to_device(self.visual_projection)\n            self.logit_scale_exp = self._fabric.to_device(self.logit_scale_exp)\n\n    for task in self.modelpool.model_names:\n        cache_file = os.path.join(\n            self.config.cache_dir,\n            f\"{os.path.basename(pretrained_path)}_{task}_zeroshot_weights.pt\",\n        )\n        if os.path.exists(cache_file):\n            log.info(f\"Loading cached zeroshot weights for task: {task}\")\n            zeroshot_weights = torch.load(cache_file, map_location=\"cpu\")\n        else:\n            log.info(f\"Construct zero shot classification head for task: {task}\")\n            classnames, templates = get_classnames_and_templates(task)\n            clip_classifier.set_classification_task(classnames, templates)\n            zeroshot_weights = clip_classifier.zeroshot_weights\n            log.info(f\"save zeroshot weights to {cache_file}\")\n            torch.save(zeroshot_weights, cache_file)\n        self.zeroshot_weights[task] = zeroshot_weights\n        if self._fabric is not None:\n            self.zeroshot_weights[task] = self._fabric.to_device(\n                self.zeroshot_weights[task]\n            )\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.CLIPLayerWiseAdaMergingAlgorithm","title":"<code>CLIPLayerWiseAdaMergingAlgorithm</code>","text":"<p>               Bases: <code>CLIPClassificationMixin</code>, <code>LayerWiseAdaMergingAlgorithm</code></p> Source code in <code>fusion_bench/method/adamerging/clip_layer_wise_adamerging.py</code> <pre><code>class CLIPLayerWiseAdaMergingAlgorithm(\n    CLIPClassificationMixin,\n    LayerWiseAdaMergingAlgorithm,\n):\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Here we load the CLIP processor and construct the zero-shot classification head for each task.\n        \"\"\"\n        self.setup_zero_shot_classification_head()\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(self, task: str):\n        return super().get_shuffled_test_loader_iter(\n            task,\n            batch_size=self.config.batch_size,\n            num_workers=self.config.num_workers,\n        )\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.CLIPLayerWiseAdaMergingAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Here we load the CLIP processor and construct the zero-shot classification head for each task.</p> Source code in <code>fusion_bench/method/adamerging/clip_layer_wise_adamerging.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Here we load the CLIP processor and construct the zero-shot classification head for each task.\n    \"\"\"\n    self.setup_zero_shot_classification_head()\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.GPT2LayerWiseAdaMergingAlgorithm","title":"<code>GPT2LayerWiseAdaMergingAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>LightningFabricMixin</code>, <code>SimpleProfilerMixin</code></p> Source code in <code>fusion_bench/method/adamerging/gpt2_layer_wise_adamerging.py</code> <pre><code>class GPT2LayerWiseAdaMergingAlgorithm(\n    BaseAlgorithm,\n    LightningFabricMixin,\n    SimpleProfilerMixin,\n):\n    scores: Dict[str, nn.Linear] = None\n\n    def __init__(\n        self,\n        optimizer: DictConfig,\n        dataloader_kwargs: DictConfig,\n        init_values: float,\n        max_steps: int,\n        merging_weights_load_path: Optional[Union[str, Path]] = None,\n        merging_weights_save_path: Optional[Union[str, Path]] = None,\n        clamp_weights: bool = False,\n        tie_weights: bool = True,\n        strict: bool = False,\n        cache_dir: str = \"outputs/cache\",\n        variant: Optional[str] = None,\n        **kwargs,\n    ):\n        self._optimizer = optimizer\n        self.dataloader_kwargs = dataloader_kwargs\n        self.init_values = init_values\n        self.merging_weights_load_path = merging_weights_load_path\n        self.merging_weights_save_path = merging_weights_save_path\n        self.clamp_weights = clamp_weights\n        self.tie_weights = tie_weights\n        self.strict = strict\n        self.max_steps = max_steps\n        self.cache_dir = cache_dir\n        self.variant = variant\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def construct_layer_wise_merged_model(\n        self, modelpool: GPT2ForSequenceClassificationPool\n    ):\n        \"\"\"\n        Constructs a wrapped layer-wise merged model from model pool.\n\n        This method creates a new wrapped model by merging the layers of a pretrained model with those of several fine-tuned models.\n        The merging is controlled by layer-wise weights, which is a `torch.Tensor` of the shape `(num_models, num_layers)`.\n        The merging weights can be initialized based on a provided configuration or loaded from a file.\n\n        Args:\n            modelpool (ModelPool): An object containing the pretrained model and fine-tuned models to be merged.\n\n        Returns:\n            LayerWiseMergedModel: An instance of the merged model with layer-wise weights applied.\n        \"\"\"\n        pretrained_model: GPT2Model = modelpool.load_model(\"_pretrained_\")\n        finetuned_models: List[GPT2Model] = [\n            modelpool.load_model(name) for name in modelpool.model_names\n        ]\n\n        # initialize layer-wise weights using the provided configuration `init_values` or load from file if `weights` is provided\n        if self.merging_weights_load_path is None:\n            layer_wise_weight = get_layer_wise_weights(\n                num_models=len(modelpool.model_names),\n                num_layers=len(\n                    tuple(\n                        filter(lambda p: p.requires_grad, pretrained_model.parameters())\n                    )\n                ),\n                init_values=self.init_values,\n            )\n        else:\n            if isinstance(self.merging_weights_load_path, str):\n                # load the merging weights from a file\n                layer_wise_weight = load_tensor_from_file(\n                    self.merging_weights_load_path\n                )\n            else:\n                raise ValueError(\n                    f\"Unsupported weights format: {self.merging_weights_load_path}\"\n                )\n\n        module = LayerWiseMergedModel(\n            layer_wise_weight=layer_wise_weight,\n            pretrained_model=pretrained_model,\n            finetuned_models=finetuned_models,\n            clamp_weights=self.clamp_weights,\n            tie_weights=self.tie_weights,\n            strict=self.strict,\n        )\n        print(f\"{layer_wise_weight.size()=}, {layer_wise_weight.numel()=}\")\n        return module\n\n    @rank_zero_only\n    def save_merging_weights(self, file_path: str, merging_weights: torch.Tensor):\n        \"\"\"\n        Save the merging weights to a file.\n\n        Args:\n            file_path (str): The path to save the merging weights.\n            merging_weights (torch.Tensor): The merging weights to save.\n        \"\"\"\n        if self.fabric.is_global_zero and self.merging_weights_save_path is not None:\n            if isinstance(file_path, str) and not file_path.startswith((\"/\", \".\")):\n                # if the file path is not absolute or relative to current working directory, save it in the log directory\n                save_path = os.path.join(self.log_dir, file_path)\n            else:\n                save_path = file_path\n            log.info(f\"saving merging weights to {save_path}.\")\n            if os.path.dirname(save_path):\n                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n            torch.save(merging_weights.detach().cpu(), save_path)\n\n    def run(self, modelpool: GPT2ForSequenceClassificationPool, **kwargs):\n        \"\"\"\n        Run the Layer-Wise AdaMerging Algorithm.\n\n        This method constructs the wrapped model and performs test-time adaptation if necessary.\n\n        Args:\n            modelpool (ModelPool): The model pool containing the pretrained and fine-tuned models.\n\n        Returns:\n            LayerWiseMergedModel: The merged model after test-time adaptation.\n        \"\"\"\n        log.info(\"Fusing models using layer-wise adaptive merging.\")\n        self.modelpool = modelpool\n\n        with self.profile(\"construct the wrapped model\"):\n            module = self.construct_layer_wise_merged_model(modelpool)\n\n        if self.merging_weights_load_path is not None:\n            # skip the test-time adaptation\n            return module.merge_and_unload()\n        else:\n            with self.profile(\"test-time adaptation\"):\n                module = self.test_time_adaptation(module)\n            if self.merging_weights_save_path is not None:\n                self.save_merging_weights(\n                    self.merging_weights_save_path, module.merge_weight\n                )\n            return module.merge_and_unload()\n\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Something to do before the test-time adaptation starts. Such as setting up the task-specific heads.\n        \"\"\"\n        self.scores = {}\n        for model_name in self.modelpool.model_names:\n            score = cast(\n                GPT2ForSequenceClassification,\n                self.modelpool.load_classifier(model_name),\n            ).score.requires_grad_(False)\n            score = score.to(self.fabric.device)\n            self.scores[model_name] = score\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n        \"\"\"\n        Loader of test dataset for test-time adaptation. labels are not needed.\n\n        Args:\n            task (str): The name of the task.\n\n        Returns:\n            DataLoader: The data loader for the test dataset.\n        \"\"\"\n        dataloader_kwargs = dict(self.dataloader_kwargs)\n        dataloader_kwargs.update(dict(shuffle=True, collate_fn=default_data_collator))\n\n        dataset = self.modelpool.load_test_dataset(task)\n        loader = DataLoader(dataset, **dataloader_kwargs)\n\n        if self.fabric is not None:\n            loader = self.fabric.setup_dataloaders(loader)\n        return iter(InfiniteDataLoader(loader))\n\n    def compute_logits(self, module: GPT2Model, batch, task: str) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for the given images and task.\n\n        Args:\n            module: The model module.\n            images (Tensor): The input images.\n            task (str): The name of the task.\n\n        Returns:\n            Tensor: The computed logits.\n        \"\"\"\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        batch_size, _ = input_ids.shape[:2]\n        pad_token_id = 50256\n\n        transformer_outputs = module(\n            input_ids,\n            past_key_values=None,\n            attention_mask=attention_mask,\n            token_type_ids=None,\n            position_ids=None,\n            head_mask=None,\n            inputs_embeds=None,\n            use_cache=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=True,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.scores[task](hidden_states)\n\n        sequence_lengths = torch.eq(input_ids, pad_token_id).int().argmax(-1) - 1\n        sequence_lengths = sequence_lengths % input_ids.shape[-1]\n        sequence_lengths = sequence_lengths.to(logits.device)\n\n        pooled_logits = logits[\n            torch.arange(batch_size, device=logits.device), sequence_lengths\n        ]\n\n        assert pooled_logits.dim() == 2\n        return pooled_logits\n\n    def test_time_adaptation(self, module: LayerWiseMergedModel):\n        \"\"\"\n        Perform test-time adaptation on the merged model.\n\n        This method adapts the merging weights during test-time to improve performance.\n\n        Args:\n            module (LayerWiseMergedModel): The merged model.\n\n        Returns:\n            LayerWiseMergedModel: The adapted merged model.\n        \"\"\"\n        self.on_test_time_adaptation_start()\n\n        # configure optimizer\n        optimizer = instantiate(self._optimizer, [module.merge_weight])\n        module, optimizer = self.fabric.setup(module, optimizer)\n\n        module.train()\n        module.merge_weights()\n        for step_idx in (\n            pbar := tqdm(\n                range(self.max_steps if not self.is_debug_mode else 1),\n                (\"[DEBUG MODE] \" if self.is_debug_mode else \"\")\n                + \"AdaMerging Test-time adaptation\",\n                dynamic_ncols=True,\n            )\n        ):\n            if self.variant == \"mgda\":\n                total_loss = self._compute_gradients_using_mgda(module)\n            else:\n                total_loss = 0\n                for task in self.modelpool.model_names:\n                    with self.profile(\"data loading\"):\n                        batch = next(self.get_shuffled_test_loader_iter(task))\n                    with self.profile(\"forward pass\"):\n                        logits = self.compute_logits(module, batch, task)\n                        logits = logits.mean(dim=0, keepdim=True)\n                        loss = entropy_loss(logits)\n                        total_loss += loss\n                    with self.profile(\"backward pass\"):\n                        self.fabric.backward(loss, retain_graph=True)\n\n            with self.profile(\"optimizer step\"):\n                optimizer.step()\n                optimizer.zero_grad()\n            with self.profile(\"merging weights\"):\n                module.merge_weights()\n\n            metrics = {\n                \"train/loss\": total_loss.item(),\n                \"train/weight_max\": module.merge_weight.max().item(),\n                \"train/weight_min\": module.merge_weight.min().item(),\n                \"train/weight_mean\": module.merge_weight.mean().item(),\n            }\n            self.fabric.log_dict(metrics, step=step_idx)\n            pbar.set_postfix(metrics)\n\n        log.info(get_memory_usage(f\"after adamerging, the memory usage of GPU is:\"))\n        self.print_profile_summary()\n        return module\n\n    def _compute_gradients_using_mgda(self, module: LayerWiseMergedModel):\n        all_grads = []\n        total_loss = 0\n        # default behavior for first-order optimizers\n        for task in self.modelpool.model_names:\n            with self.profile(\"data loading\"):\n                batch = next(self.get_shuffled_test_loader_iter(task))\n            with self.profile(\"forward pass\"):\n                logits = self.compute_logits(module, batch, task)\n                logits = logits.mean(dim=0, keepdim=True)\n                loss = entropy_loss(logits)\n                total_loss += loss\n            with self.profile(\"backward pass\"):\n                # self.fabric.backward(loss, retain_graph=True)\n                _grads = torch.autograd.grad(\n                    loss,\n                    [module.merge_weight],\n                    create_graph=False,\n                    retain_graph=True,\n                )\n                all_grads.append(_grads[0].flatten().detach())\n        sol, min_norm = MinNormSolver.find_min_norm_element(all_grads)\n        if not isinstance(sol, torch.Tensor):\n            sol = torch.from_numpy(sol)\n        sol = sol.to(\n            device=module.merge_weight.device,\n            dtype=module.merge_weight.dtype,\n        )\n        grad = torch.stack(all_grads) * sol.view(-1, 1)\n        module.merge_weight.grad = grad.sum(dim=0).view_as(module.merge_weight)\n        return total_loss\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.GPT2LayerWiseAdaMergingAlgorithm.compute_logits","title":"<code>compute_logits(module, batch, task)</code>","text":"<p>Compute the logits for the given images and task.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>GPT2Model</code>)           \u2013            <p>The model module.</p> </li> <li> <code>images</code>               (<code>Tensor</code>)           \u2013            <p>The input images.</p> </li> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The computed logits.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/gpt2_layer_wise_adamerging.py</code> <pre><code>def compute_logits(self, module: GPT2Model, batch, task: str) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for the given images and task.\n\n    Args:\n        module: The model module.\n        images (Tensor): The input images.\n        task (str): The name of the task.\n\n    Returns:\n        Tensor: The computed logits.\n    \"\"\"\n    input_ids = batch[\"input_ids\"]\n    attention_mask = batch[\"attention_mask\"]\n    batch_size, _ = input_ids.shape[:2]\n    pad_token_id = 50256\n\n    transformer_outputs = module(\n        input_ids,\n        past_key_values=None,\n        attention_mask=attention_mask,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=True,\n    )\n    hidden_states = transformer_outputs[0]\n    logits = self.scores[task](hidden_states)\n\n    sequence_lengths = torch.eq(input_ids, pad_token_id).int().argmax(-1) - 1\n    sequence_lengths = sequence_lengths % input_ids.shape[-1]\n    sequence_lengths = sequence_lengths.to(logits.device)\n\n    pooled_logits = logits[\n        torch.arange(batch_size, device=logits.device), sequence_lengths\n    ]\n\n    assert pooled_logits.dim() == 2\n    return pooled_logits\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.GPT2LayerWiseAdaMergingAlgorithm.construct_layer_wise_merged_model","title":"<code>construct_layer_wise_merged_model(modelpool)</code>","text":"<p>Constructs a wrapped layer-wise merged model from model pool.</p> <p>This method creates a new wrapped model by merging the layers of a pretrained model with those of several fine-tuned models. The merging is controlled by layer-wise weights, which is a <code>torch.Tensor</code> of the shape <code>(num_models, num_layers)</code>. The merging weights can be initialized based on a provided configuration or loaded from a file.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>ModelPool</code>)           \u2013            <p>An object containing the pretrained model and fine-tuned models to be merged.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LayerWiseMergedModel</code>          \u2013            <p>An instance of the merged model with layer-wise weights applied.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/gpt2_layer_wise_adamerging.py</code> <pre><code>@torch.no_grad()\ndef construct_layer_wise_merged_model(\n    self, modelpool: GPT2ForSequenceClassificationPool\n):\n    \"\"\"\n    Constructs a wrapped layer-wise merged model from model pool.\n\n    This method creates a new wrapped model by merging the layers of a pretrained model with those of several fine-tuned models.\n    The merging is controlled by layer-wise weights, which is a `torch.Tensor` of the shape `(num_models, num_layers)`.\n    The merging weights can be initialized based on a provided configuration or loaded from a file.\n\n    Args:\n        modelpool (ModelPool): An object containing the pretrained model and fine-tuned models to be merged.\n\n    Returns:\n        LayerWiseMergedModel: An instance of the merged model with layer-wise weights applied.\n    \"\"\"\n    pretrained_model: GPT2Model = modelpool.load_model(\"_pretrained_\")\n    finetuned_models: List[GPT2Model] = [\n        modelpool.load_model(name) for name in modelpool.model_names\n    ]\n\n    # initialize layer-wise weights using the provided configuration `init_values` or load from file if `weights` is provided\n    if self.merging_weights_load_path is None:\n        layer_wise_weight = get_layer_wise_weights(\n            num_models=len(modelpool.model_names),\n            num_layers=len(\n                tuple(\n                    filter(lambda p: p.requires_grad, pretrained_model.parameters())\n                )\n            ),\n            init_values=self.init_values,\n        )\n    else:\n        if isinstance(self.merging_weights_load_path, str):\n            # load the merging weights from a file\n            layer_wise_weight = load_tensor_from_file(\n                self.merging_weights_load_path\n            )\n        else:\n            raise ValueError(\n                f\"Unsupported weights format: {self.merging_weights_load_path}\"\n            )\n\n    module = LayerWiseMergedModel(\n        layer_wise_weight=layer_wise_weight,\n        pretrained_model=pretrained_model,\n        finetuned_models=finetuned_models,\n        clamp_weights=self.clamp_weights,\n        tie_weights=self.tie_weights,\n        strict=self.strict,\n    )\n    print(f\"{layer_wise_weight.size()=}, {layer_wise_weight.numel()=}\")\n    return module\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.GPT2LayerWiseAdaMergingAlgorithm.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(task)</code>  <code>cached</code>","text":"<p>Loader of test dataset for test-time adaptation. labels are not needed.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code> (              <code>DataLoader</code> )          \u2013            <p>The data loader for the test dataset.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/gpt2_layer_wise_adamerging.py</code> <pre><code>@functools.cache\ndef get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n    \"\"\"\n    Loader of test dataset for test-time adaptation. labels are not needed.\n\n    Args:\n        task (str): The name of the task.\n\n    Returns:\n        DataLoader: The data loader for the test dataset.\n    \"\"\"\n    dataloader_kwargs = dict(self.dataloader_kwargs)\n    dataloader_kwargs.update(dict(shuffle=True, collate_fn=default_data_collator))\n\n    dataset = self.modelpool.load_test_dataset(task)\n    loader = DataLoader(dataset, **dataloader_kwargs)\n\n    if self.fabric is not None:\n        loader = self.fabric.setup_dataloaders(loader)\n    return iter(InfiniteDataLoader(loader))\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.GPT2LayerWiseAdaMergingAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Something to do before the test-time adaptation starts. Such as setting up the task-specific heads.</p> Source code in <code>fusion_bench/method/adamerging/gpt2_layer_wise_adamerging.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Something to do before the test-time adaptation starts. Such as setting up the task-specific heads.\n    \"\"\"\n    self.scores = {}\n    for model_name in self.modelpool.model_names:\n        score = cast(\n            GPT2ForSequenceClassification,\n            self.modelpool.load_classifier(model_name),\n        ).score.requires_grad_(False)\n        score = score.to(self.fabric.device)\n        self.scores[model_name] = score\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.GPT2LayerWiseAdaMergingAlgorithm.run","title":"<code>run(modelpool, **kwargs)</code>","text":"<p>Run the Layer-Wise AdaMerging Algorithm.</p> <p>This method constructs the wrapped model and performs test-time adaptation if necessary.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>ModelPool</code>)           \u2013            <p>The model pool containing the pretrained and fine-tuned models.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LayerWiseMergedModel</code>          \u2013            <p>The merged model after test-time adaptation.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/gpt2_layer_wise_adamerging.py</code> <pre><code>def run(self, modelpool: GPT2ForSequenceClassificationPool, **kwargs):\n    \"\"\"\n    Run the Layer-Wise AdaMerging Algorithm.\n\n    This method constructs the wrapped model and performs test-time adaptation if necessary.\n\n    Args:\n        modelpool (ModelPool): The model pool containing the pretrained and fine-tuned models.\n\n    Returns:\n        LayerWiseMergedModel: The merged model after test-time adaptation.\n    \"\"\"\n    log.info(\"Fusing models using layer-wise adaptive merging.\")\n    self.modelpool = modelpool\n\n    with self.profile(\"construct the wrapped model\"):\n        module = self.construct_layer_wise_merged_model(modelpool)\n\n    if self.merging_weights_load_path is not None:\n        # skip the test-time adaptation\n        return module.merge_and_unload()\n    else:\n        with self.profile(\"test-time adaptation\"):\n            module = self.test_time_adaptation(module)\n        if self.merging_weights_save_path is not None:\n            self.save_merging_weights(\n                self.merging_weights_save_path, module.merge_weight\n            )\n        return module.merge_and_unload()\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.GPT2LayerWiseAdaMergingAlgorithm.save_merging_weights","title":"<code>save_merging_weights(file_path, merging_weights)</code>","text":"<p>Save the merging weights to a file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str</code>)           \u2013            <p>The path to save the merging weights.</p> </li> <li> <code>merging_weights</code>               (<code>Tensor</code>)           \u2013            <p>The merging weights to save.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/gpt2_layer_wise_adamerging.py</code> <pre><code>@rank_zero_only\ndef save_merging_weights(self, file_path: str, merging_weights: torch.Tensor):\n    \"\"\"\n    Save the merging weights to a file.\n\n    Args:\n        file_path (str): The path to save the merging weights.\n        merging_weights (torch.Tensor): The merging weights to save.\n    \"\"\"\n    if self.fabric.is_global_zero and self.merging_weights_save_path is not None:\n        if isinstance(file_path, str) and not file_path.startswith((\"/\", \".\")):\n            # if the file path is not absolute or relative to current working directory, save it in the log directory\n            save_path = os.path.join(self.log_dir, file_path)\n        else:\n            save_path = file_path\n        log.info(f\"saving merging weights to {save_path}.\")\n        if os.path.dirname(save_path):\n            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        torch.save(merging_weights.detach().cpu(), save_path)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.GPT2LayerWiseAdaMergingAlgorithm.test_time_adaptation","title":"<code>test_time_adaptation(module)</code>","text":"<p>Perform test-time adaptation on the merged model.</p> <p>This method adapts the merging weights during test-time to improve performance.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>LayerWiseMergedModel</code>)           \u2013            <p>The merged model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LayerWiseMergedModel</code>          \u2013            <p>The adapted merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/gpt2_layer_wise_adamerging.py</code> <pre><code>def test_time_adaptation(self, module: LayerWiseMergedModel):\n    \"\"\"\n    Perform test-time adaptation on the merged model.\n\n    This method adapts the merging weights during test-time to improve performance.\n\n    Args:\n        module (LayerWiseMergedModel): The merged model.\n\n    Returns:\n        LayerWiseMergedModel: The adapted merged model.\n    \"\"\"\n    self.on_test_time_adaptation_start()\n\n    # configure optimizer\n    optimizer = instantiate(self._optimizer, [module.merge_weight])\n    module, optimizer = self.fabric.setup(module, optimizer)\n\n    module.train()\n    module.merge_weights()\n    for step_idx in (\n        pbar := tqdm(\n            range(self.max_steps if not self.is_debug_mode else 1),\n            (\"[DEBUG MODE] \" if self.is_debug_mode else \"\")\n            + \"AdaMerging Test-time adaptation\",\n            dynamic_ncols=True,\n        )\n    ):\n        if self.variant == \"mgda\":\n            total_loss = self._compute_gradients_using_mgda(module)\n        else:\n            total_loss = 0\n            for task in self.modelpool.model_names:\n                with self.profile(\"data loading\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                with self.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, batch, task)\n                    logits = logits.mean(dim=0, keepdim=True)\n                    loss = entropy_loss(logits)\n                    total_loss += loss\n                with self.profile(\"backward pass\"):\n                    self.fabric.backward(loss, retain_graph=True)\n\n        with self.profile(\"optimizer step\"):\n            optimizer.step()\n            optimizer.zero_grad()\n        with self.profile(\"merging weights\"):\n            module.merge_weights()\n\n        metrics = {\n            \"train/loss\": total_loss.item(),\n            \"train/weight_max\": module.merge_weight.max().item(),\n            \"train/weight_min\": module.merge_weight.min().item(),\n            \"train/weight_mean\": module.merge_weight.mean().item(),\n        }\n        self.fabric.log_dict(metrics, step=step_idx)\n        pbar.set_postfix(metrics)\n\n    log.info(get_memory_usage(f\"after adamerging, the memory usage of GPU is:\"))\n    self.print_profile_summary()\n    return module\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseAdaMergingAlgorithm","title":"<code>FlanT5LayerWiseAdaMergingAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>LightningFabricMixin</code>, <code>SimpleProfilerMixin</code></p> Source code in <code>fusion_bench/method/adamerging/flan_t5_layer_wise_adamerging.py</code> <pre><code>class FlanT5LayerWiseAdaMergingAlgorithm(\n    BaseAlgorithm,\n    LightningFabricMixin,\n    SimpleProfilerMixin,\n):\n\n    def __init__(\n        self,\n        optimizer: DictConfig,\n        dataloader_kwargs: DictConfig,\n        init_values: float,\n        max_steps: int,\n        merging_weights_load_path: Optional[Union[str, Path]] = None,\n        merging_weights_save_path: Optional[Union[str, Path]] = None,\n        clamp_weights: bool = False,\n        tie_weights: bool = True,\n        strict: bool = False,\n        cache_dir: str = \"outputs/cache\",\n        variant: Optional[str] = None,\n        **kwargs,\n    ):\n        self._optimizer = optimizer\n        self.dataloader_kwargs = dataloader_kwargs\n        self.init_values = init_values\n        self.merging_weights_load_path = merging_weights_load_path\n        self.merging_weights_save_path = merging_weights_save_path\n        self.clamp_weights = clamp_weights\n        self.tie_weights = tie_weights\n        self.strict = strict\n        self.max_steps = max_steps\n        self.cache_dir = cache_dir\n        self.variant = variant\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def construct_layer_wise_merged_model(self, modelpool: Seq2SeqLMPool):\n        \"\"\"\n        Constructs a wrapped layer-wise merged model from model pool.\n\n        This method creates a new wrapped model by merging the layers of a pretrained model with those of several fine-tuned models.\n        The merging is controlled by layer-wise weights, which is a `torch.Tensor` of the shape `(num_models, num_layers)`.\n        The merging weights can be initialized based on a provided configuration or loaded from a file.\n\n        Args:\n            modelpool (ModelPool): An object containing the pretrained model and fine-tuned models to be merged.\n\n        Returns:\n            LayerWiseMergedModel: An instance of the merged model with layer-wise weights applied.\n        \"\"\"\n        pretrained_model = modelpool.load_model(\"_pretrained_\")\n        finetuned_models = [\n            modelpool.load_model(name) for name in modelpool.model_names\n        ]\n\n        # initialize layer-wise weights using the provided configuration `init_values` or load from file if `weights` is provided\n        if self.merging_weights_load_path is None:\n            layer_wise_weight = get_layer_wise_weights(\n                num_models=len(modelpool.model_names),\n                num_layers=len(\n                    tuple(\n                        filter(lambda p: p.requires_grad, pretrained_model.parameters())\n                    )\n                ),\n                init_values=self.init_values,\n            )\n        else:\n            if isinstance(self.merging_weights_load_path, str):\n                # load the merging weights from a file\n                layer_wise_weight = load_tensor_from_file(\n                    self.merging_weights_load_path\n                )\n            else:\n                raise ValueError(\n                    f\"Unsupported weights format: {self.merging_weights_load_path}\"\n                )\n\n        module = LayerWiseMergedModel(\n            layer_wise_weight=layer_wise_weight,\n            pretrained_model=pretrained_model,\n            finetuned_models=finetuned_models,\n            clamp_weights=self.clamp_weights,\n            tie_weights=self.tie_weights,\n            strict=self.strict,\n        )\n        print(f\"{layer_wise_weight.size()=}, {layer_wise_weight.numel()=}\")\n        return module\n\n    @rank_zero_only\n    def save_merging_weights(self, file_path: str, merging_weights: torch.Tensor):\n        \"\"\"\n        Save the merging weights to a file.\n\n        Args:\n            file_path (str): The path to save the merging weights.\n            merging_weights (torch.Tensor): The merging weights to save.\n        \"\"\"\n        if self.fabric.is_global_zero and self.merging_weights_save_path is not None:\n            if isinstance(file_path, str) and not file_path.startswith((\"/\", \".\")):\n                # if the file path is not absolute or relative to current working directory, save it in the log directory\n                save_path = os.path.join(self.log_dir, file_path)\n            else:\n                save_path = file_path\n            log.info(f\"saving merging weights to {save_path}.\")\n            if os.path.dirname(save_path):\n                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n            torch.save(merging_weights.detach().cpu(), save_path)\n\n    def run(self, modelpool: Seq2SeqLMPool, **kwargs):\n        \"\"\"\n        Run the Layer-Wise AdaMerging Algorithm.\n\n        This method constructs the wrapped model and performs test-time adaptation if necessary.\n\n        Args:\n            modelpool (ModelPool): The model pool containing the pretrained and fine-tuned models.\n\n        Returns:\n            LayerWiseMergedModel: The merged model after test-time adaptation.\n        \"\"\"\n        log.info(\"Fusing models using layer-wise adaptive merging.\")\n        self.modelpool = modelpool\n\n        with self.profile(\"construct the wrapped model\"):\n            module = self.construct_layer_wise_merged_model(modelpool)\n\n        if self.merging_weights_load_path is not None:\n            # skip the test-time adaptation\n            return module.merge_and_unload()\n        else:\n            with self.profile(\"test-time adaptation\"):\n                module = self.test_time_adaptation(module)\n            if self.merging_weights_save_path is not None:\n                self.save_merging_weights(\n                    self.merging_weights_save_path, module.merge_weight\n                )\n            return module.merge_and_unload()\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n        \"\"\"\n        Loader of test dataset for test-time adaptation. labels are not needed.\n\n        Args:\n            task (str): The name of the task.\n\n        Returns:\n            DataLoader: The data loader for the test dataset.\n        \"\"\"\n        dataloader_kwargs = dict(self.dataloader_kwargs)\n        dataloader_kwargs.update(dict(shuffle=True, collate_fn=default_data_collator))\n\n        dataset = self.modelpool.load_test_dataset(task)\n        loader = DataLoader(dataset, **dataloader_kwargs)\n\n        if self.fabric is not None:\n            loader = self.fabric.setup_dataloaders(loader)\n        return iter(InfiniteDataLoader(loader))\n\n    def compute_logits(\n        self,\n        module: Union[T5ForConditionalGeneration, LayerWiseMergedModel],\n        batch,\n        task: str,\n    ) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for the given images and task.\n\n        Args:\n            module: The model module.\n            images (Tensor): The input images.\n            task (str): The name of the task.\n\n        Returns:\n            Tensor: The computed logits.\n        \"\"\"\n        input_ids: Tensor = batch[\"input_ids\"]\n        attention_mask: Tensor = batch[\"attention_mask\"]\n\n        # remove padding tokens from the input\n        while attention_mask[:, -1].eq(0).all():\n            input_ids = input_ids[:, :-1]\n            attention_mask = attention_mask[:, :-1]\n\n        outputs = module(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=torch.ones(\n                input_ids.size(0), 1, dtype=torch.long, device=input_ids.device\n            ),\n        )\n        logits = outputs.logits[:, 0, :]\n        return logits\n\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Something to do before the test-time adaptation starts. Such as setting up the task-specific heads.\n        \"\"\"\n        pass\n\n    def test_time_adaptation(self, module: LayerWiseMergedModel):\n        \"\"\"\n        Perform test-time adaptation on the merged model.\n\n        This method adapts the merging weights during test-time to improve performance.\n\n        Args:\n            module (LayerWiseMergedModel): The merged model.\n\n        Returns:\n            LayerWiseMergedModel: The adapted merged model.\n        \"\"\"\n        self.on_test_time_adaptation_start()\n\n        # configure optimizer\n        optimizer = instantiate(self._optimizer, [module.merge_weight])\n        module, optimizer = self.fabric.setup(module, optimizer)\n\n        module.train()\n        module.merge_weights()\n        for step_idx in (\n            pbar := tqdm(\n                range(self.max_steps if not self.is_debug_mode else 1),\n                (\"[DEBUG MODE] \" if self.is_debug_mode else \"\")\n                + \"AdaMerging Test-time adaptation\",\n                dynamic_ncols=True,\n            )\n        ):\n            if self.variant == \"mgda\":\n                total_loss = self._compute_gradients_using_mgda(module)\n            else:\n                total_loss = 0\n                for task in self.modelpool.model_names:\n                    with self.profile(\"data loading\"):\n                        batch = next(self.get_shuffled_test_loader_iter(task))\n                    with self.profile(\"forward pass\"):\n                        logits = self.compute_logits(module, batch, task)\n                        logits = logits.mean(dim=0, keepdim=True)\n                        loss = entropy_loss(logits)\n                        total_loss += loss\n                    with self.profile(\"backward pass\"):\n                        self.fabric.backward(loss, retain_graph=True)\n\n            with self.profile(\"optimizer step\"):\n                optimizer.step()\n                optimizer.zero_grad()\n            with self.profile(\"merging weights\"):\n                module.merge_weights()\n\n            metrics = {\n                \"train/loss\": total_loss.item(),\n                \"train/weight_max\": module.merge_weight.max().item(),\n                \"train/weight_min\": module.merge_weight.min().item(),\n                \"train/weight_mean\": module.merge_weight.mean().item(),\n            }\n            self.fabric.log_dict(metrics, step=step_idx)\n            pbar.set_postfix(metrics)\n\n        log.info(get_memory_usage(f\"after adamerging, the memory usage of GPU is:\"))\n        self.print_profile_summary()\n        return module\n\n    def _compute_gradients_using_mgda(self, module: LayerWiseMergedModel):\n        all_grads = []\n        total_loss = 0\n        # default behavior for first-order optimizers\n        for task in self.modelpool.model_names:\n            with self.profile(\"data loading\"):\n                batch = next(self.get_shuffled_test_loader_iter(task))\n            with self.profile(\"forward pass\"):\n                logits = self.compute_logits(module, batch, task)\n                logits = logits.mean(dim=0, keepdim=True)\n                loss = entropy_loss(logits)\n                total_loss += loss\n            with self.profile(\"backward pass\"):\n                # self.fabric.backward(loss, retain_graph=True)\n                _grads = torch.autograd.grad(\n                    loss,\n                    [module.merge_weight],\n                    create_graph=False,\n                    retain_graph=True,\n                )\n                all_grads.append(_grads[0].flatten().detach())\n        sol, min_norm = MinNormSolver.find_min_norm_element(all_grads)\n        if not isinstance(sol, torch.Tensor):\n            sol = torch.from_numpy(sol)\n        sol = sol.to(\n            device=module.merge_weight.device,\n            dtype=module.merge_weight.dtype,\n        )\n        grad = torch.stack(all_grads) * sol.view(-1, 1)\n        module.merge_weight.grad = grad.sum(dim=0).view_as(module.merge_weight)\n        return total_loss\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseAdaMergingAlgorithm.compute_logits","title":"<code>compute_logits(module, batch, task)</code>","text":"<p>Compute the logits for the given images and task.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Union[T5ForConditionalGeneration, LayerWiseMergedModel]</code>)           \u2013            <p>The model module.</p> </li> <li> <code>images</code>               (<code>Tensor</code>)           \u2013            <p>The input images.</p> </li> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The computed logits.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/flan_t5_layer_wise_adamerging.py</code> <pre><code>def compute_logits(\n    self,\n    module: Union[T5ForConditionalGeneration, LayerWiseMergedModel],\n    batch,\n    task: str,\n) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for the given images and task.\n\n    Args:\n        module: The model module.\n        images (Tensor): The input images.\n        task (str): The name of the task.\n\n    Returns:\n        Tensor: The computed logits.\n    \"\"\"\n    input_ids: Tensor = batch[\"input_ids\"]\n    attention_mask: Tensor = batch[\"attention_mask\"]\n\n    # remove padding tokens from the input\n    while attention_mask[:, -1].eq(0).all():\n        input_ids = input_ids[:, :-1]\n        attention_mask = attention_mask[:, :-1]\n\n    outputs = module(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=torch.ones(\n            input_ids.size(0), 1, dtype=torch.long, device=input_ids.device\n        ),\n    )\n    logits = outputs.logits[:, 0, :]\n    return logits\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseAdaMergingAlgorithm.construct_layer_wise_merged_model","title":"<code>construct_layer_wise_merged_model(modelpool)</code>","text":"<p>Constructs a wrapped layer-wise merged model from model pool.</p> <p>This method creates a new wrapped model by merging the layers of a pretrained model with those of several fine-tuned models. The merging is controlled by layer-wise weights, which is a <code>torch.Tensor</code> of the shape <code>(num_models, num_layers)</code>. The merging weights can be initialized based on a provided configuration or loaded from a file.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>ModelPool</code>)           \u2013            <p>An object containing the pretrained model and fine-tuned models to be merged.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LayerWiseMergedModel</code>          \u2013            <p>An instance of the merged model with layer-wise weights applied.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/flan_t5_layer_wise_adamerging.py</code> <pre><code>@torch.no_grad()\ndef construct_layer_wise_merged_model(self, modelpool: Seq2SeqLMPool):\n    \"\"\"\n    Constructs a wrapped layer-wise merged model from model pool.\n\n    This method creates a new wrapped model by merging the layers of a pretrained model with those of several fine-tuned models.\n    The merging is controlled by layer-wise weights, which is a `torch.Tensor` of the shape `(num_models, num_layers)`.\n    The merging weights can be initialized based on a provided configuration or loaded from a file.\n\n    Args:\n        modelpool (ModelPool): An object containing the pretrained model and fine-tuned models to be merged.\n\n    Returns:\n        LayerWiseMergedModel: An instance of the merged model with layer-wise weights applied.\n    \"\"\"\n    pretrained_model = modelpool.load_model(\"_pretrained_\")\n    finetuned_models = [\n        modelpool.load_model(name) for name in modelpool.model_names\n    ]\n\n    # initialize layer-wise weights using the provided configuration `init_values` or load from file if `weights` is provided\n    if self.merging_weights_load_path is None:\n        layer_wise_weight = get_layer_wise_weights(\n            num_models=len(modelpool.model_names),\n            num_layers=len(\n                tuple(\n                    filter(lambda p: p.requires_grad, pretrained_model.parameters())\n                )\n            ),\n            init_values=self.init_values,\n        )\n    else:\n        if isinstance(self.merging_weights_load_path, str):\n            # load the merging weights from a file\n            layer_wise_weight = load_tensor_from_file(\n                self.merging_weights_load_path\n            )\n        else:\n            raise ValueError(\n                f\"Unsupported weights format: {self.merging_weights_load_path}\"\n            )\n\n    module = LayerWiseMergedModel(\n        layer_wise_weight=layer_wise_weight,\n        pretrained_model=pretrained_model,\n        finetuned_models=finetuned_models,\n        clamp_weights=self.clamp_weights,\n        tie_weights=self.tie_weights,\n        strict=self.strict,\n    )\n    print(f\"{layer_wise_weight.size()=}, {layer_wise_weight.numel()=}\")\n    return module\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseAdaMergingAlgorithm.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(task)</code>  <code>cached</code>","text":"<p>Loader of test dataset for test-time adaptation. labels are not needed.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code> (              <code>DataLoader</code> )          \u2013            <p>The data loader for the test dataset.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/flan_t5_layer_wise_adamerging.py</code> <pre><code>@functools.cache\ndef get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n    \"\"\"\n    Loader of test dataset for test-time adaptation. labels are not needed.\n\n    Args:\n        task (str): The name of the task.\n\n    Returns:\n        DataLoader: The data loader for the test dataset.\n    \"\"\"\n    dataloader_kwargs = dict(self.dataloader_kwargs)\n    dataloader_kwargs.update(dict(shuffle=True, collate_fn=default_data_collator))\n\n    dataset = self.modelpool.load_test_dataset(task)\n    loader = DataLoader(dataset, **dataloader_kwargs)\n\n    if self.fabric is not None:\n        loader = self.fabric.setup_dataloaders(loader)\n    return iter(InfiniteDataLoader(loader))\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseAdaMergingAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Something to do before the test-time adaptation starts. Such as setting up the task-specific heads.</p> Source code in <code>fusion_bench/method/adamerging/flan_t5_layer_wise_adamerging.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Something to do before the test-time adaptation starts. Such as setting up the task-specific heads.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseAdaMergingAlgorithm.run","title":"<code>run(modelpool, **kwargs)</code>","text":"<p>Run the Layer-Wise AdaMerging Algorithm.</p> <p>This method constructs the wrapped model and performs test-time adaptation if necessary.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>ModelPool</code>)           \u2013            <p>The model pool containing the pretrained and fine-tuned models.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LayerWiseMergedModel</code>          \u2013            <p>The merged model after test-time adaptation.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/flan_t5_layer_wise_adamerging.py</code> <pre><code>def run(self, modelpool: Seq2SeqLMPool, **kwargs):\n    \"\"\"\n    Run the Layer-Wise AdaMerging Algorithm.\n\n    This method constructs the wrapped model and performs test-time adaptation if necessary.\n\n    Args:\n        modelpool (ModelPool): The model pool containing the pretrained and fine-tuned models.\n\n    Returns:\n        LayerWiseMergedModel: The merged model after test-time adaptation.\n    \"\"\"\n    log.info(\"Fusing models using layer-wise adaptive merging.\")\n    self.modelpool = modelpool\n\n    with self.profile(\"construct the wrapped model\"):\n        module = self.construct_layer_wise_merged_model(modelpool)\n\n    if self.merging_weights_load_path is not None:\n        # skip the test-time adaptation\n        return module.merge_and_unload()\n    else:\n        with self.profile(\"test-time adaptation\"):\n            module = self.test_time_adaptation(module)\n        if self.merging_weights_save_path is not None:\n            self.save_merging_weights(\n                self.merging_weights_save_path, module.merge_weight\n            )\n        return module.merge_and_unload()\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseAdaMergingAlgorithm.save_merging_weights","title":"<code>save_merging_weights(file_path, merging_weights)</code>","text":"<p>Save the merging weights to a file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str</code>)           \u2013            <p>The path to save the merging weights.</p> </li> <li> <code>merging_weights</code>               (<code>Tensor</code>)           \u2013            <p>The merging weights to save.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/flan_t5_layer_wise_adamerging.py</code> <pre><code>@rank_zero_only\ndef save_merging_weights(self, file_path: str, merging_weights: torch.Tensor):\n    \"\"\"\n    Save the merging weights to a file.\n\n    Args:\n        file_path (str): The path to save the merging weights.\n        merging_weights (torch.Tensor): The merging weights to save.\n    \"\"\"\n    if self.fabric.is_global_zero and self.merging_weights_save_path is not None:\n        if isinstance(file_path, str) and not file_path.startswith((\"/\", \".\")):\n            # if the file path is not absolute or relative to current working directory, save it in the log directory\n            save_path = os.path.join(self.log_dir, file_path)\n        else:\n            save_path = file_path\n        log.info(f\"saving merging weights to {save_path}.\")\n        if os.path.dirname(save_path):\n            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        torch.save(merging_weights.detach().cpu(), save_path)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseAdaMergingAlgorithm.test_time_adaptation","title":"<code>test_time_adaptation(module)</code>","text":"<p>Perform test-time adaptation on the merged model.</p> <p>This method adapts the merging weights during test-time to improve performance.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>LayerWiseMergedModel</code>)           \u2013            <p>The merged model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LayerWiseMergedModel</code>          \u2013            <p>The adapted merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/flan_t5_layer_wise_adamerging.py</code> <pre><code>def test_time_adaptation(self, module: LayerWiseMergedModel):\n    \"\"\"\n    Perform test-time adaptation on the merged model.\n\n    This method adapts the merging weights during test-time to improve performance.\n\n    Args:\n        module (LayerWiseMergedModel): The merged model.\n\n    Returns:\n        LayerWiseMergedModel: The adapted merged model.\n    \"\"\"\n    self.on_test_time_adaptation_start()\n\n    # configure optimizer\n    optimizer = instantiate(self._optimizer, [module.merge_weight])\n    module, optimizer = self.fabric.setup(module, optimizer)\n\n    module.train()\n    module.merge_weights()\n    for step_idx in (\n        pbar := tqdm(\n            range(self.max_steps if not self.is_debug_mode else 1),\n            (\"[DEBUG MODE] \" if self.is_debug_mode else \"\")\n            + \"AdaMerging Test-time adaptation\",\n            dynamic_ncols=True,\n        )\n    ):\n        if self.variant == \"mgda\":\n            total_loss = self._compute_gradients_using_mgda(module)\n        else:\n            total_loss = 0\n            for task in self.modelpool.model_names:\n                with self.profile(\"data loading\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                with self.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, batch, task)\n                    logits = logits.mean(dim=0, keepdim=True)\n                    loss = entropy_loss(logits)\n                    total_loss += loss\n                with self.profile(\"backward pass\"):\n                    self.fabric.backward(loss, retain_graph=True)\n\n        with self.profile(\"optimizer step\"):\n            optimizer.step()\n            optimizer.zero_grad()\n        with self.profile(\"merging weights\"):\n            module.merge_weights()\n\n        metrics = {\n            \"train/loss\": total_loss.item(),\n            \"train/weight_max\": module.merge_weight.max().item(),\n            \"train/weight_min\": module.merge_weight.min().item(),\n            \"train/weight_mean\": module.merge_weight.mean().item(),\n        }\n        self.fabric.log_dict(metrics, step=step_idx)\n        pbar.set_postfix(metrics)\n\n    log.info(get_memory_usage(f\"after adamerging, the memory usage of GPU is:\"))\n    self.print_profile_summary()\n    return module\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#optimization-based-methods","title":"Optimization-based Methods","text":""},{"location":"api/fusion_bench.method/merging/#regmean","title":"RegMean","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.RegMeanAlgorithmForCLIP","title":"<code>RegMeanAlgorithmForCLIP</code>","text":"<p>               Bases: <code>CLIPClassificationMixin</code>, <code>RegMeanAlgorithm</code></p> Source code in <code>fusion_bench/method/regmean/clip_regmean.py</code> <pre><code>@auto_register_config\nclass RegMeanAlgorithmForCLIP(\n    CLIPClassificationMixin,\n    RegMeanAlgorithm,\n):\n    def __init__(self, *, dataloader_kwargs: DictConfig, **kwargs):\n        super().__init__(**kwargs)\n\n    def on_regmean_start(self):\n        self.setup_zero_shot_classification_head()\n\n    def compute_logits(self, module, batch, task: str) -&gt; Tensor:\n        images, _ = batch\n        text_embeds = self.zeroshot_weights[task]\n\n        image_embeds = module(images)[1]\n        image_embeds = self.visual_projection(image_embeds)\n\n        # normalize embeddings\n        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n        # cosine similarity\n        logits_per_text = (\n            torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n        )\n        logits_per_image = logits_per_text.t()\n\n        return logits_per_image\n\n    def get_regmean_weights(\n        self,\n        model_name: str,\n        model: Module,\n        train_dataset: torch.utils.data.Dataset,\n        linear_modules_to_merge: Dict[str, Module],\n    ):\n        # setup dataloader\n        train_dataset = CLIPDataset(train_dataset, self.clip_processor)\n        train_dataloader = DataLoader(\n            train_dataset, shuffle=True, **self.dataloader_kwargs\n        )\n        train_dataloader = self.fabric.setup_dataloaders(train_dataloader)\n        model = self.fabric.setup(model)\n\n        def compute_regmean_weights(module_name: str):\n            \"\"\"\n            compute the regmean weights, a hook function to deal with each module's input\n            :param module_name: str, module name\n            :return:\n            \"\"\"\n\n            def hook(module: nn.Module, input: tuple, output: torch.Tensor):\n                # Tensor, shape (batch_size, sequence_length, hidden_dim)\n                x = cast(Tensor, input[0]).detach()\n                batch_num_actual_examples = x.shape[0]\n                # Tensor, shape (batch_size * sequence_length, hidden_dim)\n                x = x.reshape(-1, x.shape[-1])\n                # Tensor, shape (hidden_dim, hidden_dim)\n                xtx = torch.matmul(x.transpose(0, 1), x)\n                # store the averaged weights in regmean_weights\n                if module_name not in regmean_weights.keys():\n                    regmean_weights[module_name] = xtx / x.shape[0]\n                    num_computed_examples[module_name] = x.shape[0]\n                    num_actual_examples[module_name] = batch_num_actual_examples\n                else:\n                    regmean_weights[module_name] = (\n                        regmean_weights[module_name]\n                        * num_computed_examples[module_name]\n                        + xtx\n                    ) / (num_computed_examples[module_name] + x.shape[0])\n                    num_computed_examples[module_name] += x.shape[0]\n                    num_actual_examples[module_name] += batch_num_actual_examples\n\n            return hook\n\n        handles = []\n        # dictionary, regmean matrices for each linear module inputs\n        regmean_weights = {}\n        # dictionary, number of examples (multiplied the sequence length) used for computing regmean matrices\n        num_computed_examples = {}\n        # dictionary, number of actual examples used for computing regmean matrices\n        num_actual_examples = {}\n\n        for module_name, linear_module_to_merge in linear_modules_to_merge.items():\n            # register a hook in the forward process\n            handle = linear_module_to_merge.register_forward_hook(\n                compute_regmean_weights(module_name=module_name)\n            )\n            handles.append(handle)\n        for step, batch in tqdm(\n            enumerate(train_dataloader),\n            desc=f\"computing regmean weights for model {model_name}\",\n        ):\n            if (\n                len(num_actual_examples) &gt; 0\n                and list(num_actual_examples.values())[0] &gt;= self.num_regmean_examples\n            ):\n                break\n            logits = self.compute_logits(model, batch, model_name)  # noqa: F841\n\n        # remove the added hook\n        for handle in handles:\n            handle.remove()\n\n        for module_name in regmean_weights.keys():\n            regmean_weights[module_name] = regmean_weights[module_name].detach().cpu()\n\n        return regmean_weights\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.RegMeanAlgorithmForGPT2","title":"<code>RegMeanAlgorithmForGPT2</code>","text":"<p>               Bases: <code>LightningFabricMixin</code>, <code>RegMeanAlgorithm</code></p> Source code in <code>fusion_bench/method/regmean/gpt2_regmean.py</code> <pre><code>@auto_register_config\nclass RegMeanAlgorithmForGPT2(\n    LightningFabricMixin,\n    RegMeanAlgorithm,\n):\n    _include_module_type = [Conv1D]\n    classifiers = {}\n\n    def __init__(self, cache_dir: str, batch_size: int, num_workers: int, **kwargs):\n        super().__init__(**kwargs)\n\n    def on_regmean_start(self):\n        for model_name in self.modelpool.model_names:\n            classifier = cast(\n                GPT2ForSequenceClassification,\n                self.modelpool.load_classifier(model_name),\n            ).requires_grad_(False)\n            classifier.transformer = None\n            classifier = classifier.to(self.fabric.device)\n            self.classifiers[model_name] = classifier\n\n    def compute_logits(self, module: GPT2Model, batch, task: str) -&gt; Tensor:\n        self.classifiers[task].transformer = module\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n\n        outputs = self.classifiers[task](input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        assert logits.dim() == 2\n        return logits\n\n    def get_regmean_weights(\n        self,\n        model_name: str,\n        model: Module,\n        train_dataset,\n        linear_modules_to_merge: Dict[str, Module],\n    ):\n        # setup dataloader\n        train_dataloader = DataLoader(\n            train_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            num_workers=self.config.num_workers,\n            collate_fn=default_data_collator,\n            pin_memory=True,\n        )\n        train_dataloader = self.fabric.setup_dataloaders(train_dataloader)\n        model = self.fabric.setup(model)\n\n        def compute_regmean_weights(module_name: str):\n            \"\"\"\n            compute the regmean weights, a hook function to deal with each module's input\n            :param module_name: str, module name\n            :return:\n            \"\"\"\n\n            def hook(module: nn.Module, input: tuple, output: torch.Tensor):\n                # Tensor, shape (batch_size, sequence_length, hidden_dim)\n                x = cast(Tensor, input[0]).detach()\n                batch_num_actual_examples = x.shape[0]\n                # Tensor, shape (batch_size * sequence_length, hidden_dim)\n                x = x.reshape(-1, x.shape[-1])\n                # Tensor, shape (hidden_dim, hidden_dim)\n                xtx = torch.matmul(x.transpose(0, 1), x)\n                # store the averaged weights in regmean_weights\n                if module_name not in regmean_weights.keys():\n                    regmean_weights[module_name] = xtx / x.shape[0]\n                    num_computed_examples[module_name] = x.shape[0]\n                    num_actual_examples[module_name] = batch_num_actual_examples\n                else:\n                    regmean_weights[module_name] = (\n                        regmean_weights[module_name]\n                        * num_computed_examples[module_name]\n                        + xtx\n                    ) / (num_computed_examples[module_name] + x.shape[0])\n                    num_computed_examples[module_name] += x.shape[0]\n                    num_actual_examples[module_name] += batch_num_actual_examples\n\n            return hook\n\n        handles = []\n        # dictionary, regmean matrices for each linear module inputs\n        regmean_weights = {}\n        # dictionary, number of examples (multiplied the sequence length) used for computing regmean matrices\n        num_computed_examples = {}\n        # dictionary, number of actual examples used for computing regmean matrices\n        num_actual_examples = {}\n\n        for module_name, linear_module_to_merge in linear_modules_to_merge.items():\n            # register a hook in the forward process\n            handle = linear_module_to_merge.register_forward_hook(\n                compute_regmean_weights(module_name=module_name)\n            )\n            handles.append(handle)\n        for step, batch in tqdm(\n            enumerate(train_dataloader),\n            desc=f\"computing regmean weights for model {model_name}\",\n        ):\n            if (\n                len(num_actual_examples) &gt; 0\n                and list(num_actual_examples.values())[0]\n                &gt;= self.config.num_regmean_examples\n            ):\n                break\n            logits = self.compute_logits(model, batch, model_name)\n\n        # remove the added hook\n        for handle in handles:\n            handle.remove()\n\n        for module_name in regmean_weights.keys():\n            regmean_weights[module_name] = regmean_weights[module_name].detach().cpu()\n\n        return regmean_weights\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#regmean_1","title":"RegMean++","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.RegMeanAlgorithmPlusPlus","title":"<code>RegMeanAlgorithmPlusPlus</code>","text":"<p>               Bases: <code>SimpleProfilerMixin</code>, <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/regmean_plusplus/regmean_plusplus.py</code> <pre><code>@auto_register_config\nclass RegMeanAlgorithmPlusPlus(\n    SimpleProfilerMixin,\n    BaseAlgorithm,\n):\n    _include_module_type = [nn.Linear]\n\n    def __init__(\n        self,\n        *,\n        num_regmean_examples: int,\n        exclude_param_names_regex: list,\n        reduce_non_diagonal_ratio: float,\n        weight_transpose: bool,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.num_regmean_examples = num_regmean_examples\n        self.exclude_param_names_regex = exclude_param_names_regex\n        self.reduce_non_diagonal_ratio = reduce_non_diagonal_ratio\n        self.weight_transpose = weight_transpose\n\n    def run(self, modelpool: BaseModelPool, **kwargs):\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n        self.modelpool = modelpool\n        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n        models_to_merge_dict = {\n            name: model.to(device) for name, model in modelpool.named_models()\n        }\n        self.on_regmean_start()\n\n        # initialize the merged models as the pretrained model\n        merged_model = modelpool.load_pretrained_model().to(device)\n        merged_params_dict = {}\n\n        # 1. merge embedding layer\n        merged_embedding_dict = self.merge_embedding_layer(\n            models_to_merge_dict=models_to_merge_dict\n        )\n        merged_model.load_state_dict(merged_embedding_dict, strict=False)\n\n        with torch.no_grad():\n            # 1.1. compute input for the first layer\n            with (\n                self.profile(\"merging models\"),\n                self.profile(\"computing first layer input\"),\n            ):\n                batches_input_dict = defaultdict(list)\n                for name in tqdm(\n                    models_to_merge_dict.keys(), desc=\"computing input for first layer\"\n                ):\n                    dataset = modelpool.load_train_dataset(name)\n\n                    batches_input_dict[name] = self.get_input_for_first_layer(\n                        merged_model, dataset\n                    )\n\n            # 2. iteratively merge layer by layer with regmean algorithm\n            backbone_layers = self.get_layers(merged_model)\n            num_layers = len(backbone_layers)\n\n            models_to_merge_layers_dict = defaultdict(list)\n            for name, model in models_to_merge_dict.items():\n                models_to_merge_layers_dict[name] = self.get_layers(model)\n\n            param_names_to_merge = None\n            for layer_idx, backbone_layer in tqdm(\n                enumerate(backbone_layers), desc=\"merging layers\", total=num_layers\n            ):\n                # dictionary of list, where key is the parameter name,\n                # value is a list of the corresponding parameters of all the models that need to be merged\n                models_to_merge_param_dict = defaultdict(list)\n\n                # list of dictionaries with length len(models_to_merge),\n                # each dictionary records the regmean weights (matrix) of parameters for each model that needs to be merged\n                models_to_merge_regmean_weights_list = []\n\n                for name, layers_to_merge in models_to_merge_layers_dict.items():\n                    layer_to_merge = layers_to_merge[layer_idx]\n                    param_dict = layer_to_merge.state_dict()\n\n                    # exclude parameter whose name matches element in exclude_param_names_regex\n                    if param_names_to_merge is None:\n                        param_names_to_merge = regmean_utils.get_param_names_to_merge(\n                            input_param_names=list(param_dict.keys()),\n                            exclude_param_names_regex=self.config.get(\n                                \"exclude_param_names_regex\", []\n                            ),\n                        )\n\n                    for param_name in param_names_to_merge:\n                        models_to_merge_param_dict[param_name].append(\n                            param_dict[param_name]\n                        )\n\n                    linear_modules_to_merge = regmean_utils.get_modules_to_merge(\n                        model=layer_to_merge,\n                        include_module_types=self._include_module_type,\n                    )\n                    assert (\n                        len(linear_modules_to_merge) &gt; 0\n                    ), \"No linear modules to merge\"\n\n                    # 2.1. compute regmean weights for each model\n                    with (\n                        self.profile(\"merging models\"),\n                        self.profile(\"computing regmean weights\"),\n                    ):\n                        regmean_weights = self.get_regmean_weights(\n                            name,\n                            layer_to_merge,\n                            batches_input=batches_input_dict[name],\n                            linear_modules_to_merge=linear_modules_to_merge,\n                        )\n\n                        module_subset = regmean_utils.get_param_names_to_merge(\n                            input_param_names=list(param_dict.keys()),\n                            exclude_param_names_regex=self.exclude_param_names_regex,\n                        )\n                        module_subset = [\n                            name.replace(\".weight\", \"\").replace(\".bias\", \"\")\n                            for name in module_subset\n                        ]\n                        module_subset = list(set(module_subset))\n                        regmean_weights = {\n                            module_name: regmean_weights[module_name]\n                            for module_name in module_subset\n                            if module_name in regmean_weights\n                        }\n\n                        models_to_merge_regmean_weights_list.append(regmean_weights)\n\n                # 2.2. merge parameters with regmean weights\n                with self.profile(\"merging models\"):\n                    # merging with regmean weights\n                    merged_layer_params = merging_with_regmean_weights(\n                        models_to_merge_param_dict=models_to_merge_param_dict,\n                        models_to_merge_regmean_weights_list=models_to_merge_regmean_weights_list,\n                        reduce_non_diagonal_ratio=self.reduce_non_diagonal_ratio,\n                        weight_transpose=self.config.get(\"weight_transpose\", True),\n                    )\n\n                    merged_params_dict = self.update_merged_params_dict(\n                        merged_params_dict=merged_params_dict,\n                        new_merged_params=merged_layer_params,\n                        layer_idx=layer_idx,\n                    )\n\n                # 2.3. compute input for the next layer\n                with (\n                    self.profile(\"merging models\"),\n                    self.profile(\"forwarding next layer\"),\n                ):\n                    if layer_idx &lt; num_layers - 1:\n                        backbone_layer.load_state_dict(\n                            merged_layer_params, strict=False\n                        )\n                        batches_output_dict = defaultdict(list)\n                        for name in models_to_merge_dict.keys():\n                            batches_output_dict[name] = self.layer_batches_forward(\n                                backbone_layer, batches_input_dict[name]\n                            )\n                        batches_input_dict = batches_output_dict\n\n            # 3. load state dict to the merged model\n            merged_model.load_state_dict(merged_params_dict, strict=False)\n\n        self.print_profile_summary()\n        return merged_model\n\n    def merge_embedding_layer(self, models_to_merge_dict: Dict[str, nn.Module]):\n        \"\"\"\n        Merge the embedding layer of the model with the merged model.\n        This method should be implemented in subclasses if needed.\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_input_for_first_layer(self, model: nn.Module, train_dataset):\n        raise NotImplementedError\n\n    def get_layers(self, model: nn.Module):\n        raise NotImplementedError\n\n    def update_merged_params_dict(\n        self, merged_params_dict, new_merged_params, layer_idx\n    ):\n        raise NotImplementedError\n\n    def layer_batches_forward(self, layer: nn.Module, batches_input: List[Tensor]):\n        raise NotImplementedError\n\n    def on_regmean_start(self):\n        pass\n\n    def get_regmean_weights(\n        self,\n        model_name: str,\n        layer: nn.Module,\n        batches_input: List[Tensor],\n        linear_modules_to_merge: Dict[str, nn.Module],\n    ):\n        raise NotImplementedError\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.RegMeanAlgorithmPlusPlus.merge_embedding_layer","title":"<code>merge_embedding_layer(models_to_merge_dict)</code>","text":"<p>Merge the embedding layer of the model with the merged model. This method should be implemented in subclasses if needed.</p> Source code in <code>fusion_bench/method/regmean_plusplus/regmean_plusplus.py</code> <pre><code>def merge_embedding_layer(self, models_to_merge_dict: Dict[str, nn.Module]):\n    \"\"\"\n    Merge the embedding layer of the model with the merged model.\n    This method should be implemented in subclasses if needed.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.RegMeanAlgorithmForCLIPPlusPlus","title":"<code>RegMeanAlgorithmForCLIPPlusPlus</code>","text":"<p>               Bases: <code>RegMeanAlgorithmPlusPlus</code>, <code>CLIPClassificationMixin</code></p> Source code in <code>fusion_bench/method/regmean_plusplus/clip_regmean_plusplus.py</code> <pre><code>class RegMeanAlgorithmForCLIPPlusPlus(\n    RegMeanAlgorithmPlusPlus,\n    CLIPClassificationMixin,\n):\n    _config_mapping = {\n        \"_dataloader_kwargs\": \"dataloader_kwargs\",\n    }\n\n    def __init__(self, *, dataloader_kwargs: DictConfig, **kwargs):\n        super().__init__(**kwargs)\n        self.dataloader_kwargs = dataloader_kwargs\n\n    def on_regmean_start(self):\n        self.setup_zero_shot_classification_head()\n\n    def compute_logits(self, module, batch, task: str) -&gt; Tensor:\n        images, _ = batch\n        text_embeds = self.zeroshot_weights[task]\n\n        image_embeds = module(images)[1]\n        image_embeds = self.visual_projection(image_embeds)\n\n        # normalize embeddings\n        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n        # cosine similarity\n        logits_per_text = (\n            torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n        )\n        logits_per_image = logits_per_text.t()\n\n        return logits_per_image\n\n    def get_regmean_weights(\n        self,\n        model_name: str,\n        layer: Module,\n        batches_input: List[Tensor],\n        linear_modules_to_merge: Dict[str, Module],\n    ):\n        layer = self.fabric.setup(layer)\n\n        def compute_regmean_weights(module_name: str):\n            \"\"\"\n            compute the regmean weights, a hook function to deal with each module's input\n            :param module_name: str, module name\n            :return:\n            \"\"\"\n\n            def hook(module: nn.Module, input: tuple, output: torch.Tensor):\n                # Tensor, shape (batch_size, sequence_length, hidden_dim)\n                x = cast(Tensor, input[0]).detach()\n                batch_num_actual_examples = x.shape[0]\n                # Tensor, shape (batch_size * sequence_length, hidden_dim)\n                x = x.reshape(-1, x.shape[-1])\n                # Tensor, shape (hidden_dim, hidden_dim)\n                xtx = torch.matmul(x.transpose(0, 1), x)\n                # store the averaged weights in regmean_weights\n                if module_name not in regmean_weights.keys():\n                    regmean_weights[module_name] = xtx / x.shape[0]\n                    num_computed_examples[module_name] = x.shape[0]\n                    num_actual_examples[module_name] = batch_num_actual_examples\n                else:\n                    regmean_weights[module_name] = (\n                        regmean_weights[module_name]\n                        * num_computed_examples[module_name]\n                        + xtx\n                    ) / (num_computed_examples[module_name] + x.shape[0])\n                    num_computed_examples[module_name] += x.shape[0]\n                    num_actual_examples[module_name] += batch_num_actual_examples\n\n            return hook\n\n        handles = []\n        # dictionary, regmean matrices for each linear module inputs\n        regmean_weights = {}\n        # dictionary, number of examples (multiplied the sequence length) used for computing regmean matrices\n        num_computed_examples = {}\n        # dictionary, number of actual examples used for computing regmean matrices\n        num_actual_examples = {}\n\n        for module_name, linear_module_to_merge in linear_modules_to_merge.items():\n            # register a hook in the forward process\n            handle = linear_module_to_merge.register_forward_hook(\n                compute_regmean_weights(module_name=module_name)\n            )\n            handles.append(handle)\n        _ = self.layer_batches_forward(layer, batches_input)\n\n        # remove the added hook\n        for handle in handles:\n            handle.remove()\n\n        for module_name in regmean_weights.keys():\n            regmean_weights[module_name] = regmean_weights[module_name].detach().cpu()\n\n        return regmean_weights\n\n    def merge_embedding_layer(self, models_to_merge_dict: Dict[str, nn.Module]):\n        models_to_merge_param_dict = defaultdict(list)\n\n        # get the parameters of the embedding layer from each model\n        for model_to_merge in models_to_merge_dict.values():\n            model_to_merge_state_dict = model_to_merge.state_dict()\n\n            param_dict = {}\n            for name, param in model_to_merge_state_dict.items():\n                if name.startswith(\"vision_model.embeddings\") or name.startswith(\n                    \"vision_model.pre_layrnorm\"\n                ):\n                    param_dict[name] = param\n\n            for param_name in param_dict.keys():\n                models_to_merge_param_dict[param_name].append(param_dict[param_name])\n\n        # merge the parameters of the embedding layer\n        merged_params_dict = {}\n        for param_name, param_list in models_to_merge_param_dict.items():\n            merged_params_dict[param_name] = torch.stack(param_list).mean(dim=0)\n\n        return merged_params_dict\n\n    def get_input_for_first_layer(self, model: nn.Module, train_dataset):\n        # setup dataloader\n        train_dataset = CLIPDataset(train_dataset, self.clip_processor)\n        train_dataloader = DataLoader(\n            train_dataset, shuffle=True, **self.dataloader_kwargs\n        )\n        train_dataloader = self.fabric.setup_dataloaders(train_dataloader)\n        model = self.fabric.setup(model)\n\n        def compute_input(model, batch):\n            images, _ = batch\n\n            images = images.to(model.device)\n            image_embeds = model.vision_model.embeddings(images)\n            image_embeds = model.vision_model.pre_layrnorm(image_embeds)\n            image_embeds = image_embeds.detach().cpu()\n\n            return image_embeds\n\n        num_computed_examples = 0\n        num_regmean_examples = self.num_regmean_examples\n\n        batches_input = []\n        for batch in train_dataloader:\n            if num_computed_examples &gt;= num_regmean_examples:\n                break\n            batches_input.append(compute_input(model, batch))\n            num_computed_examples += batch[0].size(0)\n\n        return batches_input\n\n    def get_layers(self, model: nn.Module):\n        return model.vision_model.encoder.layers\n\n    def update_merged_params_dict(\n        self, merged_params_dict, new_merged_params, layer_idx\n    ):\n        for key, value in new_merged_params.items():\n            key = f\"vision_model.encoder.layers.{layer_idx}.{key}\"\n            merged_params_dict[key] = value\n\n        return merged_params_dict\n\n    def layer_batches_forward(\n        self, layer: nn.Module, batches_input: List[Tensor]\n    ) -&gt; Tensor:\n        batches_output = []\n        for batch in batches_input:\n            device = next(layer.parameters()).device\n            batch = batch.to(device)\n            logits = (\n                layer(batch, attention_mask=None, causal_attention_mask=None)[0]\n                .detach()\n                .cpu()\n            )\n            batches_output.append(logits)\n        return batches_output\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#frank-wolfe-merging","title":"Frank-Wolfe Merging","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FrankWolfeSoftAlgorithm","title":"<code>FrankWolfeSoftAlgorithm</code>","text":"<p>               Bases: <code>CLIPClassificationMixin</code>, <code>ModelFusionAlgorithm</code>, <code>SimpleProfilerMixin</code></p> Source code in <code>fusion_bench/method/fw_merging/fw_soft.py</code> <pre><code>class FrankWolfeSoftAlgorithm(\n    CLIPClassificationMixin,\n    ModelFusionAlgorithm,\n    SimpleProfilerMixin,\n):\n    def __init__(\n        self,\n        max_iters: int,\n        dataset_size: int,\n        ada_iters: int,\n        ada_coeff: float,\n        merge_fn: str,\n        granularity: str = \"task\",\n        max_num_models: int = 100,\n        step_size: float = 0.3,\n        tasks: List[str] = [],\n        init_weight: str = \"\",\n        ada_loss=\"entropy_loss\",\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the TaskArithmeticAlgorithm with the given scaling factor.\n\n        Args:\n            step_size (int): The factor by which the task vectors will be scaled before merging.\n        \"\"\"\n        self.merge_fn = merge_fn\n\n        self.init_weight = init_weight\n        self.max_iters = max_iters\n        self.ada_iters = ada_iters\n        self.ada_coeff = ada_coeff\n        self.granularity = granularity\n        self.tasks = tasks\n        self.step_size = step_size\n        self.dataset_size = dataset_size\n        self.max_num_models = max_num_models\n        self.ada_loss = ada_loss\n        super().__init__(**kwargs)\n\n    def on_frank_wolfe_iteration_start(self):\n        self.setup_zero_shot_classification_head()\n\n    @functools.cache\n    def get_shuffled_train_loader_iter(self, task: str, batch_size: int = 1):\n        # get dataloader kwargs\n        dataloader_kwargs = self.dataloader_kwargs.copy()\n        dataloader_kwargs[\"shuffle\"] = True\n        dataloader_kwargs[\"batch_size\"] = batch_size\n\n        # get the test dataset\n        clip_dataset = CLIPDataset(\n            self.modelpool.load_train_dataset(task), self.clip_processor\n        )\n        # create the dataloader\n        loader = DataLoader(clip_dataset, **dataloader_kwargs)\n        loader = self.fabric.setup_dataloaders(loader)\n        return iter(InfiniteDataLoader(loader))\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(self, task: str, batch_size: int = 1):\n        return super().get_shuffled_test_loader_iter(task, batch_size=batch_size)\n\n    def run_adamerging(self, module):\n        use_entropy_loss = self.ada_loss == \"entropy_loss\"\n\n        optimizer = torch.optim.Adam([module.merge_weight], lr=1e-3)\n        module, optimizer = self.fabric.setup(module, optimizer)\n        module.train()\n        for step_idx in (\n            pbar := tqdm(\n                range(self.ada_iters),\n                \"AdaMerging (2/2)\",\n                dynamic_ncols=True,\n                disable=not self.fabric.is_global_zero,\n            )\n        ):\n            with self.profile(\"merge weights\"):\n                module.merge_weights()\n\n            metrics = {}\n            total_loss = None\n            tasks = self.modelpool.model_names if self.tasks == [] else self.tasks\n            if not use_entropy_loss:\n                loss_fn = nn.CrossEntropyLoss()\n            for task in tasks:\n                with self.profile(\"data loading\"):\n                    if use_entropy_loss:\n                        batch = next(\n                            self.get_shuffled_test_loader_iter(task, batch_size=16)\n                        )\n                    else:\n                        batch = next(\n                            self.get_shuffled_train_loader_iter(task, batch_size=16)\n                        )\n                        # NOTE: The labels are not allowed to be used during test-time adaptation\n                    images = batch[0]\n                with self.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, images, task)\n                    if use_entropy_loss:\n                        loss = entropy_loss(logits)\n                    else:\n                        loss = loss_fn(logits, batch[1])\n                    total_loss = loss if total_loss is None else total_loss + loss\n\n            optimizer.zero_grad()\n            with self.profile(\"compute grad\"):\n                self.fabric.backward(total_loss)\n\n            with self.profile(\"base optimizer step\"):\n                optimizer.step()\n\n            metrics.update({\"train/loss\": loss.item()})\n            self.fabric.log_dict(metrics, step=step_idx)\n            pbar.set_postfix(metrics)\n        return module\n\n    def frank_wolfe_iteration(self, merged_model, task):\n\n        merged_model.train()\n        # zero the gradients\n        requires_grad_dict = {}\n        for name, param in merged_model.named_parameters():\n            requires_grad_dict[name] = param.requires_grad\n            param.requires_grad = True\n            param.grad = None\n\n        loss_fn = nn.CrossEntropyLoss()\n        avg_loss = defaultdict(list)\n        log.info(f\"Processing task {task}\")\n        for i in range(self.dataset_size):\n            with self.profile(\"data loading\"):\n                batch = next(self.get_shuffled_train_loader_iter(task))\n            with self.profile(\"forward pass\"):\n                logits = self.compute_logits(merged_model, batch[0], task)\n                loss = loss_fn(logits, batch[1]) / (\n                    self.dataset_size * len(self.modelpool.model_names)\n                )\n            with self.profile(\"backward pass\"):\n                loss.backward()\n            avg_loss[task].append(loss.item())\n\n        # calculate the loss\n        avg_loss = {\n            task: sum(losses) / len(losses) for task, losses in avg_loss.items()\n        }\n        log.info(\n            f\"Average Loss: {avg_loss}, Total Loss: {sum(avg_loss.values()) / len(avg_loss)}\"\n        )\n\n        gradients = {\n            name: param.grad.clone().to(\"cpu\")\n            for name, param in merged_model.named_parameters()\n            if param.requires_grad\n        }\n        for name, param in merged_model.named_parameters():\n            param.requires_grad = requires_grad_dict[name]\n            param.grad = None\n        merged_model.eval()\n\n        return gradients\n\n    def frank_wolfe_selection(\n        self, gradients, checkpoints, model_to_merge_names=[], type=\"task\"\n    ):\n        assert type in [\n            \"task\",\n            \"layer\",\n        ], f\"Unsupported FW selection type: {type}, supported types are ['task', 'layer']\"\n        min_inner_product = float(\"inf\")\n        min_model = None\n        min_model_name = None\n        log_dict = {}\n        if type == \"task\":\n            for model_name, model_to_merge in checkpoints.items():\n                model_to_merge = model_to_merge.to(\"cpu\").state_dict()\n                inner_product_sum = 0\n                for param_name, param_value in model_to_merge.items():\n                    # caclulate consine similarity\n                    grad = gradients[param_name]\n                    ckpt = model_to_merge[param_name]\n                    param_alignment = torch.dot(grad.flatten(), ckpt.flatten()) / (\n                        torch.norm(grad) * torch.norm(ckpt)\n                    )\n                    inner_product_sum += param_alignment\n                log_dict[model_name] = inner_product_sum.item()\n                if (\n                    inner_product_sum &lt; min_inner_product\n                    and model_name not in model_to_merge_names\n                ):\n                    min_inner_product = inner_product_sum\n                    min_model = deepcopy(model_to_merge)\n                    min_model_name = model_name\n        else:\n            min_model = {}\n            min_inner_product = {}\n            min_idx = {}\n            min_model_name = {}\n            for model_name, model_to_merge in checkpoints.items():\n                model_to_merge = model_to_merge.to(\"cpu\").state_dict()\n                for param_name, param_value in model_to_merge.items():\n                    # caclulate consine similarity\n                    grad = gradients[param_name]\n                    ckpt = model_to_merge[param_name]\n                    param_alignment = torch.dot(grad.flatten(), ckpt.flatten()) / (\n                        torch.norm(grad) * torch.norm(ckpt)\n                    )\n                    if (\n                        param_name not in min_inner_product\n                        or param_alignment &lt; min_inner_product[param_name]\n                    ) and model_name not in model_to_merge_names[param_name]:\n                        min_inner_product[param_name] = param_alignment\n                        min_model[param_name] = param_value\n                        min_idx[param_name] = model_name\n                        min_model_name[param_name] = model_name\n            min_inner_product = sum(min_inner_product.values())\n            log_dict = {model_name: 0 for model_name in checkpoints.keys()}\n            for k in min_idx.values():\n                log_dict[k] += 1\n\n        return min_model, min_model_name, min_inner_product, log_dict\n\n    def run(self, modelpool: HuggingFaceClipVisionPool):\n        log.info(\"Fusing models using FW merging.\")\n        self.modelpool = modelpool\n        tasks = self.tasks if self.tasks else self.modelpool.model_names\n        self.log_hyperparams(self.config)\n        self.on_frank_wolfe_iteration_start()\n\n        assert modelpool.has_pretrained, \"Pretrained model is required.\"\n        finetuned_models = {\n            name: modelpool.load_model(name)\n            for name in modelpool.model_names[: self.max_num_models]\n        }\n\n        if self.init_weight == \"base\" or self.init_weight == \"\":\n            merged_model = modelpool.load_model(\"_pretrained_\")\n        else:\n            log.info(\"Initializing the merged model with the initial weight\")\n            if isinstance(self.init_weight, str):\n                # self.config.weights is a path to a saved tensor\n                layer_wise_weight = load_tensor_from_file(self.init_weight)\n            else:\n                raise ValueError(f\"Unsupported weights format: {self.init_weight}\")\n\n            pretrained_model = modelpool.load_model(\"_pretrained_\")\n            layerwise_merged_model = LayerWiseMergedModel(\n                layer_wise_weight=layer_wise_weight,\n                pretrained_model=pretrained_model,\n                finetuned_models=list(finetuned_models.values())[: self.max_num_models],\n                clamp_weights=False,\n                tie_weights=True,\n                strict=False,\n            ).cuda()\n            merged_model = layerwise_merged_model.merge_and_unload()\n\n        initial_model = modelpool.load_model(\"_pretrained_\")\n        self.set_requires_grad(merged_model, initial_model)\n        # initial_model.load_state_dict(deepcopy(merged_model.state_dict()))\n        # finetuned_models['initial'] = initial_model\n        for step_idx in (\n            pbar := tqdm(\n                range(self.max_iters if not self.is_debug_mode else 1),\n                (\"[DEBUG MODE] \" if self.is_debug_mode else \"\") + \"Frank-Wolfe Merging\",\n                dynamic_ncols=True,\n            )\n        ):\n            # Find the task vector with the most alignment to the gradient\n            models_dict_to_merge = []\n            model_to_merge_names = (\n                []\n                if self.granularity == \"task\"\n                else {name: [] for name in merged_model.state_dict().keys()}\n            )\n            inner_products = []\n            for task in tasks:\n                torch.set_grad_enabled(True)\n                torch.cuda.empty_cache()\n                gradients = self.frank_wolfe_iteration(merged_model.cuda(), task)\n                torch.set_grad_enabled(False)\n                grad_norm = torch.norm(\n                    torch.stack([torch.norm(g) for g in gradients.values()])\n                )\n\n                min_model, min_model_name, min_inner_product, log_dict = (\n                    self.frank_wolfe_selection(\n                        gradients,\n                        finetuned_models,\n                        model_to_merge_names,\n                        type=self.granularity,\n                    )\n                )\n                if self.granularity == \"task\":\n                    model_to_merge_names.append(min_model_name)\n                else:\n                    for k, v in min_model_name.items():\n                        model_to_merge_names[k].append(v)\n                models_dict_to_merge.append(min_model)\n                inner_products.append(min_inner_product)\n\n                log.info(f\"Task: {task}, Inner Products: {log_dict}\")\n                if (\n                    len(models_dict_to_merge) &gt;= len(self.modelpool.model_names)\n                    or len(models_dict_to_merge) &gt;= self.max_num_models\n                ):\n                    log.info(f\"Breaking at {len(models_dict_to_merge)}\")\n                    break\n\n            # print iteration information\n            log.info(\n                f\"Iteration {step_idx+1}, Task Vector: {model_to_merge_names}, Gradient Norm: {grad_norm:.6f}, Inner Products: {inner_products}\"\n            )\n\n            if self.merge_fn == \"adamerging\":\n                models_to_merge = [\n                    modelpool.load_model(\"_pretrained_\")\n                    for _ in range(len(models_dict_to_merge))\n                ]\n                layer_wise_weight = get_layer_wise_weights(\n                    num_models=len(models_to_merge),\n                    num_layers=len(\n                        tuple(\n                            filter(\n                                lambda p: p.requires_grad,\n                                models_to_merge[0].parameters(),\n                            )\n                        )\n                    ),\n                    init_values=self.ada_coeff if step_idx &gt; 0 else 0.3,\n                )\n                for model_to_merge, model_to_merge_dict in zip(\n                    models_to_merge, models_dict_to_merge\n                ):\n                    model_to_merge.load_state_dict(model_to_merge_dict)\n                layerwise_merged_model = LayerWiseMergedModel(\n                    layer_wise_weight=layer_wise_weight,\n                    pretrained_model=merged_model.to(\"cpu\"),\n                    finetuned_models=models_to_merge,\n                    clamp_weights=False,\n                    tie_weights=True,\n                    strict=False,\n                ).cuda()\n                torch.set_grad_enabled(True)\n                layerwise_merged_model = self.run_adamerging(layerwise_merged_model)\n                torch.set_grad_enabled(False)\n                with torch.no_grad():\n                    merged_model = layerwise_merged_model.merge_and_unload()\n                    self.set_requires_grad(merged_model, initial_model)\n                del (\n                    models_to_merge,\n                    layerwise_merged_model,\n                    layer_wise_weight,\n                    models_dict_to_merge,\n                )\n            else:\n                step = 2 / (step_idx + 2) * self.step_size if step_idx &gt; 0 else 1\n                merged_model = task_arithmetic_merge(\n                    merged_model.to(\"cpu\"), models_dict_to_merge, 0.3 * step\n                )\n                del models_dict_to_merge\n\n        torch.set_grad_enabled(False)\n        merged_model = merged_model.cuda().eval()\n        return merged_model\n\n    def set_requires_grad(self, merged_model, initial_model):\n        for name, param in initial_model.named_parameters():\n            for n, p in merged_model.named_parameters():\n                if name == n:\n                    p.requires_grad = param.requires_grad\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FrankWolfeSoftAlgorithm.__init__","title":"<code>__init__(max_iters, dataset_size, ada_iters, ada_coeff, merge_fn, granularity='task', max_num_models=100, step_size=0.3, tasks=[], init_weight='', ada_loss='entropy_loss', **kwargs)</code>","text":"<p>Initializes the TaskArithmeticAlgorithm with the given scaling factor.</p> <p>Parameters:</p> <ul> <li> <code>step_size</code>               (<code>int</code>, default:                   <code>0.3</code> )           \u2013            <p>The factor by which the task vectors will be scaled before merging.</p> </li> </ul> Source code in <code>fusion_bench/method/fw_merging/fw_soft.py</code> <pre><code>def __init__(\n    self,\n    max_iters: int,\n    dataset_size: int,\n    ada_iters: int,\n    ada_coeff: float,\n    merge_fn: str,\n    granularity: str = \"task\",\n    max_num_models: int = 100,\n    step_size: float = 0.3,\n    tasks: List[str] = [],\n    init_weight: str = \"\",\n    ada_loss=\"entropy_loss\",\n    **kwargs,\n):\n    \"\"\"\n    Initializes the TaskArithmeticAlgorithm with the given scaling factor.\n\n    Args:\n        step_size (int): The factor by which the task vectors will be scaled before merging.\n    \"\"\"\n    self.merge_fn = merge_fn\n\n    self.init_weight = init_weight\n    self.max_iters = max_iters\n    self.ada_iters = ada_iters\n    self.ada_coeff = ada_coeff\n    self.granularity = granularity\n    self.tasks = tasks\n    self.step_size = step_size\n    self.dataset_size = dataset_size\n    self.max_num_models = max_num_models\n    self.ada_loss = ada_loss\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FrankWolfeHardAlgorithm","title":"<code>FrankWolfeHardAlgorithm</code>","text":"<p>               Bases: <code>CLIPClassificationMixin</code>, <code>ModelFusionAlgorithm</code>, <code>SimpleProfilerMixin</code></p> Source code in <code>fusion_bench/method/fw_merging/fw_hard.py</code> <pre><code>class FrankWolfeHardAlgorithm(\n    CLIPClassificationMixin,\n    ModelFusionAlgorithm,\n    SimpleProfilerMixin,\n):\n\n    def __init__(\n        self,\n        merge_fn: str,\n        step_size: float,\n        max_iters: int,\n        dataset_size: int,\n        tasks: List[str] = [],\n        granularity: str = \"task\",\n        max_num_models: int = 100,\n        loss_fn: str = \"cross_entropy\",\n        init_weight: str = \"\",\n        scaling_factor: float = 1.0,\n        threshold: int = 20,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the TaskArithmeticAlgorithm with the given scaling factor.\n\n        Args:\n            scaling_factor (int): The factor by which the task vectors will be scaled before merging.\n        \"\"\"\n        self.merger = merge_fn\n        if merge_fn == \"task_arithmetic\":\n            self.merge_fn = task_arithmetic_merge\n        elif merge_fn == \"ties\":\n            self.merge_fn = partial(ties_merge, threshold=threshold)\n        # elif merge_fn == \"concrete_ta\":\n        #     self.merge_fn = ConcreteTaskArithmeticAlgorithmForCLIP(\n        #         instantiate(OmegaConf.load(\"config/method/concrete_subspace/clip_concrete_task_arithmetic.yaml\"))\n        #     )\n        else:\n            raise ValueError(f\"Unsupported merge_fn: {merge_fn}\")\n        self.scaling_factor = scaling_factor\n\n        self.init_weight = init_weight\n        self.step_size = step_size\n        self.max_iters = max_iters\n        self.granularity = granularity\n        self.loss_fn = loss_fn\n        self.tasks = tasks\n        self.dataset_size = dataset_size\n        self.max_num_models = max_num_models\n        super().__init__(**kwargs)\n\n    def on_frank_wolfe_iteration_start(self):\n        self.setup_zero_shot_classification_head()\n\n    @functools.cache\n    def get_shuffled_loader_iter(self, task: str):\n        if self.loss_fn == \"cross_entropy\":\n            # get dataloader kwargs\n            dataloader_kwargs = self.dataloader_kwargs.copy()\n            dataloader_kwargs[\"shuffle\"] = True\n            dataloader_kwargs[\"batch_size\"] = 1\n\n            # get the test dataset\n            clip_dataset = CLIPDataset(\n                self.modelpool.load_train_dataset(task), self.clip_processor\n            )\n            # create the dataloader\n            loader = DataLoader(clip_dataset, **dataloader_kwargs)\n            loader = self.fabric.setup_dataloaders(loader)\n            return iter(InfiniteDataLoader(loader))\n        elif self.loss_fn == \"entropy\":\n            return super().get_shuffled_test_loader_iter(\n                task,\n                batch_size=1,\n            )\n        else:\n            raise ValueError(f\"Unsupported loss function: {self.loss_fn}\")\n\n    def frank_wolfe_iteration(self, merged_model):\n\n        merged_model.train()\n        # zero the gradients\n        for name, param in merged_model.named_parameters():\n            param.requires_grad = True\n            param.grad = None\n\n        if self.loss_fn == \"cross_entropy\":\n            loss_fn = nn.CrossEntropyLoss()\n        elif self.loss_fn == \"entropy\":\n            loss_fn = entropy_loss\n        avg_loss = defaultdict(list)\n        tasks = self.tasks if self.tasks else self.modelpool.model_names\n        for task in tasks:\n            log.info(f\"Processing task {task}\")\n            for _ in range(self.dataset_size):\n                with self.profile(\"data loading\"):\n                    batch = next(self.get_shuffled_loader_iter(task))\n                with self.profile(\"forward pass\"):\n                    logits = self.compute_logits(merged_model, batch[0], task)\n                    loss = loss_fn(logits, batch[1]) / (\n                        self.dataset_size * len(self.modelpool.model_names)\n                    )\n                with self.profile(\"backward pass\"):\n                    # self.fabric.backward(loss, retain_graph=True)\n                    loss.backward()\n                avg_loss[task].append(loss.item())\n\n        # calculate the loss\n        avg_loss = {\n            task: sum(losses) / len(losses) for task, losses in avg_loss.items()\n        }\n        log.info(\n            f\"Average Loss: {avg_loss}, Total Loss: {sum(avg_loss.values()) / len(avg_loss)}\"\n        )\n\n        gradients = {\n            name: param.grad.clone().to(\"cpu\")\n            for name, param in merged_model.named_parameters()\n            if param.requires_grad\n        }\n        for name, param in merged_model.named_parameters():\n            param.grad = None\n        merged_model.eval()\n\n        return gradients\n\n    def frank_wolfe_selection(\n        self, gradients, checkpoints, model_to_merge_names={}, type=\"task\"\n    ):\n        assert type in [\n            \"task\",\n            \"layer\",\n        ], f\"Unsupported FW selection type: {type}, supported types are ['task', 'layer']\"\n        min_inner_product = float(\"inf\")\n        min_model = None\n        min_model_name = None\n        log_dict = {}\n        if type == \"task\":\n            for model_name, model_to_merge in checkpoints.items():\n                model_to_merge = model_to_merge.to(\"cpu\").state_dict()\n                inner_product_sum = 0\n                for param_name, param_value in model_to_merge.items():\n                    # caclulate consine similarity\n                    grad = gradients[param_name]\n                    ckpt = model_to_merge[param_name]\n                    param_alignment = torch.dot(grad.flatten(), ckpt.flatten()) / (\n                        torch.norm(grad) * torch.norm(ckpt)\n                    )\n                    inner_product_sum += param_alignment\n                log_dict[model_name] = inner_product_sum.item()\n                if (\n                    inner_product_sum &lt; min_inner_product\n                    and model_name not in model_to_merge_names\n                ):\n                    min_inner_product = inner_product_sum\n                    min_model = deepcopy(model_to_merge)\n                    min_model_name = model_name\n        else:\n            min_model = {}\n            min_inner_product = {}\n            min_idx = {}\n            min_model_name = {}\n            for model_name, model_to_merge in checkpoints.items():\n                model_to_merge = model_to_merge.to(\"cpu\").state_dict()\n                for param_name, param_value in model_to_merge.items():\n                    # caclulate consine similarity\n                    grad = gradients[param_name]\n                    ckpt = model_to_merge[param_name]\n                    param_alignment = torch.dot(grad.flatten(), ckpt.flatten()) / (\n                        torch.norm(grad) * torch.norm(ckpt)\n                    )\n                    if (\n                        param_name not in min_inner_product\n                        or param_alignment &lt; min_inner_product[param_name]\n                    ) and model_name not in model_to_merge_names[param_name]:\n                        min_inner_product[param_name] = param_alignment\n                        # if min_inner_product[param_name] &lt; 0:\n                        min_model[param_name] = param_value\n                        min_idx[param_name] = model_name\n                        min_model_name[param_name] = model_name\n                        # else:\n                        # min_model[param_name] = torch.zeros_like(param_value)\n            min_inner_product = sum(min_inner_product.values())\n            log_dict = {model_name: 0 for model_name in checkpoints.keys()}\n            for k in min_idx.values():\n                log_dict[k] += 1\n\n        return min_model, min_model_name, min_inner_product, log_dict\n\n    def run(self, modelpool: HuggingFaceClipVisionPool):\n        log.info(\"Fusing models using FW merging.\")\n        self.modelpool = modelpool\n        self.log_hyperparams(self.config)\n        self.on_frank_wolfe_iteration_start()\n\n        assert modelpool.has_pretrained, \"Pretrained model is required.\"\n        finetuned_models = {\n            name: modelpool.load_model(name)\n            for name in modelpool.model_names[: self.max_num_models]\n        }\n        pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n        if self.init_weight:\n            if self.init_weight == \"base\":\n                log.info(\"Initializing the merged model with the base model\")\n                merged_model = pretrained_model\n            else:\n                log.info(\"Initializing the merged model with the initial weight\")\n                if isinstance(self.init_weight, str):\n                    # self.config.weights is a path to a saved tensor\n                    layer_wise_weight = load_tensor_from_file(self.init_weight)\n                else:\n                    raise ValueError(f\"Unsupported weights format: {self.init_weight}\")\n\n                merged_model = LayerWiseMergedModel(\n                    layer_wise_weight=layer_wise_weight,\n                    pretrained_model=modelpool.load_model(\"_pretrained_\"),\n                    finetuned_models=list(finetuned_models.values()),\n                    clamp_weights=False,\n                    tie_weights=True,\n                    strict=False,\n                ).cuda()\n                merged_model = merged_model.merge_and_unload()\n        else:\n            log.info(\"Initializing the merged model with merge function\")\n            merged_model = self.merge_fn(\n                pretrained_model=modelpool.load_model(\"_pretrained_\"),\n                finetuned_models=list(finetuned_models.values()),\n                scaling_factor=self.scaling_factor,\n            ).cuda()\n        # merged_model = self.fabric.setup(merged_model)\n\n        initial_model = modelpool.load_model(\"_pretrained_\")\n        initial_model.load_state_dict(deepcopy(merged_model.state_dict()))\n        finetuned_models[\"initial\"] = initial_model\n        for step_idx in (\n            pbar := tqdm(\n                range(self.max_iters if not self.is_debug_mode else 1),\n                (\"[DEBUG MODE] \" if self.is_debug_mode else \"\") + \"Frank-Wolfe Merging\",\n                dynamic_ncols=True,\n            )\n        ):\n            torch.cuda.empty_cache()\n            torch.set_grad_enabled(True)\n            gradients = self.frank_wolfe_iteration(merged_model.cuda())\n            torch.set_grad_enabled(False)\n            grad_norm = torch.norm(\n                torch.stack([torch.norm(g) for g in gradients.values()])\n            )\n\n            model_to_merge_names = (\n                []\n                if self.granularity == \"task\"\n                else {name: [] for name in merged_model.state_dict().keys()}\n            )\n            min_model, min_model_name, min_alignment, chosen_model = (\n                self.frank_wolfe_selection(\n                    gradients,\n                    finetuned_models,\n                    model_to_merge_names=model_to_merge_names,\n                    type=self.granularity,\n                )\n            )\n\n            # Determine step size\n            step = 2 / (step_idx + 2) * self.step_size\n\n            # print iteration information\n            log.info(\n                f\"Iteration {step_idx+1}, Task Vector: {min_model_name}, Gradient Norm: {grad_norm:.6f}, Inner Products: {min_alignment:.6f}, Chosen Model: {chosen_model}\"\n            )\n\n            merged_model = self.merge_fn(\n                pretrained_model=merged_model.to(\"cpu\"),\n                finetuned_models=[min_model],\n                scaling_factor=step * self.scaling_factor,\n            )\n\n        torch.set_grad_enabled(False)\n        merged_model = merged_model.cuda().eval()\n        return merged_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FrankWolfeHardAlgorithm.__init__","title":"<code>__init__(merge_fn, step_size, max_iters, dataset_size, tasks=[], granularity='task', max_num_models=100, loss_fn='cross_entropy', init_weight='', scaling_factor=1.0, threshold=20, **kwargs)</code>","text":"<p>Initializes the TaskArithmeticAlgorithm with the given scaling factor.</p> <p>Parameters:</p> <ul> <li> <code>scaling_factor</code>               (<code>int</code>, default:                   <code>1.0</code> )           \u2013            <p>The factor by which the task vectors will be scaled before merging.</p> </li> </ul> Source code in <code>fusion_bench/method/fw_merging/fw_hard.py</code> <pre><code>def __init__(\n    self,\n    merge_fn: str,\n    step_size: float,\n    max_iters: int,\n    dataset_size: int,\n    tasks: List[str] = [],\n    granularity: str = \"task\",\n    max_num_models: int = 100,\n    loss_fn: str = \"cross_entropy\",\n    init_weight: str = \"\",\n    scaling_factor: float = 1.0,\n    threshold: int = 20,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the TaskArithmeticAlgorithm with the given scaling factor.\n\n    Args:\n        scaling_factor (int): The factor by which the task vectors will be scaled before merging.\n    \"\"\"\n    self.merger = merge_fn\n    if merge_fn == \"task_arithmetic\":\n        self.merge_fn = task_arithmetic_merge\n    elif merge_fn == \"ties\":\n        self.merge_fn = partial(ties_merge, threshold=threshold)\n    # elif merge_fn == \"concrete_ta\":\n    #     self.merge_fn = ConcreteTaskArithmeticAlgorithmForCLIP(\n    #         instantiate(OmegaConf.load(\"config/method/concrete_subspace/clip_concrete_task_arithmetic.yaml\"))\n    #     )\n    else:\n        raise ValueError(f\"Unsupported merge_fn: {merge_fn}\")\n    self.scaling_factor = scaling_factor\n\n    self.init_weight = init_weight\n    self.step_size = step_size\n    self.max_iters = max_iters\n    self.granularity = granularity\n    self.loss_fn = loss_fn\n    self.tasks = tasks\n    self.dataset_size = dataset_size\n    self.max_num_models = max_num_models\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#wudi-merging","title":"WUDI-Merging","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.WUDIMerging","title":"<code>WUDIMerging</code>","text":"<p>               Bases: <code>LightningFabricMixin</code>, <code>BaseAlgorithm</code></p> <p>Whoever Started the Interference Should End It:  Guiding Data-Free Model Merging via Task Vectors</p> Source code in <code>fusion_bench/method/wudi/wudi.py</code> <pre><code>@auto_register_config\nclass WUDIMerging(\n    LightningFabricMixin,\n    BaseAlgorithm,\n):\n    \"\"\"\n    Whoever Started the Interference Should End It:  Guiding Data-Free Model Merging via Task Vectors\n    \"\"\"\n\n    def __init__(\n        self,\n        iter_num: int,\n        exclude_keys: List[str] = None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n    def run(self, modelpool: BaseModelPool):\n        # load the pretrained model and the task vectors of all the finetuned models\n        with torch.no_grad():\n            pretrained_model = modelpool.load_pretrained_model()\n            task_vectors = []\n            for model_name in modelpool.model_names:\n                finetuned_model = modelpool.load_model(model_name)\n                task_vectors.append(\n                    state_dict_sub(\n                        finetuned_model.state_dict(), pretrained_model.state_dict()\n                    )\n                )\n                del finetuned_model  # free memory\n\n        merged_tv = wudi_merging(\n            task_vectors,\n            accelerator=self.fabric.device,\n            iter_num=self.iter_num,\n            exclude_keys=self.exclude_keys,\n        )\n\n        pretrained_model.load_state_dict(\n            state_dict_add(pretrained_model.state_dict(), merged_tv)\n        )\n\n        return pretrained_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#subspace-based-methods","title":"Subspace-based Methods","text":""},{"location":"api/fusion_bench.method/merging/#concrete-subspace","title":"Concrete Subspace","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.ConcreteTaskArithmeticAlgorithmForCLIP","title":"<code>ConcreteTaskArithmeticAlgorithmForCLIP</code>","text":"<p>               Bases: <code>CLIPClassificationMixin</code>, <code>SimpleProfilerMixin</code>, <code>ModelFusionAlgorithm</code></p> <p>ConcreteTaskArithmeticAlgorithmForCLIP is a class for performing task arithmetic on CLIP models with learned masking.</p> <p>This class extends the CLIPClassificationMixin, SimpleProfilerMixin, and ModelFusionAlgorithm classes. It provides methods for setting up models, training masks, and running the task arithmetic algorithm.</p> <p>Attributes:</p> <ul> <li> <code>merge_dtype</code>               (<code>dtype</code>)           \u2013            <p>The data type for merging weights.</p> </li> <li> <code>modelpool</code>               (<code>HuggingFaceClipVisionPool</code>)           \u2013            <p>The model pool containing the pretrained and fine-tuned models.</p> </li> </ul> Source code in <code>fusion_bench/method/concrete_subspace/clip_concrete_task_arithmetic.py</code> <pre><code>class ConcreteTaskArithmeticAlgorithmForCLIP(\n    CLIPClassificationMixin,\n    SimpleProfilerMixin,\n    ModelFusionAlgorithm,\n):\n    \"\"\"\n    ConcreteTaskArithmeticAlgorithmForCLIP is a class for performing task arithmetic on CLIP models with learned masking.\n\n    This class extends the CLIPClassificationMixin, SimpleProfilerMixin, and ModelFusionAlgorithm classes.\n    It provides methods for setting up models, training masks, and running the task arithmetic algorithm.\n\n    Attributes:\n        merge_dtype (torch.dtype): The data type for merging weights.\n        modelpool (HuggingFaceClipVisionPool): The model pool containing the pretrained and fine-tuned models.\n    \"\"\"\n\n    @torch.no_grad()\n    def setup_models(self):\n        \"\"\"\n        Set up the pretrained model, fine-tuned models, and mask model.\n\n        This method loads the pretrained model, constructs the PGE mask model, and loads the fine-tuned models.\n        It also creates a wrapped model with task-wise weights.\n\n        Returns:\n            Tuple[TaskWiseMergedModel, MaskModel]: The wrapped model and mask model.\n        \"\"\"\n        config = self.config\n        self.merge_dtype = parse_dtype(config.get(\"merge_dtype\", None))\n        modelpool = self.modelpool\n\n        # Load the pretrained model\n        pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n        # construct PGE mask model\n        mask_model = MaskModel(\n            pretrained_model,\n            ignore_untrained_params=True,\n            parameter_type=\"logits\",\n        )\n        if self.merge_dtype is not None:\n            mask_model.to(self.merge_dtype)\n        mask_model.fill_(self.config.initial_logits)\n        # TODO: ablation study for the initialization of mask model\n        # for param in mask_model.parameters():\n        #     param.data = param + 0.1 * torch.randn_like(param)\n        print(\"Summary of mask model:\")\n        print_parameters(mask_model)\n\n        # Load the fine-tuned models\n        finetuned_models = [\n            modelpool.load_model(name) for name in modelpool.model_names\n        ]\n\n        task_wise_weight = get_task_wise_weights(\n            num_models=len(modelpool.model_names),\n            init_values=self.config.scaling_factor,\n        )\n\n        # create a warpped model\n        module = TaskWiseMergedModel(\n            task_wise_weight=task_wise_weight,\n            pretrained_model=pretrained_model,\n            finetuned_models=finetuned_models,\n            clamp_weights=self.config.clamp_weights,\n            tie_weights=self.config.tie_weights,\n            strict=self.config.strict,\n            task_vector_dtype=self.merge_dtype,\n        )\n\n        return module, mask_model\n\n    def train_mask(self, module: TaskWiseMergedModel, mask_model: MaskModel):\n        \"\"\"\n        Train the mask model using the provided module.\n\n        This method configures the optimizer, sets up the mask model, and performs test-time adaptation to train the mask model.\n\n        Args:\n            module (TaskWiseMergedModel): The wrapped model with task-wise weights.\n            mask_model (MaskModel): The mask model to be trained.\n        \"\"\"\n        config = self.config\n        # mask_model: MaskModel = self.fabric.to_device(mask_model)\n\n        # configure optimizer\n        lr_scheduler = None\n        if self.config.optimizer == \"adam\":\n            optimizer = torch.optim.Adam(\n                filter(lambda p: p.requires_grad, mask_model.parameters()),\n                lr=self.config.lr,\n            )\n            print(f\"{optimizer=}\")\n            # TODO: ablation study for the learning rate scheduler. It should yield similar results.\n            # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            #     optimizer, self.config.max_steps, eta_min=0.1\n            # )\n            mask_model, optimizer = self.fabric.setup(mask_model, optimizer)\n        elif self.config.optimizer == \"sgd\":\n            optimizer = torch.optim.SGD(mask_model.parameters(), lr=self.config.lr)\n            print(f\"{optimizer=}\")\n            lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n                optimizer, self.config.max_steps, eta_min=0.1\n            )\n            mask_model, optimizer = self.fabric.setup(mask_model, optimizer)\n        else:\n            raise ValueError(f\"Unsupported optimizer: {self.config.optimizer}\")\n\n        module.to(mask_model.device)\n        module.requires_grad_(False)\n\n        mask_model.train()\n        optimizer.zero_grad()\n        for step_idx in (\n            pbar := tqdm(\n                range(self.config.max_steps if not self.is_debug_mode else 5),\n                (\"[DEBUG MODE] \" if self.is_debug_mode else \"\")\n                + \"Concrete Task Arithmetic Test-Time Adaptation\",\n                dynamic_ncols=True,\n                disable=not self.fabric.is_global_zero,\n            )\n        ):\n            metrics = {}\n            # sample a shared mask and merge weights\n            with self.profile(\"sample mask\"):\n                mask = mask_model.sample_mask(\n                    mask_type=\"continuous\", temperature=config.temperature\n                )\n                metrics[\"train/sparsity\"] = mask_sparsity(mask)\n            with self.profile(\"merge weights\"):\n                # rescale mask\n                for name, m in mask.items():\n                    mask[name] = m / torch.mean(m)\n                module.merge_weights(task_vector_mask=mask)\n\n            # ------ inner optimization goes here ------\n            # NOTE:\n            #   Because the algorithmic parameters of task arithmetic are assumed to be chosen on a validation test\n            #   set, we do not need to perform inner optimization here. So here we skip the inner optimization step.\n            # ------------------------------------------\n\n            total_loss = None\n            for task in self.modelpool.model_names:\n                with self.profile(\"data loading\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                    # NOTE: The labels are not allowed to be used during test-time adaptation\n                    images = batch[0].to(dtype=self.merge_dtype)\n                with self.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, images, task)\n                    loss = entropy_loss(logits)\n                    total_loss = loss if total_loss is None else total_loss + loss\n\n            with self.profile(\"compute grad\"):\n                self.fabric.backward(total_loss)\n\n            with self.profile(\"optimizer step\"):\n                optimizer.step()\n                optimizer.zero_grad()\n\n                if lr_scheduler is not None:\n                    lr_scheduler.step()\n\n            metrics.update({\"train/loss\": loss.item()})\n            self.fabric.log_dict(metrics, step=step_idx)\n            pbar.set_postfix(metrics)\n\n            if (step_idx + 1) % self.config.save_interval == 0:\n                with self.profiler.profile(\"save checkpoint\"):\n                    save_dir = os.path.join(self.fabric.logger.log_dir, \"checkpoints\")\n                    if not os.path.exists(save_dir):\n                        os.makedirs(save_dir, exist_ok=True)\n                    save_path = os.path.join(save_dir, f\"mask_steps_{step_idx}.pt\")\n                    print(f\"saving checkpoint to {save_path}\")\n                    state = {\"model\": mask_model}\n                    self.fabric.save(save_path, state)\n\n                    # Create or update a symbolic link to the latest checkpoint\n                    if self.fabric.is_global_zero:\n                        symlink_path = os.path.join(save_dir, \"latest_checkpoint.pt\")\n                        if os.path.exists(symlink_path):\n                            os.remove(symlink_path)\n                        os.link(os.path.abspath(save_path), symlink_path)\n\n                self.print_profile_summary()\n\n    def run(self, modelpool: HuggingFaceClipVisionPool):\n        \"\"\"\n        Run the Concrete Task Arithmetic algorithm.\n\n        This method sets up the models, trains the mask model if necessary, and performs the final merging of weights.\n\n        Args:\n            modelpool (HuggingFaceClipVisionPool): The model pool containing the pretrained and fine-tuned models.\n\n        Returns:\n            torch.nn.Module: The final merged model.\n        \"\"\"\n        self.modelpool = to_modelpool(modelpool)\n        config = self.config\n        self.log_hyperparams(config, filename=\"method_config.yaml\")\n\n        with self.profile(\"setup models\"):\n            module, mask_model = self.setup_models()\n            self.setup_zero_shot_classification_head()\n\n        if config.mask_checkpoint is None:\n            if not config.skip_training:\n                torch.cuda.empty_cache()\n                self.train_mask(module=module, mask_model=mask_model)\n        else:\n            if self.fabric.is_global_zero:\n                print(\"loading mask from checkpoint\", config.mask_checkpoint)\n            self.fabric.load(config.mask_checkpoint, {\"model\": mask_model})\n\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            mask = mask_model.sample_mask(\n                mask_type=config.eval_mask_type,\n                temperature=config.temperature,\n            )\n            # rescale mask\n            for name, m in mask.items():\n                mask[name] = m / torch.mean(m)\n            model = module.merge_and_unload(mask)\n        return model.to(dtype=torch.float32)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.ConcreteTaskArithmeticAlgorithmForCLIP.run","title":"<code>run(modelpool)</code>","text":"<p>Run the Concrete Task Arithmetic algorithm.</p> <p>This method sets up the models, trains the mask model if necessary, and performs the final merging of weights.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>HuggingFaceClipVisionPool</code>)           \u2013            <p>The model pool containing the pretrained and fine-tuned models.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>torch.nn.Module: The final merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/concrete_subspace/clip_concrete_task_arithmetic.py</code> <pre><code>def run(self, modelpool: HuggingFaceClipVisionPool):\n    \"\"\"\n    Run the Concrete Task Arithmetic algorithm.\n\n    This method sets up the models, trains the mask model if necessary, and performs the final merging of weights.\n\n    Args:\n        modelpool (HuggingFaceClipVisionPool): The model pool containing the pretrained and fine-tuned models.\n\n    Returns:\n        torch.nn.Module: The final merged model.\n    \"\"\"\n    self.modelpool = to_modelpool(modelpool)\n    config = self.config\n    self.log_hyperparams(config, filename=\"method_config.yaml\")\n\n    with self.profile(\"setup models\"):\n        module, mask_model = self.setup_models()\n        self.setup_zero_shot_classification_head()\n\n    if config.mask_checkpoint is None:\n        if not config.skip_training:\n            torch.cuda.empty_cache()\n            self.train_mask(module=module, mask_model=mask_model)\n    else:\n        if self.fabric.is_global_zero:\n            print(\"loading mask from checkpoint\", config.mask_checkpoint)\n        self.fabric.load(config.mask_checkpoint, {\"model\": mask_model})\n\n    with torch.no_grad():\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        mask = mask_model.sample_mask(\n            mask_type=config.eval_mask_type,\n            temperature=config.temperature,\n        )\n        # rescale mask\n        for name, m in mask.items():\n            mask[name] = m / torch.mean(m)\n        model = module.merge_and_unload(mask)\n    return model.to(dtype=torch.float32)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.ConcreteTaskArithmeticAlgorithmForCLIP.setup_models","title":"<code>setup_models()</code>","text":"<p>Set up the pretrained model, fine-tuned models, and mask model.</p> <p>This method loads the pretrained model, constructs the PGE mask model, and loads the fine-tuned models. It also creates a wrapped model with task-wise weights.</p> <p>Returns:</p> <ul> <li>           \u2013            <p>Tuple[TaskWiseMergedModel, MaskModel]: The wrapped model and mask model.</p> </li> </ul> Source code in <code>fusion_bench/method/concrete_subspace/clip_concrete_task_arithmetic.py</code> <pre><code>@torch.no_grad()\ndef setup_models(self):\n    \"\"\"\n    Set up the pretrained model, fine-tuned models, and mask model.\n\n    This method loads the pretrained model, constructs the PGE mask model, and loads the fine-tuned models.\n    It also creates a wrapped model with task-wise weights.\n\n    Returns:\n        Tuple[TaskWiseMergedModel, MaskModel]: The wrapped model and mask model.\n    \"\"\"\n    config = self.config\n    self.merge_dtype = parse_dtype(config.get(\"merge_dtype\", None))\n    modelpool = self.modelpool\n\n    # Load the pretrained model\n    pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n    # construct PGE mask model\n    mask_model = MaskModel(\n        pretrained_model,\n        ignore_untrained_params=True,\n        parameter_type=\"logits\",\n    )\n    if self.merge_dtype is not None:\n        mask_model.to(self.merge_dtype)\n    mask_model.fill_(self.config.initial_logits)\n    # TODO: ablation study for the initialization of mask model\n    # for param in mask_model.parameters():\n    #     param.data = param + 0.1 * torch.randn_like(param)\n    print(\"Summary of mask model:\")\n    print_parameters(mask_model)\n\n    # Load the fine-tuned models\n    finetuned_models = [\n        modelpool.load_model(name) for name in modelpool.model_names\n    ]\n\n    task_wise_weight = get_task_wise_weights(\n        num_models=len(modelpool.model_names),\n        init_values=self.config.scaling_factor,\n    )\n\n    # create a warpped model\n    module = TaskWiseMergedModel(\n        task_wise_weight=task_wise_weight,\n        pretrained_model=pretrained_model,\n        finetuned_models=finetuned_models,\n        clamp_weights=self.config.clamp_weights,\n        tie_weights=self.config.tie_weights,\n        strict=self.config.strict,\n        task_vector_dtype=self.merge_dtype,\n    )\n\n    return module, mask_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.ConcreteTaskArithmeticAlgorithmForCLIP.train_mask","title":"<code>train_mask(module, mask_model)</code>","text":"<p>Train the mask model using the provided module.</p> <p>This method configures the optimizer, sets up the mask model, and performs test-time adaptation to train the mask model.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>TaskWiseMergedModel</code>)           \u2013            <p>The wrapped model with task-wise weights.</p> </li> <li> <code>mask_model</code>               (<code>MaskModel</code>)           \u2013            <p>The mask model to be trained.</p> </li> </ul> Source code in <code>fusion_bench/method/concrete_subspace/clip_concrete_task_arithmetic.py</code> <pre><code>def train_mask(self, module: TaskWiseMergedModel, mask_model: MaskModel):\n    \"\"\"\n    Train the mask model using the provided module.\n\n    This method configures the optimizer, sets up the mask model, and performs test-time adaptation to train the mask model.\n\n    Args:\n        module (TaskWiseMergedModel): The wrapped model with task-wise weights.\n        mask_model (MaskModel): The mask model to be trained.\n    \"\"\"\n    config = self.config\n    # mask_model: MaskModel = self.fabric.to_device(mask_model)\n\n    # configure optimizer\n    lr_scheduler = None\n    if self.config.optimizer == \"adam\":\n        optimizer = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, mask_model.parameters()),\n            lr=self.config.lr,\n        )\n        print(f\"{optimizer=}\")\n        # TODO: ablation study for the learning rate scheduler. It should yield similar results.\n        # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        #     optimizer, self.config.max_steps, eta_min=0.1\n        # )\n        mask_model, optimizer = self.fabric.setup(mask_model, optimizer)\n    elif self.config.optimizer == \"sgd\":\n        optimizer = torch.optim.SGD(mask_model.parameters(), lr=self.config.lr)\n        print(f\"{optimizer=}\")\n        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, self.config.max_steps, eta_min=0.1\n        )\n        mask_model, optimizer = self.fabric.setup(mask_model, optimizer)\n    else:\n        raise ValueError(f\"Unsupported optimizer: {self.config.optimizer}\")\n\n    module.to(mask_model.device)\n    module.requires_grad_(False)\n\n    mask_model.train()\n    optimizer.zero_grad()\n    for step_idx in (\n        pbar := tqdm(\n            range(self.config.max_steps if not self.is_debug_mode else 5),\n            (\"[DEBUG MODE] \" if self.is_debug_mode else \"\")\n            + \"Concrete Task Arithmetic Test-Time Adaptation\",\n            dynamic_ncols=True,\n            disable=not self.fabric.is_global_zero,\n        )\n    ):\n        metrics = {}\n        # sample a shared mask and merge weights\n        with self.profile(\"sample mask\"):\n            mask = mask_model.sample_mask(\n                mask_type=\"continuous\", temperature=config.temperature\n            )\n            metrics[\"train/sparsity\"] = mask_sparsity(mask)\n        with self.profile(\"merge weights\"):\n            # rescale mask\n            for name, m in mask.items():\n                mask[name] = m / torch.mean(m)\n            module.merge_weights(task_vector_mask=mask)\n\n        # ------ inner optimization goes here ------\n        # NOTE:\n        #   Because the algorithmic parameters of task arithmetic are assumed to be chosen on a validation test\n        #   set, we do not need to perform inner optimization here. So here we skip the inner optimization step.\n        # ------------------------------------------\n\n        total_loss = None\n        for task in self.modelpool.model_names:\n            with self.profile(\"data loading\"):\n                batch = next(self.get_shuffled_test_loader_iter(task))\n                # NOTE: The labels are not allowed to be used during test-time adaptation\n                images = batch[0].to(dtype=self.merge_dtype)\n            with self.profile(\"forward pass\"):\n                logits = self.compute_logits(module, images, task)\n                loss = entropy_loss(logits)\n                total_loss = loss if total_loss is None else total_loss + loss\n\n        with self.profile(\"compute grad\"):\n            self.fabric.backward(total_loss)\n\n        with self.profile(\"optimizer step\"):\n            optimizer.step()\n            optimizer.zero_grad()\n\n            if lr_scheduler is not None:\n                lr_scheduler.step()\n\n        metrics.update({\"train/loss\": loss.item()})\n        self.fabric.log_dict(metrics, step=step_idx)\n        pbar.set_postfix(metrics)\n\n        if (step_idx + 1) % self.config.save_interval == 0:\n            with self.profiler.profile(\"save checkpoint\"):\n                save_dir = os.path.join(self.fabric.logger.log_dir, \"checkpoints\")\n                if not os.path.exists(save_dir):\n                    os.makedirs(save_dir, exist_ok=True)\n                save_path = os.path.join(save_dir, f\"mask_steps_{step_idx}.pt\")\n                print(f\"saving checkpoint to {save_path}\")\n                state = {\"model\": mask_model}\n                self.fabric.save(save_path, state)\n\n                # Create or update a symbolic link to the latest checkpoint\n                if self.fabric.is_global_zero:\n                    symlink_path = os.path.join(save_dir, \"latest_checkpoint.pt\")\n                    if os.path.exists(symlink_path):\n                        os.remove(symlink_path)\n                    os.link(os.path.abspath(save_path), symlink_path)\n\n            self.print_profile_summary()\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.ConcreteTaskWiseAdaMergingForCLIP","title":"<code>ConcreteTaskWiseAdaMergingForCLIP</code>","text":"<p>               Bases: <code>CLIPClassificationMixin</code>, <code>SimpleProfilerMixin</code>, <code>ModelFusionAlgorithm</code></p> Source code in <code>fusion_bench/method/concrete_subspace/clip_concrete_adamerging.py</code> <pre><code>class ConcreteTaskWiseAdaMergingForCLIP(\n    CLIPClassificationMixin,\n    SimpleProfilerMixin,\n    ModelFusionAlgorithm,\n):\n    @torch.no_grad()\n    def setup_models(self):\n        config = self.config\n        self.merge_dtype = parse_dtype(config.get(\"merge_dtype\", None))\n        modelpool = self.modelpool\n\n        # Load the pretrained model\n        pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n        # construct PGE mask model\n        mask_model = MaskModel(\n            pretrained_model,\n            ignore_untrained_params=True,\n            parameter_type=\"logits\",\n        )\n        if self.merge_dtype is not None:\n            mask_model.to(self.merge_dtype)\n        mask_model.fill_(self.config.initial_logits)\n        # TODO: ablation study for the initialization of mask model\n        # for param in mask_model.parameters():\n        #     param.data = param + 0.1 * torch.randn_like(param)\n        print(\"Summary of mask model:\")\n        print_parameters(mask_model)\n\n        # Load the fine-tuned models\n        finetuned_models = [\n            modelpool.load_model(name) for name in modelpool.model_names\n        ]\n\n        task_wise_weight = get_task_wise_weights(\n            num_models=len(modelpool.model_names),\n            init_values=self.config.scaling_factor,\n        )\n        self.init_task_wise_weight = deepcopy(task_wise_weight)\n\n        # create a warpped model\n        module = TaskWiseMergedModel(\n            task_wise_weight=task_wise_weight,\n            pretrained_model=pretrained_model,\n            finetuned_models=finetuned_models,\n            clamp_weights=self.config.clamp_weights,\n            tie_weights=self.config.tie_weights,\n            strict=self.config.strict,\n            task_vector_dtype=self.merge_dtype,\n        )\n        return module, mask_model\n\n    def train_mask(self, module: TaskWiseMergedModel, mask_model: MaskModel):\n        config = self.config\n        self.init_task_wise_weight = self.to_device(self.init_task_wise_weight)\n\n        # configure optimizer\n        lr_scheduler = None\n        if self.config.optimizer == \"adam\":\n            base_optimizer = torch.optim.Adam(\n                [module.merge_weight], lr=self.config.base_lr\n            )\n            optimizer = torch.optim.Adam(mask_model.parameters(), lr=self.config.lr)\n            print(f\"{optimizer=}\")\n            # TODO: ablation study for the learning rate scheduler. It should yield similar results.\n            # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            #     optimizer, self.config.max_steps, eta_min=0.1\n            # )\n            module, base_optimizer = self.fabric.setup(module, base_optimizer)\n            mask_model, optimizer = self.fabric.setup(mask_model, optimizer)\n        else:\n            raise ValueError(f\"Unsupported optimizer: {self.config.optimizer}\")\n\n        module.train()\n        mask_model.train()\n        for step_idx in (\n            pbar := tqdm(\n                range(self.config.max_steps if not self.is_debug_mode else 5),\n                (\"[DEBUG MODE] \" if self.is_debug_mode else \"\")\n                + \"Concrete AdaMerging Meta-Learn Mask (1/2)\",\n                dynamic_ncols=True,\n                disable=not self.fabric.is_global_zero,\n            )\n        ):\n            metrics = {}\n            # sample a shared mask and merge weights\n            with self.profile(\"sample mask\"):\n                mask = mask_model.sample_mask(\n                    mask_type=\"continuous\", temperature=config.temperature\n                )\n                metrics[\"train/sparsity\"] = mask_sparsity(mask)\n            with self.profile(\"merge weights\"):\n                # rescale mask\n                for name, m in mask.items():\n                    mask[name] = m / torch.mean(m)\n\n                # for inner optimization, we do not optimize the mask, so we detach it\n                module.merge_weights(\n                    task_vector_mask={name: m.detach() for name, m in mask.items()}\n                )\n\n            # ------ inner optimization goes here ------\n            module.merge_weight.data = deepcopy(self.init_task_wise_weight)\n            total_loss = None\n            for task in self.modelpool.model_names:\n                with self.profile(\"data loading\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                    # NOTE: The labels are not allowed to be used during test-time adaptation\n                    images = batch[0]\n                with self.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, images, task)\n                    loss = entropy_loss(logits)\n                    total_loss = loss if total_loss is None else total_loss + loss\n\n            base_optimizer.zero_grad()\n            with self.profile(\"compute grad\"):\n                self.fabric.backward(total_loss)\n\n            with self.profile(\"base optimizer step\"):\n                base_optimizer.step()\n\n            with self.profile(\"merge weights\"):\n                module.merge_weights(task_vector_mask=mask)\n\n            # ------------------------------------------\n\n            total_loss = None\n            for task in self.modelpool.model_names:\n                with self.profile(\"data loading\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                    # NOTE: The labels are not allowed to be used during test-time adaptation\n                    images = batch[0]\n                with self.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, images, task)\n                    loss = entropy_loss(logits)\n                    total_loss = loss if total_loss is None else total_loss + loss\n\n            optimizer.zero_grad()\n            with self.profile(\"compute grad\"):\n                self.fabric.backward(total_loss)\n\n            with self.profile(\"optimizer step\"):\n                optimizer.step()\n\n                if lr_scheduler is not None:\n                    lr_scheduler.step()\n\n            metrics.update({\"train/loss\": loss.item()})\n            self.fabric.log_dict(metrics, step=step_idx)\n            pbar.set_postfix(metrics)\n\n            if (step_idx + 1) % self.config.save_interval == 0:\n                with self.profiler.profile(\"save checkpoint\"):\n                    save_dir = os.path.join(self.fabric.logger.log_dir, \"checkpoints\")\n                    if not os.path.exists(save_dir):\n                        os.makedirs(save_dir, exist_ok=True)\n                    save_path = os.path.join(save_dir, f\"mask_steps_{step_idx}.pt\")\n                    print(f\"saving checkpoint to {save_path}\")\n                    state = {\"model\": mask_model}\n                    self.fabric.save(save_path, state)\n\n                    # Create or update a symbolic link to the latest checkpoint\n                    if self.fabric.is_global_zero:\n                        symlink_path = os.path.join(save_dir, \"latest_checkpoint.pt\")\n                        if os.path.exists(symlink_path):\n                            os.remove(symlink_path)\n                        os.link(os.path.abspath(save_path), symlink_path)\n\n                self.print_profile_summary()\n\n    def run_adamerging(self, module: TaskWiseMergedModel, mask):\n        module.merge_weight.data = deepcopy(self.init_task_wise_weight)\n        optimizer = torch.optim.Adam(\n            [module.merge_weight], lr=self.config.adamerging_lr\n        )\n        module, optimizer = self.fabric.setup(module, optimizer)\n        module.train()\n        for step_idx in (\n            pbar := tqdm(\n                range(\n                    self.config.max_adamerging_steps if not self.is_debug_mode else 5\n                ),\n                (\"[DEBUG MODE] \" if self.is_debug_mode else \"\")\n                + \"Concrete AdaMerging AdaMerging (2/2)\",\n                dynamic_ncols=True,\n                disable=not self.fabric.is_global_zero,\n            )\n        ):\n            step_idx = step_idx + self.config.max_steps\n            with self.profile(\"merge weights\"):\n                module.merge_weights(task_vector_mask=mask)\n\n            metrics = {}\n            total_loss = None\n            for task in self.modelpool.model_names:\n                with self.profile(\"data loading\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                    # NOTE: The labels are not allowed to be used during test-time adaptation\n                    images = batch[0]\n                with self.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, images, task)\n                    loss = entropy_loss(logits)\n                    total_loss = loss if total_loss is None else total_loss + loss\n\n            optimizer.zero_grad()\n            with self.profile(\"compute grad\"):\n                self.fabric.backward(total_loss)\n\n            with self.profile(\"base optimizer step\"):\n                optimizer.step()\n\n            metrics.update({\"train/loss\": loss.item()})\n            self.fabric.log_dict(metrics, step=step_idx)\n            pbar.set_postfix(metrics)\n\n            if (step_idx + 1) % self.config.save_interval == 0:\n                with self.profiler.profile(\"save checkpoint\"):\n                    save_dir = os.path.join(self.fabric.logger.log_dir, \"checkpoints\")\n                    if not os.path.exists(save_dir):\n                        os.makedirs(save_dir, exist_ok=True)\n                    save_path = os.path.join(save_dir, f\"merge_weight_{step_idx}.pt\")\n                    print(f\"saving checkpoint to {save_path}\")\n                    state = {\"merge_weight\": module.merge_weight}\n                    self.fabric.save(save_path, state)\n\n                    # Create or update a symbolic link to the latest checkpoint\n                    if self.fabric.is_global_zero:\n                        symlink_path = os.path.join(\n                            save_dir, \"merge_weight_latest_checkpoint.pt\"\n                        )\n                        if os.path.exists(symlink_path):\n                            os.remove(symlink_path)\n                        os.link(os.path.abspath(save_path), symlink_path)\n\n                self.print_profile_summary()\n        return module\n\n    def run(self, modelpool: HuggingFaceClipVisionPool):\n        self.modelpool = to_modelpool(modelpool)\n        config = self.config\n        self.log_hyperparams(config, filename=\"method_config.yaml\")\n\n        with self.profile(\"setup models\"):\n            module, mask_model = self.setup_models()\n            mask_model: MaskModel = self.fabric.to_device(mask_model)\n            module: TaskWiseMergedModel = self.fabric.to_device(module)\n            self.setup_zero_shot_classification_head()\n\n        if config.mask_checkpoint is None:\n            self.train_mask(module=module, mask_model=mask_model)\n        else:\n            if self.fabric.is_global_zero:\n                print(\"loading mask from checkpoint\", config.mask_checkpoint)\n            self.fabric.load(config.mask_checkpoint, {\"model\": mask_model})\n\n        # run adamerging\n        with torch.no_grad():\n            mask = mask_model.sample_mask(\n                mask_type=config.eval_mask_type,\n                temperature=config.temperature,\n            )\n            # rescale mask\n            for name, m in mask.items():\n                mask[name] = m / torch.mean(m)\n        module = self.run_adamerging(module, mask=mask)\n\n        with torch.no_grad():\n            model = module.merge_and_unload(mask)\n        return model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.ConcreteLayerWiseAdaMergingForCLIP","title":"<code>ConcreteLayerWiseAdaMergingForCLIP</code>","text":"<p>               Bases: <code>CLIPClassificationMixin</code>, <code>SimpleProfilerMixin</code>, <code>ModelFusionAlgorithm</code></p> Source code in <code>fusion_bench/method/concrete_subspace/clip_concrete_adamerging.py</code> <pre><code>class ConcreteLayerWiseAdaMergingForCLIP(\n    CLIPClassificationMixin,\n    SimpleProfilerMixin,\n    ModelFusionAlgorithm,\n):\n    @torch.no_grad()\n    def setup_models(self):\n        config = self.config\n        self.merge_dtype = parse_dtype(config.get(\"merge_dtype\", None))\n        modelpool = self.modelpool\n\n        # Load the pretrained model\n        pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n        # construct PGE mask model\n        mask_model = MaskModel(\n            pretrained_model,\n            ignore_untrained_params=True,\n            parameter_type=\"logits\",\n        )\n        if self.merge_dtype is not None:\n            mask_model.to(self.merge_dtype)\n        mask_model.fill_(self.config.initial_logits)\n        # TODO: ablation study for the initialization of mask model\n        # for param in mask_model.parameters():\n        #     param.data = param + 0.1 * torch.randn_like(param)\n        print(\"Summary of mask model:\")\n        print_parameters(mask_model)\n\n        # Load the fine-tuned models\n        finetuned_models = [\n            modelpool.load_model(name) for name in modelpool.model_names\n        ]\n\n        layer_wise_weight = get_layer_wise_weights(\n            num_models=len(modelpool.model_names),\n            num_layers=len(\n                tuple(filter(lambda p: p.requires_grad, pretrained_model.parameters()))\n            ),\n            init_values=self.config.scaling_factor,\n        )\n        self.init_layer_wise_weight = deepcopy(layer_wise_weight)\n\n        # create a warpped model\n        module = LayerWiseMergedModel(\n            layer_wise_weight=layer_wise_weight,\n            pretrained_model=pretrained_model,\n            finetuned_models=finetuned_models,\n            clamp_weights=self.config.clamp_weights,\n            tie_weights=self.config.tie_weights,\n            strict=self.config.strict,\n        )\n        return module, mask_model\n\n    def train_mask(self, module: LayerWiseMergedModel, mask_model: MaskModel):\n        config = self.config\n        self.init_layer_wise_weight = self.to_device(self.init_layer_wise_weight)\n\n        # configure optimizer\n        lr_scheduler = None\n        if self.config.optimizer == \"adam\":\n            base_optimizer = torch.optim.Adam(\n                [module.merge_weight], lr=self.config.base_lr\n            )\n            optimizer = torch.optim.Adam(mask_model.parameters(), lr=self.config.lr)\n            print(f\"{optimizer=}\")\n            # TODO: ablation study for the learning rate scheduler. It should yield similar results.\n            # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            #     optimizer, self.config.max_steps, eta_min=0.1\n            # )\n            module, base_optimizer = self.fabric.setup(module, base_optimizer)\n            mask_model, optimizer = self.fabric.setup(mask_model, optimizer)\n        else:\n            raise ValueError(f\"Unsupported optimizer: {self.config.optimizer}\")\n\n        module.train()\n        mask_model.train()\n        for step_idx in (\n            pbar := tqdm(\n                range(self.config.max_steps if not self.is_debug_mode else 5),\n                (\"[DEBUG MODE] \" if self.is_debug_mode else \"\")\n                + \"Concrete AdaMerging Meta-Learn Mask (1/2)\",\n                dynamic_ncols=True,\n                disable=not self.fabric.is_global_zero,\n            )\n        ):\n            metrics = {}\n            # sample a shared mask and merge weights\n            with self.profile(\"sample mask\"):\n                mask = mask_model.sample_mask(\n                    mask_type=\"continuous\", temperature=config.temperature\n                )\n                metrics[\"train/sparsity\"] = mask_sparsity(mask)\n            with self.profile(\"merge weights\"):\n                # rescale mask\n                for name, m in mask.items():\n                    mask[name] = m / torch.mean(m)\n\n                # for inner optimization, we do not optimize the mask, so we detach it\n                module.merge_weights(\n                    task_vector_mask={name: m.detach() for name, m in mask.items()}\n                )\n\n            # ------ inner optimization goes here ------\n            module.merge_weight.data = deepcopy(self.init_layer_wise_weight)\n            total_loss = None\n            for task in self.modelpool.model_names:\n                with self.profile(\"data loading\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                    # NOTE: The labels are not allowed to be used during test-time adaptation\n                    images = batch[0]\n                with self.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, images, task)\n                    loss = entropy_loss(logits)\n                    total_loss = loss if total_loss is None else total_loss + loss\n\n            base_optimizer.zero_grad()\n            with self.profile(\"compute grad\"):\n                self.fabric.backward(total_loss)\n\n            with self.profile(\"base optimizer step\"):\n                base_optimizer.step()\n\n            with self.profile(\"merge weights\"):\n                module.merge_weights(task_vector_mask=mask)\n\n            # ------------------------------------------\n\n            total_loss = None\n            for task in self.modelpool.model_names:\n                with self.profile(\"data loading\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                    # NOTE: The labels are not allowed to be used during test-time adaptation\n                    images = batch[0]\n                with self.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, images, task)\n                    loss = entropy_loss(logits)\n                    total_loss = loss if total_loss is None else total_loss + loss\n\n            optimizer.zero_grad()\n            with self.profile(\"compute grad\"):\n                self.fabric.backward(total_loss)\n\n            with self.profile(\"optimizer step\"):\n                optimizer.step()\n\n                if lr_scheduler is not None:\n                    lr_scheduler.step()\n\n            metrics.update({\"train/loss\": loss.item()})\n            self.fabric.log_dict(metrics, step=step_idx)\n            pbar.set_postfix(metrics)\n\n            if (step_idx + 1) % self.config.save_interval == 0:\n                with self.profiler.profile(\"save checkpoint\"):\n                    save_dir = os.path.join(self.fabric.logger.log_dir, \"checkpoints\")\n                    if not os.path.exists(save_dir):\n                        os.makedirs(save_dir, exist_ok=True)\n                    save_path = os.path.join(save_dir, f\"mask_steps_{step_idx}.pt\")\n                    print(f\"saving checkpoint to {save_path}\")\n                    state = {\"model\": mask_model}\n                    self.fabric.save(save_path, state)\n\n                    # Create or update a symbolic link to the latest checkpoint\n                    if self.fabric.is_global_zero:\n                        symlink_path = os.path.join(save_dir, \"latest_checkpoint.pt\")\n                        if os.path.exists(symlink_path):\n                            os.remove(symlink_path)\n                        os.link(os.path.abspath(save_path), symlink_path)\n\n                self.print_profile_summary()\n\n    def run_adamerging(self, module: LayerWiseMergedModel, mask):\n        module.merge_weight.data = deepcopy(self.init_layer_wise_weight)\n        optimizer = torch.optim.Adam(\n            [module.merge_weight], lr=self.config.adamerging_lr\n        )\n        module, optimizer = self.fabric.setup(module, optimizer)\n        module.train()\n        for step_idx in (\n            pbar := tqdm(\n                range(\n                    self.config.max_adamerging_steps if not self.is_debug_mode else 5\n                ),\n                (\"[DEBUG MODE] \" if self.is_debug_mode else \"\")\n                + \"Concrete AdaMerging AdaMerging (2/2)\",\n                dynamic_ncols=True,\n                disable=not self.fabric.is_global_zero,\n            )\n        ):\n            step_idx = step_idx + self.config.max_steps\n            with self.profile(\"merge weights\"):\n                module.merge_weights(task_vector_mask=mask)\n\n            metrics = {}\n            total_loss = None\n            for task in self.modelpool.model_names:\n                with self.profile(\"data loading\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                    # NOTE: The labels are not allowed to be used during test-time adaptation\n                    images = batch[0]\n                with self.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, images, task)\n                    loss = entropy_loss(logits)\n                    total_loss = loss if total_loss is None else total_loss + loss\n\n            optimizer.zero_grad()\n            with self.profile(\"compute grad\"):\n                self.fabric.backward(total_loss)\n\n            with self.profile(\"base optimizer step\"):\n                optimizer.step()\n\n            metrics.update({\"train/loss\": loss.item()})\n            self.fabric.log_dict(metrics, step=step_idx)\n            pbar.set_postfix(metrics)\n\n            if (step_idx + 1) % self.config.save_interval == 0:\n                with self.profiler.profile(\"save checkpoint\"):\n                    save_dir = os.path.join(self.fabric.logger.log_dir, \"checkpoints\")\n                    if not os.path.exists(save_dir):\n                        os.makedirs(save_dir, exist_ok=True)\n                    save_path = os.path.join(save_dir, f\"merge_weight_{step_idx}.pt\")\n                    print(f\"saving checkpoint to {save_path}\")\n                    state = {\"merge_weight\": module.merge_weight}\n                    self.fabric.save(save_path, state)\n\n                    # Create or update a symbolic link to the latest checkpoint\n                    if self.fabric.is_global_zero:\n                        symlink_path = os.path.join(\n                            save_dir, \"merge_weight_latest_checkpoint.pt\"\n                        )\n                        if os.path.exists(symlink_path):\n                            os.remove(symlink_path)\n                        os.link(os.path.abspath(save_path), symlink_path)\n\n                self.print_profile_summary()\n        return module\n\n    def run(self, modelpool: HuggingFaceClipVisionPool):\n        self.modelpool = to_modelpool(modelpool)\n        config = self.config\n        self.log_hyperparams(config, filename=\"method_config.yaml\")\n\n        with self.profile(\"setup models\"):\n            module, mask_model = self.setup_models()\n            mask_model: MaskModel = self.fabric.to_device(mask_model)\n            module: LayerWiseMergedModel = self.fabric.to_device(module)\n            self.setup_zero_shot_classification_head()\n\n        if config.mask_checkpoint is None:\n            self.train_mask(module=module, mask_model=mask_model)\n        else:\n            if self.fabric.is_global_zero:\n                print(\"loading mask from checkpoint\", config.mask_checkpoint)\n            self.fabric.load(config.mask_checkpoint, {\"model\": mask_model})\n\n        # run adamerging\n        with torch.no_grad():\n            mask = mask_model.sample_mask(\n                mask_type=config.eval_mask_type,\n                temperature=config.temperature,\n            )\n            # rescale mask\n            for name, m in mask.items():\n                mask[name] = m / torch.mean(m)\n        module = self.run_adamerging(module, mask=mask)\n\n        with torch.no_grad():\n            model = module.merge_and_unload(mask)\n        return model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#task-singular-vector-merging-tsvm","title":"Task Singular Vector Merging (TSVM)","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.TaskSingularVectorMerging","title":"<code>TaskSingularVectorMerging</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>LightningFabricMixin</code></p> <p>Task Singular Vector Merging (TSVM) Algorithm</p> <p>This class implements a model merging technique that leverages Singular Value Decomposition (SVD) to identify and combine the most important directions in the task vector space. The algorithm is particularly effective for merging multiple models fine-tuned on different tasks while preserving their essential capabilities.</p> <p>Key Concepts: - Task Vector: The difference between a fine-tuned model and its pretrained base model,   representing the knowledge gained during fine-tuning for a specific task. - Singular Value Decomposition: A matrix factorization technique used to find the principal   components (most important directions) in the space of task vectors. - Model Merging: The process of combining multiple models into a single unified model that   retains capabilities from all constituent models.</p> <p>Algorithm Steps: 1. Extract task vectors from all fine-tuned models by subtracting the pretrained model 2. Apply SVD to the matrix of task vectors to find principal components 3. Reconstruct task vectors using only the most significant singular vectors 4. Merge the reconstructed task vectors (either individually scaled or as a sum) 5. Add the final merged task vector to the pretrained model to create the unified model</p> <p>see <code>docs/algorithms/task_singular_vector.md</code> for comprehensive algorithmic details.</p> Source code in <code>fusion_bench/method/task_singular_vector/TSVM.py</code> <pre><code>class TaskSingularVectorMerging(BaseAlgorithm, LightningFabricMixin):\n    \"\"\"\n    Task Singular Vector Merging (TSVM) Algorithm\n\n    This class implements a model merging technique that leverages Singular Value\n    Decomposition (SVD) to identify and combine the most important directions in the task vector\n    space. The algorithm is particularly effective for merging multiple models fine-tuned on\n    different tasks while preserving their essential capabilities.\n\n    Key Concepts:\n    - Task Vector: The difference between a fine-tuned model and its pretrained base model,\n      representing the knowledge gained during fine-tuning for a specific task.\n    - Singular Value Decomposition: A matrix factorization technique used to find the principal\n      components (most important directions) in the space of task vectors.\n    - Model Merging: The process of combining multiple models into a single unified model that\n      retains capabilities from all constituent models.\n\n    Algorithm Steps:\n    1. Extract task vectors from all fine-tuned models by subtracting the pretrained model\n    2. Apply SVD to the matrix of task vectors to find principal components\n    3. Reconstruct task vectors using only the most significant singular vectors\n    4. Merge the reconstructed task vectors (either individually scaled or as a sum)\n    5. Add the final merged task vector to the pretrained model to create the unified model\n\n    see `docs/algorithms/task_singular_vector.md` for comprehensive algorithmic details.\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: Optional[Union[float, Iterable[float]]] = None,\n        exclude_keys: Optional[List[str]] = None,\n        return_single_task_models: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the Task Singular Vector Merging algorithm.\n\n        Args:\n            alpha (Union[float, Iterable[float]], optional): Scaling factor(s) for task vectors.\n                This parameter controls the strength of the task-specific adaptations in the final model.\n\n                - If a single float: Applied to the final merged task vector after SVD reconstruction.\n                  This uniformly scales the entire merged adaptation.\n\n                - If an iterable of floats: Applied to individual task vectors before SVD and merging.\n                  Must have the same length as the number of models in the modelpool.\n                  This allows for task-specific weighting (e.g., giving more importance to certain tasks).\n\n                - If None: No scaling is applied (equivalent to alpha=1.0).\n\n                Example: alpha=[0.8, 1.2, 0.5] would apply different weights to three different task vectors.\n\n            exclude_keys (Optional[List[str]], optional): List of parameter names to exclude from TSVM.\n                These parameters will not participate in the SVD computation and merging process.\n                Useful for excluding certain layers (e.g., task-specific heads, normalization layers)\n                that should not be merged across tasks. Defaults to an empty list.\n\n                Example: exclude_keys=['classifier.weight', 'classifier.bias'] to skip classification heads.\n\n            return_single_task_models (bool, optional): Whether to return individual transformed models.\n\n                - If True: Returns a dictionary containing both individual models with their transformed\n                  task vectors applied AND the final merged model. The dictionary has the structure:\n\n                  &gt;&gt;&gt; {'model_name_1': transformed_model_1, ..., 'merged': final_merged_model}\n\n                - If False: Returns only the final merged model.\n\n                This is useful for analysis or when you need access to intermediate results.\n                Defaults to False.\n\n            **kwargs: Additional arguments passed to the parent BaseAlgorithm class.\n\n        Note:\n            The choice between single alpha vs. list of alphas affects the merging strategy:\n            - Single alpha: SVD is applied first, then the result is scaled\n            - List of alphas: Individual task vectors are scaled first, then SVD is applied\n        \"\"\"\n        self.alpha = alpha\n        self.exclude_keys = exclude_keys if exclude_keys is not None else []\n        self.return_single_task_models = return_single_task_models\n        super().__init__(**kwargs)\n\n    def load_pretrained_model_and_task_vectors(self, modelpool: fb.BaseModelPool):\n        \"\"\"\n        Load the pretrained base model and compute task vectors from all fine-tuned models.\n\n        This method performs the initial step of the TSVM algorithm by:\n        1. Loading the original pretrained model (before any task-specific fine-tuning)\n        2. For each fine-tuned model in the pool:\n           - Load the fine-tuned model\n           - Compute the task vector (fine-tuned params - pretrained params)\n           - Optionally apply individual scaling if alpha is provided as a list\n\n        Task vectors represent the knowledge gained during fine-tuning and are the core\n        data structure that TSVM operates on.\n\n        Args:\n            modelpool (fb.BaseModelPool): Pool containing the pretrained model and all\n                fine-tuned models to be merged.\n\n        Returns:\n            tuple: A tuple containing:\n                - pretrained_model: The original pretrained model (torch.nn.Module)\n                - task_vectors: List of task vectors (List[StateDictType]), where each\n                  task vector is a state dictionary representing the parameter differences\n                  for one specific task\n        \"\"\"\n        # Load the original pretrained model that serves as the base for all fine-tuned variants\n        pretrained_model = modelpool.load_pretrained_model()\n\n        # Initialize list to store computed task vectors\n        task_vectors = []\n\n        # Process each fine-tuned model in the modelpool\n        for model_idx, model_name in enumerate(modelpool.model_names):\n            # Load the current fine-tuned model\n            finetuned_model = modelpool.load_model(model_name)\n\n            # Compute task vector: difference between fine-tuned and pretrained parameters\n            # This captures the task-specific adaptations learned during fine-tuning\n            task_vector = state_dict_sub(\n                finetuned_model.state_dict(), pretrained_model.state_dict()\n            )\n            task_vectors.append(task_vector)\n\n            # Apply individual scaling to task vectors if alpha is provided as a list\n            # This allows for task-specific weighting before the SVD computation\n            if self.alpha is not None and isinstance(self.alpha, Iterable):\n                # Ensure the number of alpha values matches the number of models\n                assert len(self.alpha) == len(\n                    modelpool.model_names\n                ), f\"Alpha list length ({len(self.alpha)}) must match number of models ({len(modelpool.model_names)})\"\n\n                # Scale the current task vector by its corresponding alpha value\n                task_vectors[-1] = state_dict_mul(\n                    state_dict=task_vectors[-1], scalar=self.alpha[model_idx]\n                )\n\n        return pretrained_model, task_vectors\n\n    def run(self, modelpool: fb.BaseModelPool):\n        \"\"\"\n        Execute the complete Task Singular Vector Merging algorithm.\n\n        This is the main entry point that orchestrates the entire TSVM process:\n\n        The algorithm leverages the mathematical insight that task vectors often lie in a\n        lower-dimensional subspace, and SVD helps identify the most important directions\n        in this subspace while filtering out noise and interference.\n\n        Args:\n            modelpool (fb.BaseModelPool): Pool of models to merge, including:\n                - The pretrained base model\n                - Multiple fine-tuned models (one per task)\n                All models must have compatible architectures.\n\n        Returns:\n            Union[torch.nn.Module, Dict[str, torch.nn.Module]]:\n                - If return_single_task_models=False: Returns the merged model\n                - If return_single_task_models=True: Returns a dictionary with:\n                  * Individual transformed models keyed by their original names\n                  * Final merged model under the key 'merged'\n\n        Raises:\n            AssertionError: If alpha list length doesn't match the number of models\n        \"\"\"\n        # Determine the compute device for SVD operations (GPU if available for faster computation)\n        accelerator = self.fabric.device\n\n        # Phase 1: Load pretrained model and compute task vectors from all fine-tuned models\n        pretrained_model, task_vectors = self.load_pretrained_model_and_task_vectors(\n            modelpool\n        )\n\n        # Phase 2: Apply SVD-based merging to the task vectors\n        # This is the core of the TSVM algorithm where:\n        # - Task vectors are organized into a matrix\n        # - SVD finds the principal components (most important directions)\n        # - Task vectors are reconstructed using only the most significant components\n        # - The reconstructed vectors are merged (summed) to create a unified task vector\n        new_merged_tv = TSVM_utils.compute_and_sum_svd_mem_reduction(\n            task_vectors,\n            exclude_keys=self.exclude_keys,  # Skip certain parameters from SVD\n            accelerator=accelerator,  # Use GPU if available\n            return_single_task_models=self.return_single_task_models,\n        )\n\n        # Handle the case where individual transformed task vectors are also returned\n        if self.return_single_task_models:\n            new_merged_tv, single_task_models = new_merged_tv\n\n        # Phase 3: Apply global scaling to the merged task vector (if alpha is a single value)\n        # This is different from individual scaling applied earlier - here we scale the\n        # final merged result, which affects the overall strength of all merged adaptations\n        if self.alpha is not None and isinstance(self.alpha, (float, int)):\n            print(f\"Scaling new merged task vector by alpha: {self.alpha}\")\n            new_merged_tv = state_dict_mul(state_dict=new_merged_tv, scalar=self.alpha)\n\n        # Phase 4: Prepare individual transformed models if requested\n        if self.return_single_task_models:\n            models = {}\n            # Create individual models by adding each transformed task vector to the pretrained base\n            for model_idx, model_name in enumerate(modelpool.model_names):\n                # Create a deep copy to avoid modifying the original pretrained model\n                model = deepcopy(pretrained_model)\n                # Apply the transformed task vector to get the individual model\n                model.load_state_dict(\n                    state_dict_add(model.state_dict(), single_task_models[model_idx])\n                )\n                models[model_name] = model\n\n        # Phase 5: Create the final merged model by adding the merged task vector to pretrained model\n        # This produces a single model that combines capabilities from all input models\n        pretrained_model.load_state_dict(\n            state_dict_add(new_merged_tv, pretrained_model.state_dict())\n        )\n\n        # Phase 6: Return results based on the requested output format\n        if self.return_single_task_models:\n            # Include the final merged model in the dictionary of results\n            models[\"merged\"] = pretrained_model\n            return models\n        else:\n            # Return only the merged model\n            return pretrained_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.TaskSingularVectorMerging.__init__","title":"<code>__init__(alpha=None, exclude_keys=None, return_single_task_models=False, **kwargs)</code>","text":"<p>Initialize the Task Singular Vector Merging algorithm.</p> <p>Parameters:</p> <ul> <li> <code>alpha</code>               (<code>Union[float, Iterable[float]]</code>, default:                   <code>None</code> )           \u2013            <p>Scaling factor(s) for task vectors. This parameter controls the strength of the task-specific adaptations in the final model.</p> <ul> <li> <p>If a single float: Applied to the final merged task vector after SVD reconstruction.   This uniformly scales the entire merged adaptation.</p> </li> <li> <p>If an iterable of floats: Applied to individual task vectors before SVD and merging.   Must have the same length as the number of models in the modelpool.   This allows for task-specific weighting (e.g., giving more importance to certain tasks).</p> </li> <li> <p>If None: No scaling is applied (equivalent to alpha=1.0).</p> </li> </ul> <p>Example: alpha=[0.8, 1.2, 0.5] would apply different weights to three different task vectors.</p> </li> <li> <code>exclude_keys</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of parameter names to exclude from TSVM. These parameters will not participate in the SVD computation and merging process. Useful for excluding certain layers (e.g., task-specific heads, normalization layers) that should not be merged across tasks. Defaults to an empty list.</p> <p>Example: exclude_keys=['classifier.weight', 'classifier.bias'] to skip classification heads.</p> </li> <li> <code>return_single_task_models</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return individual transformed models.</p> <ul> <li>If True: Returns a dictionary containing both individual models with their transformed   task vectors applied AND the final merged model. The dictionary has the structure:</li> </ul> <p>{'model_name_1': transformed_model_1, ..., 'merged': final_merged_model}</p> <ul> <li>If False: Returns only the final merged model.</li> </ul> <p>This is useful for analysis or when you need access to intermediate results. Defaults to False.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to the parent BaseAlgorithm class.</p> </li> </ul> Note <p>The choice between single alpha vs. list of alphas affects the merging strategy: - Single alpha: SVD is applied first, then the result is scaled - List of alphas: Individual task vectors are scaled first, then SVD is applied</p> Source code in <code>fusion_bench/method/task_singular_vector/TSVM.py</code> <pre><code>def __init__(\n    self,\n    alpha: Optional[Union[float, Iterable[float]]] = None,\n    exclude_keys: Optional[List[str]] = None,\n    return_single_task_models: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the Task Singular Vector Merging algorithm.\n\n    Args:\n        alpha (Union[float, Iterable[float]], optional): Scaling factor(s) for task vectors.\n            This parameter controls the strength of the task-specific adaptations in the final model.\n\n            - If a single float: Applied to the final merged task vector after SVD reconstruction.\n              This uniformly scales the entire merged adaptation.\n\n            - If an iterable of floats: Applied to individual task vectors before SVD and merging.\n              Must have the same length as the number of models in the modelpool.\n              This allows for task-specific weighting (e.g., giving more importance to certain tasks).\n\n            - If None: No scaling is applied (equivalent to alpha=1.0).\n\n            Example: alpha=[0.8, 1.2, 0.5] would apply different weights to three different task vectors.\n\n        exclude_keys (Optional[List[str]], optional): List of parameter names to exclude from TSVM.\n            These parameters will not participate in the SVD computation and merging process.\n            Useful for excluding certain layers (e.g., task-specific heads, normalization layers)\n            that should not be merged across tasks. Defaults to an empty list.\n\n            Example: exclude_keys=['classifier.weight', 'classifier.bias'] to skip classification heads.\n\n        return_single_task_models (bool, optional): Whether to return individual transformed models.\n\n            - If True: Returns a dictionary containing both individual models with their transformed\n              task vectors applied AND the final merged model. The dictionary has the structure:\n\n              &gt;&gt;&gt; {'model_name_1': transformed_model_1, ..., 'merged': final_merged_model}\n\n            - If False: Returns only the final merged model.\n\n            This is useful for analysis or when you need access to intermediate results.\n            Defaults to False.\n\n        **kwargs: Additional arguments passed to the parent BaseAlgorithm class.\n\n    Note:\n        The choice between single alpha vs. list of alphas affects the merging strategy:\n        - Single alpha: SVD is applied first, then the result is scaled\n        - List of alphas: Individual task vectors are scaled first, then SVD is applied\n    \"\"\"\n    self.alpha = alpha\n    self.exclude_keys = exclude_keys if exclude_keys is not None else []\n    self.return_single_task_models = return_single_task_models\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.TaskSingularVectorMerging.load_pretrained_model_and_task_vectors","title":"<code>load_pretrained_model_and_task_vectors(modelpool)</code>","text":"<p>Load the pretrained base model and compute task vectors from all fine-tuned models.</p> <p>This method performs the initial step of the TSVM algorithm by: 1. Loading the original pretrained model (before any task-specific fine-tuning) 2. For each fine-tuned model in the pool:    - Load the fine-tuned model    - Compute the task vector (fine-tuned params - pretrained params)    - Optionally apply individual scaling if alpha is provided as a list</p> <p>Task vectors represent the knowledge gained during fine-tuning and are the core data structure that TSVM operates on.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool</code>)           \u2013            <p>Pool containing the pretrained model and all fine-tuned models to be merged.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code>          \u2013            <p>A tuple containing: - pretrained_model: The original pretrained model (torch.nn.Module) - task_vectors: List of task vectors (List[StateDictType]), where each   task vector is a state dictionary representing the parameter differences   for one specific task</p> </li> </ul> Source code in <code>fusion_bench/method/task_singular_vector/TSVM.py</code> <pre><code>def load_pretrained_model_and_task_vectors(self, modelpool: fb.BaseModelPool):\n    \"\"\"\n    Load the pretrained base model and compute task vectors from all fine-tuned models.\n\n    This method performs the initial step of the TSVM algorithm by:\n    1. Loading the original pretrained model (before any task-specific fine-tuning)\n    2. For each fine-tuned model in the pool:\n       - Load the fine-tuned model\n       - Compute the task vector (fine-tuned params - pretrained params)\n       - Optionally apply individual scaling if alpha is provided as a list\n\n    Task vectors represent the knowledge gained during fine-tuning and are the core\n    data structure that TSVM operates on.\n\n    Args:\n        modelpool (fb.BaseModelPool): Pool containing the pretrained model and all\n            fine-tuned models to be merged.\n\n    Returns:\n        tuple: A tuple containing:\n            - pretrained_model: The original pretrained model (torch.nn.Module)\n            - task_vectors: List of task vectors (List[StateDictType]), where each\n              task vector is a state dictionary representing the parameter differences\n              for one specific task\n    \"\"\"\n    # Load the original pretrained model that serves as the base for all fine-tuned variants\n    pretrained_model = modelpool.load_pretrained_model()\n\n    # Initialize list to store computed task vectors\n    task_vectors = []\n\n    # Process each fine-tuned model in the modelpool\n    for model_idx, model_name in enumerate(modelpool.model_names):\n        # Load the current fine-tuned model\n        finetuned_model = modelpool.load_model(model_name)\n\n        # Compute task vector: difference between fine-tuned and pretrained parameters\n        # This captures the task-specific adaptations learned during fine-tuning\n        task_vector = state_dict_sub(\n            finetuned_model.state_dict(), pretrained_model.state_dict()\n        )\n        task_vectors.append(task_vector)\n\n        # Apply individual scaling to task vectors if alpha is provided as a list\n        # This allows for task-specific weighting before the SVD computation\n        if self.alpha is not None and isinstance(self.alpha, Iterable):\n            # Ensure the number of alpha values matches the number of models\n            assert len(self.alpha) == len(\n                modelpool.model_names\n            ), f\"Alpha list length ({len(self.alpha)}) must match number of models ({len(modelpool.model_names)})\"\n\n            # Scale the current task vector by its corresponding alpha value\n            task_vectors[-1] = state_dict_mul(\n                state_dict=task_vectors[-1], scalar=self.alpha[model_idx]\n            )\n\n    return pretrained_model, task_vectors\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.TaskSingularVectorMerging.run","title":"<code>run(modelpool)</code>","text":"<p>Execute the complete Task Singular Vector Merging algorithm.</p> <p>This is the main entry point that orchestrates the entire TSVM process:</p> <p>The algorithm leverages the mathematical insight that task vectors often lie in a lower-dimensional subspace, and SVD helps identify the most important directions in this subspace while filtering out noise and interference.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool</code>)           \u2013            <p>Pool of models to merge, including: - The pretrained base model - Multiple fine-tuned models (one per task) All models must have compatible architectures.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Union[torch.nn.Module, Dict[str, torch.nn.Module]]: - If return_single_task_models=False: Returns the merged model - If return_single_task_models=True: Returns a dictionary with:   * Individual transformed models keyed by their original names   * Final merged model under the key 'merged'</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AssertionError</code>             \u2013            <p>If alpha list length doesn't match the number of models</p> </li> </ul> Source code in <code>fusion_bench/method/task_singular_vector/TSVM.py</code> <pre><code>def run(self, modelpool: fb.BaseModelPool):\n    \"\"\"\n    Execute the complete Task Singular Vector Merging algorithm.\n\n    This is the main entry point that orchestrates the entire TSVM process:\n\n    The algorithm leverages the mathematical insight that task vectors often lie in a\n    lower-dimensional subspace, and SVD helps identify the most important directions\n    in this subspace while filtering out noise and interference.\n\n    Args:\n        modelpool (fb.BaseModelPool): Pool of models to merge, including:\n            - The pretrained base model\n            - Multiple fine-tuned models (one per task)\n            All models must have compatible architectures.\n\n    Returns:\n        Union[torch.nn.Module, Dict[str, torch.nn.Module]]:\n            - If return_single_task_models=False: Returns the merged model\n            - If return_single_task_models=True: Returns a dictionary with:\n              * Individual transformed models keyed by their original names\n              * Final merged model under the key 'merged'\n\n    Raises:\n        AssertionError: If alpha list length doesn't match the number of models\n    \"\"\"\n    # Determine the compute device for SVD operations (GPU if available for faster computation)\n    accelerator = self.fabric.device\n\n    # Phase 1: Load pretrained model and compute task vectors from all fine-tuned models\n    pretrained_model, task_vectors = self.load_pretrained_model_and_task_vectors(\n        modelpool\n    )\n\n    # Phase 2: Apply SVD-based merging to the task vectors\n    # This is the core of the TSVM algorithm where:\n    # - Task vectors are organized into a matrix\n    # - SVD finds the principal components (most important directions)\n    # - Task vectors are reconstructed using only the most significant components\n    # - The reconstructed vectors are merged (summed) to create a unified task vector\n    new_merged_tv = TSVM_utils.compute_and_sum_svd_mem_reduction(\n        task_vectors,\n        exclude_keys=self.exclude_keys,  # Skip certain parameters from SVD\n        accelerator=accelerator,  # Use GPU if available\n        return_single_task_models=self.return_single_task_models,\n    )\n\n    # Handle the case where individual transformed task vectors are also returned\n    if self.return_single_task_models:\n        new_merged_tv, single_task_models = new_merged_tv\n\n    # Phase 3: Apply global scaling to the merged task vector (if alpha is a single value)\n    # This is different from individual scaling applied earlier - here we scale the\n    # final merged result, which affects the overall strength of all merged adaptations\n    if self.alpha is not None and isinstance(self.alpha, (float, int)):\n        print(f\"Scaling new merged task vector by alpha: {self.alpha}\")\n        new_merged_tv = state_dict_mul(state_dict=new_merged_tv, scalar=self.alpha)\n\n    # Phase 4: Prepare individual transformed models if requested\n    if self.return_single_task_models:\n        models = {}\n        # Create individual models by adding each transformed task vector to the pretrained base\n        for model_idx, model_name in enumerate(modelpool.model_names):\n            # Create a deep copy to avoid modifying the original pretrained model\n            model = deepcopy(pretrained_model)\n            # Apply the transformed task vector to get the individual model\n            model.load_state_dict(\n                state_dict_add(model.state_dict(), single_task_models[model_idx])\n            )\n            models[model_name] = model\n\n    # Phase 5: Create the final merged model by adding the merged task vector to pretrained model\n    # This produces a single model that combines capabilities from all input models\n    pretrained_model.load_state_dict(\n        state_dict_add(new_merged_tv, pretrained_model.state_dict())\n    )\n\n    # Phase 6: Return results based on the requested output format\n    if self.return_single_task_models:\n        # Include the final merged model in the dictionary of results\n        models[\"merged\"] = pretrained_model\n        return models\n    else:\n        # Return only the merged model\n        return pretrained_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#isotropic-merging","title":"Isotropic Merging","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.ISO_C_Merge","title":"<code>ISO_C_Merge = IsotropicMergingInCommonSubspace</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.ISO_CTS_Merge","title":"<code>ISO_CTS_Merge = IsotropicMergingInCommonAndTaskSubspace</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.IsotropicMergingInCommonSubspace","title":"<code>IsotropicMergingInCommonSubspace</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>LightningFabricMixin</code></p> <p>Isotropic Merging in Common Subspace (Iso-C)</p> Source code in <code>fusion_bench/method/isotropic_merging/iso.py</code> <pre><code>class IsotropicMergingInCommonSubspace(BaseAlgorithm, LightningFabricMixin):\n    \"\"\"\n    Isotropic Merging in Common Subspace (Iso-C)\n    \"\"\"\n\n    def __init__(\n        self,\n        scaling_factor: float,\n        exclude_keys: List[str] = None,\n    ):\n        self.scaling_factor = scaling_factor\n        self.exclude_keys = exclude_keys\n        super().__init__()\n\n    def run(self, modelpool: BaseModelPool):\n        # load the pretrained model and the task vectors of all the finetuned models\n        with torch.no_grad():\n            pretrained_model = modelpool.load_pretrained_model()\n            task_vectors = []\n            for model_name in modelpool.model_names:\n                finetuned_model = modelpool.load_model(model_name)\n                task_vectors.append(\n                    state_dict_sub(\n                        finetuned_model.state_dict(), pretrained_model.state_dict()\n                    )\n                )\n                del finetuned_model  # free memory\n            check_parameterNamesMatch(task_vectors)\n\n        # compute the merged task vector\n        merged_tv = iso_c(\n            task_vectors,\n            accelerator=self.fabric.device,\n            exclude_keys=self.exclude_keys,\n        )\n\n        # merged_parameters = pretrained_parameters + scaling_factor * merged_task_vector\n        pretrained_model.load_state_dict(\n            state_dict_add(\n                pretrained_model.state_dict(),\n                state_dict_mul(merged_tv, self.scaling_factor),\n            )\n        )\n\n        return pretrained_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.IsotropicMergingInCommonAndTaskSubspace","title":"<code>IsotropicMergingInCommonAndTaskSubspace</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>LightningFabricMixin</code></p> <p>Isotropic Merging in Common and Task-Specific Subspaces (Iso-CTS)</p> Source code in <code>fusion_bench/method/isotropic_merging/iso.py</code> <pre><code>class IsotropicMergingInCommonAndTaskSubspace(BaseAlgorithm, LightningFabricMixin):\n    \"\"\"\n    Isotropic Merging in Common and Task-Specific Subspaces (Iso-CTS)\n    \"\"\"\n\n    def __init__(\n        self,\n        scaling_factor: float,\n        common_space_fraction: float,\n        exclude_keys: List[str] = None,\n    ):\n        self.common_space_fraction = common_space_fraction\n        self.scaling_factor = scaling_factor\n        self.exclude_keys = exclude_keys\n        super().__init__()\n\n    def run(self, modelpool: BaseModelPool):\n        # load the pretrained model and the task vectors of all the finetuned models\n        with torch.no_grad():\n            pretrained_model = modelpool.load_pretrained_model()\n            task_vectors = []\n            for model_name in modelpool.model_names:\n                finetuned_model = modelpool.load_model(model_name)\n                task_vectors.append(\n                    state_dict_sub(\n                        finetuned_model.state_dict(), pretrained_model.state_dict()\n                    )\n                )\n                del finetuned_model  # free memory\n            check_parameterNamesMatch(task_vectors)\n\n        # compute the merged task vector\n        merged_tv = iso_cts(\n            task_vectors,\n            common_space_fraction=self.common_space_fraction,\n            accelerator=self.fabric.device,\n            exclude_keys=self.exclude_keys,\n        )\n\n        # merged_parameters = pretrained_parameters + scaling_factor * merged_task_vector\n        pretrained_model.load_state_dict(\n            state_dict_add(\n                pretrained_model.state_dict(),\n                state_dict_mul(merged_tv, self.scaling_factor),\n            )\n        )\n\n        return pretrained_model\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#distributed-model-merging","title":"Distributed Model Merging","text":""},{"location":"api/fusion_bench.method/merging/#gossip","title":"Gossip","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.CLIPTaskWiseGossipAlgorithm","title":"<code>CLIPTaskWiseGossipAlgorithm</code>","text":"<p>               Bases: <code>TaskWiseGossipAlgorithm</code></p> <p>A class for task-wise adaptive merging of CLIP models.</p> <p>This class extends the TaskWiseGossipAlgorithm to provide specific functionality for CLIP models, including loading datasets, constructing zero-shot classification heads, and computing logits.</p> <p>Attributes:</p> <ul> <li> <code>modelpool</code>               (<code>CLIPVisionModelPool</code>)           \u2013            <p>The model pool containing CLIP models.</p> </li> <li> <code>_clip_processor</code>               (<code>CLIPProcessor</code>)           \u2013            <p>The CLIP processor for preparing inputs.</p> </li> <li> <code>zeroshot_weights</code>               (<code>dict</code>)           \u2013            <p>A dictionary to store zero-shot weights for each task.</p> </li> </ul> Source code in <code>fusion_bench/method/gossip/clip_task_wise_gossip.py</code> <pre><code>class CLIPTaskWiseGossipAlgorithm(TaskWiseGossipAlgorithm):\n    \"\"\"\n    A class for task-wise adaptive merging of CLIP models.\n\n    This class extends the TaskWiseGossipAlgorithm to provide specific\n    functionality for CLIP models, including loading datasets, constructing\n    zero-shot classification heads, and computing logits.\n\n    Attributes:\n        modelpool (CLIPVisionModelPool): The model pool containing CLIP models.\n        _clip_processor (CLIPProcessor): The CLIP processor for preparing inputs.\n        zeroshot_weights (dict): A dictionary to store zero-shot weights for each task.\n    \"\"\"\n\n    modelpool: CLIPVisionModelPool = None\n    _clip_processor: CLIPProcessor = None\n    zeroshot_weights = {}\n\n    def __init__(self, algorithm_config: DictConfig):\n        super().__init__(algorithm_config)\n\n    @functools.cache\n    def get_test_dataset(self, task: str):\n        \"\"\"\n        Load the test dataset for the task.\n        This method is cached, so the dataset is loaded only once.\n\n        Args:\n            task (str): The name of the task.\n\n        Returns:\n            CLIPDataset: The test dataset for the task.\n        \"\"\"\n        log.info(f\"Loading test dataset: {task}\")\n        dataset = self.modelpool.load_test_dataset(task)\n        dataset = CLIPDataset(dataset, self._clip_processor)\n        return dataset\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(self, task: str):\n        \"\"\"\n        Get an iterator over the shuffled test DataLoader for the task.\n\n        Args:\n            task (str): The name of the task.\n\n        Returns:\n            iterator: An iterator over the shuffled test DataLoader.\n        \"\"\"\n        loader = DataLoader(\n            self.get_test_dataset(task),\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            num_workers=self.config.num_workers,\n            pin_memory=True,\n        )\n        if self._fabric is not None:\n            loader = self._fabric.setup_dataloaders(loader)\n        return iter(InfiniteDataLoader(loader))\n\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Prepare for test-time adaptation.\n\n        This method loads the CLIP processor and constructs the zero-shot\n        classification head for each task.\n        \"\"\"\n        if self._clip_processor is not None and self.zeroshot_weights is not None:\n            return  # this can be reused in Gossip\n\n        clip_model_config = self.modelpool.get_model_config(\"_pretrained_\")\n        pretrained_path = (\n            clip_model_config.pretrained_model_name_or_path\n            if hasattr(clip_model_config, \"pretrained_model_name_or_path\")\n            else clip_model_config.path\n        )\n\n        with timeit_context(\"Loading CLIP processor and pretrained CLIP model.\"):\n            self._clip_processor = CLIPProcessor.from_pretrained(pretrained_path)\n            clip_model: CLIPModel = CLIPModel.from_pretrained(pretrained_path)\n\n            clip_classifier = HFCLIPClassifier(clip_model, self._clip_processor)\n            self.visual_projection = clip_model.visual_projection.requires_grad_(False)\n            self.logit_scale_exp = clip_model.logit_scale.exp()\n            if self._fabric is not None:\n                self.visual_projection = self._fabric.to_device(self.visual_projection)\n                self.logit_scale_exp = self._fabric.to_device(self.logit_scale_exp)\n\n        for task in self.modelpool.model_names:\n            cache_file = os.path.join(\n                self.config.cache_dir,\n                f\"{os.path.basename(pretrained_path)}_{task}_zeroshot_weights.pt\",\n            )\n            if os.path.exists(cache_file):\n                log.info(f\"Loading cached zeroshot weights for task: {task}\")\n                zeroshot_weights = torch.load(cache_file, map_location=\"cpu\")\n            else:\n                log.info(f\"Construct zero shot classification head for task: {task}\")\n                classnames, templates = get_classnames_and_templates(task)\n                clip_classifier.set_classification_task(classnames, templates)\n                zeroshot_weights = clip_classifier.zeroshot_weights\n                log.info(f\"save zeroshot weights to {cache_file}\")\n                torch.save(zeroshot_weights, cache_file)\n            self.zeroshot_weights[task] = zeroshot_weights\n            if self._fabric is not None:\n                self.zeroshot_weights[task] = self._fabric.to_device(\n                    self.zeroshot_weights[task]\n                )\n\n    def compute_logits(self, module, batch, task: str) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for the given batch and task.\n\n        This method computes the image embeddings, normalizes them, and calculates\n        the cosine similarity with the text embeddings to produce classification logits.\n\n        Args:\n            module (nn.Module): The model module.\n            batch (tuple): A batch of input data.\n            task (str): The name of the task.\n\n        Returns:\n            Tensor: The classification logits for the batch.\n        \"\"\"\n        images, _ = batch\n        text_embeds = self.zeroshot_weights[task]\n\n        image_embeds = module(images)[1]\n        image_embeds = self.visual_projection(image_embeds)\n\n        # normalize embeddings\n        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n        # cosine similarity\n        logits_per_text = (\n            torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n        )\n        logits_per_image = logits_per_text.t()\n\n        return logits_per_image\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.CLIPTaskWiseGossipAlgorithm.compute_logits","title":"<code>compute_logits(module, batch, task)</code>","text":"<p>Compute the logits for the given batch and task.</p> <p>This method computes the image embeddings, normalizes them, and calculates the cosine similarity with the text embeddings to produce classification logits.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The model module.</p> </li> <li> <code>batch</code>               (<code>tuple</code>)           \u2013            <p>A batch of input data.</p> </li> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The classification logits for the batch.</p> </li> </ul> Source code in <code>fusion_bench/method/gossip/clip_task_wise_gossip.py</code> <pre><code>def compute_logits(self, module, batch, task: str) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for the given batch and task.\n\n    This method computes the image embeddings, normalizes them, and calculates\n    the cosine similarity with the text embeddings to produce classification logits.\n\n    Args:\n        module (nn.Module): The model module.\n        batch (tuple): A batch of input data.\n        task (str): The name of the task.\n\n    Returns:\n        Tensor: The classification logits for the batch.\n    \"\"\"\n    images, _ = batch\n    text_embeds = self.zeroshot_weights[task]\n\n    image_embeds = module(images)[1]\n    image_embeds = self.visual_projection(image_embeds)\n\n    # normalize embeddings\n    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n    # cosine similarity\n    logits_per_text = (\n        torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n    )\n    logits_per_image = logits_per_text.t()\n\n    return logits_per_image\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.CLIPTaskWiseGossipAlgorithm.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(task)</code>  <code>cached</code>","text":"<p>Get an iterator over the shuffled test DataLoader for the task.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>iterator</code>          \u2013            <p>An iterator over the shuffled test DataLoader.</p> </li> </ul> Source code in <code>fusion_bench/method/gossip/clip_task_wise_gossip.py</code> <pre><code>@functools.cache\ndef get_shuffled_test_loader_iter(self, task: str):\n    \"\"\"\n    Get an iterator over the shuffled test DataLoader for the task.\n\n    Args:\n        task (str): The name of the task.\n\n    Returns:\n        iterator: An iterator over the shuffled test DataLoader.\n    \"\"\"\n    loader = DataLoader(\n        self.get_test_dataset(task),\n        batch_size=self.config.batch_size,\n        shuffle=True,\n        num_workers=self.config.num_workers,\n        pin_memory=True,\n    )\n    if self._fabric is not None:\n        loader = self._fabric.setup_dataloaders(loader)\n    return iter(InfiniteDataLoader(loader))\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.CLIPTaskWiseGossipAlgorithm.get_test_dataset","title":"<code>get_test_dataset(task)</code>  <code>cached</code>","text":"<p>Load the test dataset for the task. This method is cached, so the dataset is loaded only once.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>CLIPDataset</code>          \u2013            <p>The test dataset for the task.</p> </li> </ul> Source code in <code>fusion_bench/method/gossip/clip_task_wise_gossip.py</code> <pre><code>@functools.cache\ndef get_test_dataset(self, task: str):\n    \"\"\"\n    Load the test dataset for the task.\n    This method is cached, so the dataset is loaded only once.\n\n    Args:\n        task (str): The name of the task.\n\n    Returns:\n        CLIPDataset: The test dataset for the task.\n    \"\"\"\n    log.info(f\"Loading test dataset: {task}\")\n    dataset = self.modelpool.load_test_dataset(task)\n    dataset = CLIPDataset(dataset, self._clip_processor)\n    return dataset\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.CLIPTaskWiseGossipAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Prepare for test-time adaptation.</p> <p>This method loads the CLIP processor and constructs the zero-shot classification head for each task.</p> Source code in <code>fusion_bench/method/gossip/clip_task_wise_gossip.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Prepare for test-time adaptation.\n\n    This method loads the CLIP processor and constructs the zero-shot\n    classification head for each task.\n    \"\"\"\n    if self._clip_processor is not None and self.zeroshot_weights is not None:\n        return  # this can be reused in Gossip\n\n    clip_model_config = self.modelpool.get_model_config(\"_pretrained_\")\n    pretrained_path = (\n        clip_model_config.pretrained_model_name_or_path\n        if hasattr(clip_model_config, \"pretrained_model_name_or_path\")\n        else clip_model_config.path\n    )\n\n    with timeit_context(\"Loading CLIP processor and pretrained CLIP model.\"):\n        self._clip_processor = CLIPProcessor.from_pretrained(pretrained_path)\n        clip_model: CLIPModel = CLIPModel.from_pretrained(pretrained_path)\n\n        clip_classifier = HFCLIPClassifier(clip_model, self._clip_processor)\n        self.visual_projection = clip_model.visual_projection.requires_grad_(False)\n        self.logit_scale_exp = clip_model.logit_scale.exp()\n        if self._fabric is not None:\n            self.visual_projection = self._fabric.to_device(self.visual_projection)\n            self.logit_scale_exp = self._fabric.to_device(self.logit_scale_exp)\n\n    for task in self.modelpool.model_names:\n        cache_file = os.path.join(\n            self.config.cache_dir,\n            f\"{os.path.basename(pretrained_path)}_{task}_zeroshot_weights.pt\",\n        )\n        if os.path.exists(cache_file):\n            log.info(f\"Loading cached zeroshot weights for task: {task}\")\n            zeroshot_weights = torch.load(cache_file, map_location=\"cpu\")\n        else:\n            log.info(f\"Construct zero shot classification head for task: {task}\")\n            classnames, templates = get_classnames_and_templates(task)\n            clip_classifier.set_classification_task(classnames, templates)\n            zeroshot_weights = clip_classifier.zeroshot_weights\n            log.info(f\"save zeroshot weights to {cache_file}\")\n            torch.save(zeroshot_weights, cache_file)\n        self.zeroshot_weights[task] = zeroshot_weights\n        if self._fabric is not None:\n            self.zeroshot_weights[task] = self._fabric.to_device(\n                self.zeroshot_weights[task]\n            )\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.CLIPLayerWiseGossipAlgorithm","title":"<code>CLIPLayerWiseGossipAlgorithm</code>","text":"<p>               Bases: <code>CLIPClassificationMixin</code>, <code>LayerWiseGossipAlgorithm</code></p> Source code in <code>fusion_bench/method/gossip/clip_layer_wise_gossip.py</code> <pre><code>class CLIPLayerWiseGossipAlgorithm(\n    CLIPClassificationMixin,\n    LayerWiseGossipAlgorithm,\n):\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Here we load the CLIP processor and construct the zero-shot classification head for each task.\n        \"\"\"\n        if self.whether_setup_zero_shot_classification_head == False:\n            self.setup_zero_shot_classification_head()\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(self, task: str):\n        return super().get_shuffled_test_loader_iter(\n            task,\n            batch_size=self.config.batch_size,\n            num_workers=self.config.num_workers,\n        )\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.CLIPLayerWiseGossipAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Here we load the CLIP processor and construct the zero-shot classification head for each task.</p> Source code in <code>fusion_bench/method/gossip/clip_layer_wise_gossip.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Here we load the CLIP processor and construct the zero-shot classification head for each task.\n    \"\"\"\n    if self.whether_setup_zero_shot_classification_head == False:\n        self.setup_zero_shot_classification_head()\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseGossipAlgorithm","title":"<code>FlanT5LayerWiseGossipAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>LightningFabricMixin</code>, <code>SimpleProfilerMixin</code></p> Source code in <code>fusion_bench/method/gossip/flan_t5_layer_wise_gossip.py</code> <pre><code>class FlanT5LayerWiseGossipAlgorithm(\n    BaseAlgorithm,\n    LightningFabricMixin,\n    SimpleProfilerMixin,\n):\n\n    def __init__(\n        self,\n        optimizer: DictConfig,\n        dataloader_kwargs: DictConfig,\n        init_values: float,\n        max_steps: int,\n        merging_weights_load_path: Optional[Union[str, Path]] = None,\n        merging_weights_save_path: Optional[Union[str, Path]] = None,\n        clamp_weights: bool = False,\n        tie_weights: bool = True,\n        strict: bool = False,\n        cache_dir: str = \"outputs/cache\",\n        variant: Optional[str] = None,\n        **kwargs,\n    ):\n        self._optimizer = optimizer\n        self.dataloader_kwargs = dataloader_kwargs\n        self.init_values = init_values\n        self.merging_weights_load_path = merging_weights_load_path\n        self.merging_weights_save_path = merging_weights_save_path\n        self.clamp_weights = clamp_weights\n        self.tie_weights = tie_weights\n        self.strict = strict\n        self.max_steps = max_steps\n        self.cache_dir = cache_dir\n        self.variant = variant\n\n        self.configs = SimpleNamespace(**kwargs)\n        self.configs.init_values = init_values\n        self.configs.clamp_weights = clamp_weights\n        self.configs.tie_weights = tie_weights\n        self.configs.strict = strict\n        if isinstance(self.configs.accuracy_test_interval, ListConfig):\n            self.configs.accuracy_test_interval = list(\n                self.configs.accuracy_test_interval\n            )\n        elif isinstance(self.configs.accuracy_test_interval, int):\n            pass\n        else:\n            log.warning(\n                f\"Unexpected type of accuracy_test_interval: {type(self.configs.accuracy_test_interval)}\"\n            )\n        super().__init__(**kwargs)\n\n    @rank_zero_only\n    def save_merging_weights(self, file_path: str, merging_weights: torch.Tensor):\n        \"\"\"\n        Save the merging weights to a file.\n\n        Args:\n            file_path (str): The path to save the merging weights.\n            merging_weights (torch.Tensor): The merging weights to save.\n        \"\"\"\n        if self.fabric.is_global_zero and self.merging_weights_save_path is not None:\n            if isinstance(file_path, str) and not file_path.startswith((\"/\", \".\")):\n                # if the file path is not absolute or relative to current working directory, save it in the log directory\n                save_path = os.path.join(self.log_dir, file_path)\n            else:\n                save_path = file_path\n            log.info(f\"saving merging weights to {save_path}.\")\n            if os.path.dirname(save_path):\n                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n            torch.save(merging_weights.detach().cpu(), save_path)\n\n    def free_gpu_memory(self, module: LayerWiseMergedModel):\n        module.pretrained_model.to(\"cpu\")\n        for model in module.task_vectors:\n            model.to(\"cpu\")\n        del module\n        gc.collect()\n        torch.cuda.empty_cache()\n        log.info(get_memory_usage(\"after freeing memory, the memory usage of GPU is:\"))\n\n    def update_datasets(self, datasets):\n        \"\"\"\n        for evary epoch of local adamerging, we only use the data set corresponding to the model involved in the fusion\n        \"\"\"\n        num_datasets = len(datasets)\n        datasets_copy = datasets.copy()\n        for i in range(num_datasets):\n            datasets[i] = (\n                datasets_copy[i]\n                .union(datasets_copy[(i + 1) % num_datasets])\n                .union(datasets_copy[(i - 1) % num_datasets])\n            )\n        return datasets\n\n    def run(self, modelpool: Seq2SeqLMPool, **kwargs):\n        \"\"\"\n        Run the Layer-Wise AdaMerging Algorithm.\n\n        This method constructs the wrapped model and performs test-time adaptation if necessary.\n\n        Args:\n            modelpool (ModelPool): The model pool containing the pretrained and fine-tuned models.\n\n        Returns:\n            LayerWiseMergedModel: The merged model after test-time adaptation.\n        \"\"\"\n        log.info(\"Fusing models using layer-wise adaptive merging.\")\n        self.modelpool = modelpool\n        self.num_finetuned_models = len(modelpool.model_names)\n        datasets = [{dataset} for dataset in modelpool.model_names]\n\n        with self.profile(\"construct the wrapped model\"):\n            model_scheduler = ModelScheduler(self.configs, self.modelpool)\n\n        if self.merging_weights_load_path is not None:\n            # skip the test-time adaptation\n            return module.merge_and_unload()\n        else:\n            for step_idx in tqdm(\n                range(self.configs.gossip_max_steps),\n                \"Gossip merging\",\n                dynamic_ncols=True,\n            ):\n                datasets = self.update_datasets(datasets)\n                log.info(f\"Gossip merging step:, {step_idx}\")\n                for model_id in tqdm(\n                    range(self.num_finetuned_models),\n                    \"local admerging\",\n                    dynamic_ncols=True,\n                ):\n                    if self.configs.gossip_skip_adamerging == True:\n                        # skip adamerging, only merge\n                        with self.profile(\"construct the local wrapped model\"):\n                            module = model_scheduler(model_id)\n                        log.info(\n                            f\"skip adamerging, only merge ({modelpool.model_names[model_id]})\"\n                        )\n                        model_scheduler.store_model(module.merge_weights(), model_id)\n                        self.free_gpu_memory(module)\n                    else:\n                        with self.profile(\"construct the local wrapped model\"):\n                            module = model_scheduler(model_id)\n\n                        if self.configs.improve_dataset == True:\n                            log.info(\n                                f\"improved datasets, the datasets used in this local merging is {datasets[model_id]}\"\n                            )\n                        else:\n                            log.info(\n                                f\"unimproved datasets, the datasets used in this local merging is {modelpool.model_names}\"\n                            )\n                        with self.profile(\"test-time adaptation\"):\n                            module = self.test_time_adaptation(\n                                module, datasets[model_id]\n                            )\n                        # if self.configs.get(\"save_merging_weights\", False):\n                        #     self.save_merging_weights(\n                        #         self.configs.save_merging_weights, module.merge_weight\n                        #     )\n                        model_scheduler.store_model(module.merge_weights(), model_id)\n                        log.info(\n                            get_memory_usage(\n                                f\"after local merging ({modelpool.model_names[model_id]}), the memory usage of GPU is:\"\n                            )\n                        )\n                        self.free_gpu_memory(\n                            module\n                        )  # simulate distributed GPU memory usage as much as possible\n\n                model_scheduler.update_models()\n                do_evaluation = False  # whether to do evaluation after each Gossip step\n                if isinstance(self.configs.accuracy_test_interval, list):\n                    if (step_idx + 1) in self.configs.accuracy_test_interval:\n                        do_evaluation = True\n                elif isinstance(self.configs.accuracy_test_interval, int):\n                    if (\n                        self.configs.accuracy_test_interval != 0\n                        and (step_idx + 1) % self.configs.accuracy_test_interval == 0\n                    ):\n                        do_evaluation = True\n                if do_evaluation:\n                    self._program.evaluate_merged_model(\n                        self._program.taskpool, model_scheduler.get_final_models()\n                    )\n                    model_scheduler.move_to(\"cpu\")\n\n        return model_scheduler.get_final_models()\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n        \"\"\"\n        Loader of test dataset for test-time adaptation. labels are not needed.\n\n        Args:\n            task (str): The name of the task.\n\n        Returns:\n            DataLoader: The data loader for the test dataset.\n        \"\"\"\n        dataloader_kwargs = dict(self.dataloader_kwargs)\n        dataloader_kwargs.update(dict(shuffle=True, collate_fn=default_data_collator))\n\n        dataset = self.modelpool.load_test_dataset(task)\n        loader = DataLoader(dataset, **dataloader_kwargs)\n\n        if self.fabric is not None:\n            loader = self.fabric.setup_dataloaders(loader)\n        return iter(InfiniteDataLoader(loader))\n\n    def compute_logits(\n        self,\n        module: Union[T5ForConditionalGeneration, LayerWiseMergedModel],\n        batch,\n        task: str,\n    ) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for the given images and task.\n\n        Args:\n            module: The model module.\n            images (Tensor): The input images.\n            task (str): The name of the task.\n\n        Returns:\n            Tensor: The computed logits.\n        \"\"\"\n        input_ids: Tensor = batch[\"input_ids\"]\n        attention_mask: Tensor = batch[\"attention_mask\"]\n\n        # remove padding tokens from the input\n        while attention_mask[:, -1].eq(0).all():\n            input_ids = input_ids[:, :-1]\n            attention_mask = attention_mask[:, :-1]\n\n        outputs = module(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=torch.ones(\n                input_ids.size(0), 1, dtype=torch.long, device=input_ids.device\n            ),\n        )\n        logits = outputs.logits[:, 0, :]\n        return logits\n\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Something to do before the test-time adaptation starts. Such as setting up the task-specific heads.\n        \"\"\"\n        pass\n\n    def test_time_adaptation(self, module: LayerWiseMergedModel, datasets):\n        \"\"\"\n        Perform test-time adaptation on the merged model.\n\n        This method adapts the merging weights during test-time to improve performance.\n\n        Args:\n            module (LayerWiseMergedModel): The merged model.\n\n        Returns:\n            LayerWiseMergedModel: The adapted merged model.\n        \"\"\"\n        self.on_test_time_adaptation_start()\n\n        # configure optimizer\n        optimizer = instantiate(self._optimizer, [module.merge_weight])\n        module, optimizer = self.fabric.setup(module, optimizer)\n\n        module.train()\n        module.merge_weights()\n        for step_idx in (\n            pbar := tqdm(\n                range(self.max_steps if not self.is_debug_mode else 1),\n                (\"[DEBUG MODE] \" if self.is_debug_mode else \"\")\n                + \"AdaMerging Test-time adaptation\",\n                dynamic_ncols=True,\n            )\n        ):\n            if self.variant == \"mgda\":\n                total_loss = self._compute_gradients_using_mgda(module)\n            else:\n                total_loss = 0\n                for task in self.modelpool.model_names:\n                    with self.profile(\"data loading\"):\n                        batch = next(self.get_shuffled_test_loader_iter(task))\n                    with self.profile(\"forward pass\"):\n                        logits = self.compute_logits(module, batch, task)\n                        logits = logits.mean(dim=0, keepdim=True)\n                        loss = entropy_loss(logits)\n                        total_loss += loss\n                    with self.profile(\"backward pass\"):\n                        self.fabric.backward(loss, retain_graph=True)\n\n            with self.profile(\"optimizer step\"):\n                optimizer.step()\n                optimizer.zero_grad()\n            with self.profile(\"merging weights\"):\n                module.merge_weights()\n\n            metrics = {\n                \"train/loss\": total_loss.item(),\n                \"train/weight_max\": module.merge_weight.max().item(),\n                \"train/weight_min\": module.merge_weight.min().item(),\n                \"train/weight_mean\": module.merge_weight.mean().item(),\n            }\n            self.fabric.log_dict(metrics, step=step_idx)\n            pbar.set_postfix(metrics)\n\n        self.print_profile_summary()\n        del optimizer\n        gc.collect()\n        torch.cuda.empty_cache()\n        return module\n\n    def _compute_gradients_using_mgda(self, module: LayerWiseMergedModel):\n        all_grads = []\n        total_loss = 0\n        # default behavior for first-order optimizers\n        for task in self.modelpool.model_names:\n            with self.profile(\"data loading\"):\n                batch = next(self.get_shuffled_test_loader_iter(task))\n            with self.profile(\"forward pass\"):\n                logits = self.compute_logits(module, batch, task)\n                logits = logits.mean(dim=0, keepdim=True)\n                loss = entropy_loss(logits)\n                total_loss += loss\n            with self.profile(\"backward pass\"):\n                # self.fabric.backward(loss, retain_graph=True)\n                _grads = torch.autograd.grad(\n                    loss,\n                    [module.merge_weight],\n                    create_graph=False,\n                    retain_graph=True,\n                )\n                all_grads.append(_grads[0].flatten().detach())\n        sol, min_norm = MinNormSolver.find_min_norm_element(all_grads)\n        if not isinstance(sol, torch.Tensor):\n            sol = torch.from_numpy(sol)\n        sol = sol.to(\n            device=module.merge_weight.device,\n            dtype=module.merge_weight.dtype,\n        )\n        grad = torch.stack(all_grads) * sol.view(-1, 1)\n        module.merge_weight.grad = grad.sum(dim=0).view_as(module.merge_weight)\n        return total_loss\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseGossipAlgorithm.compute_logits","title":"<code>compute_logits(module, batch, task)</code>","text":"<p>Compute the logits for the given images and task.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Union[T5ForConditionalGeneration, LayerWiseMergedModel]</code>)           \u2013            <p>The model module.</p> </li> <li> <code>images</code>               (<code>Tensor</code>)           \u2013            <p>The input images.</p> </li> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The computed logits.</p> </li> </ul> Source code in <code>fusion_bench/method/gossip/flan_t5_layer_wise_gossip.py</code> <pre><code>def compute_logits(\n    self,\n    module: Union[T5ForConditionalGeneration, LayerWiseMergedModel],\n    batch,\n    task: str,\n) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for the given images and task.\n\n    Args:\n        module: The model module.\n        images (Tensor): The input images.\n        task (str): The name of the task.\n\n    Returns:\n        Tensor: The computed logits.\n    \"\"\"\n    input_ids: Tensor = batch[\"input_ids\"]\n    attention_mask: Tensor = batch[\"attention_mask\"]\n\n    # remove padding tokens from the input\n    while attention_mask[:, -1].eq(0).all():\n        input_ids = input_ids[:, :-1]\n        attention_mask = attention_mask[:, :-1]\n\n    outputs = module(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=torch.ones(\n            input_ids.size(0), 1, dtype=torch.long, device=input_ids.device\n        ),\n    )\n    logits = outputs.logits[:, 0, :]\n    return logits\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseGossipAlgorithm.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(task)</code>  <code>cached</code>","text":"<p>Loader of test dataset for test-time adaptation. labels are not needed.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code> (              <code>DataLoader</code> )          \u2013            <p>The data loader for the test dataset.</p> </li> </ul> Source code in <code>fusion_bench/method/gossip/flan_t5_layer_wise_gossip.py</code> <pre><code>@functools.cache\ndef get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n    \"\"\"\n    Loader of test dataset for test-time adaptation. labels are not needed.\n\n    Args:\n        task (str): The name of the task.\n\n    Returns:\n        DataLoader: The data loader for the test dataset.\n    \"\"\"\n    dataloader_kwargs = dict(self.dataloader_kwargs)\n    dataloader_kwargs.update(dict(shuffle=True, collate_fn=default_data_collator))\n\n    dataset = self.modelpool.load_test_dataset(task)\n    loader = DataLoader(dataset, **dataloader_kwargs)\n\n    if self.fabric is not None:\n        loader = self.fabric.setup_dataloaders(loader)\n    return iter(InfiniteDataLoader(loader))\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseGossipAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Something to do before the test-time adaptation starts. Such as setting up the task-specific heads.</p> Source code in <code>fusion_bench/method/gossip/flan_t5_layer_wise_gossip.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Something to do before the test-time adaptation starts. Such as setting up the task-specific heads.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseGossipAlgorithm.run","title":"<code>run(modelpool, **kwargs)</code>","text":"<p>Run the Layer-Wise AdaMerging Algorithm.</p> <p>This method constructs the wrapped model and performs test-time adaptation if necessary.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>ModelPool</code>)           \u2013            <p>The model pool containing the pretrained and fine-tuned models.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LayerWiseMergedModel</code>          \u2013            <p>The merged model after test-time adaptation.</p> </li> </ul> Source code in <code>fusion_bench/method/gossip/flan_t5_layer_wise_gossip.py</code> <pre><code>def run(self, modelpool: Seq2SeqLMPool, **kwargs):\n    \"\"\"\n    Run the Layer-Wise AdaMerging Algorithm.\n\n    This method constructs the wrapped model and performs test-time adaptation if necessary.\n\n    Args:\n        modelpool (ModelPool): The model pool containing the pretrained and fine-tuned models.\n\n    Returns:\n        LayerWiseMergedModel: The merged model after test-time adaptation.\n    \"\"\"\n    log.info(\"Fusing models using layer-wise adaptive merging.\")\n    self.modelpool = modelpool\n    self.num_finetuned_models = len(modelpool.model_names)\n    datasets = [{dataset} for dataset in modelpool.model_names]\n\n    with self.profile(\"construct the wrapped model\"):\n        model_scheduler = ModelScheduler(self.configs, self.modelpool)\n\n    if self.merging_weights_load_path is not None:\n        # skip the test-time adaptation\n        return module.merge_and_unload()\n    else:\n        for step_idx in tqdm(\n            range(self.configs.gossip_max_steps),\n            \"Gossip merging\",\n            dynamic_ncols=True,\n        ):\n            datasets = self.update_datasets(datasets)\n            log.info(f\"Gossip merging step:, {step_idx}\")\n            for model_id in tqdm(\n                range(self.num_finetuned_models),\n                \"local admerging\",\n                dynamic_ncols=True,\n            ):\n                if self.configs.gossip_skip_adamerging == True:\n                    # skip adamerging, only merge\n                    with self.profile(\"construct the local wrapped model\"):\n                        module = model_scheduler(model_id)\n                    log.info(\n                        f\"skip adamerging, only merge ({modelpool.model_names[model_id]})\"\n                    )\n                    model_scheduler.store_model(module.merge_weights(), model_id)\n                    self.free_gpu_memory(module)\n                else:\n                    with self.profile(\"construct the local wrapped model\"):\n                        module = model_scheduler(model_id)\n\n                    if self.configs.improve_dataset == True:\n                        log.info(\n                            f\"improved datasets, the datasets used in this local merging is {datasets[model_id]}\"\n                        )\n                    else:\n                        log.info(\n                            f\"unimproved datasets, the datasets used in this local merging is {modelpool.model_names}\"\n                        )\n                    with self.profile(\"test-time adaptation\"):\n                        module = self.test_time_adaptation(\n                            module, datasets[model_id]\n                        )\n                    # if self.configs.get(\"save_merging_weights\", False):\n                    #     self.save_merging_weights(\n                    #         self.configs.save_merging_weights, module.merge_weight\n                    #     )\n                    model_scheduler.store_model(module.merge_weights(), model_id)\n                    log.info(\n                        get_memory_usage(\n                            f\"after local merging ({modelpool.model_names[model_id]}), the memory usage of GPU is:\"\n                        )\n                    )\n                    self.free_gpu_memory(\n                        module\n                    )  # simulate distributed GPU memory usage as much as possible\n\n            model_scheduler.update_models()\n            do_evaluation = False  # whether to do evaluation after each Gossip step\n            if isinstance(self.configs.accuracy_test_interval, list):\n                if (step_idx + 1) in self.configs.accuracy_test_interval:\n                    do_evaluation = True\n            elif isinstance(self.configs.accuracy_test_interval, int):\n                if (\n                    self.configs.accuracy_test_interval != 0\n                    and (step_idx + 1) % self.configs.accuracy_test_interval == 0\n                ):\n                    do_evaluation = True\n            if do_evaluation:\n                self._program.evaluate_merged_model(\n                    self._program.taskpool, model_scheduler.get_final_models()\n                )\n                model_scheduler.move_to(\"cpu\")\n\n    return model_scheduler.get_final_models()\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseGossipAlgorithm.save_merging_weights","title":"<code>save_merging_weights(file_path, merging_weights)</code>","text":"<p>Save the merging weights to a file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str</code>)           \u2013            <p>The path to save the merging weights.</p> </li> <li> <code>merging_weights</code>               (<code>Tensor</code>)           \u2013            <p>The merging weights to save.</p> </li> </ul> Source code in <code>fusion_bench/method/gossip/flan_t5_layer_wise_gossip.py</code> <pre><code>@rank_zero_only\ndef save_merging_weights(self, file_path: str, merging_weights: torch.Tensor):\n    \"\"\"\n    Save the merging weights to a file.\n\n    Args:\n        file_path (str): The path to save the merging weights.\n        merging_weights (torch.Tensor): The merging weights to save.\n    \"\"\"\n    if self.fabric.is_global_zero and self.merging_weights_save_path is not None:\n        if isinstance(file_path, str) and not file_path.startswith((\"/\", \".\")):\n            # if the file path is not absolute or relative to current working directory, save it in the log directory\n            save_path = os.path.join(self.log_dir, file_path)\n        else:\n            save_path = file_path\n        log.info(f\"saving merging weights to {save_path}.\")\n        if os.path.dirname(save_path):\n            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        torch.save(merging_weights.detach().cpu(), save_path)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseGossipAlgorithm.test_time_adaptation","title":"<code>test_time_adaptation(module, datasets)</code>","text":"<p>Perform test-time adaptation on the merged model.</p> <p>This method adapts the merging weights during test-time to improve performance.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>LayerWiseMergedModel</code>)           \u2013            <p>The merged model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LayerWiseMergedModel</code>          \u2013            <p>The adapted merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/gossip/flan_t5_layer_wise_gossip.py</code> <pre><code>def test_time_adaptation(self, module: LayerWiseMergedModel, datasets):\n    \"\"\"\n    Perform test-time adaptation on the merged model.\n\n    This method adapts the merging weights during test-time to improve performance.\n\n    Args:\n        module (LayerWiseMergedModel): The merged model.\n\n    Returns:\n        LayerWiseMergedModel: The adapted merged model.\n    \"\"\"\n    self.on_test_time_adaptation_start()\n\n    # configure optimizer\n    optimizer = instantiate(self._optimizer, [module.merge_weight])\n    module, optimizer = self.fabric.setup(module, optimizer)\n\n    module.train()\n    module.merge_weights()\n    for step_idx in (\n        pbar := tqdm(\n            range(self.max_steps if not self.is_debug_mode else 1),\n            (\"[DEBUG MODE] \" if self.is_debug_mode else \"\")\n            + \"AdaMerging Test-time adaptation\",\n            dynamic_ncols=True,\n        )\n    ):\n        if self.variant == \"mgda\":\n            total_loss = self._compute_gradients_using_mgda(module)\n        else:\n            total_loss = 0\n            for task in self.modelpool.model_names:\n                with self.profile(\"data loading\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                with self.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, batch, task)\n                    logits = logits.mean(dim=0, keepdim=True)\n                    loss = entropy_loss(logits)\n                    total_loss += loss\n                with self.profile(\"backward pass\"):\n                    self.fabric.backward(loss, retain_graph=True)\n\n        with self.profile(\"optimizer step\"):\n            optimizer.step()\n            optimizer.zero_grad()\n        with self.profile(\"merging weights\"):\n            module.merge_weights()\n\n        metrics = {\n            \"train/loss\": total_loss.item(),\n            \"train/weight_max\": module.merge_weight.max().item(),\n            \"train/weight_min\": module.merge_weight.min().item(),\n            \"train/weight_mean\": module.merge_weight.mean().item(),\n        }\n        self.fabric.log_dict(metrics, step=step_idx)\n        pbar.set_postfix(metrics)\n\n    self.print_profile_summary()\n    del optimizer\n    gc.collect()\n    torch.cuda.empty_cache()\n    return module\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.FlanT5LayerWiseGossipAlgorithm.update_datasets","title":"<code>update_datasets(datasets)</code>","text":"<p>for evary epoch of local adamerging, we only use the data set corresponding to the model involved in the fusion</p> Source code in <code>fusion_bench/method/gossip/flan_t5_layer_wise_gossip.py</code> <pre><code>def update_datasets(self, datasets):\n    \"\"\"\n    for evary epoch of local adamerging, we only use the data set corresponding to the model involved in the fusion\n    \"\"\"\n    num_datasets = len(datasets)\n    datasets_copy = datasets.copy()\n    for i in range(num_datasets):\n        datasets[i] = (\n            datasets_copy[i]\n            .union(datasets_copy[(i + 1) % num_datasets])\n            .union(datasets_copy[(i - 1) % num_datasets])\n        )\n    return datasets\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#continual-model-merging","title":"Continual Model Merging","text":""},{"location":"api/fusion_bench.method/merging/#orthogonal-projection-based-continual-merging-opcm","title":"Orthogonal Projection-based Continual Merging (OPCM)","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.OPCMForCLIP","title":"<code>OPCMForCLIP</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>LightningFabricMixin</code>, <code>SimpleProfilerMixin</code></p> Source code in <code>fusion_bench/method/opcm/opcm.py</code> <pre><code>class OPCMForCLIP(\n    BaseAlgorithm,\n    LightningFabricMixin,\n    SimpleProfilerMixin,\n):\n    def __init__(\n        self,\n        alpha: float,\n        shuffle_order: bool = True,\n        seed: Optional[int] = None,\n        save_on_every_step: bool = True,\n        evaluate_on_every_step: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Continual Model Merging via SVD Projection.\n\n        Args:\n            alpha (float): the scaling factor for the SVD projection.\n            shuffle_order (bool): whether to shuffle the order of the models.\n            seed (Optional[int]): the seed to use.\n            save_on_every_step (bool): whether to save the merged model on every step.\n            evaluate_on_every_step (bool): whether to evaluate the merged model on every step.\n        \"\"\"\n        self.alpha = alpha\n        self.shuffle_order = shuffle_order\n        self.seed = seed\n        self.save_on_every_step = save_on_every_step\n        self.evaluate_on_every_step = evaluate_on_every_step\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool):\n        if self.seed is not None:\n            L.seed_everything(self.seed)\n        accelerator = self.fabric.device\n\n        with self.profile(\"loading model\"):\n            pretrained_model = modelpool.load_pretrained_model()\n\n        model_names = modelpool.model_names\n        if self.shuffle_order:\n            random.shuffle(model_names)\n\n        self.taskpool = cast(CLIPVisionModelTaskPool, self._program.taskpool)\n        self._test_datasets = deepcopy(self.taskpool._test_datasets)\n        \"\"\"Configuration for the test datasets\"\"\"\n\n        # log the model names\n        if self.log_dir is not None:\n            save_to_json(model_names, Path(self.log_dir) / \"model_names.json\")\n            tensorboard_summarywriter: \"SummaryWriter\" = self.tensorboard_summarywriter\n            tensorboard_summarywriter.add_text(\n                \"global/model_names\", str(model_names), global_step=0\n            )\n\n        # get the average model\n        with self.profile(\"loading model\"):\n            merged_model = modelpool.load_model(model_names[0])\n            assert merged_model is not None, \"Failed to load the first model\"\n\n        if self.evaluate_on_every_step:\n            with self.profile(\"evaluating model\"):\n                self.taskpool._is_setup = False\n                self.taskpool._test_datasets = DictConfig(\n                    {model_names[0]: self._test_datasets[model_names[0]]}\n                )\n                report = self.taskpool.evaluate(deepcopy(merged_model))\n                save_to_json(report, Path(self.log_dir) / \"report_0.json\")\n\n        self.avg_task_vector_norm = get_task_vector_norm(merged_model, pretrained_model)\n        self.all_task_vector_norm = [self.avg_task_vector_norm]\n        self.fabric.log(\"model/task_vector_norm\", self.avg_task_vector_norm, step=0)\n        self.fabric.log(\"model/avg_task_vector_norm\", self.avg_task_vector_norm, step=0)\n        self.fabric.log(\n            \"model/merged_task_vector_norm\", self.avg_task_vector_norm, step=0\n        )\n\n        self.previous_lambda_t = 1\n        self.lambda_t = None\n        self.fabric.log(\"model/lambda_t\", self.previous_lambda_t, step=0)\n        self.fabric.log(\"empirical/lambda_t\", 1, step=0)\n\n        if self.save_on_every_step:\n            self.save_merged_model(merged_model, 0)\n\n        for model_idx, model_name in tqdm(\n            enumerate(model_names[1:]), desc=\"Processing models\"\n        ):\n            model_idx += 1\n            with self.profile(\"loading model\"):\n                task_model = modelpool.load_model(model_name)\n\n            with self.profile(\"merging model\"):\n                self.all_task_vector_norm.append(\n                    get_task_vector_norm(task_model, pretrained_model)\n                )\n                self.avg_task_vector_norm = np.mean(self.all_task_vector_norm)\n                self.fabric.log(\n                    \"model/task_vector_norm\",\n                    self.all_task_vector_norm[-1],\n                    step=model_idx,\n                )\n                self.fabric.log(\n                    \"model/avg_task_vector_norm\",\n                    self.avg_task_vector_norm,\n                    step=model_idx,\n                )\n\n                self.lambda_t = 1  # temporary value\n\n                for module_name, module in tqdm(\n                    list(merged_model.named_modules()),\n                    desc=f\"Processing {model_name}\",\n                    leave=False,\n                ):\n                    if not is_leaf_module(module):\n                        continue\n\n                    if isinstance(module, nn.Linear):\n                        module.weight.data = self.merge_linear_weights(\n                            module.weight,\n                            pretrained_model.get_submodule(module_name).weight,\n                            task_model.get_submodule(module_name).weight,\n                            param_name=\".\".join([module_name, \"weight\"]),\n                            alpha=self.alpha,\n                            accelerator=accelerator,\n                        )\n                        if module.bias is not None:\n                            module.bias.data = self.merge_other_parameters(\n                                module.bias,\n                                pretrained_model.get_submodule(module_name).bias,\n                                task_model.get_submodule(module_name).bias,\n                                param_name=\".\".join([module_name, \"bias\"]),\n                                accelerator=accelerator,\n                            )\n                    else:\n                        for param_name, param in module.named_parameters():\n                            param.data = self.merge_other_parameters(\n                                merged_W=param,\n                                pretrained_W=pretrained_model.get_submodule(\n                                    module_name\n                                ).get_parameter(param_name),\n                                task_W=task_model.get_submodule(\n                                    module_name\n                                ).get_parameter(param_name),\n                                param_name=\".\".join([module_name, param_name]),\n                                accelerator=accelerator,\n                            )\n\n                task_vector_norm = get_task_vector_norm(merged_model, pretrained_model)\n                self.lambda_t *= task_vector_norm / self.avg_task_vector_norm\n                for param_name, param in merged_model.named_parameters():\n                    param.data = pretrained_model.get_parameter(param_name) + (\n                        param - pretrained_model.get_parameter(param_name)\n                    ) * (self.avg_task_vector_norm / task_vector_norm)\n                self.fabric.log(\"model/lambda_t\", self.lambda_t, step=model_idx)\n                self.fabric.log(\n                    \"empirical/lambda_t\", np.sqrt(model_idx + 1), step=model_idx\n                )\n                self.previous_lambda_t = self.lambda_t\n                self.lambda_t = None\n\n                self.fabric.log(\n                    \"model/merged_task_vector_norm\",\n                    get_task_vector_norm(merged_model, pretrained_model),\n                    step=model_idx,\n                )\n\n            if self.save_on_every_step:\n                with self.profile(\"saving model\"):\n                    self.save_merged_model(merged_model, model_idx)\n\n            if self.evaluate_on_every_step:\n                with self.profile(\"evaluating model\"):\n                    self.taskpool._is_setup = False\n                    self.taskpool._test_datasets = DictConfig(\n                        {\n                            n: self._test_datasets[n]\n                            for n in model_names[: model_idx + 1]\n                        }\n                    )\n                    report = self.taskpool.evaluate(deepcopy(merged_model))\n                    save_to_json(\n                        report, Path(self.log_dir) / f\"report_{model_idx}.json\"\n                    )\n\n        self.print_profile_summary()\n        return merged_model\n\n    def save_merged_model(self, merged_model: CLIPVisionModel, step: int):\n        os.makedirs(Path(self.log_dir) / \"checkpoints\", exist_ok=True)\n        merged_model.save_pretrained(\n            Path(self.log_dir) / \"checkpoints\" / f\"merged_model_{step}\"\n        )\n\n    def merge_linear_weights(\n        self,\n        merged_W: Tensor,\n        pretrained_W: Tensor,\n        task_W: Tensor,\n        param_name: str,\n        alpha: float,\n        accelerator: str = \"cpu\",\n    ):\n        original_device = merged_W.device\n        merged_W = merged_W.to(accelerator)\n        pretrained_W = pretrained_W.to(accelerator)\n        task_W = task_W.to(accelerator)\n\n        previous_merged_tv = merged_W - pretrained_W\n        task_tv = task_W - pretrained_W\n\n        u, s, v = svd(previous_merged_tv)\n        rank = s.size(0)\n        split_rank = (s.cumsum(dim=0) / s.sum() &gt; alpha).float().argmax().item()\n\n        projected_task_tv = u.T @ task_tv @ v\n        projected_task_tv.diagonal().fill_(0)\n\n        projected_task_tv[:split_rank, :split_rank] = 0\n\n        cleaned_task_tv = u @ projected_task_tv @ v.T\n\n        previous_lambda_t = self.previous_lambda_t\n        lambda_t = self.lambda_t\n        new_merged_W = (\n            pretrained_W\n            + (previous_lambda_t * previous_merged_tv + cleaned_task_tv) / lambda_t\n        )\n        return new_merged_W.to(original_device)\n\n    def merge_other_parameters(\n        self,\n        merged_W: Tensor,\n        pretrained_W: Tensor,\n        task_W: Tensor,\n        param_name: str,\n        accelerator: str = \"cpu\",\n    ):\n        original_device = merged_W.device\n        merged_W = merged_W.to(accelerator)\n        pretrained_W = pretrained_W.to(accelerator)\n        task_W = task_W.to(accelerator)\n\n        previous_merged_tv = merged_W - pretrained_W\n        task_tv = task_W - pretrained_W\n\n        previous_lambda_t = self.previous_lambda_t\n        lambda_t = self.lambda_t\n\n        new_merged_W = (\n            pretrained_W + (previous_lambda_t * previous_merged_tv + task_tv) / lambda_t\n        )\n        return new_merged_W.to(original_device)\n\n    def compute_lambda_t(\n        self, previous_merged_tv: Tensor, task_tv: Tensor, previous_lambda_t: float\n    ):\n        previous_merged_tv = torch.flatten(previous_merged_tv)\n        task_tv = torch.flatten(task_tv)\n\n        lambda_t = torch.linalg.vector_norm(\n            previous_lambda_t * previous_merged_tv + task_tv\n        ) / torch.linalg.vector_norm(previous_merged_tv)\n        return lambda_t.item()\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.OPCMForCLIP.__init__","title":"<code>__init__(alpha, shuffle_order=True, seed=None, save_on_every_step=True, evaluate_on_every_step=False, **kwargs)</code>","text":"<p>Continual Model Merging via SVD Projection.</p> <p>Parameters:</p> <ul> <li> <code>alpha</code>               (<code>float</code>)           \u2013            <p>the scaling factor for the SVD projection.</p> </li> <li> <code>shuffle_order</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to shuffle the order of the models.</p> </li> <li> <code>seed</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>the seed to use.</p> </li> <li> <code>save_on_every_step</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to save the merged model on every step.</p> </li> <li> <code>evaluate_on_every_step</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to evaluate the merged model on every step.</p> </li> </ul> Source code in <code>fusion_bench/method/opcm/opcm.py</code> <pre><code>def __init__(\n    self,\n    alpha: float,\n    shuffle_order: bool = True,\n    seed: Optional[int] = None,\n    save_on_every_step: bool = True,\n    evaluate_on_every_step: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Continual Model Merging via SVD Projection.\n\n    Args:\n        alpha (float): the scaling factor for the SVD projection.\n        shuffle_order (bool): whether to shuffle the order of the models.\n        seed (Optional[int]): the seed to use.\n        save_on_every_step (bool): whether to save the merged model on every step.\n        evaluate_on_every_step (bool): whether to evaluate the merged model on every step.\n    \"\"\"\n    self.alpha = alpha\n    self.shuffle_order = shuffle_order\n    self.seed = seed\n    self.save_on_every_step = save_on_every_step\n    self.evaluate_on_every_step = evaluate_on_every_step\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/merging/#dual-projections-dop","title":"Dual Projections (DOP)","text":""},{"location":"api/fusion_bench.method/merging/#fusion_bench.method.ContinualDOPForCLIP","title":"<code>ContinualDOPForCLIP</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>LightningFabricMixin</code></p> Source code in <code>fusion_bench/method/dop/dop.py</code> <pre><code>@auto_register_config\nclass ContinualDOPForCLIP(BaseAlgorithm, LightningFabricMixin):\n\n    def __init__(\n        self,\n        seed: Optional[int] = None,\n        shuffle_order: bool = False,\n        save_on_every_step: bool = True,\n        evaluate_on_every_step: bool = False,\n        lr: float = 1e-4,\n        num_steps: int = 200,\n        mgda: bool = True,\n        ema: bool = True,\n        ema_beta: float = 0.99,\n        alpha: float = None,\n        svd_epsilon: float = 1.0,\n        svd_proj_space: str = \"uv\",\n        **kwargs,\n    ):\n        self.lr = lr\n        self.num_steps = num_steps\n        self.mgda = mgda\n        self.ema = ema\n        self.ema_beta = ema_beta\n        self.alpha = alpha\n        self.svd_epsilon = svd_epsilon\n        self.svd_proj_space = svd_proj_space\n        self.seed = seed\n        self.shuffle_order = shuffle_order\n        self.save_on_every_step = save_on_every_step\n        self.evaluate_on_every_step = evaluate_on_every_step\n\n        assert (\n            self.svd_epsilon &gt;= 0 and self.svd_epsilon &lt;= 1\n        ), \"The svd_epsilon should be in the range of [0, 1]\"\n        assert (\n            self.alpha &gt;= 0 and self.alpha &lt;= 1\n        ), \"The alpha should be in the range of [0, 1]\"\n        super().__init__(**kwargs)\n\n    def print_params(self, pretrained_model):\n        total_params = 0\n        linear_params = 0\n        linear_weight_params = 0\n        for module_name, module in pretrained_model.named_modules():\n            if not is_leaf_module(module):\n                continue\n            if isinstance(module, nn.Linear):\n                linear_params += sum(p.numel() for n, p in module.named_parameters())\n                linear_weight_params += sum(\n                    p.numel() for n, p in module.named_parameters() if \"weight\" in n\n                )\n            total_params += sum(p.numel() for p in module.parameters())\n\n        linear_ratio = linear_params / total_params * 100\n        linear_weight_ratio = linear_weight_params / total_params * 100\n        print(f\"Total Parameters: {total_params}\")\n        print(f\"Linear Parameters: {linear_params}\")\n        print(f\"Linear Weight Parameters: {linear_weight_params}\")\n        print(f\"Linear Ratio: {linear_ratio:.2f}%\")\n        print(f\"Linear Weight Ratio: {linear_weight_ratio:.2f}%\")\n\n    def run(self, modelpool: BaseModelPool):\n        if self.seed is not None:\n            L.seed_everything(self.seed)\n        else:\n            seed_everything_by_time(self.fabric)\n\n        # get the model names, shuffle if needed\n        # the model names will be saved to the log directory as `model_names.json`\n        model_names = modelpool.model_names\n        if self.shuffle_order:\n            random.shuffle(model_names)\n        if self.log_dir is not None:\n            save_to_json(model_names, os.path.join(self.log_dir, \"model_names.json\"))\n\n        if self.evaluate_on_every_step:\n            \"\"\"Configuration for the test datasets\"\"\"\n            self.taskpool = cast(CLIPVisionModelTaskPool, self._program.taskpool)\n            self._test_datasets = deepcopy(self.taskpool._test_datasets)\n\n        pretrained_model = modelpool.load_pretrained_model()\n\n        merged_model = None\n        for model_idx, model_name in enumerate(model_names):\n            print(\n                f\"--------- Optimizing {model_idx + 1}/{len(model_names)}-th with {model_name} ---------\"\n            )\n            if model_idx == 0:\n                merged_model = modelpool.load_model(model_names[0])\n            else:\n                merged_model = self._layer_wise_optimize(\n                    model_names=[\"merged\", model_name],\n                    pretrained_model=deepcopy(pretrained_model),\n                    finetuned_models={\n                        \"merged\": merged_model,\n                        model_name: modelpool.load_model(model_name),\n                    },\n                    model_idx=model_idx,\n                )\n\n            if self.save_on_every_step:\n                self.save_merged_model(merged_model, model_idx)\n\n            if self.evaluate_on_every_step:\n                self.taskpool._is_setup = False\n                self.taskpool._test_datasets = DictConfig(\n                    {n: self._test_datasets[n] for n in model_names[: model_idx + 1]}\n                )\n                report = self.taskpool.evaluate(deepcopy(merged_model))\n                save_to_json(report, Path(self.log_dir) / f\"report_{model_idx}.json\")\n\n        return merged_model\n\n    def _layer_wise_optimize(\n        self,\n        model_names: List[str],\n        pretrained_model: nn.Module,\n        finetuned_models: Dict[str, nn.Module],\n        model_idx: int,\n    ):\n        time_cost = []\n        for module_name, module in pretrained_model.named_modules():\n            if not is_leaf_module(module):\n                continue\n\n            if isinstance(module, nn.Linear):\n                if module.weight.requires_grad:\n                    import time\n\n                    start_time = time.time()\n                    merged_weight = self._optimize_weight(\n                        module.weight,\n                        {\n                            model_name: finetuned_models[model_name]\n                            .get_submodule(module_name)\n                            .weight\n                            for model_name in model_names\n                        },\n                        module_name,\n                        model_idx,\n                    )\n                    end_time = time.time()\n                    time_cost.append(end_time - start_time)\n                    module.weight.data = merged_weight.data\n                else:\n                    module.weight.data = simple_average(\n                        [\n                            finetuned_models[model_name]\n                            .get_submodule(module_name)\n                            .weight\n                            for model_name in model_names\n                        ]\n                    )\n                if module.bias is not None:\n                    module.bias.data = simple_average(\n                        [\n                            finetuned_models[model_name].get_submodule(module_name).bias\n                            for model_name in model_names\n                        ]\n                    )\n            else:\n                simple_average(\n                    [\n                        finetuned_models[model_name].get_submodule(module_name)\n                        for model_name in model_names\n                    ],\n                    base_module=module,\n                )\n\n        return pretrained_model\n\n    def _optimize_weight(\n        self,\n        pretrained_weight: Tensor,\n        finetuned_weights: Dict[str, Tensor],\n        module_name: str,\n        model_idx: int,\n    ):\n        assert (\n            self.fabric.world_size == 1\n        ), \"This algorithm is not currently supported in distributed training\"\n\n        pretrained_weight = self.fabric.to_device(pretrained_weight.detach())\n        finetuned_weights = {\n            model_name: self.fabric.to_device(finetuned_weight.detach())\n            for model_name, finetuned_weight in finetuned_weights.items()\n        }\n\n        merged_weight = self.fabric.to_device(\n            nn.Parameter(\n                simple_average(\n                    [\n                        finetuned_weight.detach()\n                        for finetuned_weight in finetuned_weights.values()\n                    ]\n                ),\n                requires_grad=True,\n            )\n        )\n\n        # Compute SVD of the difference between the finetuned and pretrained weights\n        proj_u_dict = {}\n        proj_v_dict = {}\n        proj_s_dict = {}\n        for i, finetuned_weight in enumerate(finetuned_weights.values()):\n            finetuned_tv = finetuned_weight - pretrained_weight\n            u, s, v = svd(finetuned_tv, full_matrices=True)\n            epsilon = 1.0 if self.svd_epsilon &gt; 1.0 else self.svd_epsilon\n            cumsum_ratio = s.cumsum(dim=0) / s.sum()\n            split_rank = torch.searchsorted(cumsum_ratio, epsilon).item()\n            u_main = u[:, :split_rank]\n            v_main = v[:, :split_rank]\n            s_main = s[:split_rank]\n            proj_u_dict[i] = u_main\n            proj_v_dict[i] = v_main\n            proj_s_dict[i] = s_main\n\n        if self.mgda:\n            if self.ema:\n                ema_sol = [self.alpha, 1 - self.alpha]\n            # This is multiple-gradient descent algorithm (MGDA) optimization\n            optimizer = torch.optim.Adam([merged_weight], lr=self.lr)\n            all_losses = [[], []]\n            all_alphas = [[], []]\n            for step_idx in tqdm(\n                range(self.num_steps), desc=f\"Optimizing {module_name} weight\"\n            ):\n                # Scaling the loss functions based on the algorithm choice\n                loss_data = {}\n                grads = {}\n                for i, finetuned_weight in enumerate(finetuned_weights.values()):\n                    proj_u = proj_u_dict[i]\n                    proj_v = proj_v_dict[i]\n                    proj_s = proj_s_dict[i]\n                    delta_tv = merged_weight - finetuned_weight\n                    loss_i = self.cal_loss_i(delta_tv, proj_s, proj_u, proj_v)\n                    loss_data[i] = float(loss_i.data)\n\n                    all_losses[i].append(float(loss_i.data))\n\n                    optimizer.zero_grad()\n                    loss_i.backward()\n                    grads[i] = Variable(\n                        merged_weight.grad.data.clone(), requires_grad=False\n                    )\n\n                # Normalize all gradients\n                gn = gradient_normalizers(\n                    grads=grads, losses=loss_data, normalization_type=\"loss\"\n                )\n                for i, _ in enumerate(finetuned_weights.values()):\n                    grads[i] = grads[i] / float(gn[i])\n\n                # Frank-Wolfe iteration to compute scales.\n                sol, min_norm = MinNormSolver.find_min_norm_element(\n                    [[grads[i]] for i in range(len(finetuned_weights.values()))]\n                )\n\n                if self.ema:\n                    ema_sol = [\n                        self.ema_beta * ema_sol[i] + (1 - self.ema_beta) * float(sol[i])\n                        for i in range(len(sol))\n                    ]\n                    sol = ema_sol\n                    all_alphas[0].append(ema_sol[0])\n                    all_alphas[1].append(ema_sol[1])\n\n                # Scaled back-propagation\n                loss = 0\n                for i, finetuned_weight in enumerate(finetuned_weights.values()):\n                    # Comptue gradients of each loss function wrt parameters\n                    proj_u = proj_u_dict[i]\n                    proj_v = proj_v_dict[i]\n                    proj_s = proj_s_dict[i]\n                    delta_tv = merged_weight - finetuned_weight\n                    loss_i = self.cal_loss_i(delta_tv, proj_s, proj_u, proj_v)\n                    loss += float(sol[i]) * loss_i\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n        else:\n            # This is a naive weighted optimization\n            optimizer = torch.optim.Adam([merged_weight], lr=self.lr)\n            for step_idx in tqdm(\n                range(self.num_steps), desc=f\"Optimizing {module_name} weight\"\n            ):\n                loss = 0\n                for i, finetuned_weight in enumerate(finetuned_weights.values()):\n                    proj_u = proj_u_dict[i]\n                    proj_v = proj_v_dict[i]\n                    proj_s = proj_s_dict[i]\n                    delta_tv = merged_weight - finetuned_weight\n                    loss_i = self.cal_loss_i(delta_tv, proj_s, proj_u, proj_v)\n                    loss += self.alpha * loss_i if i == 0 else (1 - self.alpha) * loss_i\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n        return merged_weight.detach().cpu()\n\n    def cal_loss_i(self, delta_tv, proj_s, proj_u, proj_v):\n        proj_delta_1 = torch.diag(proj_s) @ proj_u.T @ delta_tv\n        proj_delta_2 = delta_tv @ proj_v @ torch.diag(proj_s)\n        loss_i_u = torch.linalg.matrix_norm(proj_delta_1, ord=\"fro\") ** 2\n        loss_i_v = torch.linalg.matrix_norm(proj_delta_2, ord=\"fro\") ** 2\n        if self.svd_proj_space == \"uv\":\n            loss_i = loss_i_u + loss_i_v\n        elif self.svd_proj_space == \"u\":\n            loss_i = loss_i_u\n        elif self.svd_proj_space == \"v\":\n            loss_i = loss_i_v\n        else:\n            raise ValueError(\"Invalid svd_proj_space\")\n\n        return loss_i\n\n    def save_merged_model(self, merged_model: CLIPVisionModel, step: int):\n        os.makedirs(Path(self.log_dir) / \"checkpoints\", exist_ok=True)\n        merged_model.save_pretrained(\n            Path(self.log_dir) / \"checkpoints\" / f\"merged_model_{step}\"\n        )\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/","title":"Model Mixing","text":""},{"location":"api/fusion_bench.method/mixing/#layer-level-mixing","title":"Layer-level Mixing","text":""},{"location":"api/fusion_bench.method/mixing/#depth-upscaling","title":"Depth Upscaling","text":""},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.DepthUpscalingAlgorithm","title":"<code>DepthUpscalingAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>Implements the Depth Upscaling Algorithm.</p> <ul> <li>Kim et al. SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling. http://arxiv.org/abs/2312.15166</li> </ul> <p>This class extends the <code>BaseModelFusionAlgorithm</code> to handle depth upscaling of models. It supports upscaling the depth of a model by duplicating specified layers.</p> <p>Parameters:</p> <ul> <li> <code>layer_indices</code>               (<code>list</code>)           \u2013            <p>List of layer indices to duplicate.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments.</p> </li> </ul> Source code in <code>fusion_bench/method/depth_upscaling/depth_upscaling.py</code> <pre><code>@auto_register_config\nclass DepthUpscalingAlgorithm(BaseAlgorithm):\n    R\"\"\"\n    Implements the Depth Upscaling Algorithm.\n\n    - Kim et al. SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling. http://arxiv.org/abs/2312.15166\n\n    This class extends the `BaseModelFusionAlgorithm` to handle depth upscaling of models.\n    It supports upscaling the depth of a model by duplicating specified layers.\n\n    Args:\n        layer_indices (list): List of layer indices to duplicate.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n\n    def __init__(self, layer_indices: Union[str, List[int]], **kwargs: Any):\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(self, modelpool: nn.ModuleList | BaseModelPool) -&gt; nn.ModuleList:\n        \"\"\"\n        Executes the depth upscaling algorithm on a given model pool.\n\n        This method checks the type of the model pool, ensures that it contains only one model, and verifies that the model is an instance of `nn.ModuleList`.\n\n        Args:\n            modelpool (nn.ModuleList | ModelPool): The pool of models to upscale. Must contain only one model.\n\n        Returns:\n            nn.ModuleList: The upscaled model.\n\n        Raises:\n            AssertionError: If the model pool contains more than one model or if the model is not an instance of `nn.ModuleList`.\n            ValueError: If an invalid layer specification is provided in the configuration.\n        \"\"\"\n        # check the modelpool type\n        if isinstance(modelpool, BaseModelPool):\n            assert len(modelpool) == 1, \"DepthUpscaling only support one model\"\n            model = modelpool.load_model(modelpool.model_names[0])\n            assert isinstance(\n                model, nn.ModuleList\n            ), f\"The model should be a `nn.ModuleList`, but got {type(model)}\"\n        elif isinstance(modelpool, nn.ModuleList):\n            model = modelpool\n        else:\n            raise AssertionError(\n                f\"Invalid modelpool type: {type(modelpool)}. Expected `ModelPool` or `nn.ModuleList`.\"\n            )\n\n        # parse the layers\n        layer_indices = self.layer_indices\n        parsed_layer_indices = []\n        for layer in layer_indices:\n            if isinstance(layer, int):\n                parsed_layer_indices.append(layer)\n            elif isinstance(layer, str):\n                parsed_layer_indices.extend(eval(layer))\n            else:\n                raise ValueError(\"Invalid layer specification: {}\".format(layer))\n\n        # create a new model with the specified layers\n        new_model = nn.ModuleList(\n            [\n                deepcopy(model[i])\n                for i in tqdm(\n                    parsed_layer_indices, desc=\"constructing depth-upscaled model\"\n                )\n            ]\n        )\n\n        return new_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.DepthUpscalingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Executes the depth upscaling algorithm on a given model pool.</p> <p>This method checks the type of the model pool, ensures that it contains only one model, and verifies that the model is an instance of <code>nn.ModuleList</code>.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>ModuleList | ModelPool</code>)           \u2013            <p>The pool of models to upscale. Must contain only one model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ModuleList</code>           \u2013            <p>nn.ModuleList: The upscaled model.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AssertionError</code>             \u2013            <p>If the model pool contains more than one model or if the model is not an instance of <code>nn.ModuleList</code>.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If an invalid layer specification is provided in the configuration.</p> </li> </ul> Source code in <code>fusion_bench/method/depth_upscaling/depth_upscaling.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: nn.ModuleList | BaseModelPool) -&gt; nn.ModuleList:\n    \"\"\"\n    Executes the depth upscaling algorithm on a given model pool.\n\n    This method checks the type of the model pool, ensures that it contains only one model, and verifies that the model is an instance of `nn.ModuleList`.\n\n    Args:\n        modelpool (nn.ModuleList | ModelPool): The pool of models to upscale. Must contain only one model.\n\n    Returns:\n        nn.ModuleList: The upscaled model.\n\n    Raises:\n        AssertionError: If the model pool contains more than one model or if the model is not an instance of `nn.ModuleList`.\n        ValueError: If an invalid layer specification is provided in the configuration.\n    \"\"\"\n    # check the modelpool type\n    if isinstance(modelpool, BaseModelPool):\n        assert len(modelpool) == 1, \"DepthUpscaling only support one model\"\n        model = modelpool.load_model(modelpool.model_names[0])\n        assert isinstance(\n            model, nn.ModuleList\n        ), f\"The model should be a `nn.ModuleList`, but got {type(model)}\"\n    elif isinstance(modelpool, nn.ModuleList):\n        model = modelpool\n    else:\n        raise AssertionError(\n            f\"Invalid modelpool type: {type(modelpool)}. Expected `ModelPool` or `nn.ModuleList`.\"\n        )\n\n    # parse the layers\n    layer_indices = self.layer_indices\n    parsed_layer_indices = []\n    for layer in layer_indices:\n        if isinstance(layer, int):\n            parsed_layer_indices.append(layer)\n        elif isinstance(layer, str):\n            parsed_layer_indices.extend(eval(layer))\n        else:\n            raise ValueError(\"Invalid layer specification: {}\".format(layer))\n\n    # create a new model with the specified layers\n    new_model = nn.ModuleList(\n        [\n            deepcopy(model[i])\n            for i in tqdm(\n                parsed_layer_indices, desc=\"constructing depth-upscaled model\"\n            )\n        ]\n    )\n\n    return new_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.DepthUpscalingForLlama","title":"<code>DepthUpscalingForLlama</code>","text":"<p>               Bases: <code>DepthUpscalingAlgorithm</code></p> <p>Implements depth upscaling for Llama models.</p> <p>This class extends the DepthUpscalingAlgorithm to handle Llama models specifically. It supports saving the upscaled model to a specified path.</p> <p>Parameters:</p> <ul> <li> <code>layer_indices</code>               (<code>list</code>)           \u2013            <p>List of layer indices to upscale.</p> </li> <li> <code>model_save_path</code>               (<code>Optional[str]</code>)           \u2013            <p>Path to save the upscaled model.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments.</p> </li> </ul> Source code in <code>fusion_bench/method/depth_upscaling/depth_upscaling_for_llama.py</code> <pre><code>class DepthUpscalingForLlama(DepthUpscalingAlgorithm):\n    \"\"\"\n    Implements depth upscaling for Llama models.\n\n    This class extends the DepthUpscalingAlgorithm to handle Llama models specifically.\n    It supports saving the upscaled model to a specified path.\n\n    Args:\n        layer_indices (list): List of layer indices to upscale.\n        model_save_path (Optional[str]): Path to save the upscaled model.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n\n    def __init__(self, layer_indices: list, model_save_path: Optional[str], **kwargs):\n        if isinstance(model_save_path, str):\n            model_save_path = os.path.expanduser(model_save_path)\n        self.model_save_path = model_save_path\n        super().__init__(layer_indices, **kwargs)\n\n    @override\n    def run(self, modelpool: CausalLMPool):\n        \"\"\"\n        Executes the depth upscaling algorithm on a given model pool.\n\n        This method loads the pretrained model or the first model in the pool,\n        applies the depth upscaling algorithm, and updates the number of hidden layers in the model configuration.\n        If a save path is provided, it saves the upscaled model and tokenizer to the specified path.\n\n        Args:\n            modelpool (CausalLMPool): The pool of models to upscale.\n\n        Returns:\n            CausalLM: The upscaled model.\n        \"\"\"\n        if self.model_save_path is not None:\n            tokenizer = modelpool.load_tokenizer()\n\n        model: PreTrainedModel = modelpool.load_pretrained_or_first_model()\n        model.model.layers = super().run(model.model.layers)\n        model.config.num_hidden_layers = len(model.model.layers)\n\n        if self.model_save_path is not None:\n            with timeit_context(f\"Saving the model to {self.model_save_path}\"):\n                tokenizer.save_pretrained(self.model_save_path)\n                model.save_pretrained(self.model_save_path)\n        return model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.DepthUpscalingForLlama.run","title":"<code>run(modelpool)</code>","text":"<p>Executes the depth upscaling algorithm on a given model pool.</p> <p>This method loads the pretrained model or the first model in the pool, applies the depth upscaling algorithm, and updates the number of hidden layers in the model configuration. If a save path is provided, it saves the upscaled model and tokenizer to the specified path.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>CausalLMPool</code>)           \u2013            <p>The pool of models to upscale.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>CausalLM</code>          \u2013            <p>The upscaled model.</p> </li> </ul> Source code in <code>fusion_bench/method/depth_upscaling/depth_upscaling_for_llama.py</code> <pre><code>@override\ndef run(self, modelpool: CausalLMPool):\n    \"\"\"\n    Executes the depth upscaling algorithm on a given model pool.\n\n    This method loads the pretrained model or the first model in the pool,\n    applies the depth upscaling algorithm, and updates the number of hidden layers in the model configuration.\n    If a save path is provided, it saves the upscaled model and tokenizer to the specified path.\n\n    Args:\n        modelpool (CausalLMPool): The pool of models to upscale.\n\n    Returns:\n        CausalLM: The upscaled model.\n    \"\"\"\n    if self.model_save_path is not None:\n        tokenizer = modelpool.load_tokenizer()\n\n    model: PreTrainedModel = modelpool.load_pretrained_or_first_model()\n    model.model.layers = super().run(model.model.layers)\n    model.config.num_hidden_layers = len(model.model.layers)\n\n    if self.model_save_path is not None:\n        with timeit_context(f\"Saving the model to {self.model_save_path}\"):\n            tokenizer.save_pretrained(self.model_save_path)\n            model.save_pretrained(self.model_save_path)\n    return model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#model-recombination","title":"Model Recombination","text":""},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.ModelRecombinationAlgorithm","title":"<code>ModelRecombinationAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>Model recombination recombinates the layers of the given models, to create a new set of models.</p> Source code in <code>fusion_bench/method/model_recombination.py</code> <pre><code>@auto_register_config\nclass ModelRecombinationAlgorithm(BaseAlgorithm):\n    \"\"\"\n    Model recombination recombinates the layers of the given models, to create a new set of models.\n    \"\"\"\n\n    def __init__(self, return_modelpool: bool, **kwargs):\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(\n        self,\n        modelpool: BaseModelPool,\n        return_modelpool: bool = True,\n    ) -&gt; Union[nn.Module, BaseModelPool]:\n        \"\"\"\n        Executes the model recombination algorithm on a given model pool.\n\n        This method loads models from the model pool, determines their type, and applies the appropriate recombination method.\n        It then creates a new model pool with the recombined models. Depending on the `return_modelpool` flag, it either returns\n        the entire new model pool or just the first model from it.\n\n        - If the models in the model pool are of type `nn.ModuleList`, the recombination method `recombine_modellist` is used. Where each module in the list is shuffled across the models.\n        - If the models are of type `nn.ModuleDict`, the recombination method `recombine_modeldict` is used. Where each module in the dictionary is shuffled across the models.\n        - If the models are of type `nn.Module`, the recombination method `recombine_state_dict` is used. Where the state dictionaries of the models are shuffled across the models.\n\n        Args:\n            modelpool (BaseModelPool): The pool of models to recombine.\n            return_modelpool (bool, optional): Flag indicating whether to return the entire model pool or just the first model. Defaults to True. If this algorithm is initialized with config, the value of `return_modelpool` in the config will be used and this argument passed to the method will be ignored.\n\n        Returns:\n            Union[nn.Module, BaseModelPool]: The recombined model pool or the first model from the recombined pool, depending on the `return_modelpool` flag.\n\n        Raises:\n            ValueError: If the models in the model pool are of an unsupported type.\n        \"\"\"\n        # If the config has a return_modelpool flag, use that, otherwise use the argument\n        if self.config.get(\"return_modelpool\", None) is not None:\n            return_modelpool = self.config.return_modelpool\n        # check the modelpool type\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n\n        log.info(f\"Running model recombination algorithm with {len(modelpool)} models\")\n\n        # TODO: optimize the `recombine_*` functions, if `return_modelpool` is False, we don't need to create the new modelpool, just the first model\n        models = [modelpool.load_model(m) for m in modelpool.model_names]\n        if isinstance(models[0], nn.ModuleList):\n            new_models = recombine_modellist(models)\n        elif isinstance(models[0], nn.ModuleDict):\n            new_models = recombine_modeldict(models)\n        elif isinstance(models[0], nn.Module):\n            new_models = recombine_state_dict(models)\n        else:\n            raise ValueError(f\"Unsupported model type {type(models[0])}\")\n\n        new_modelpool = BaseModelPool(\n            {n: m for n, m in zip(modelpool.model_names, new_models)}\n        )\n        if return_modelpool:\n            return new_modelpool\n        else:\n            return new_modelpool.load_model(new_modelpool.model_names[0])\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.ModelRecombinationAlgorithm.run","title":"<code>run(modelpool, return_modelpool=True)</code>","text":"<p>Executes the model recombination algorithm on a given model pool.</p> <p>This method loads models from the model pool, determines their type, and applies the appropriate recombination method. It then creates a new model pool with the recombined models. Depending on the <code>return_modelpool</code> flag, it either returns the entire new model pool or just the first model from it.</p> <ul> <li>If the models in the model pool are of type <code>nn.ModuleList</code>, the recombination method <code>recombine_modellist</code> is used. Where each module in the list is shuffled across the models.</li> <li>If the models are of type <code>nn.ModuleDict</code>, the recombination method <code>recombine_modeldict</code> is used. Where each module in the dictionary is shuffled across the models.</li> <li>If the models are of type <code>nn.Module</code>, the recombination method <code>recombine_state_dict</code> is used. Where the state dictionaries of the models are shuffled across the models.</li> </ul> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool</code>)           \u2013            <p>The pool of models to recombine.</p> </li> <li> <code>return_modelpool</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Flag indicating whether to return the entire model pool or just the first model. Defaults to True. If this algorithm is initialized with config, the value of <code>return_modelpool</code> in the config will be used and this argument passed to the method will be ignored.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Module, BaseModelPool]</code>           \u2013            <p>Union[nn.Module, BaseModelPool]: The recombined model pool or the first model from the recombined pool, depending on the <code>return_modelpool</code> flag.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the models in the model pool are of an unsupported type.</p> </li> </ul> Source code in <code>fusion_bench/method/model_recombination.py</code> <pre><code>@torch.no_grad()\ndef run(\n    self,\n    modelpool: BaseModelPool,\n    return_modelpool: bool = True,\n) -&gt; Union[nn.Module, BaseModelPool]:\n    \"\"\"\n    Executes the model recombination algorithm on a given model pool.\n\n    This method loads models from the model pool, determines their type, and applies the appropriate recombination method.\n    It then creates a new model pool with the recombined models. Depending on the `return_modelpool` flag, it either returns\n    the entire new model pool or just the first model from it.\n\n    - If the models in the model pool are of type `nn.ModuleList`, the recombination method `recombine_modellist` is used. Where each module in the list is shuffled across the models.\n    - If the models are of type `nn.ModuleDict`, the recombination method `recombine_modeldict` is used. Where each module in the dictionary is shuffled across the models.\n    - If the models are of type `nn.Module`, the recombination method `recombine_state_dict` is used. Where the state dictionaries of the models are shuffled across the models.\n\n    Args:\n        modelpool (BaseModelPool): The pool of models to recombine.\n        return_modelpool (bool, optional): Flag indicating whether to return the entire model pool or just the first model. Defaults to True. If this algorithm is initialized with config, the value of `return_modelpool` in the config will be used and this argument passed to the method will be ignored.\n\n    Returns:\n        Union[nn.Module, BaseModelPool]: The recombined model pool or the first model from the recombined pool, depending on the `return_modelpool` flag.\n\n    Raises:\n        ValueError: If the models in the model pool are of an unsupported type.\n    \"\"\"\n    # If the config has a return_modelpool flag, use that, otherwise use the argument\n    if self.config.get(\"return_modelpool\", None) is not None:\n        return_modelpool = self.config.return_modelpool\n    # check the modelpool type\n    if not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(modelpool)\n\n    log.info(f\"Running model recombination algorithm with {len(modelpool)} models\")\n\n    # TODO: optimize the `recombine_*` functions, if `return_modelpool` is False, we don't need to create the new modelpool, just the first model\n    models = [modelpool.load_model(m) for m in modelpool.model_names]\n    if isinstance(models[0], nn.ModuleList):\n        new_models = recombine_modellist(models)\n    elif isinstance(models[0], nn.ModuleDict):\n        new_models = recombine_modeldict(models)\n    elif isinstance(models[0], nn.Module):\n        new_models = recombine_state_dict(models)\n    else:\n        raise ValueError(f\"Unsupported model type {type(models[0])}\")\n\n    new_modelpool = BaseModelPool(\n        {n: m for n, m in zip(modelpool.model_names, new_models)}\n    )\n    if return_modelpool:\n        return new_modelpool\n    else:\n        return new_modelpool.load_model(new_modelpool.model_names[0])\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.model_recombination.recombine_modellist","title":"<code>recombine_modellist(models)</code>","text":"Source code in <code>fusion_bench/method/model_recombination.py</code> <pre><code>def recombine_modellist(models: List[nn.ModuleList]):\n    num_models = len(models)\n    num_layers = len(models[0])\n\n    new_models = [[] for _ in range(num_models)]\n    for layer_idx in range(num_layers):\n        shuffled_layers = [m[layer_idx] for m in models]\n        random.shuffle(shuffled_layers)\n        for model_idx in range(num_models):\n            new_models[model_idx].append(shuffled_layers[model_idx])\n    new_models = [nn.ModuleList(m) for m in new_models]\n    return new_models\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.model_recombination.recombine_modeldict","title":"<code>recombine_modeldict(models)</code>","text":"Source code in <code>fusion_bench/method/model_recombination.py</code> <pre><code>def recombine_modeldict(models: List[nn.ModuleDict]):\n    num_models = len(models)\n\n    new_models = [{} for _ in range(num_models)]\n    for layer_name in models[0].keys():\n        shuffled_layers = [m[layer_name] for m in models]\n        random.shuffle(shuffled_layers)\n        for model_idx in range(num_models):\n            new_models[model_idx][layer_name] = shuffled_layers[model_idx]\n    new_models = [nn.ModuleDict(m) for m in new_models]\n    return new_models\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.model_recombination.recombine_state_dict","title":"<code>recombine_state_dict(models)</code>","text":"Source code in <code>fusion_bench/method/model_recombination.py</code> <pre><code>def recombine_state_dict(models: List[nn.Module]):\n    num_models = len(models)\n    state_dicts = [model.state_dict() for model in models]\n    new_state_dict = [{} for _ in range(num_models)]\n    for key in state_dicts[0].keys():\n        shuffled_layers = [state_dict[key] for state_dict in state_dicts]\n        random.shuffle(shuffled_layers)\n        for model_idx in range(num_models):\n            new_state_dict[model_idx][key] = shuffled_layers[model_idx]\n    for model_idx in range(num_models):\n        models[model_idx].load_state_dict(new_state_dict[model_idx])\n    return models\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#moe-based-mixing","title":"MoE-based Mixing","text":""},{"location":"api/fusion_bench.method/mixing/#moe-upscaling","title":"MoE Upscaling","text":""},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.MixtralUpscalingAlgorithm","title":"<code>MixtralUpscalingAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>This class is responsible for upscaling a model to a MixtralModel. It inherits from the ModelFusionAlgorithm class.</p> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_upcycling.py</code> <pre><code>@auto_register_config\nclass MixtralUpscalingAlgorithm(BaseAlgorithm):\n    \"\"\"\n    This class is responsible for upscaling a model to a MixtralModel.\n    It inherits from the ModelFusionAlgorithm class.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_experts: int,\n        experts_per_token: int,\n        save_checkpoint: str,\n        **kwargs: Any,\n    ):\n        \"\"\"\n        Initialize the MixtralUpscalingAlgorithm.\n\n        Args:\n            num_experts (int): The number of experts in the Mixtral model.\n            experts_per_token (int): The number of experts per token.\n            save_checkpoint (str): The path to save the checkpoint.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def _run(\n        self, modelpool: BaseModelPool | LlamaModel | MistralModel\n    ) -&gt; MixtralModel:\n        \"\"\"\n        Internal method to run the upscaling process.\n\n        Args:\n            modelpool (BaseModelPool | LlamaModel | MistralModel): The model to be upscaled.\n\n        Returns:\n            MixtralModel: The upscaled model.\n        \"\"\"\n        if isinstance(modelpool, BaseModelPool):\n            assert modelpool.has_pretrained, \"ModelPool must have pretrained model.\"\n            pretrained_model = modelpool.load_model(\"_pretrained_\")\n        elif isinstance(modelpool, (LlamaModel, MistralModel)):\n            pretrained_model = modelpool\n        else:\n            raise ValueError(\"Invalid modelpool type\")\n\n        mixtral_config = _convert_config_to_mixtral(\n            pretrained_model.config,\n            self.config.num_experts,\n            self.config.experts_per_token,\n        )\n\n        with ContextManagers([no_init_weights(True)]):\n            for _ in tqdm(range(1), desc=\"Initializing Mixtral model\"):\n                mixtral_model = MixtralModel(mixtral_config)\n        upscale_to_mixtral_model(pretrained_model, mixtral_model)\n\n        return mixtral_model\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool | LlamaModel | MistralModel) -&gt; MixtralModel:\n        \"\"\"\n        Runs the upscaling process.\n\n        Args:\n            modelpool (ModelPool | LlamaModel | MistralModel): The model to be upscaled.\n\n        Returns:\n            MixtralModel: The upscaled model.\n        \"\"\"\n        mixtral_model = self._run(modelpool)\n\n        if self.config.get(\"save_checkpoint\", None) is not None:\n            mixtral_model.save_pretrained(self.config.save_checkpoint)\n        return mixtral_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.MixtralUpscalingAlgorithm.__init__","title":"<code>__init__(num_experts, experts_per_token, save_checkpoint, **kwargs)</code>","text":"<p>Initialize the MixtralUpscalingAlgorithm.</p> <p>Parameters:</p> <ul> <li> <code>num_experts</code>               (<code>int</code>)           \u2013            <p>The number of experts in the Mixtral model.</p> </li> <li> <code>experts_per_token</code>               (<code>int</code>)           \u2013            <p>The number of experts per token.</p> </li> <li> <code>save_checkpoint</code>               (<code>str</code>)           \u2013            <p>The path to save the checkpoint.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments.</p> </li> </ul> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_upcycling.py</code> <pre><code>def __init__(\n    self,\n    num_experts: int,\n    experts_per_token: int,\n    save_checkpoint: str,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize the MixtralUpscalingAlgorithm.\n\n    Args:\n        num_experts (int): The number of experts in the Mixtral model.\n        experts_per_token (int): The number of experts per token.\n        save_checkpoint (str): The path to save the checkpoint.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.MixtralUpscalingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Runs the upscaling process.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>ModelPool | LlamaModel | MistralModel</code>)           \u2013            <p>The model to be upscaled.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MixtralModel</code> (              <code>MixtralModel</code> )          \u2013            <p>The upscaled model.</p> </li> </ul> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_upcycling.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool | LlamaModel | MistralModel) -&gt; MixtralModel:\n    \"\"\"\n    Runs the upscaling process.\n\n    Args:\n        modelpool (ModelPool | LlamaModel | MistralModel): The model to be upscaled.\n\n    Returns:\n        MixtralModel: The upscaled model.\n    \"\"\"\n    mixtral_model = self._run(modelpool)\n\n    if self.config.get(\"save_checkpoint\", None) is not None:\n        mixtral_model.save_pretrained(self.config.save_checkpoint)\n    return mixtral_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.MixtralForCausalLMUpscalingAlgorithm","title":"<code>MixtralForCausalLMUpscalingAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>This class is responsible for upscaling a model to a MixtralForCausalLM. It inherits from the ModelFusionAlgorithm class.</p> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_upcycling.py</code> <pre><code>@auto_register_config\nclass MixtralForCausalLMUpscalingAlgorithm(BaseAlgorithm):\n    \"\"\"\n    This class is responsible for upscaling a model to a MixtralForCausalLM.\n    It inherits from the ModelFusionAlgorithm class.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_experts: int,\n        experts_per_token: int,\n        save_checkpoint: str,\n        **kwargs: Any,\n    ):\n        \"\"\"\n        Initialize the MixtralForCausalLMUpscalingAlgorithm.\n\n        Args:\n            num_experts (int): The number of experts in the Mixtral model.\n            experts_per_token (int): The number of experts per token.\n            save_checkpoint (str): The path to save the checkpoint.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def _run(\n        self, modelpool: BaseModelPool | LlamaForCausalLM | MistralForCausalLM\n    ) -&gt; MixtralForCausalLM:\n        \"\"\"\n        Internal method to run the upscaling process.\n\n        Args:\n            modelpool (BaseModelPool | LlamaForCausalLM | MistralForCausalLM): The model to be upscaled.\n\n        Returns:\n            MixtralForCausalLM: The upscaled model.\n        \"\"\"\n        if isinstance(modelpool, BaseModelPool):\n            assert modelpool.has_pretrained, \"ModelPool must have pretrained model.\"\n            pretrained_model = modelpool.load_model(\"_pretrained_\")\n        elif isinstance(modelpool, (LlamaForCausalLM, MistralForCausalLM)):\n            pretrained_model = modelpool\n        else:\n            raise ValueError(\"Invalid modelpool type\")\n\n        mixtral_config = _convert_config_to_mixtral(\n            pretrained_model.config,\n            self.config.num_experts,\n            self.config.experts_per_token,\n        )\n\n        with ContextManagers([no_init_weights()]):\n            for _ in tqdm(range(1), desc=\"Initializing Mixtral model\"):\n                mixtral_model = MixtralForCausalLM(mixtral_config)\n        upscale_to_mixtral_for_causal_lm(pretrained_model, mixtral_model)\n\n        return mixtral_model\n\n    @torch.no_grad()\n    def run(\n        self, modelpool: BaseModelPool | LlamaForCausalLM | MistralForCausalLM\n    ) -&gt; MixtralForCausalLM:\n        \"\"\"\n        Runs the upscaling process.\n\n        Args:\n            modelpool (ModelPool | LlamaForCausalLM | MistralForCausalLM): The model to be upscaled.\n\n        Returns:\n            MixtralForCausalLM: The upscaled model.\n        \"\"\"\n        mixtral_model = self._run(modelpool)\n\n        if self.config.get(\"save_checkpoint\", None) is not None:\n            mixtral_model.save_pretrained(self.config.save_checkpoint)\n        return mixtral_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.MixtralForCausalLMUpscalingAlgorithm.__init__","title":"<code>__init__(num_experts, experts_per_token, save_checkpoint, **kwargs)</code>","text":"<p>Initialize the MixtralForCausalLMUpscalingAlgorithm.</p> <p>Parameters:</p> <ul> <li> <code>num_experts</code>               (<code>int</code>)           \u2013            <p>The number of experts in the Mixtral model.</p> </li> <li> <code>experts_per_token</code>               (<code>int</code>)           \u2013            <p>The number of experts per token.</p> </li> <li> <code>save_checkpoint</code>               (<code>str</code>)           \u2013            <p>The path to save the checkpoint.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments.</p> </li> </ul> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_upcycling.py</code> <pre><code>def __init__(\n    self,\n    num_experts: int,\n    experts_per_token: int,\n    save_checkpoint: str,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize the MixtralForCausalLMUpscalingAlgorithm.\n\n    Args:\n        num_experts (int): The number of experts in the Mixtral model.\n        experts_per_token (int): The number of experts per token.\n        save_checkpoint (str): The path to save the checkpoint.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.MixtralForCausalLMUpscalingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Runs the upscaling process.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>ModelPool | LlamaForCausalLM | MistralForCausalLM</code>)           \u2013            <p>The model to be upscaled.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MixtralForCausalLM</code> (              <code>MixtralForCausalLM</code> )          \u2013            <p>The upscaled model.</p> </li> </ul> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_upcycling.py</code> <pre><code>@torch.no_grad()\ndef run(\n    self, modelpool: BaseModelPool | LlamaForCausalLM | MistralForCausalLM\n) -&gt; MixtralForCausalLM:\n    \"\"\"\n    Runs the upscaling process.\n\n    Args:\n        modelpool (ModelPool | LlamaForCausalLM | MistralForCausalLM): The model to be upscaled.\n\n    Returns:\n        MixtralForCausalLM: The upscaled model.\n    \"\"\"\n    mixtral_model = self._run(modelpool)\n\n    if self.config.get(\"save_checkpoint\", None) is not None:\n        mixtral_model.save_pretrained(self.config.save_checkpoint)\n    return mixtral_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.MixtralMoEMergingAlgorithm","title":"<code>MixtralMoEMergingAlgorithm</code>","text":"<p>               Bases: <code>MixtralUpscalingAlgorithm</code></p> <p>This class is responsible for merging models into a MixtralModel.</p> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_merging.py</code> <pre><code>class MixtralMoEMergingAlgorithm(MixtralUpscalingAlgorithm):\n    \"\"\"\n    This class is responsible for merging models into a MixtralModel.\n    \"\"\"\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool) -&gt; MixtralModel:\n        \"\"\"\n        Runs the merging process.\n\n        Args:\n            modelpool (ModelPool): The pool of models to be merged. Each model in the pool will be treated as an expert, and should be a `MistralModel` or `LlamaModel`.\n\n        Returns:\n            MixtralModel: The merged model.\n        \"\"\"\n        with open_dict(self.config):\n            self.config.num_experts = len(modelpool)\n\n        # firstly, we upscale the models to MixtralModel\n        mixtral_model = super()._run(modelpool)\n\n        # then we substitute the experts of the MixtralModel with the models from the modelpool\n        for model_idx, model_name in enumerate(modelpool.model_names):\n            expert_model: MistralModel | LlamaModel = modelpool.load_model(model_name)\n            _substitute_experts(model_idx, expert_model, mixtral_model)\n\n        if self.config.get(\"save_checkpoint\", None) is not None:\n            mixtral_model.save_pretrained(self.config.save_checkpoint)\n        return mixtral_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.MixtralMoEMergingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Runs the merging process.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>ModelPool</code>)           \u2013            <p>The pool of models to be merged. Each model in the pool will be treated as an expert, and should be a <code>MistralModel</code> or <code>LlamaModel</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MixtralModel</code> (              <code>MixtralModel</code> )          \u2013            <p>The merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_merging.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool) -&gt; MixtralModel:\n    \"\"\"\n    Runs the merging process.\n\n    Args:\n        modelpool (ModelPool): The pool of models to be merged. Each model in the pool will be treated as an expert, and should be a `MistralModel` or `LlamaModel`.\n\n    Returns:\n        MixtralModel: The merged model.\n    \"\"\"\n    with open_dict(self.config):\n        self.config.num_experts = len(modelpool)\n\n    # firstly, we upscale the models to MixtralModel\n    mixtral_model = super()._run(modelpool)\n\n    # then we substitute the experts of the MixtralModel with the models from the modelpool\n    for model_idx, model_name in enumerate(modelpool.model_names):\n        expert_model: MistralModel | LlamaModel = modelpool.load_model(model_name)\n        _substitute_experts(model_idx, expert_model, mixtral_model)\n\n    if self.config.get(\"save_checkpoint\", None) is not None:\n        mixtral_model.save_pretrained(self.config.save_checkpoint)\n    return mixtral_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.MixtralForCausalLMMergingAlgorithm","title":"<code>MixtralForCausalLMMergingAlgorithm</code>","text":"<p>               Bases: <code>MixtralForCausalLMUpscalingAlgorithm</code></p> <p>This class is responsible for merging models into a <code>MixtralForCausalLM</code>.</p> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_merging.py</code> <pre><code>class MixtralForCausalLMMergingAlgorithm(MixtralForCausalLMUpscalingAlgorithm):\n    \"\"\"\n    This class is responsible for merging models into a `MixtralForCausalLM`.\n    \"\"\"\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool) -&gt; MixtralForCausalLM:\n        \"\"\"\n        Runs the merging process. It first upscales the models to MixtralForCausalLM,\n        then substitutes the experts of the MixtralForCausalLM with the models from the modelpool.\n\n        Args:\n            modelpool (ModelPool): The pool of models to be merged. Each model in the pool will be treated as an expert, and should be a `MistralForCausalLM` or `LlamaForCausalLM`.\n\n        Returns:\n            MixtralForCausalLM: The merged model.\n        \"\"\"\n        with open_dict(self.config):\n            self.config.num_experts = len(modelpool)\n\n        # firstly, we upscale the models to MixtralForCausalLM\n        mixtral_model = super()._run(modelpool)\n\n        # then we substitute the experts of the MixtralForCausalLM with the models from the modelpool\n        for model_idx, model_name in enumerate(modelpool.model_names):\n            expert_model: MistralForCausalLM | LlamaForCausalLM = modelpool.load_model(\n                model_name\n            )\n            _substitute_experts(model_idx, expert_model.model, mixtral_model.model)\n\n        if self.config.get(\"save_checkpoint\", None) is not None:\n            mixtral_model.save_pretrained(self.config.save_checkpoint)\n        return mixtral_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.MixtralForCausalLMMergingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Runs the merging process. It first upscales the models to MixtralForCausalLM, then substitutes the experts of the MixtralForCausalLM with the models from the modelpool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>ModelPool</code>)           \u2013            <p>The pool of models to be merged. Each model in the pool will be treated as an expert, and should be a <code>MistralForCausalLM</code> or <code>LlamaForCausalLM</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MixtralForCausalLM</code> (              <code>MixtralForCausalLM</code> )          \u2013            <p>The merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_merging.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool) -&gt; MixtralForCausalLM:\n    \"\"\"\n    Runs the merging process. It first upscales the models to MixtralForCausalLM,\n    then substitutes the experts of the MixtralForCausalLM with the models from the modelpool.\n\n    Args:\n        modelpool (ModelPool): The pool of models to be merged. Each model in the pool will be treated as an expert, and should be a `MistralForCausalLM` or `LlamaForCausalLM`.\n\n    Returns:\n        MixtralForCausalLM: The merged model.\n    \"\"\"\n    with open_dict(self.config):\n        self.config.num_experts = len(modelpool)\n\n    # firstly, we upscale the models to MixtralForCausalLM\n    mixtral_model = super()._run(modelpool)\n\n    # then we substitute the experts of the MixtralForCausalLM with the models from the modelpool\n    for model_idx, model_name in enumerate(modelpool.model_names):\n        expert_model: MistralForCausalLM | LlamaForCausalLM = modelpool.load_model(\n            model_name\n        )\n        _substitute_experts(model_idx, expert_model.model, mixtral_model.model)\n\n    if self.config.get(\"save_checkpoint\", None) is not None:\n        mixtral_model.save_pretrained(self.config.save_checkpoint)\n    return mixtral_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#weight-ensembling-mixture-of-experts-we-moe","title":"Weight-Ensembling Mixture of Experts (WE-MoE)","text":""},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.CLIPWeightEnsemblingMoEAlgorithm","title":"<code>CLIPWeightEnsemblingMoEAlgorithm</code>","text":"<p>               Bases: <code>WeightEnsemblingMoEAlgorithm</code>, <code>CLIPClassificationMixin</code></p> <p>CLIPWeightEnsemblingMoEAlgorithm is a class that implements the WeightEnsemblingMoEAlgorithm for CLIP models. It extends the WeightEnsemblingMoEAlgorithm and CLIPClassificationMixin classes.</p> <p>Attributes:</p> <ul> <li> <code>modelpool</code>               (<code>CLIPVisionModelPool</code>)           \u2013            <p>The model pool containing the CLIP models.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/clip_we_moe.py</code> <pre><code>class CLIPWeightEnsemblingMoEAlgorithm(\n    WeightEnsemblingMoEAlgorithm,\n    CLIPClassificationMixin,\n):\n    \"\"\"\n    CLIPWeightEnsemblingMoEAlgorithm is a class that implements the WeightEnsemblingMoEAlgorithm\n    for CLIP models. It extends the WeightEnsemblingMoEAlgorithm and CLIPClassificationMixin classes.\n\n    Attributes:\n        modelpool (CLIPVisionModelPool): The model pool containing the CLIP models.\n    \"\"\"\n\n    modelpool: CLIPVisionModelPool = None\n\n    def load_checkpoint(self, model: Any, checkpoint: Any):\n        \"\"\"\n        Load the checkpoint file.\n\n        Args:\n            model: The model to load the checkpoint into.\n            checkpoint: The path to the checkpoint file.\n        \"\"\"\n        state = {\"model\": model}\n        self._fabric.load(checkpoint, state)\n\n    def save_checkpoint(self, model: Any, checkpoint: Any):\n        \"\"\"\n        Save the checkpoint file.\n\n        Args:\n            model: The model to save the checkpoint from.\n            checkpoint: The path to the checkpoint file.\n        \"\"\"\n        self._fabric.save(checkpoint, {\"model\": model})\n\n    def construct_moe_model(self) -&gt; WeightEnsemblingMoE:\n        \"\"\"\n        Construct the Mixture of Experts (MoE) model using the models in the model pool.\n\n        Returns:\n            WeightEnsemblingMoE: The constructed MoE model.\n        \"\"\"\n        base_model = self.modelpool.load_model(\"_pretrained_\")\n        expert_models = [\n            self.modelpool.load_model(m) for m in self.modelpool.model_names\n        ]\n\n        # Merge the models using task arithmetic\n        moe_model = task_arithmetic_merge(\n            # This function modifies the model in place, so we need to pass a deepcopy\n            deepcopy(base_model),\n            expert_models,\n            scaling_factor=self.config.init_lambda,\n        ).requires_grad_(False)\n\n        # Up-scale MLP modules\n        base_encoder: CLIPEncoder = base_model.vision_model.encoder\n        moe_encoder: CLIPEncoder = moe_model.vision_model.encoder\n        expert_encoders = [m.vision_model.encoder for m in expert_models]\n\n        num_layers = len(base_encoder.layers)\n        for layer_idx in range(num_layers):\n            base_mlp = base_encoder.layers[layer_idx].mlp\n            expert_mlps = [e.layers[layer_idx].mlp for e in expert_encoders]\n\n            moe_encoder.layers[layer_idx].mlp = WeightEnsemblingMoE(\n                hidden_size=base_encoder.config.hidden_size,\n                base_model=base_mlp,\n                expert_models=expert_mlps,\n                init_lambda=self.config.init_lambda,\n                batch_first=True,  # For open_clip models this is False\n                router_hidden_layers=self.config.router_hidden_layers,\n                batch_reduce=self.config.batch_reduce,\n            )\n\n        return moe_model\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(self, tta_dataset: str) -&gt; Iterator:\n        \"\"\"\n        Get an iterator for the shuffled test data loader.\n\n        Args:\n            tta_dataset (str): The name of the test-time adaptation dataset.\n\n        Returns:\n            Iterator: An iterator for the shuffled test data loader.\n        \"\"\"\n        dataset = self.modelpool.load_test_dataset(tta_dataset)\n        dataset = CLIPDataset(dataset, processor=self.clip_processor)\n        log.info(\"get_shuffled_test_loader_iter\")\n        loader = DataLoader(\n            dataset,\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            num_workers=self.config.num_workers,\n            pin_memory=True,\n        )\n        loader = self.fabric.setup_dataloaders(loader)\n        return iter(InfiniteDataLoader(loader))\n\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Load the CLIP processor and construct the zero-shot classification head for each task.\n        \"\"\"\n        self.setup_zero_shot_classification_head()\n\n    def compute_logits(self, module: Any, batch: Any, task: Any) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for the given batch and task.\n\n        Args:\n            module: The model module.\n            batch: The input batch.\n            task: The task name.\n\n        Returns:\n            Tensor: The computed logits.\n        \"\"\"\n        images, _ = batch\n        text_embeds = self.zeroshot_weights[task]\n\n        image_embeds = module(images)[1]\n        image_embeds = self.visual_projection(image_embeds)\n\n        # Normalize embeddings\n        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n        # Cosine similarity\n        logits_per_text = (\n            torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n        )\n        logits_per_image = logits_per_text.t()\n\n        return logits_per_image\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.CLIPWeightEnsemblingMoEAlgorithm.compute_logits","title":"<code>compute_logits(module, batch, task)</code>","text":"<p>Compute the logits for the given batch and task.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Any</code>)           \u2013            <p>The model module.</p> </li> <li> <code>batch</code>               (<code>Any</code>)           \u2013            <p>The input batch.</p> </li> <li> <code>task</code>               (<code>Any</code>)           \u2013            <p>The task name.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The computed logits.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/clip_we_moe.py</code> <pre><code>def compute_logits(self, module: Any, batch: Any, task: Any) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for the given batch and task.\n\n    Args:\n        module: The model module.\n        batch: The input batch.\n        task: The task name.\n\n    Returns:\n        Tensor: The computed logits.\n    \"\"\"\n    images, _ = batch\n    text_embeds = self.zeroshot_weights[task]\n\n    image_embeds = module(images)[1]\n    image_embeds = self.visual_projection(image_embeds)\n\n    # Normalize embeddings\n    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n    # Cosine similarity\n    logits_per_text = (\n        torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n    )\n    logits_per_image = logits_per_text.t()\n\n    return logits_per_image\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.CLIPWeightEnsemblingMoEAlgorithm.construct_moe_model","title":"<code>construct_moe_model()</code>","text":"<p>Construct the Mixture of Experts (MoE) model using the models in the model pool.</p> <p>Returns:</p> <ul> <li> <code>WeightEnsemblingMoE</code> (              <code>WeightEnsemblingMoE</code> )          \u2013            <p>The constructed MoE model.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/clip_we_moe.py</code> <pre><code>def construct_moe_model(self) -&gt; WeightEnsemblingMoE:\n    \"\"\"\n    Construct the Mixture of Experts (MoE) model using the models in the model pool.\n\n    Returns:\n        WeightEnsemblingMoE: The constructed MoE model.\n    \"\"\"\n    base_model = self.modelpool.load_model(\"_pretrained_\")\n    expert_models = [\n        self.modelpool.load_model(m) for m in self.modelpool.model_names\n    ]\n\n    # Merge the models using task arithmetic\n    moe_model = task_arithmetic_merge(\n        # This function modifies the model in place, so we need to pass a deepcopy\n        deepcopy(base_model),\n        expert_models,\n        scaling_factor=self.config.init_lambda,\n    ).requires_grad_(False)\n\n    # Up-scale MLP modules\n    base_encoder: CLIPEncoder = base_model.vision_model.encoder\n    moe_encoder: CLIPEncoder = moe_model.vision_model.encoder\n    expert_encoders = [m.vision_model.encoder for m in expert_models]\n\n    num_layers = len(base_encoder.layers)\n    for layer_idx in range(num_layers):\n        base_mlp = base_encoder.layers[layer_idx].mlp\n        expert_mlps = [e.layers[layer_idx].mlp for e in expert_encoders]\n\n        moe_encoder.layers[layer_idx].mlp = WeightEnsemblingMoE(\n            hidden_size=base_encoder.config.hidden_size,\n            base_model=base_mlp,\n            expert_models=expert_mlps,\n            init_lambda=self.config.init_lambda,\n            batch_first=True,  # For open_clip models this is False\n            router_hidden_layers=self.config.router_hidden_layers,\n            batch_reduce=self.config.batch_reduce,\n        )\n\n    return moe_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.CLIPWeightEnsemblingMoEAlgorithm.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(tta_dataset)</code>  <code>cached</code>","text":"<p>Get an iterator for the shuffled test data loader.</p> <p>Parameters:</p> <ul> <li> <code>tta_dataset</code>               (<code>str</code>)           \u2013            <p>The name of the test-time adaptation dataset.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator</code> (              <code>Iterator</code> )          \u2013            <p>An iterator for the shuffled test data loader.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/clip_we_moe.py</code> <pre><code>@functools.cache\ndef get_shuffled_test_loader_iter(self, tta_dataset: str) -&gt; Iterator:\n    \"\"\"\n    Get an iterator for the shuffled test data loader.\n\n    Args:\n        tta_dataset (str): The name of the test-time adaptation dataset.\n\n    Returns:\n        Iterator: An iterator for the shuffled test data loader.\n    \"\"\"\n    dataset = self.modelpool.load_test_dataset(tta_dataset)\n    dataset = CLIPDataset(dataset, processor=self.clip_processor)\n    log.info(\"get_shuffled_test_loader_iter\")\n    loader = DataLoader(\n        dataset,\n        batch_size=self.config.batch_size,\n        shuffle=True,\n        num_workers=self.config.num_workers,\n        pin_memory=True,\n    )\n    loader = self.fabric.setup_dataloaders(loader)\n    return iter(InfiniteDataLoader(loader))\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.CLIPWeightEnsemblingMoEAlgorithm.load_checkpoint","title":"<code>load_checkpoint(model, checkpoint)</code>","text":"<p>Load the checkpoint file.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Any</code>)           \u2013            <p>The model to load the checkpoint into.</p> </li> <li> <code>checkpoint</code>               (<code>Any</code>)           \u2013            <p>The path to the checkpoint file.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/clip_we_moe.py</code> <pre><code>def load_checkpoint(self, model: Any, checkpoint: Any):\n    \"\"\"\n    Load the checkpoint file.\n\n    Args:\n        model: The model to load the checkpoint into.\n        checkpoint: The path to the checkpoint file.\n    \"\"\"\n    state = {\"model\": model}\n    self._fabric.load(checkpoint, state)\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.CLIPWeightEnsemblingMoEAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Load the CLIP processor and construct the zero-shot classification head for each task.</p> Source code in <code>fusion_bench/method/we_moe/clip_we_moe.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Load the CLIP processor and construct the zero-shot classification head for each task.\n    \"\"\"\n    self.setup_zero_shot_classification_head()\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.CLIPWeightEnsemblingMoEAlgorithm.save_checkpoint","title":"<code>save_checkpoint(model, checkpoint)</code>","text":"<p>Save the checkpoint file.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Any</code>)           \u2013            <p>The model to save the checkpoint from.</p> </li> <li> <code>checkpoint</code>               (<code>Any</code>)           \u2013            <p>The path to the checkpoint file.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/clip_we_moe.py</code> <pre><code>def save_checkpoint(self, model: Any, checkpoint: Any):\n    \"\"\"\n    Save the checkpoint file.\n\n    Args:\n        model: The model to save the checkpoint from.\n        checkpoint: The path to the checkpoint file.\n    \"\"\"\n    self._fabric.save(checkpoint, {\"model\": model})\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#sparse-we-moe","title":"Sparse WE-MoE","text":""},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseWeightEnsemblingMoEAlgorithm","title":"<code>SparseWeightEnsemblingMoEAlgorithm</code>","text":"<p>               Bases: <code>ModelFusionAlgorithm</code></p> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_we_moe.py</code> <pre><code>class SparseWeightEnsemblingMoEAlgorithm(ModelFusionAlgorithm):\n    _fabric: L.Fabric = None\n    modelpool: BaseModelPool = None\n\n    def __init__(self, algorithm_config: DictConfig):\n        \"\"\"\n        Initialize the SparseWeightEnsemblingMoEAlgorithm with the given configuration.\n\n        Args:\n            algorithm_config (DictConfig): The configuration for the algorithm.\n        \"\"\"\n        super().__init__(algorithm_config)\n\n        self.profiler = SimpleProfiler(\n            self.config.get(\"cache_dir\", \"outputs\"), \"we_moe_profiler.txt\"\n        )\n\n    @abstractmethod\n    def load_checkpoint(self, model, checkpoint):\n        \"\"\"\n        Load the checkpoint file.\n\n        Args:\n            model (nn.Module): The model to load the checkpoint into.\n            checkpoint (str): The path to the checkpoint file.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def save_checkpoint(self, model, checkpoint):\n        \"\"\"\n        Save the checkpoint file.\n\n        Args:\n            model (nn.Module): The model to save the checkpoint from.\n            checkpoint (str): The path to the checkpoint file.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def construct_moe_model(self) -&gt; SparseWeightEnsemblingMoE:\n        \"\"\"\n        Construct the Mixture of Experts model using the models in the model pool.\n\n        Returns:\n            SparseWeightEnsemblingMoE: The constructed Mixture of Experts model.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def construct_moe_model_sharedgate(self) -&gt; SparseWeightEnsemblingMoE_ShardGate:\n        \"\"\"\n        Construct the Mixture of Experts model using the models in the model pool.\n\n        Returns:\n            SparseWeightEnsemblingMoE_ShardGate: The constructed Mixture of Experts model with shared gate.\n        \"\"\"\n        pass\n\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Hook that is called at the start of test-time adaptation.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n        \"\"\"\n        Get an iterator for the shuffled test DataLoader for a specific task.\n\n        Args:\n            task (str): The task for which to get the DataLoader iterator.\n\n        Returns:\n            DataLoader: The DataLoader iterator for the specified task.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def compute_logits(self, module, batch, task) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for a given batch and task.\n\n        Args:\n            module (nn.Module): The model module.\n            batch (Any): The input batch.\n            task (str): The task for which to compute the logits.\n\n        Returns:\n            Tensor: The computed logits.\n        \"\"\"\n        pass\n\n    def dynamic_prune(self, module, prune_ratio):\n        \"\"\"\n        Dynamically prune the parameters of a module based on the given prune ratio.\n\n        Args:\n            module (nn.Module): The module to prune.\n            prune_ratio (float): The ratio of parameters to prune.\n        \"\"\"\n        for param in module.parameters():\n            if param.requires_grad:\n                param.data = _magnitude_prune(param, prune_ratio)\n\n    def l1_regularization(self, module, l1_lambda):\n        \"\"\"\n        Compute the L1 regularization loss for a module.\n\n        Args:\n            module (nn.Module): The module for which to compute the L1 regularization loss.\n            l1_lambda (float): The L1 regularization coefficient.\n\n        Returns:\n            Tensor: The L1 regularization loss.\n        \"\"\"\n        l1_norm = sum(\n            param.abs().sum() for param in module.parameters() if param.requires_grad\n        )\n        return l1_lambda * l1_norm\n\n    def test_time_adaptation(self, module: SparseWeightEnsemblingMoE):\n        \"\"\"\n        Perform test-time adaptation for the given module.\n\n        Args:\n            module (SparseWeightEnsemblingMoE): The module to adapt.\n\n        Returns:\n            SparseWeightEnsemblingMoE: The adapted module.\n        \"\"\"\n        self.on_test_time_adaptation_start()\n\n        # configure optimizer\n        if self.config.optimizer == \"adam\":\n            optimizer = torch.optim.Adam(\n                [p for p in module.parameters() if p.requires_grad], lr=self.config.lr\n            )\n        else:\n            raise ValueError(f\"Unsupported optimizer: {self.config.optimizer}\")\n\n        if self._fabric is not None:\n            module, optimizer = self._fabric.setup(module, optimizer)\n\n        module.train()\n\n        if self.config.get(\"fast_dev_run\", False):\n            log.info(\"Running fast_dev_run, only one step\")\n            pbar = tqdm(\n                range(1),\n                \"Test-time adaptation\",\n                dynamic_ncols=True,\n            )\n        else:\n            pbar = tqdm(\n                range(self.config.max_steps),\n                \"Test-time adaptation\",\n                dynamic_ncols=True,\n            )\n\n        for step_idx in pbar:\n            if self.config.use_grad_accumulate:\n                for task in self.modelpool.model_names:\n                    with self.profiler.profile(\"data time\"):\n                        batch = next(self.get_shuffled_test_loader_iter(task))\n                    with self.profiler.profile(\"forward pass\"):\n                        logits = self.compute_logits(module, batch, task)\n                        assert (\n                            logits.dim() == 2\n                        ), f\"Expected logits to be 2D, got {logits.dim()}\"\n                        loss = entropy_loss(logits)\n                    # .backward() accumulates when .zero_grad() wasn't called\n                    # this can save memory\n                    with self.profiler.profile(\"backward pass\"):\n                        self._fabric.backward(loss, retain_graph=True)\n            else:\n                loss = 0\n                for task in self.modelpool.model_names:\n                    with self.profiler.profile(\"data time\"):\n                        batch = next(self.get_shuffled_test_loader_iter(task))\n                    with self.profiler.profile(\"forward pass\"):\n                        logits = self.compute_logits(module, batch, task)\n                        assert (\n                            logits.dim() == 2\n                        ), f\"Expected logits to be 2D, got {logits.dim()}\"\n                        loss = loss + entropy_loss(logits)\n\n                with self.profiler.profile(\"backward pass\"):\n                    self._fabric.backward(loss, retain_graph=True)\n\n            with self.profiler.profile(\"optimizer step\"):\n                optimizer.step()\n                optimizer.zero_grad()\n\n        return module\n\n    def construct_post_spare_gate_model(self, moe_model, gate_prune_ratio):\n        \"\"\"\n        Construct a (post) sparse gated model.\n\n        Args:\n            moe_model (SparseWeightEnsemblingMoE): The Mixture of Experts model.\n            gate_prune_ratio (float): The ratio of parameters to prune in the gate.\n\n        Returns:\n            SparseWeightEnsemblingMoE: The constructed (post) sparse gated model.\n        \"\"\"\n        moe_encoder = moe_model.vision_model.encoder\n        num_layers = len(moe_encoder.layers)\n        for layer_idx in range(num_layers):\n            gate = moe_encoder.layers[layer_idx].mlp.gate\n            sparse_gate = _module_magnitude_prune(gate, gate_prune_ratio, layer_idx)\n            moe_encoder.layers[layer_idx].mlp.gate = sparse_gate\n        return moe_model\n\n    def run(self, modelpool: BaseModelPool):\n        \"\"\"\n        Run the SparseWeightEnsemblingMoEAlgorithm with the given model pool.\n\n        Args:\n            modelpool (BaseModelPool): The model pool to use for the algorithm.\n\n        Returns:\n            SparseWeightEnsemblingMoE: The final Mixture of Experts model.\n        \"\"\"\n        log.info(\"Fusing models using WeightEnsembling Mixture of Experts modules.\")\n        self.modelpool = modelpool\n\n        with timeit_context(\"upscaling models to a weight-ensembling MoE model\"):\n            if self.config.shared_gate:\n                moe_model = self.construct_moe_model_sharedgate()\n            else:\n                moe_model = self.construct_moe_model()\n            print_parameters(moe_model)\n\n        if self.config.get(\"checkpoint\", False):\n            log.info(\n                f\"load checkpoint from {self.config.checkpoint}, test-time adaptation will be skipped.\"\n            )\n            self.load_checkpoint(moe_model, self.config.checkpoint)\n        else:\n            with self.profiler.profile(\"test-time adaptation\"):\n                moe_model = self.test_time_adaptation(moe_model)\n            if self.config.get(\"save_checkpoint\", False):\n                log.info(f\"save checkpoint to {self.config.save_checkpoint}\")\n                self.save_checkpoint(moe_model, self.config.save_checkpoint)\n\n            if lightning.fabric.wrappers.is_wrapped(moe_model):\n                moe_model = lightning.fabric.wrappers._unwrap_objects(moe_model)\n\n        #  (post) sparse gate model\n        if self.config.post_sparse_gate:\n            moe_model = self.construct_post_spare_gate_model(\n                moe_model, self.config.gate_prune_ratio\n            )\n\n        # enable sample-wise adaptation\n        moe_model.batch_reduce = False\n        print(self.profiler.summary())\n        return moe_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseWeightEnsemblingMoEAlgorithm.__init__","title":"<code>__init__(algorithm_config)</code>","text":"<p>Initialize the SparseWeightEnsemblingMoEAlgorithm with the given configuration.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_config</code>               (<code>DictConfig</code>)           \u2013            <p>The configuration for the algorithm.</p> </li> </ul> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_we_moe.py</code> <pre><code>def __init__(self, algorithm_config: DictConfig):\n    \"\"\"\n    Initialize the SparseWeightEnsemblingMoEAlgorithm with the given configuration.\n\n    Args:\n        algorithm_config (DictConfig): The configuration for the algorithm.\n    \"\"\"\n    super().__init__(algorithm_config)\n\n    self.profiler = SimpleProfiler(\n        self.config.get(\"cache_dir\", \"outputs\"), \"we_moe_profiler.txt\"\n    )\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseWeightEnsemblingMoEAlgorithm.compute_logits","title":"<code>compute_logits(module, batch, task)</code>  <code>abstractmethod</code>","text":"<p>Compute the logits for a given batch and task.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The model module.</p> </li> <li> <code>batch</code>               (<code>Any</code>)           \u2013            <p>The input batch.</p> </li> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The task for which to compute the logits.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The computed logits.</p> </li> </ul> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_we_moe.py</code> <pre><code>@abstractmethod\ndef compute_logits(self, module, batch, task) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for a given batch and task.\n\n    Args:\n        module (nn.Module): The model module.\n        batch (Any): The input batch.\n        task (str): The task for which to compute the logits.\n\n    Returns:\n        Tensor: The computed logits.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseWeightEnsemblingMoEAlgorithm.construct_moe_model","title":"<code>construct_moe_model()</code>  <code>abstractmethod</code>","text":"<p>Construct the Mixture of Experts model using the models in the model pool.</p> <p>Returns:</p> <ul> <li> <code>SparseWeightEnsemblingMoE</code> (              <code>SparseWeightEnsemblingMoE</code> )          \u2013            <p>The constructed Mixture of Experts model.</p> </li> </ul> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_we_moe.py</code> <pre><code>@abstractmethod\ndef construct_moe_model(self) -&gt; SparseWeightEnsemblingMoE:\n    \"\"\"\n    Construct the Mixture of Experts model using the models in the model pool.\n\n    Returns:\n        SparseWeightEnsemblingMoE: The constructed Mixture of Experts model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseWeightEnsemblingMoEAlgorithm.construct_moe_model_sharedgate","title":"<code>construct_moe_model_sharedgate()</code>  <code>abstractmethod</code>","text":"<p>Construct the Mixture of Experts model using the models in the model pool.</p> <p>Returns:</p> <ul> <li> <code>SparseWeightEnsemblingMoE_ShardGate</code> (              <code>SparseWeightEnsemblingMoE_ShardGate</code> )          \u2013            <p>The constructed Mixture of Experts model with shared gate.</p> </li> </ul> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_we_moe.py</code> <pre><code>@abstractmethod\ndef construct_moe_model_sharedgate(self) -&gt; SparseWeightEnsemblingMoE_ShardGate:\n    \"\"\"\n    Construct the Mixture of Experts model using the models in the model pool.\n\n    Returns:\n        SparseWeightEnsemblingMoE_ShardGate: The constructed Mixture of Experts model with shared gate.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseWeightEnsemblingMoEAlgorithm.construct_post_spare_gate_model","title":"<code>construct_post_spare_gate_model(moe_model, gate_prune_ratio)</code>","text":"<p>Construct a (post) sparse gated model.</p> <p>Parameters:</p> <ul> <li> <code>moe_model</code>               (<code>SparseWeightEnsemblingMoE</code>)           \u2013            <p>The Mixture of Experts model.</p> </li> <li> <code>gate_prune_ratio</code>               (<code>float</code>)           \u2013            <p>The ratio of parameters to prune in the gate.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SparseWeightEnsemblingMoE</code>          \u2013            <p>The constructed (post) sparse gated model.</p> </li> </ul> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_we_moe.py</code> <pre><code>def construct_post_spare_gate_model(self, moe_model, gate_prune_ratio):\n    \"\"\"\n    Construct a (post) sparse gated model.\n\n    Args:\n        moe_model (SparseWeightEnsemblingMoE): The Mixture of Experts model.\n        gate_prune_ratio (float): The ratio of parameters to prune in the gate.\n\n    Returns:\n        SparseWeightEnsemblingMoE: The constructed (post) sparse gated model.\n    \"\"\"\n    moe_encoder = moe_model.vision_model.encoder\n    num_layers = len(moe_encoder.layers)\n    for layer_idx in range(num_layers):\n        gate = moe_encoder.layers[layer_idx].mlp.gate\n        sparse_gate = _module_magnitude_prune(gate, gate_prune_ratio, layer_idx)\n        moe_encoder.layers[layer_idx].mlp.gate = sparse_gate\n    return moe_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseWeightEnsemblingMoEAlgorithm.dynamic_prune","title":"<code>dynamic_prune(module, prune_ratio)</code>","text":"<p>Dynamically prune the parameters of a module based on the given prune ratio.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The module to prune.</p> </li> <li> <code>prune_ratio</code>               (<code>float</code>)           \u2013            <p>The ratio of parameters to prune.</p> </li> </ul> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_we_moe.py</code> <pre><code>def dynamic_prune(self, module, prune_ratio):\n    \"\"\"\n    Dynamically prune the parameters of a module based on the given prune ratio.\n\n    Args:\n        module (nn.Module): The module to prune.\n        prune_ratio (float): The ratio of parameters to prune.\n    \"\"\"\n    for param in module.parameters():\n        if param.requires_grad:\n            param.data = _magnitude_prune(param, prune_ratio)\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseWeightEnsemblingMoEAlgorithm.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(task)</code>  <code>abstractmethod</code>","text":"<p>Get an iterator for the shuffled test DataLoader for a specific task.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The task for which to get the DataLoader iterator.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code> (              <code>DataLoader</code> )          \u2013            <p>The DataLoader iterator for the specified task.</p> </li> </ul> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_we_moe.py</code> <pre><code>@abstractmethod\ndef get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n    \"\"\"\n    Get an iterator for the shuffled test DataLoader for a specific task.\n\n    Args:\n        task (str): The task for which to get the DataLoader iterator.\n\n    Returns:\n        DataLoader: The DataLoader iterator for the specified task.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseWeightEnsemblingMoEAlgorithm.l1_regularization","title":"<code>l1_regularization(module, l1_lambda)</code>","text":"<p>Compute the L1 regularization loss for a module.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The module for which to compute the L1 regularization loss.</p> </li> <li> <code>l1_lambda</code>               (<code>float</code>)           \u2013            <p>The L1 regularization coefficient.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>          \u2013            <p>The L1 regularization loss.</p> </li> </ul> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_we_moe.py</code> <pre><code>def l1_regularization(self, module, l1_lambda):\n    \"\"\"\n    Compute the L1 regularization loss for a module.\n\n    Args:\n        module (nn.Module): The module for which to compute the L1 regularization loss.\n        l1_lambda (float): The L1 regularization coefficient.\n\n    Returns:\n        Tensor: The L1 regularization loss.\n    \"\"\"\n    l1_norm = sum(\n        param.abs().sum() for param in module.parameters() if param.requires_grad\n    )\n    return l1_lambda * l1_norm\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseWeightEnsemblingMoEAlgorithm.load_checkpoint","title":"<code>load_checkpoint(model, checkpoint)</code>  <code>abstractmethod</code>","text":"<p>Load the checkpoint file.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to load the checkpoint into.</p> </li> <li> <code>checkpoint</code>               (<code>str</code>)           \u2013            <p>The path to the checkpoint file.</p> </li> </ul> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_we_moe.py</code> <pre><code>@abstractmethod\ndef load_checkpoint(self, model, checkpoint):\n    \"\"\"\n    Load the checkpoint file.\n\n    Args:\n        model (nn.Module): The model to load the checkpoint into.\n        checkpoint (str): The path to the checkpoint file.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseWeightEnsemblingMoEAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Hook that is called at the start of test-time adaptation.</p> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_we_moe.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Hook that is called at the start of test-time adaptation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseWeightEnsemblingMoEAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Run the SparseWeightEnsemblingMoEAlgorithm with the given model pool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool</code>)           \u2013            <p>The model pool to use for the algorithm.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SparseWeightEnsemblingMoE</code>          \u2013            <p>The final Mixture of Experts model.</p> </li> </ul> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_we_moe.py</code> <pre><code>def run(self, modelpool: BaseModelPool):\n    \"\"\"\n    Run the SparseWeightEnsemblingMoEAlgorithm with the given model pool.\n\n    Args:\n        modelpool (BaseModelPool): The model pool to use for the algorithm.\n\n    Returns:\n        SparseWeightEnsemblingMoE: The final Mixture of Experts model.\n    \"\"\"\n    log.info(\"Fusing models using WeightEnsembling Mixture of Experts modules.\")\n    self.modelpool = modelpool\n\n    with timeit_context(\"upscaling models to a weight-ensembling MoE model\"):\n        if self.config.shared_gate:\n            moe_model = self.construct_moe_model_sharedgate()\n        else:\n            moe_model = self.construct_moe_model()\n        print_parameters(moe_model)\n\n    if self.config.get(\"checkpoint\", False):\n        log.info(\n            f\"load checkpoint from {self.config.checkpoint}, test-time adaptation will be skipped.\"\n        )\n        self.load_checkpoint(moe_model, self.config.checkpoint)\n    else:\n        with self.profiler.profile(\"test-time adaptation\"):\n            moe_model = self.test_time_adaptation(moe_model)\n        if self.config.get(\"save_checkpoint\", False):\n            log.info(f\"save checkpoint to {self.config.save_checkpoint}\")\n            self.save_checkpoint(moe_model, self.config.save_checkpoint)\n\n        if lightning.fabric.wrappers.is_wrapped(moe_model):\n            moe_model = lightning.fabric.wrappers._unwrap_objects(moe_model)\n\n    #  (post) sparse gate model\n    if self.config.post_sparse_gate:\n        moe_model = self.construct_post_spare_gate_model(\n            moe_model, self.config.gate_prune_ratio\n        )\n\n    # enable sample-wise adaptation\n    moe_model.batch_reduce = False\n    print(self.profiler.summary())\n    return moe_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseWeightEnsemblingMoEAlgorithm.save_checkpoint","title":"<code>save_checkpoint(model, checkpoint)</code>  <code>abstractmethod</code>","text":"<p>Save the checkpoint file.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to save the checkpoint from.</p> </li> <li> <code>checkpoint</code>               (<code>str</code>)           \u2013            <p>The path to the checkpoint file.</p> </li> </ul> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_we_moe.py</code> <pre><code>@abstractmethod\ndef save_checkpoint(self, model, checkpoint):\n    \"\"\"\n    Save the checkpoint file.\n\n    Args:\n        model (nn.Module): The model to save the checkpoint from.\n        checkpoint (str): The path to the checkpoint file.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseWeightEnsemblingMoEAlgorithm.test_time_adaptation","title":"<code>test_time_adaptation(module)</code>","text":"<p>Perform test-time adaptation for the given module.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>SparseWeightEnsemblingMoE</code>)           \u2013            <p>The module to adapt.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SparseWeightEnsemblingMoE</code>          \u2013            <p>The adapted module.</p> </li> </ul> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_we_moe.py</code> <pre><code>def test_time_adaptation(self, module: SparseWeightEnsemblingMoE):\n    \"\"\"\n    Perform test-time adaptation for the given module.\n\n    Args:\n        module (SparseWeightEnsemblingMoE): The module to adapt.\n\n    Returns:\n        SparseWeightEnsemblingMoE: The adapted module.\n    \"\"\"\n    self.on_test_time_adaptation_start()\n\n    # configure optimizer\n    if self.config.optimizer == \"adam\":\n        optimizer = torch.optim.Adam(\n            [p for p in module.parameters() if p.requires_grad], lr=self.config.lr\n        )\n    else:\n        raise ValueError(f\"Unsupported optimizer: {self.config.optimizer}\")\n\n    if self._fabric is not None:\n        module, optimizer = self._fabric.setup(module, optimizer)\n\n    module.train()\n\n    if self.config.get(\"fast_dev_run\", False):\n        log.info(\"Running fast_dev_run, only one step\")\n        pbar = tqdm(\n            range(1),\n            \"Test-time adaptation\",\n            dynamic_ncols=True,\n        )\n    else:\n        pbar = tqdm(\n            range(self.config.max_steps),\n            \"Test-time adaptation\",\n            dynamic_ncols=True,\n        )\n\n    for step_idx in pbar:\n        if self.config.use_grad_accumulate:\n            for task in self.modelpool.model_names:\n                with self.profiler.profile(\"data time\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                with self.profiler.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, batch, task)\n                    assert (\n                        logits.dim() == 2\n                    ), f\"Expected logits to be 2D, got {logits.dim()}\"\n                    loss = entropy_loss(logits)\n                # .backward() accumulates when .zero_grad() wasn't called\n                # this can save memory\n                with self.profiler.profile(\"backward pass\"):\n                    self._fabric.backward(loss, retain_graph=True)\n        else:\n            loss = 0\n            for task in self.modelpool.model_names:\n                with self.profiler.profile(\"data time\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                with self.profiler.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, batch, task)\n                    assert (\n                        logits.dim() == 2\n                    ), f\"Expected logits to be 2D, got {logits.dim()}\"\n                    loss = loss + entropy_loss(logits)\n\n            with self.profiler.profile(\"backward pass\"):\n                self._fabric.backward(loss, retain_graph=True)\n\n        with self.profiler.profile(\"optimizer step\"):\n            optimizer.step()\n            optimizer.zero_grad()\n\n    return module\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseCLIPWeightEnsemblingMoEAlgorithm","title":"<code>SparseCLIPWeightEnsemblingMoEAlgorithm</code>","text":"<p>               Bases: <code>SparseWeightEnsemblingMoEAlgorithm</code>, <code>CLIPClassificationMixin</code></p> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_clip_we_moe.py</code> <pre><code>class SparseCLIPWeightEnsemblingMoEAlgorithm(\n    SparseWeightEnsemblingMoEAlgorithm,\n    CLIPClassificationMixin,\n):\n    modelpool: CLIPVisionModelPool = None\n\n    def load_checkpoint(self, model, checkpoint):\n        \"\"\"\n        Load the checkpoint file.\n        \"\"\"\n        state = {\"model\": model}\n        self._fabric.load(checkpoint, state)\n\n    def save_checkpoint(self, model, checkpoint):\n        \"\"\"\n        Save the checkpoint file.\n        \"\"\"\n        self._fabric.save(checkpoint, {\"model\": model})\n\n    def construct_moe_model(self) -&gt; SparseWeightEnsemblingMoE:\n        \"\"\"\n        Construct the Mixture of Experts model using the models in the model pool.\n        \"\"\"\n        base_model = self.modelpool.load_model(\"_pretrained_\")\n        expert_models = [\n            self.modelpool.load_model(m) for m in self.modelpool.model_names\n        ]\n\n        # merge the models using task arithmetic\n        moe_model = task_arithmetic_merge(\n            # this function modifies the model in place, so we need to pass a deepcopy\n            deepcopy(base_model),\n            expert_models,\n            scaling_factor=self.config.init_lambda,\n        ).requires_grad_(False)\n\n        # up-scale MLP modules\n        base_encoder: CLIPEncoder = base_model.vision_model.encoder\n        moe_encoder: CLIPEncoder = moe_model.vision_model.encoder\n        expert_encoders = [m.vision_model.encoder for m in expert_models]\n\n        num_layers = len(base_encoder.layers)\n        for layer_idx in range(num_layers):\n            base_mlp = base_encoder.layers[layer_idx].mlp\n            expert_mlps = [e.layers[layer_idx].mlp for e in expert_encoders]\n\n            moe_encoder.layers[layer_idx].mlp = SparseWeightEnsemblingMoE(\n                hidden_size=base_encoder.config.hidden_size,\n                base_model=base_mlp,\n                expert_models=expert_mlps,\n                init_lambda=self.config.init_lambda,\n                batch_first=True,  # for open_clip models this is False\n                router_hidden_layers=self.config.router_hidden_layers,\n                batch_reduce=self.config.batch_reduce,\n                num_layers=num_layers,\n                layer_idx=layer_idx,\n                tv_prune_ratio=self.config.tv_prune_ratio,\n            )\n\n        return moe_model\n\n    def construct_moe_model_sharedgate(self) -&gt; SparseWeightEnsemblingMoE_ShardGate:\n        \"\"\"\n        Construct the Mixture of Experts model using the models in the model pool with a shared gate.\n        \"\"\"\n        base_model = self.modelpool.load_model(\"_pretrained_\")\n        expert_models = [\n            self.modelpool.load_model(m) for m in self.modelpool.model_names\n        ]\n\n        # merge the models using task arithmetic\n        moe_model = task_arithmetic_merge(\n            # this function modifies the model in place, so we need to pass a deepcopy\n            deepcopy(base_model),\n            expert_models,\n            scaling_factor=self.config.init_lambda,\n        ).requires_grad_(False)\n\n        # up-scale MLP modules\n        base_encoder: CLIPEncoder = base_model.vision_model.encoder\n        moe_encoder: CLIPEncoder = moe_model.vision_model.encoder\n        expert_encoders = [m.vision_model.encoder for m in expert_models]\n\n        # shared gate\n        shared_gate = construct_weight_ensembling_gate(\n            hidden_size=(\n                base_encoder.config.hidden_size + self.config.position_encoding_dim\n                if self.config.position_encoding\n                else base_encoder.config.hidden_size\n            ),\n            num_experts=len(expert_models),\n            init_lambda=self.config.init_lambda,\n            num_hidden_layers=self.config.router_hidden_layers,\n        )\n\n        # ------------------------------------------------------------------------------------\n        # Calculate magnitude\n        # num_layers = len(base_encoder.layers)\n        # exp_id = 0\n        # for e in expert_encoders:\n        #     for layer_idx in range(num_layers):\n        #         if layer_idx in [0,3,5,7,9,11]:\n        #             print(f\"layer_idx: {layer_idx}\")\n        #             v_e = torch.cat([param.view(-1) for param in e.layers[layer_idx].mlp.parameters()])\n        #             v_base = torch.cat([param.view(-1) for param in base_encoder.layers[layer_idx].mlp.parameters()])\n        #             absolute_vector = torch.abs(v_e - v_base)\n        #             np.save(f\"/home/enneng/fusion_bench/outputs/sparse_we_moe/magnitude/absolute_vector_expert_{exp_id}_layer_{layer_idx}.npy\", absolute_vector.detach().numpy())\n        #     exp_id += 1\n        # print('succ')\n        # ------------------------------------------------------------------------------------\n\n        # ------------------------------------------------------------------------------------\n        # Calculate l2 distance and cos similarity\n        # key = 'att' # 'mlp' or 'att'\n        # num_layers = len(base_encoder.layers)\n        # l2_distance_ss = []\n        # cos_sim_ss = []\n        # for e in expert_encoders:\n        #     l2_distance_s = []\n        #     cos_sim_s = []\n        #     for layer_idx in range(num_layers):\n        #         print(f\"layer_idx: {layer_idx}\")\n        #         v_e = torch.cat([param.view(-1) for param in e.layers[layer_idx].mlp.parameters()]) if key == 'mlp' \\\n        #             else torch.cat([param.view(-1) for param in e.layers[layer_idx].self_attn.parameters()])\n        #         v_base = torch.cat([param.view(-1) for param in base_encoder.layers[layer_idx].mlp.parameters()]) if key == 'mlp' \\\n        #             else torch.cat([param.view(-1) for param in base_encoder.layers[layer_idx].self_attn.parameters()])\n        #         l2_distance = torch.norm(v_e - v_base, p=2)\n        #         print(f\"L2 Distance: {l2_distance}\")\n        #         cos_sim = torch.nn.functional.cosine_similarity(v_e, v_base, dim=0)\n        #         print(f\"Cosine Similarity: {cos_sim}\")\n        #\n        #         l2_distance_s.append(l2_distance.item())\n        #         cos_sim_s.append(cos_sim.item())\n        #     l2_distance_ss.append(l2_distance_s)\n        #     cos_sim_ss.append(cos_sim_s)\n        #\n        # print(\"L2 Distances:\")\n        # print(l2_distance_ss)\n        # print(\"Cosine Similarity:\")\n        # print(cos_sim_ss)\n        # ------------------------------------------------------------------------------------\n\n        num_layers = len(base_encoder.layers)\n        for layer_idx in range(num_layers):\n            base_mlp = base_encoder.layers[layer_idx].mlp\n            expert_mlps = [e.layers[layer_idx].mlp for e in expert_encoders]\n\n            moe_encoder.layers[layer_idx].mlp = SparseWeightEnsemblingMoE_ShardGate(\n                hidden_size=base_encoder.config.hidden_size,\n                base_model=base_mlp,\n                expert_models=expert_mlps,\n                init_lambda=self.config.init_lambda,\n                batch_first=True,  # for open_clip models this is False\n                router_hidden_layers=self.config.router_hidden_layers,\n                batch_reduce=self.config.batch_reduce,\n                num_layers=num_layers,\n                layer_idx=layer_idx,\n                tv_prune_ratio=self.config.tv_prune_ratio,\n                sharedgate=shared_gate,\n                position_encoding=self.config.position_encoding,\n                position_encoding_dim=self.config.position_encoding_dim,\n            )\n\n        return moe_model\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(self, tta_dataset: str):\n        \"\"\"\n        Get an iterator for the shuffled test data loader.\n        \"\"\"\n        log.info(\"get_shuffled_test_loader_iter\")\n        dataset = self.modelpool.load_test_dataset(tta_dataset)\n        dataset = CLIPDataset(dataset, processor=self.clip_processor)\n        loader = DataLoader(\n            dataset,\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            num_workers=self.config.num_workers,\n            pin_memory=True,\n        )\n        if self._fabric is not None:\n            loader = self._fabric.setup_dataloaders(loader)\n        return iter(InfiniteDataLoader(loader))\n\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Here we load the CLIP processor and construct the zero-shot classification head for each task.\n        \"\"\"\n        self.setup_zero_shot_classification_head()\n\n    def compute_logits(\n        self, module: CLIPVisionModel, batch: Tuple[Tensor, Tensor], task: str\n    ) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for the given batch and task.\n\n        Args:\n            module (CLIPVisionModel): The vision model to use for computing logits.\n            batch (Tuple[Tensor, Tensor]): The batch of data.\n            task (str): The task for which to compute logits.\n\n        Returns:\n            Tensor: The computed logits.\n        \"\"\"\n        images, _ = batch\n        text_embeds = self.zeroshot_weights[task]\n\n        image_embeds = module(images)[1]\n        image_embeds = self.visual_projection(image_embeds)\n\n        # normalize embeddings\n        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n        # cosine similarity\n        logits_per_text = (\n            torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n        )\n        logits_per_image = logits_per_text.t()\n\n        return logits_per_image\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseCLIPWeightEnsemblingMoEAlgorithm.compute_logits","title":"<code>compute_logits(module, batch, task)</code>","text":"<p>Compute the logits for the given batch and task.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>CLIPVisionModel</code>)           \u2013            <p>The vision model to use for computing logits.</p> </li> <li> <code>batch</code>               (<code>Tuple[Tensor, Tensor]</code>)           \u2013            <p>The batch of data.</p> </li> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The task for which to compute logits.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The computed logits.</p> </li> </ul> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_clip_we_moe.py</code> <pre><code>def compute_logits(\n    self, module: CLIPVisionModel, batch: Tuple[Tensor, Tensor], task: str\n) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for the given batch and task.\n\n    Args:\n        module (CLIPVisionModel): The vision model to use for computing logits.\n        batch (Tuple[Tensor, Tensor]): The batch of data.\n        task (str): The task for which to compute logits.\n\n    Returns:\n        Tensor: The computed logits.\n    \"\"\"\n    images, _ = batch\n    text_embeds = self.zeroshot_weights[task]\n\n    image_embeds = module(images)[1]\n    image_embeds = self.visual_projection(image_embeds)\n\n    # normalize embeddings\n    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n    # cosine similarity\n    logits_per_text = (\n        torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n    )\n    logits_per_image = logits_per_text.t()\n\n    return logits_per_image\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseCLIPWeightEnsemblingMoEAlgorithm.construct_moe_model","title":"<code>construct_moe_model()</code>","text":"<p>Construct the Mixture of Experts model using the models in the model pool.</p> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_clip_we_moe.py</code> <pre><code>def construct_moe_model(self) -&gt; SparseWeightEnsemblingMoE:\n    \"\"\"\n    Construct the Mixture of Experts model using the models in the model pool.\n    \"\"\"\n    base_model = self.modelpool.load_model(\"_pretrained_\")\n    expert_models = [\n        self.modelpool.load_model(m) for m in self.modelpool.model_names\n    ]\n\n    # merge the models using task arithmetic\n    moe_model = task_arithmetic_merge(\n        # this function modifies the model in place, so we need to pass a deepcopy\n        deepcopy(base_model),\n        expert_models,\n        scaling_factor=self.config.init_lambda,\n    ).requires_grad_(False)\n\n    # up-scale MLP modules\n    base_encoder: CLIPEncoder = base_model.vision_model.encoder\n    moe_encoder: CLIPEncoder = moe_model.vision_model.encoder\n    expert_encoders = [m.vision_model.encoder for m in expert_models]\n\n    num_layers = len(base_encoder.layers)\n    for layer_idx in range(num_layers):\n        base_mlp = base_encoder.layers[layer_idx].mlp\n        expert_mlps = [e.layers[layer_idx].mlp for e in expert_encoders]\n\n        moe_encoder.layers[layer_idx].mlp = SparseWeightEnsemblingMoE(\n            hidden_size=base_encoder.config.hidden_size,\n            base_model=base_mlp,\n            expert_models=expert_mlps,\n            init_lambda=self.config.init_lambda,\n            batch_first=True,  # for open_clip models this is False\n            router_hidden_layers=self.config.router_hidden_layers,\n            batch_reduce=self.config.batch_reduce,\n            num_layers=num_layers,\n            layer_idx=layer_idx,\n            tv_prune_ratio=self.config.tv_prune_ratio,\n        )\n\n    return moe_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseCLIPWeightEnsemblingMoEAlgorithm.construct_moe_model_sharedgate","title":"<code>construct_moe_model_sharedgate()</code>","text":"<p>Construct the Mixture of Experts model using the models in the model pool with a shared gate.</p> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_clip_we_moe.py</code> <pre><code>def construct_moe_model_sharedgate(self) -&gt; SparseWeightEnsemblingMoE_ShardGate:\n    \"\"\"\n    Construct the Mixture of Experts model using the models in the model pool with a shared gate.\n    \"\"\"\n    base_model = self.modelpool.load_model(\"_pretrained_\")\n    expert_models = [\n        self.modelpool.load_model(m) for m in self.modelpool.model_names\n    ]\n\n    # merge the models using task arithmetic\n    moe_model = task_arithmetic_merge(\n        # this function modifies the model in place, so we need to pass a deepcopy\n        deepcopy(base_model),\n        expert_models,\n        scaling_factor=self.config.init_lambda,\n    ).requires_grad_(False)\n\n    # up-scale MLP modules\n    base_encoder: CLIPEncoder = base_model.vision_model.encoder\n    moe_encoder: CLIPEncoder = moe_model.vision_model.encoder\n    expert_encoders = [m.vision_model.encoder for m in expert_models]\n\n    # shared gate\n    shared_gate = construct_weight_ensembling_gate(\n        hidden_size=(\n            base_encoder.config.hidden_size + self.config.position_encoding_dim\n            if self.config.position_encoding\n            else base_encoder.config.hidden_size\n        ),\n        num_experts=len(expert_models),\n        init_lambda=self.config.init_lambda,\n        num_hidden_layers=self.config.router_hidden_layers,\n    )\n\n    # ------------------------------------------------------------------------------------\n    # Calculate magnitude\n    # num_layers = len(base_encoder.layers)\n    # exp_id = 0\n    # for e in expert_encoders:\n    #     for layer_idx in range(num_layers):\n    #         if layer_idx in [0,3,5,7,9,11]:\n    #             print(f\"layer_idx: {layer_idx}\")\n    #             v_e = torch.cat([param.view(-1) for param in e.layers[layer_idx].mlp.parameters()])\n    #             v_base = torch.cat([param.view(-1) for param in base_encoder.layers[layer_idx].mlp.parameters()])\n    #             absolute_vector = torch.abs(v_e - v_base)\n    #             np.save(f\"/home/enneng/fusion_bench/outputs/sparse_we_moe/magnitude/absolute_vector_expert_{exp_id}_layer_{layer_idx}.npy\", absolute_vector.detach().numpy())\n    #     exp_id += 1\n    # print('succ')\n    # ------------------------------------------------------------------------------------\n\n    # ------------------------------------------------------------------------------------\n    # Calculate l2 distance and cos similarity\n    # key = 'att' # 'mlp' or 'att'\n    # num_layers = len(base_encoder.layers)\n    # l2_distance_ss = []\n    # cos_sim_ss = []\n    # for e in expert_encoders:\n    #     l2_distance_s = []\n    #     cos_sim_s = []\n    #     for layer_idx in range(num_layers):\n    #         print(f\"layer_idx: {layer_idx}\")\n    #         v_e = torch.cat([param.view(-1) for param in e.layers[layer_idx].mlp.parameters()]) if key == 'mlp' \\\n    #             else torch.cat([param.view(-1) for param in e.layers[layer_idx].self_attn.parameters()])\n    #         v_base = torch.cat([param.view(-1) for param in base_encoder.layers[layer_idx].mlp.parameters()]) if key == 'mlp' \\\n    #             else torch.cat([param.view(-1) for param in base_encoder.layers[layer_idx].self_attn.parameters()])\n    #         l2_distance = torch.norm(v_e - v_base, p=2)\n    #         print(f\"L2 Distance: {l2_distance}\")\n    #         cos_sim = torch.nn.functional.cosine_similarity(v_e, v_base, dim=0)\n    #         print(f\"Cosine Similarity: {cos_sim}\")\n    #\n    #         l2_distance_s.append(l2_distance.item())\n    #         cos_sim_s.append(cos_sim.item())\n    #     l2_distance_ss.append(l2_distance_s)\n    #     cos_sim_ss.append(cos_sim_s)\n    #\n    # print(\"L2 Distances:\")\n    # print(l2_distance_ss)\n    # print(\"Cosine Similarity:\")\n    # print(cos_sim_ss)\n    # ------------------------------------------------------------------------------------\n\n    num_layers = len(base_encoder.layers)\n    for layer_idx in range(num_layers):\n        base_mlp = base_encoder.layers[layer_idx].mlp\n        expert_mlps = [e.layers[layer_idx].mlp for e in expert_encoders]\n\n        moe_encoder.layers[layer_idx].mlp = SparseWeightEnsemblingMoE_ShardGate(\n            hidden_size=base_encoder.config.hidden_size,\n            base_model=base_mlp,\n            expert_models=expert_mlps,\n            init_lambda=self.config.init_lambda,\n            batch_first=True,  # for open_clip models this is False\n            router_hidden_layers=self.config.router_hidden_layers,\n            batch_reduce=self.config.batch_reduce,\n            num_layers=num_layers,\n            layer_idx=layer_idx,\n            tv_prune_ratio=self.config.tv_prune_ratio,\n            sharedgate=shared_gate,\n            position_encoding=self.config.position_encoding,\n            position_encoding_dim=self.config.position_encoding_dim,\n        )\n\n    return moe_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseCLIPWeightEnsemblingMoEAlgorithm.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(tta_dataset)</code>  <code>cached</code>","text":"<p>Get an iterator for the shuffled test data loader.</p> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_clip_we_moe.py</code> <pre><code>@functools.cache\ndef get_shuffled_test_loader_iter(self, tta_dataset: str):\n    \"\"\"\n    Get an iterator for the shuffled test data loader.\n    \"\"\"\n    log.info(\"get_shuffled_test_loader_iter\")\n    dataset = self.modelpool.load_test_dataset(tta_dataset)\n    dataset = CLIPDataset(dataset, processor=self.clip_processor)\n    loader = DataLoader(\n        dataset,\n        batch_size=self.config.batch_size,\n        shuffle=True,\n        num_workers=self.config.num_workers,\n        pin_memory=True,\n    )\n    if self._fabric is not None:\n        loader = self._fabric.setup_dataloaders(loader)\n    return iter(InfiniteDataLoader(loader))\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseCLIPWeightEnsemblingMoEAlgorithm.load_checkpoint","title":"<code>load_checkpoint(model, checkpoint)</code>","text":"<p>Load the checkpoint file.</p> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_clip_we_moe.py</code> <pre><code>def load_checkpoint(self, model, checkpoint):\n    \"\"\"\n    Load the checkpoint file.\n    \"\"\"\n    state = {\"model\": model}\n    self._fabric.load(checkpoint, state)\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseCLIPWeightEnsemblingMoEAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Here we load the CLIP processor and construct the zero-shot classification head for each task.</p> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_clip_we_moe.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Here we load the CLIP processor and construct the zero-shot classification head for each task.\n    \"\"\"\n    self.setup_zero_shot_classification_head()\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SparseCLIPWeightEnsemblingMoEAlgorithm.save_checkpoint","title":"<code>save_checkpoint(model, checkpoint)</code>","text":"<p>Save the checkpoint file.</p> Source code in <code>fusion_bench/method/sparse_we_moe/sparse_clip_we_moe.py</code> <pre><code>def save_checkpoint(self, model, checkpoint):\n    \"\"\"\n    Save the checkpoint file.\n    \"\"\"\n    self._fabric.save(checkpoint, {\"model\": model})\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#rank-one-moe","title":"Rank-One MoE","text":""},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.RankOneMoEAlgorithm","title":"<code>RankOneMoEAlgorithm</code>","text":"<p>               Bases: <code>ModelFusionAlgorithm</code></p> <p>Algorithm for fusing models using RankOne-MoE (https://github.com/EnnengYang/RankOne-MoE).</p> <p>This class provides methods for constructing the MoE model, performing test-time adaptation, and running the fusion process.</p> <p>Attributes:</p> <ul> <li> <code>_fabric</code>               (<code>Fabric</code>)           \u2013            <p>The fabric for distributed training.</p> </li> <li> <code>modelpool</code>               (<code>ModelPool</code>)           \u2013            <p>The pool of models to be fused.</p> </li> <li> <code>profiler</code>               (<code>SimpleProfiler</code>)           \u2013            <p>The profiler for measuring performance.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/rankone_moe.py</code> <pre><code>class RankOneMoEAlgorithm(ModelFusionAlgorithm):\n    \"\"\"\n    Algorithm for fusing models using RankOne-MoE (https://github.com/EnnengYang/RankOne-MoE).\n\n    This class provides methods for constructing the MoE model, performing test-time adaptation,\n    and running the fusion process.\n\n    Attributes:\n        _fabric (L.Fabric): The fabric for distributed training.\n        modelpool (ModelPool): The pool of models to be fused.\n        profiler (SimpleProfiler): The profiler for measuring performance.\n    \"\"\"\n\n    _fabric: L.Fabric = None\n    modelpool: ModelPool = None\n\n    def __init__(self, algorithm_config: DictConfig):\n        \"\"\"\n        Initialize the RankOneMoEAlgorithm with the given configuration.\n\n        Args:\n            algorithm_config (DictConfig): The configuration for the algorithm.\n        \"\"\"\n        super().__init__(algorithm_config)\n\n        if self._fabric is None and torch.cuda.is_available():\n            self._fabric = L.Fabric(\n                devices=self.config.get(\"devices\", 1),\n            )\n            self._fabric.launch()\n        else:\n            assert \"No CUDA device available.\"\n        self.profiler = SimpleProfiler(\n            self.config.get(\"cache_dir\", \"outputs\"), \"we_moe_profiler.txt\"\n        )\n\n    @abstractmethod\n    def load_checkpoint(self, model, checkpoint):\n        \"\"\"\n        Load the checkpoint file.\n\n        Args:\n            model: The model to load the checkpoint into.\n            checkpoint: The checkpoint file to load.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def save_checkpoint(self, model, checkpoint):\n        \"\"\"\n        Save the checkpoint file.\n\n        Args:\n            model: The model to save the checkpoint from.\n            checkpoint: The checkpoint file to save.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def construct_moe_model(self) -&gt; RankOneMoE:\n        \"\"\"\n        Construct the Mixture of Experts model using the models in the model pool.\n\n        Returns:\n            RankOne-MoE: The constructed MoE model.\n        \"\"\"\n        pass\n\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Hook method called at the start of test-time adaptation.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n        \"\"\"\n        Get an iterator for the shuffled test data loader for a specific task.\n\n        Args:\n            task (str): The task for which to get the test data loader.\n\n        Returns:\n            DataLoader: The shuffled test data loader iterator.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def compute_logits(self, module, batch, task) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for a given batch and task.\n\n        Args:\n            module: The model module to use for computing logits.\n            batch: The batch of data.\n            task: The task for which to compute logits.\n\n        Returns:\n            Tensor: The computed logits.\n        \"\"\"\n        pass\n\n    def test_time_adaptation(self, module: RankOneMoE):\n        \"\"\"\n        Perform test-time adaptation for the given module.\n\n        Args:\n            module (RankOne-MoE): The MoE module to adapt.\n\n        Returns:\n            RankOne-MoE: The adapted MoE module.\n        \"\"\"\n        self.on_test_time_adaptation_start()\n\n        # configure optimizer\n        if self.config.optimizer == \"adam\":\n            optimizer = torch.optim.Adam(\n                [p for p in module.parameters() if p.requires_grad], lr=self.config.lr\n            )\n        else:\n            raise ValueError(f\"Unsupported optimizer: {self.config.optimizer}\")\n\n        if self._fabric is not None:\n            module, optimizer = self._fabric.setup(module, optimizer)\n\n        module.train()\n\n        if self.config.get(\"fast_dev_run\", False):\n            log.info(\"Running fast_dev_run, only one step\")\n            pbar = tqdm(\n                range(1),\n                \"Test-time adaptation\",\n                dynamic_ncols=True,\n            )\n        else:\n            pbar = tqdm(\n                range(self.config.max_steps),\n                \"Test-time adaptation\",\n                dynamic_ncols=True,\n            )\n        for step_idx in pbar:\n            if self.config.use_grad_accumulate:\n                for task in self.modelpool.model_names:\n                    with self.profiler.profile(\"data time\"):\n                        batch = next(self.get_shuffled_test_loader_iter(task))\n                    with self.profiler.profile(\"forward pass\"):\n                        logits = self.compute_logits(module, batch, task)\n                        assert (\n                            logits.dim() == 2\n                        ), f\"Expected logits to be 2D, got {logits.dim()}\"\n                        loss = entropy_loss(logits)\n                    # .backward() accumulates when .zero_grad() wasn't called\n                    # this can save memory\n                    with self.profiler.profile(\"backward pass\"):\n                        self._fabric.backward(loss, retain_graph=True)\n            else:\n                loss = 0\n                for task in self.modelpool.model_names:\n                    with self.profiler.profile(\"data time\"):\n                        batch = next(self.get_shuffled_test_loader_iter(task))\n                    with self.profiler.profile(\"forward pass\"):\n                        logits = self.compute_logits(module, batch, task)\n                        assert (\n                            logits.dim() == 2\n                        ), f\"Expected logits to be 2D, got {logits.dim()}\"\n                        loss = loss + entropy_loss(logits)\n                with self.profiler.profile(\"backward pass\"):\n                    self._fabric.backward(loss, retain_graph=True)\n\n            with self.profiler.profile(\"optimizer step\"):\n                optimizer.step()\n                optimizer.zero_grad()\n\n            # print([m for m in module.parameters() if m.requires_grad][0])\n\n        return module\n\n    def run(self, modelpool: ModelPool):\n        \"\"\"\n        Run the RankOneMoEAlgorithm to fuse models using RankOne-MoE.\n\n        Args:\n            modelpool (ModelPool): The pool of models to be fused.\n\n        Returns:\n            RankOne-MoE: The fused RankOne MoE model.\n        \"\"\"\n        log.info(\"Fusing models using RankOne-MoE modules.\")\n        self.modelpool = modelpool\n\n        with timeit_context(\"upscaling models to a RankOne-MoE model\"):\n            moe_model = self.construct_moe_model()\n            print_parameters(moe_model)\n\n        if self.config.get(\"checkpoint\", False):\n            log.info(\n                f\"load checkpoint from {self.config.checkpoint}, test-time adaptation will be skipped.\"\n            )\n            self.load_checkpoint(moe_model, self.config.checkpoint)\n        else:\n            with self.profiler.profile(\"test-time adaptation\"):\n                moe_model = self.test_time_adaptation(moe_model)\n            if self.config.get(\"save_checkpoint\", False):\n                log.info(f\"save checkpoint to {self.config.save_checkpoint}\")\n                self.save_checkpoint(moe_model, self.config.save_checkpoint)\n\n            if lightning.fabric.wrappers.is_wrapped(moe_model):\n                moe_model = lightning.fabric.wrappers._unwrap_objects(moe_model)\n\n        # enable sample-wise adaptation\n        moe_model.batch_reduce = False\n        print(self.profiler.summary())\n        return moe_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.RankOneMoEAlgorithm.__init__","title":"<code>__init__(algorithm_config)</code>","text":"<p>Initialize the RankOneMoEAlgorithm with the given configuration.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_config</code>               (<code>DictConfig</code>)           \u2013            <p>The configuration for the algorithm.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/rankone_moe.py</code> <pre><code>def __init__(self, algorithm_config: DictConfig):\n    \"\"\"\n    Initialize the RankOneMoEAlgorithm with the given configuration.\n\n    Args:\n        algorithm_config (DictConfig): The configuration for the algorithm.\n    \"\"\"\n    super().__init__(algorithm_config)\n\n    if self._fabric is None and torch.cuda.is_available():\n        self._fabric = L.Fabric(\n            devices=self.config.get(\"devices\", 1),\n        )\n        self._fabric.launch()\n    else:\n        assert \"No CUDA device available.\"\n    self.profiler = SimpleProfiler(\n        self.config.get(\"cache_dir\", \"outputs\"), \"we_moe_profiler.txt\"\n    )\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.RankOneMoEAlgorithm.compute_logits","title":"<code>compute_logits(module, batch, task)</code>  <code>abstractmethod</code>","text":"<p>Compute the logits for a given batch and task.</p> <p>Parameters:</p> <ul> <li> <code>module</code>           \u2013            <p>The model module to use for computing logits.</p> </li> <li> <code>batch</code>           \u2013            <p>The batch of data.</p> </li> <li> <code>task</code>           \u2013            <p>The task for which to compute logits.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The computed logits.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/rankone_moe.py</code> <pre><code>@abstractmethod\ndef compute_logits(self, module, batch, task) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for a given batch and task.\n\n    Args:\n        module: The model module to use for computing logits.\n        batch: The batch of data.\n        task: The task for which to compute logits.\n\n    Returns:\n        Tensor: The computed logits.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.RankOneMoEAlgorithm.construct_moe_model","title":"<code>construct_moe_model()</code>  <code>abstractmethod</code>","text":"<p>Construct the Mixture of Experts model using the models in the model pool.</p> <p>Returns:</p> <ul> <li> <code>RankOneMoE</code>           \u2013            <p>RankOne-MoE: The constructed MoE model.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/rankone_moe.py</code> <pre><code>@abstractmethod\ndef construct_moe_model(self) -&gt; RankOneMoE:\n    \"\"\"\n    Construct the Mixture of Experts model using the models in the model pool.\n\n    Returns:\n        RankOne-MoE: The constructed MoE model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.RankOneMoEAlgorithm.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(task)</code>  <code>abstractmethod</code>","text":"<p>Get an iterator for the shuffled test data loader for a specific task.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>str</code>)           \u2013            <p>The task for which to get the test data loader.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code> (              <code>DataLoader</code> )          \u2013            <p>The shuffled test data loader iterator.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/rankone_moe.py</code> <pre><code>@abstractmethod\ndef get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n    \"\"\"\n    Get an iterator for the shuffled test data loader for a specific task.\n\n    Args:\n        task (str): The task for which to get the test data loader.\n\n    Returns:\n        DataLoader: The shuffled test data loader iterator.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.RankOneMoEAlgorithm.load_checkpoint","title":"<code>load_checkpoint(model, checkpoint)</code>  <code>abstractmethod</code>","text":"<p>Load the checkpoint file.</p> <p>Parameters:</p> <ul> <li> <code>model</code>           \u2013            <p>The model to load the checkpoint into.</p> </li> <li> <code>checkpoint</code>           \u2013            <p>The checkpoint file to load.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/rankone_moe.py</code> <pre><code>@abstractmethod\ndef load_checkpoint(self, model, checkpoint):\n    \"\"\"\n    Load the checkpoint file.\n\n    Args:\n        model: The model to load the checkpoint into.\n        checkpoint: The checkpoint file to load.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.RankOneMoEAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Hook method called at the start of test-time adaptation.</p> Source code in <code>fusion_bench/method/rankone_moe/rankone_moe.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Hook method called at the start of test-time adaptation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.RankOneMoEAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Run the RankOneMoEAlgorithm to fuse models using RankOne-MoE.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>ModelPool</code>)           \u2013            <p>The pool of models to be fused.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>RankOne-MoE: The fused RankOne MoE model.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/rankone_moe.py</code> <pre><code>def run(self, modelpool: ModelPool):\n    \"\"\"\n    Run the RankOneMoEAlgorithm to fuse models using RankOne-MoE.\n\n    Args:\n        modelpool (ModelPool): The pool of models to be fused.\n\n    Returns:\n        RankOne-MoE: The fused RankOne MoE model.\n    \"\"\"\n    log.info(\"Fusing models using RankOne-MoE modules.\")\n    self.modelpool = modelpool\n\n    with timeit_context(\"upscaling models to a RankOne-MoE model\"):\n        moe_model = self.construct_moe_model()\n        print_parameters(moe_model)\n\n    if self.config.get(\"checkpoint\", False):\n        log.info(\n            f\"load checkpoint from {self.config.checkpoint}, test-time adaptation will be skipped.\"\n        )\n        self.load_checkpoint(moe_model, self.config.checkpoint)\n    else:\n        with self.profiler.profile(\"test-time adaptation\"):\n            moe_model = self.test_time_adaptation(moe_model)\n        if self.config.get(\"save_checkpoint\", False):\n            log.info(f\"save checkpoint to {self.config.save_checkpoint}\")\n            self.save_checkpoint(moe_model, self.config.save_checkpoint)\n\n        if lightning.fabric.wrappers.is_wrapped(moe_model):\n            moe_model = lightning.fabric.wrappers._unwrap_objects(moe_model)\n\n    # enable sample-wise adaptation\n    moe_model.batch_reduce = False\n    print(self.profiler.summary())\n    return moe_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.RankOneMoEAlgorithm.save_checkpoint","title":"<code>save_checkpoint(model, checkpoint)</code>  <code>abstractmethod</code>","text":"<p>Save the checkpoint file.</p> <p>Parameters:</p> <ul> <li> <code>model</code>           \u2013            <p>The model to save the checkpoint from.</p> </li> <li> <code>checkpoint</code>           \u2013            <p>The checkpoint file to save.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/rankone_moe.py</code> <pre><code>@abstractmethod\ndef save_checkpoint(self, model, checkpoint):\n    \"\"\"\n    Save the checkpoint file.\n\n    Args:\n        model: The model to save the checkpoint from.\n        checkpoint: The checkpoint file to save.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.RankOneMoEAlgorithm.test_time_adaptation","title":"<code>test_time_adaptation(module)</code>","text":"<p>Perform test-time adaptation for the given module.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>RankOne - MoE</code>)           \u2013            <p>The MoE module to adapt.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>RankOne-MoE: The adapted MoE module.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/rankone_moe.py</code> <pre><code>def test_time_adaptation(self, module: RankOneMoE):\n    \"\"\"\n    Perform test-time adaptation for the given module.\n\n    Args:\n        module (RankOne-MoE): The MoE module to adapt.\n\n    Returns:\n        RankOne-MoE: The adapted MoE module.\n    \"\"\"\n    self.on_test_time_adaptation_start()\n\n    # configure optimizer\n    if self.config.optimizer == \"adam\":\n        optimizer = torch.optim.Adam(\n            [p for p in module.parameters() if p.requires_grad], lr=self.config.lr\n        )\n    else:\n        raise ValueError(f\"Unsupported optimizer: {self.config.optimizer}\")\n\n    if self._fabric is not None:\n        module, optimizer = self._fabric.setup(module, optimizer)\n\n    module.train()\n\n    if self.config.get(\"fast_dev_run\", False):\n        log.info(\"Running fast_dev_run, only one step\")\n        pbar = tqdm(\n            range(1),\n            \"Test-time adaptation\",\n            dynamic_ncols=True,\n        )\n    else:\n        pbar = tqdm(\n            range(self.config.max_steps),\n            \"Test-time adaptation\",\n            dynamic_ncols=True,\n        )\n    for step_idx in pbar:\n        if self.config.use_grad_accumulate:\n            for task in self.modelpool.model_names:\n                with self.profiler.profile(\"data time\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                with self.profiler.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, batch, task)\n                    assert (\n                        logits.dim() == 2\n                    ), f\"Expected logits to be 2D, got {logits.dim()}\"\n                    loss = entropy_loss(logits)\n                # .backward() accumulates when .zero_grad() wasn't called\n                # this can save memory\n                with self.profiler.profile(\"backward pass\"):\n                    self._fabric.backward(loss, retain_graph=True)\n        else:\n            loss = 0\n            for task in self.modelpool.model_names:\n                with self.profiler.profile(\"data time\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                with self.profiler.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, batch, task)\n                    assert (\n                        logits.dim() == 2\n                    ), f\"Expected logits to be 2D, got {logits.dim()}\"\n                    loss = loss + entropy_loss(logits)\n            with self.profiler.profile(\"backward pass\"):\n                self._fabric.backward(loss, retain_graph=True)\n\n        with self.profiler.profile(\"optimizer step\"):\n            optimizer.step()\n            optimizer.zero_grad()\n\n        # print([m for m in module.parameters() if m.requires_grad][0])\n\n    return module\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.CLIPRankOneMoEAlgorithm","title":"<code>CLIPRankOneMoEAlgorithm</code>","text":"<p>               Bases: <code>RankOneMoEAlgorithm</code>, <code>CLIPClassificationMixin</code></p> <p>CLIPRankOneMoEAlgorithm is a class that implements the RankOneMoEAlgorithm (https://github.com/EnnengYang/RankOne-MoE) for CLIP models. It extends the RankOneMoEAlgorithm and CLIPClassificationMixin classes.</p> <p>Attributes:</p> <ul> <li> <code>modelpool</code>               (<code>CLIPVisionModelPool</code>)           \u2013            <p>The model pool containing the CLIP models.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/clip_rankone_moe.py</code> <pre><code>class CLIPRankOneMoEAlgorithm(\n    RankOneMoEAlgorithm,\n    CLIPClassificationMixin,\n):\n    \"\"\"\n    CLIPRankOneMoEAlgorithm is a class that implements the RankOneMoEAlgorithm (https://github.com/EnnengYang/RankOne-MoE)\n    for CLIP models. It extends the RankOneMoEAlgorithm and CLIPClassificationMixin classes.\n\n    Attributes:\n        modelpool (CLIPVisionModelPool): The model pool containing the CLIP models.\n    \"\"\"\n\n    modelpool: CLIPVisionModelPool = None\n\n    def load_checkpoint(self, model, checkpoint):\n        \"\"\"\n        Load the checkpoint file.\n\n        Args:\n            model: The model to load the checkpoint into.\n            checkpoint: The path to the checkpoint file.\n        \"\"\"\n        state = {\"model\": model}\n        self._fabric.load(checkpoint, state)\n\n    def save_checkpoint(self, model, checkpoint):\n        \"\"\"\n        Save the checkpoint file.\n\n        Args:\n            model: The model to save the checkpoint from.\n            checkpoint: The path to the checkpoint file.\n        \"\"\"\n        self._fabric.save(checkpoint, {\"model\": model})\n\n    def construct_moe_model(self) -&gt; RankOneMoE:\n        \"\"\"\n        Construct the RankOne-MoE model using the models in the model pool.\n\n        Returns:\n            RankOne-MoE: The constructed MoE model.\n        \"\"\"\n        base_model = self.modelpool.load_model(\"_pretrained_\")\n        expert_models = [\n            self.modelpool.load_model(m) for m in self.modelpool.model_names\n        ]\n\n        # Merge the models using task arithmetic\n        moe_model = task_arithmetic_merge(\n            # This function modifies the model in place, so we need to pass a deepcopy\n            deepcopy(base_model),\n            expert_models,\n            scaling_factor=self.config.init_lambda,\n        ).requires_grad_(False)\n\n        # Up-scale MLP modules\n        base_encoder: CLIPEncoder = base_model.vision_model.encoder\n        moe_encoder: CLIPEncoder = moe_model.vision_model.encoder\n        expert_encoders = [m.vision_model.encoder for m in expert_models]\n\n        num_layers = len(base_encoder.layers)\n        for layer_idx in range(num_layers):\n            base_mlp = base_encoder.layers[layer_idx].mlp\n            expert_mlps = [e.layers[layer_idx].mlp for e in expert_encoders]\n\n            moe_encoder.layers[layer_idx].mlp = RankOneMoE(\n                hidden_size=base_encoder.config.hidden_size,\n                base_model=base_mlp,\n                expert_models=expert_mlps,\n                init_lambda=self.config.init_lambda,\n                batch_first=True,  # For open_clip models this is False\n                router_hidden_layers=self.config.router_hidden_layers,\n                batch_reduce=self.config.batch_reduce,\n                svd_accelerator=self.config.svd_accelerator,\n                rank_k=self.config.rank_k,\n                select_k=self.config.select_k,\n            )\n\n        return moe_model\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(self, tta_dataset: str):\n        \"\"\"\n        Get an iterator for the shuffled test data loader.\n\n        Args:\n            tta_dataset (str): The name of the test-time adaptation dataset.\n\n        Returns:\n            Iterator: An iterator for the shuffled test data loader.\n        \"\"\"\n        dataset = self.modelpool.load_test_dataset(tta_dataset)\n        dataset = CLIPDataset(dataset, processor=self.clip_processor)\n        log.info(\"get_shuffled_test_loader_iter\")\n        loader = DataLoader(\n            dataset,\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            num_workers=self.config.num_workers,\n            pin_memory=True,\n        )\n        loader = self.fabric.setup_dataloaders(loader)\n        return iter(InfiniteDataLoader(loader))\n\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Load the CLIP processor and construct the zero-shot classification head for each task.\n        \"\"\"\n        self.setup_zero_shot_classification_head()\n\n    def compute_logits(self, module, batch, task) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for the given batch and task.\n\n        Args:\n            module: The model module.\n            batch: The input batch.\n            task: The task name.\n\n        Returns:\n            Tensor: The computed logits.\n        \"\"\"\n        images, _ = batch\n        text_embeds = self.zeroshot_weights[task]\n\n        image_embeds = module(images)[1]\n        image_embeds = self.visual_projection(image_embeds)\n\n        # Normalize embeddings\n        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n        # Cosine similarity\n        logits_per_text = (\n            torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n        )\n        logits_per_image = logits_per_text.t()\n\n        return logits_per_image\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.CLIPRankOneMoEAlgorithm.compute_logits","title":"<code>compute_logits(module, batch, task)</code>","text":"<p>Compute the logits for the given batch and task.</p> <p>Parameters:</p> <ul> <li> <code>module</code>           \u2013            <p>The model module.</p> </li> <li> <code>batch</code>           \u2013            <p>The input batch.</p> </li> <li> <code>task</code>           \u2013            <p>The task name.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The computed logits.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/clip_rankone_moe.py</code> <pre><code>def compute_logits(self, module, batch, task) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for the given batch and task.\n\n    Args:\n        module: The model module.\n        batch: The input batch.\n        task: The task name.\n\n    Returns:\n        Tensor: The computed logits.\n    \"\"\"\n    images, _ = batch\n    text_embeds = self.zeroshot_weights[task]\n\n    image_embeds = module(images)[1]\n    image_embeds = self.visual_projection(image_embeds)\n\n    # Normalize embeddings\n    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n    # Cosine similarity\n    logits_per_text = (\n        torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n    )\n    logits_per_image = logits_per_text.t()\n\n    return logits_per_image\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.CLIPRankOneMoEAlgorithm.construct_moe_model","title":"<code>construct_moe_model()</code>","text":"<p>Construct the RankOne-MoE model using the models in the model pool.</p> <p>Returns:</p> <ul> <li> <code>RankOneMoE</code>           \u2013            <p>RankOne-MoE: The constructed MoE model.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/clip_rankone_moe.py</code> <pre><code>def construct_moe_model(self) -&gt; RankOneMoE:\n    \"\"\"\n    Construct the RankOne-MoE model using the models in the model pool.\n\n    Returns:\n        RankOne-MoE: The constructed MoE model.\n    \"\"\"\n    base_model = self.modelpool.load_model(\"_pretrained_\")\n    expert_models = [\n        self.modelpool.load_model(m) for m in self.modelpool.model_names\n    ]\n\n    # Merge the models using task arithmetic\n    moe_model = task_arithmetic_merge(\n        # This function modifies the model in place, so we need to pass a deepcopy\n        deepcopy(base_model),\n        expert_models,\n        scaling_factor=self.config.init_lambda,\n    ).requires_grad_(False)\n\n    # Up-scale MLP modules\n    base_encoder: CLIPEncoder = base_model.vision_model.encoder\n    moe_encoder: CLIPEncoder = moe_model.vision_model.encoder\n    expert_encoders = [m.vision_model.encoder for m in expert_models]\n\n    num_layers = len(base_encoder.layers)\n    for layer_idx in range(num_layers):\n        base_mlp = base_encoder.layers[layer_idx].mlp\n        expert_mlps = [e.layers[layer_idx].mlp for e in expert_encoders]\n\n        moe_encoder.layers[layer_idx].mlp = RankOneMoE(\n            hidden_size=base_encoder.config.hidden_size,\n            base_model=base_mlp,\n            expert_models=expert_mlps,\n            init_lambda=self.config.init_lambda,\n            batch_first=True,  # For open_clip models this is False\n            router_hidden_layers=self.config.router_hidden_layers,\n            batch_reduce=self.config.batch_reduce,\n            svd_accelerator=self.config.svd_accelerator,\n            rank_k=self.config.rank_k,\n            select_k=self.config.select_k,\n        )\n\n    return moe_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.CLIPRankOneMoEAlgorithm.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(tta_dataset)</code>  <code>cached</code>","text":"<p>Get an iterator for the shuffled test data loader.</p> <p>Parameters:</p> <ul> <li> <code>tta_dataset</code>               (<code>str</code>)           \u2013            <p>The name of the test-time adaptation dataset.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator</code>          \u2013            <p>An iterator for the shuffled test data loader.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/clip_rankone_moe.py</code> <pre><code>@functools.cache\ndef get_shuffled_test_loader_iter(self, tta_dataset: str):\n    \"\"\"\n    Get an iterator for the shuffled test data loader.\n\n    Args:\n        tta_dataset (str): The name of the test-time adaptation dataset.\n\n    Returns:\n        Iterator: An iterator for the shuffled test data loader.\n    \"\"\"\n    dataset = self.modelpool.load_test_dataset(tta_dataset)\n    dataset = CLIPDataset(dataset, processor=self.clip_processor)\n    log.info(\"get_shuffled_test_loader_iter\")\n    loader = DataLoader(\n        dataset,\n        batch_size=self.config.batch_size,\n        shuffle=True,\n        num_workers=self.config.num_workers,\n        pin_memory=True,\n    )\n    loader = self.fabric.setup_dataloaders(loader)\n    return iter(InfiniteDataLoader(loader))\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.CLIPRankOneMoEAlgorithm.load_checkpoint","title":"<code>load_checkpoint(model, checkpoint)</code>","text":"<p>Load the checkpoint file.</p> <p>Parameters:</p> <ul> <li> <code>model</code>           \u2013            <p>The model to load the checkpoint into.</p> </li> <li> <code>checkpoint</code>           \u2013            <p>The path to the checkpoint file.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/clip_rankone_moe.py</code> <pre><code>def load_checkpoint(self, model, checkpoint):\n    \"\"\"\n    Load the checkpoint file.\n\n    Args:\n        model: The model to load the checkpoint into.\n        checkpoint: The path to the checkpoint file.\n    \"\"\"\n    state = {\"model\": model}\n    self._fabric.load(checkpoint, state)\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.CLIPRankOneMoEAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Load the CLIP processor and construct the zero-shot classification head for each task.</p> Source code in <code>fusion_bench/method/rankone_moe/clip_rankone_moe.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Load the CLIP processor and construct the zero-shot classification head for each task.\n    \"\"\"\n    self.setup_zero_shot_classification_head()\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.CLIPRankOneMoEAlgorithm.save_checkpoint","title":"<code>save_checkpoint(model, checkpoint)</code>","text":"<p>Save the checkpoint file.</p> <p>Parameters:</p> <ul> <li> <code>model</code>           \u2013            <p>The model to save the checkpoint from.</p> </li> <li> <code>checkpoint</code>           \u2013            <p>The path to the checkpoint file.</p> </li> </ul> Source code in <code>fusion_bench/method/rankone_moe/clip_rankone_moe.py</code> <pre><code>def save_checkpoint(self, model, checkpoint):\n    \"\"\"\n    Save the checkpoint file.\n\n    Args:\n        model: The model to save the checkpoint from.\n        checkpoint: The path to the checkpoint file.\n    \"\"\"\n    self._fabric.save(checkpoint, {\"model\": model})\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#pareto-driven-weight-ensembling-moe-pwe-moe","title":"Pareto-driven Weight-Ensembling MoE (PWE-MoE)","text":""},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoEAlgorithmForCLIP","title":"<code>PWEMoEAlgorithmForCLIP</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code>, <code>CLIPClassificationMixin</code></p> Source code in <code>fusion_bench/method/pwe_moe/clip_pwe_moe.py</code> <pre><code>@auto_register_config\nclass PWEMoEAlgorithmForCLIP(\n    BaseAlgorithm,\n    SimpleProfilerMixin,\n    CLIPClassificationMixin,\n):\n    modelpool: CLIPVisionModelPool = None\n\n    def __init__(\n        self,\n        *,\n        upscale_mlp: bool,\n        upscale_attn: bool,\n        init_lambda: float,\n        router_hidden_layers: int,\n        lr: float,\n        num_steps: int,\n        save_interval: int,\n        alpha: float,\n        checkpoint_path: str,\n        eval_grid: bool,\n        eval_grid_n: int,\n        eval_grid_m: int,\n        dataloader_kwargs: DictConfig,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n    @override\n    def run(self, modelpool: CLIPVisionModelPool):\n        self.modelpool = modelpool\n\n        model = self.setup_model()\n        if self.checkpoint_path is not None:\n            model.load_state_dict(torch.load(self.checkpoint_path, map_location=\"cpu\"))\n        else:\n            train_loaders = self.setup_train_loaders()\n            model = self.train(model, train_loaders)\n\n        if self.eval_grid:\n            return map(\n                lambda m, r: {\n                    \"model\": ParetoWeightEnsemblingModule.set_preferenece_vector(\n                        m,\n                        torch.as_tensor(\n                            r, device=self.fabric.device, dtype=torch.float32\n                        ),\n                    ),\n                    \"preference_vector\": r,\n                },\n                itertools.cycle([model]),\n                generate_simplex_grid(self.eval_grid_n, self.eval_grid_m),\n            )\n        return model\n\n    def load_clip_models(self):\n        \"\"\"\n        Loads the pretrained CLIP model and the fine-tuned models for each dataset specified in the configuration.\n        \"\"\"\n        # load pretrained and fine-tuned model\n        with timeit_context():\n            log.info(\"load models\")\n            pretrained_model: CLIPVisionModel = self.modelpool.load_model(\n                \"_pretrained_\"\n            )\n            finetuned_models = {\n                model_name: self.modelpool.load_model(model_name)\n                for model_name in self.modelpool.model_names\n            }\n\n        log.info(\"pretrained model statistics:\")\n        print_parameters(pretrained_model)\n        return pretrained_model, finetuned_models\n\n    def setup_model(self):\n        pretrained_model, finetuned_models = self.load_clip_models()\n        self.setup_zero_shot_classification_head()\n\n        with timeit_context(\"Building PWEMoE model\"):\n            model = deepcopy(pretrained_model)\n\n            # merge the remaining layers using task arithmetic\n            if self.init_lambda != 0:\n                task_arithmetic_merge(\n                    model,\n                    finetuned_models.values(),\n                    scaling_factor=self.init_lambda,\n                    inplace=True,\n                )\n            # fix all parameters\n            model.requires_grad_(False)\n\n            num_layers = len(model.vision_model.encoder.layers)\n\n            def get_layer(m, i):\n                return cast(CLIPEncoderLayer, m.vision_model.encoder.layers[i])\n\n            for layer_idx in tqdm(range(num_layers)):\n                if self.upscale_mlp:\n                    # upscale the mlp layer\n                    get_layer(model, layer_idx).mlp = ParetoWeightEnsemblingModule(\n                        base_model=get_layer(pretrained_model, layer_idx).mlp,\n                        expert_models=[\n                            get_layer(m, layer_idx).mlp\n                            for m in finetuned_models.values()\n                        ],\n                        init_lambda=self.init_lambda,\n                        fix_base_model_and_experts=True,\n                        router_hidden_layers=self.router_hidden_layers,\n                    )\n\n                if self.upscale_attn:\n                    # upscale the Attention layer\n                    get_layer(model, layer_idx).self_attn = (\n                        ParetoWeightEnsemblingModule(\n                            base_model=get_layer(pretrained_model, layer_idx).self_attn,\n                            expert_models=[\n                                get_layer(m, layer_idx).self_attn\n                                for m in finetuned_models.values()\n                            ],\n                            init_lambda=self.init_lambda,\n                            fix_base_model_and_experts=True,\n                            router_hidden_layers=self.router_hidden_layers,\n                        )\n                    )\n\n            print(\"model statistics after upscaling:\")\n            print_parameters(model)\n            return model\n\n    def setup_train_loaders(self):\n        \"\"\"\n        Loads the datasets specified in the configuration.\n        \"\"\"\n        train_datasets = {\n            dataset_name: CLIPDataset(\n                self.modelpool.load_train_dataset(dataset_name),\n                processor=self.clip_processor,\n            )\n            for dataset_name in self.modelpool.model_names\n        }\n        train_loaders = {\n            dataset_name: DataLoader(dataset, shuffle=True, **self.dataloader_kwargs)\n            for dataset_name, dataset in train_datasets.items()\n        }\n        train_loaders = {\n            dataset_name: self.fabric.setup_dataloaders(loader)\n            for dataset_name, loader in train_loaders.items()\n        }\n        return train_loaders\n\n    def train(self, model: nn.Module, train_loaders: Dict[str, DataLoader]):\n        config = self.config\n\n        # save the configuration\n        self.log_hyperparams(config, filename=\"method_config.yaml\")\n\n        # setup the model\n        num_objectives = len(self.modelpool.model_names)\n        model = model\n\n        # setup data loaders\n        train_loaders = {\n            name: InfiniteDataLoader(loader) for name, loader in train_loaders.items()\n        }\n\n        # set up the optimizer and learning rate scheduler\n        optimizer = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, model.parameters()),\n            lr=config.lr,\n        )\n        model, optimizer = self.fabric.setup(model, optimizer)\n        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer=optimizer, T_max=config.num_steps, eta_min=config.lr * 0.1\n        )\n\n        model.train()\n        device = self.fabric.device\n        for step_idx in tqdm(\n            range(1, 1 + config.num_steps), \"training\", dynamic_ncols=True\n        ):\n            # sample a preference ray\n            ray = torch.from_numpy(\n                np.random.dirichlet((config.alpha,) * num_objectives, 1)\n                .astype(np.float32)\n                .flatten()\n            ).to(device)\n            ParetoWeightEnsemblingModule.set_preferenece_vector(model, ray)\n\n            losses = []\n            for dataset_idx, dataset_name in enumerate(train_loaders):\n                batch = next(train_loaders[dataset_name])\n                images, labels = batch\n\n                logits = self.compute_logits(model, images, dataset_name)\n                _loss = F.cross_entropy(logits, labels)\n                losses.append(_loss)\n\n            loss = self.compute_loss(model, ray, losses)\n\n            optimizer.zero_grad()\n            self.fabric.backward(loss)\n            optimizer.step()\n\n            lr_scheduler.step()\n\n            self.fabric.log(\"train/loss\", loss.item(), step=step_idx)\n\n            if step_idx % config.save_interval == 0:\n                (Path(self.log_dir) / \"checkpoints\").mkdir(exist_ok=True, parents=True)\n                save_path = (\n                    Path(self.log_dir) / \"checkpoints\" / f\"model_step={step_idx}.pt\"\n                )\n                torch.save(model.state_dict(), save_path)\n\n        return model\n\n    @abstractmethod\n    def compute_loss(\n        self, model: nn.Module, ray: Tensor, losses: List[Tensor]\n    ) -&gt; Tensor:\n        \"\"\"\n        Computes the overall losses using the given preference ray.\n\n        Args:\n            model (nn.Module): The model being trained.\n            ray (Tensor): A tensor representing the preference ray, which contains the weights for each objective.\n            losses (List[Tensor]): A list of loss values for each objective.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoEAlgorithmForCLIP.compute_loss","title":"<code>compute_loss(model, ray, losses)</code>  <code>abstractmethod</code>","text":"<p>Computes the overall losses using the given preference ray.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model being trained.</p> </li> <li> <code>ray</code>               (<code>Tensor</code>)           \u2013            <p>A tensor representing the preference ray, which contains the weights for each objective.</p> </li> <li> <code>losses</code>               (<code>List[Tensor]</code>)           \u2013            <p>A list of loss values for each objective.</p> </li> </ul> Source code in <code>fusion_bench/method/pwe_moe/clip_pwe_moe.py</code> <pre><code>@abstractmethod\ndef compute_loss(\n    self, model: nn.Module, ray: Tensor, losses: List[Tensor]\n) -&gt; Tensor:\n    \"\"\"\n    Computes the overall losses using the given preference ray.\n\n    Args:\n        model (nn.Module): The model being trained.\n        ray (Tensor): A tensor representing the preference ray, which contains the weights for each objective.\n        losses (List[Tensor]): A list of loss values for each objective.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoEAlgorithmForCLIP.load_clip_models","title":"<code>load_clip_models()</code>","text":"<p>Loads the pretrained CLIP model and the fine-tuned models for each dataset specified in the configuration.</p> Source code in <code>fusion_bench/method/pwe_moe/clip_pwe_moe.py</code> <pre><code>def load_clip_models(self):\n    \"\"\"\n    Loads the pretrained CLIP model and the fine-tuned models for each dataset specified in the configuration.\n    \"\"\"\n    # load pretrained and fine-tuned model\n    with timeit_context():\n        log.info(\"load models\")\n        pretrained_model: CLIPVisionModel = self.modelpool.load_model(\n            \"_pretrained_\"\n        )\n        finetuned_models = {\n            model_name: self.modelpool.load_model(model_name)\n            for model_name in self.modelpool.model_names\n        }\n\n    log.info(\"pretrained model statistics:\")\n    print_parameters(pretrained_model)\n    return pretrained_model, finetuned_models\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoEAlgorithmForCLIP.setup_train_loaders","title":"<code>setup_train_loaders()</code>","text":"<p>Loads the datasets specified in the configuration.</p> Source code in <code>fusion_bench/method/pwe_moe/clip_pwe_moe.py</code> <pre><code>def setup_train_loaders(self):\n    \"\"\"\n    Loads the datasets specified in the configuration.\n    \"\"\"\n    train_datasets = {\n        dataset_name: CLIPDataset(\n            self.modelpool.load_train_dataset(dataset_name),\n            processor=self.clip_processor,\n        )\n        for dataset_name in self.modelpool.model_names\n    }\n    train_loaders = {\n        dataset_name: DataLoader(dataset, shuffle=True, **self.dataloader_kwargs)\n        for dataset_name, dataset in train_datasets.items()\n    }\n    train_loaders = {\n        dataset_name: self.fabric.setup_dataloaders(loader)\n        for dataset_name, loader in train_loaders.items()\n    }\n    return train_loaders\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoELinearScalarizationForCLIP","title":"<code>PWEMoELinearScalarizationForCLIP</code>","text":"<p>               Bases: <code>PWEMoEAlgorithmForCLIP</code></p> Source code in <code>fusion_bench/method/pwe_moe/clip_pwe_moe.py</code> <pre><code>class PWEMoELinearScalarizationForCLIP(PWEMoEAlgorithmForCLIP):\n    def compute_loss(self, model, ray, losses):\n        loss = 0\n        for r, l in zip(ray, losses):\n            loss += r * l\n        return loss\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoExactParetoOptimalForCLIP","title":"<code>PWEMoExactParetoOptimalForCLIP</code>","text":"<p>               Bases: <code>PWEMoEAlgorithmForCLIP</code></p> Source code in <code>fusion_bench/method/pwe_moe/clip_pwe_moe.py</code> <pre><code>class PWEMoExactParetoOptimalForCLIP(PWEMoEAlgorithmForCLIP):\n    def compute_loss(self, model: nn.Module, ray: Tensor, losses: Tuple[Tensor]):\n        from phn.solvers import EPOSolver\n\n        if self.epo_solver is None:\n            num_objectives = len(self.finetuned_models)\n            self.epo_solver = EPOSolver(n_tasks=num_objectives, n_params=None)\n        epo_solver = self.epo_solver\n\n        losses = torch.stack(losses)\n        loss = epo_solver.get_weighted_loss(\n            losses,\n            ray,\n            tuple(filter(lambda p: p.requires_grad, model.parameters())),\n        )\n        return loss\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#smile-upscaling","title":"Smile Upscaling","text":""},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SmileUpscalingAlgorithm","title":"<code>SmileUpscalingAlgorithm</code>","text":"<p>               Bases: <code>SimpleProfilerMixin</code>, <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/smile_upscaling/smile_upscaling.py</code> <pre><code>class SmileUpscalingAlgorithm(\n    SimpleProfilerMixin,\n    BaseAlgorithm,\n):\n    _linear_layer_cls = (nn.Linear,)\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"device\": \"device\",\n        \"upscaling_accelerator\": \"upscaling_accelerator\",\n        \"full_matrices\": \"full_matrices\",\n        \"gate_k\": \"gate_k\",\n        \"k\": \"k\",\n        \"top_k\": \"top_k\",\n        \"routing_use_diff\": \"routing_use_diff\",\n        \"average_experts\": \"average_experts\",\n        \"model_path\": \"model_path\",\n    }\n\n    def __init__(\n        self,\n        *,\n        device: str = \"cuda\",\n        upscaling_accelerator: str = None,\n        full_matrices: bool = True,\n        gate_k: int = 256,\n        k: int = 256,\n        top_k: int = 1,\n        routing_use_diff: bool = True,\n        average_experts: bool = False,\n        model_path: str = None,\n        **kwargs: Any,\n    ):\n        \"\"\"\n        Initialize the SmileUpscalingAlgorithm.\n\n        Args:\n            device (str): The device to perform the computation on.\n            upscaling_accelerator (str): The device to perform the SVD computation on.\n            full_matrices (bool): Whether to compute the full-sized U and V matrices.\n            gate_k (int): The number of singular values to keep for the gate.\n            k (int): The number of singular values to keep for the experts.\n            top_k (int): The number of top experts to select.\n            routing_use_diff (bool): Whether to use weight differences for routing.\n            average_experts (bool): Whether to average the experts.\n            model_path (str): The path to save/load the model.\n            **kwargs: Additional arguments.\n        \"\"\"\n        super().__init__()\n        self.device = device\n        self.upscaling_accelerator = upscaling_accelerator\n        self.full_matrices = full_matrices\n        self.gate_k = gate_k\n        self.k = k\n        self.top_k = top_k\n        self.routing_use_diff = routing_use_diff\n        self.average_experts = average_experts\n        self.model_path = model_path\n        for key, value in kwargs.items():\n            log.warning(f\"Unrecognized argument: {key}\")\n            setattr(self, key, value)\n\n        # print `self.config` as yaml\n        print(f\"=== Config for `{type(self).__name__}` ===\")\n        print(OmegaConf.to_yaml(self.config))\n        print(f\"=== Config for `{type(self).__name__}` ===\")\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool) -&gt; nn.Module:\n        \"\"\"\n        Executes the upscaling process.\n\n        Args:\n            modelpool (ModelPool): The pool of models to be used for upscaling.\n\n        Returns:\n            nn.Module: The upscaled model.\n        \"\"\"\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n\n        if self.config.model_path is not None and os.path.exists(\n            self.config.model_path\n        ):\n            log.info(f\"Loading model from {self.config.model_path}\")\n            model = torch.load(self.config.model_path)\n            print_parameters(model)\n            return model\n\n        with self.profile(\"loading model\"):\n            # load models and move to GPU if available\n            with self.profile(\"load pretrained model\"):\n                pretrained_model = modelpool.load_model(\"_pretrained_\")\n            with self.profile(\"load fine-tuned model\"):\n                finetuned_models = [\n                    m\n                    for m in tqdm(modelpool.models(), total=len(modelpool.model_names))\n                ]\n\n            if self.config.device == \"cuda\" and torch.cuda.is_available():\n                pretrained_model = pretrained_model.cuda()\n                finetuned_models = [m.cuda() for m in finetuned_models]\n\n        with self.profile(\"merge model\"):\n            model = self.merge(pretrained_model, finetuned_models)\n\n        self.print_profile_summary()\n        if self.config.model_path is not None:\n            os.makedirs(os.path.dirname(self.config.model_path), exist_ok=True)\n            log.info(f\"Saving model to {self.config.model_path}\")\n            torch.save(model, self.config.model_path)\n        print_parameters(model)\n        return model\n\n    def merge(\n        self,\n        pretrained_model: nn.Module,\n        finetuned_models: List[nn.Module],\n        in_place: bool = True,\n    ) -&gt; nn.Module:\n        \"\"\"\n        Merges the pretrained model with the fine-tuned models to create an upscaled model.\n\n        Args:\n            pretrained_model (nn.Module): The pretrained model.\n            finetuned_models (List[nn.Module]): A list of fine-tuned models.\n            in_place (bool): If True, modifies the pretrained model in place. Otherwise, creates a copy.\n\n        Returns:\n            nn.Module: The merged model.\n        \"\"\"\n        if in_place:\n            model = pretrained_model\n        else:\n            model = deepcopy(pretrained_model)\n\n        self._upscale_submodules(model, finetuned_models)\n        return model\n\n    def _upscale_linear_layer(\n        self,\n        pretrained_model,\n        finetuned_models,\n        name: str,\n    ):\n        \"\"\"\n        Upscale a linear layer by merging it with the corresponding layers from the fine-tuned models.\n\n        Args:\n            pretrained_model (nn.Module): The pretrained model.\n            finetuned_models (List[nn.Module]): A list of fine-tuned models.\n            name (str): The name of the linear layer to upscale.\n        \"\"\"\n        config = self.config\n\n        name_list = name.split(\".\")\n        module = get_attr(pretrained_model, name_list)\n        original_device = get_device(module)\n        module = module.to(self.device, non_blocking=True)\n        experts = [\n            get_attr(m, name_list).to(self.device, non_blocking=True)\n            for m in finetuned_models\n        ]\n        try:\n            moe_linear = SmileMoELinear(\n                module,\n                experts,\n                gate_k=config.gate_k,\n                k=config.k,\n                top_k=config.top_k,\n                routing_use_diff=self.routing_use_diff,\n                full_matrices=self.full_matrices,\n                upscaling_accelerator=self.upscaling_accelerator,\n            )\n            moe_linear = moe_linear.to(original_device, non_blocking=True)\n        except ExpertNotTrainedError:\n            print(f\"skip {name} because the experts are not trained.\")\n            return\n        set_attr(pretrained_model, name_list, moe_linear)\n        # remove the original module from fine-tuned models to save memory\n        for m in finetuned_models:\n            set_attr(m, name_list, None)\n\n    def _average_experts(self, pretarined_model, finetuned_models, name: str):\n        \"\"\"\n        Average the experts for a given layer.\n\n        Args:\n            pretarined_model (nn.Module): The pretrained model.\n            finetuned_models (List[nn.Module]): A list of fine-tuned models.\n            name (str): The name of the layer to average.\n        \"\"\"\n        name_list = name.split(\".\")\n        experts = [get_attr(m, name_list) for m in finetuned_models]\n        averaged_module = simple_average(experts)\n        set_attr(pretarined_model, name_list, averaged_module)\n\n    def _upscale_submodules(\n        self,\n        pretrained_model: nn.Module,\n        finetuned_models: List[nn.Module],\n        tqdm_desc: str = \"Upscaling Linear Modules\",\n    ):\n        \"\"\"\n        Upscales the submodules of the pretrained model by merging them with the corresponding submodules from the fine-tuned models.\n\n        Args:\n            pretrained_model (nn.Module): The pretrained model.\n            finetuned_models (List[nn.Module]): A list of fine-tuned models.\n            tqdm_desc (str): Description for the tqdm progress bar.\n        \"\"\"\n        config = self.config\n        for name, module in tqdm(\n            tuple(pretrained_model.named_modules()),\n            tqdm_desc,\n            leave=False,\n            dynamic_ncols=True,\n        ):\n            if isinstance(module, self._linear_layer_cls):\n                self._upscale_linear_layer(\n                    pretrained_model=pretrained_model,\n                    finetuned_models=finetuned_models,\n                    name=name,\n                )\n            elif config.average_experts and len(tuple(module.named_modules())) == 1:\n                # if the module is a leaf module, we perform a parameter average\n                self._average_experts(pretrained_model, finetuned_models, name)\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SmileUpscalingAlgorithm.__init__","title":"<code>__init__(*, device='cuda', upscaling_accelerator=None, full_matrices=True, gate_k=256, k=256, top_k=1, routing_use_diff=True, average_experts=False, model_path=None, **kwargs)</code>","text":"<p>Initialize the SmileUpscalingAlgorithm.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda'</code> )           \u2013            <p>The device to perform the computation on.</p> </li> <li> <code>upscaling_accelerator</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The device to perform the SVD computation on.</p> </li> <li> <code>full_matrices</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to compute the full-sized U and V matrices.</p> </li> <li> <code>gate_k</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The number of singular values to keep for the gate.</p> </li> <li> <code>k</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The number of singular values to keep for the experts.</p> </li> <li> <code>top_k</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of top experts to select.</p> </li> <li> <code>routing_use_diff</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use weight differences for routing.</p> </li> <li> <code>average_experts</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to average the experts.</p> </li> <li> <code>model_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The path to save/load the model.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments.</p> </li> </ul> Source code in <code>fusion_bench/method/smile_upscaling/smile_upscaling.py</code> <pre><code>def __init__(\n    self,\n    *,\n    device: str = \"cuda\",\n    upscaling_accelerator: str = None,\n    full_matrices: bool = True,\n    gate_k: int = 256,\n    k: int = 256,\n    top_k: int = 1,\n    routing_use_diff: bool = True,\n    average_experts: bool = False,\n    model_path: str = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize the SmileUpscalingAlgorithm.\n\n    Args:\n        device (str): The device to perform the computation on.\n        upscaling_accelerator (str): The device to perform the SVD computation on.\n        full_matrices (bool): Whether to compute the full-sized U and V matrices.\n        gate_k (int): The number of singular values to keep for the gate.\n        k (int): The number of singular values to keep for the experts.\n        top_k (int): The number of top experts to select.\n        routing_use_diff (bool): Whether to use weight differences for routing.\n        average_experts (bool): Whether to average the experts.\n        model_path (str): The path to save/load the model.\n        **kwargs: Additional arguments.\n    \"\"\"\n    super().__init__()\n    self.device = device\n    self.upscaling_accelerator = upscaling_accelerator\n    self.full_matrices = full_matrices\n    self.gate_k = gate_k\n    self.k = k\n    self.top_k = top_k\n    self.routing_use_diff = routing_use_diff\n    self.average_experts = average_experts\n    self.model_path = model_path\n    for key, value in kwargs.items():\n        log.warning(f\"Unrecognized argument: {key}\")\n        setattr(self, key, value)\n\n    # print `self.config` as yaml\n    print(f\"=== Config for `{type(self).__name__}` ===\")\n    print(OmegaConf.to_yaml(self.config))\n    print(f\"=== Config for `{type(self).__name__}` ===\")\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SmileUpscalingAlgorithm.merge","title":"<code>merge(pretrained_model, finetuned_models, in_place=True)</code>","text":"<p>Merges the pretrained model with the fine-tuned models to create an upscaled model.</p> <p>Parameters:</p> <ul> <li> <code>pretrained_model</code>               (<code>Module</code>)           \u2013            <p>The pretrained model.</p> </li> <li> <code>finetuned_models</code>               (<code>List[Module]</code>)           \u2013            <p>A list of fine-tuned models.</p> </li> <li> <code>in_place</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, modifies the pretrained model in place. Otherwise, creates a copy.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: The merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/smile_upscaling/smile_upscaling.py</code> <pre><code>def merge(\n    self,\n    pretrained_model: nn.Module,\n    finetuned_models: List[nn.Module],\n    in_place: bool = True,\n) -&gt; nn.Module:\n    \"\"\"\n    Merges the pretrained model with the fine-tuned models to create an upscaled model.\n\n    Args:\n        pretrained_model (nn.Module): The pretrained model.\n        finetuned_models (List[nn.Module]): A list of fine-tuned models.\n        in_place (bool): If True, modifies the pretrained model in place. Otherwise, creates a copy.\n\n    Returns:\n        nn.Module: The merged model.\n    \"\"\"\n    if in_place:\n        model = pretrained_model\n    else:\n        model = deepcopy(pretrained_model)\n\n    self._upscale_submodules(model, finetuned_models)\n    return model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SmileUpscalingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Executes the upscaling process.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>ModelPool</code>)           \u2013            <p>The pool of models to be used for upscaling.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: The upscaled model.</p> </li> </ul> Source code in <code>fusion_bench/method/smile_upscaling/smile_upscaling.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool) -&gt; nn.Module:\n    \"\"\"\n    Executes the upscaling process.\n\n    Args:\n        modelpool (ModelPool): The pool of models to be used for upscaling.\n\n    Returns:\n        nn.Module: The upscaled model.\n    \"\"\"\n    if not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(modelpool)\n\n    if self.config.model_path is not None and os.path.exists(\n        self.config.model_path\n    ):\n        log.info(f\"Loading model from {self.config.model_path}\")\n        model = torch.load(self.config.model_path)\n        print_parameters(model)\n        return model\n\n    with self.profile(\"loading model\"):\n        # load models and move to GPU if available\n        with self.profile(\"load pretrained model\"):\n            pretrained_model = modelpool.load_model(\"_pretrained_\")\n        with self.profile(\"load fine-tuned model\"):\n            finetuned_models = [\n                m\n                for m in tqdm(modelpool.models(), total=len(modelpool.model_names))\n            ]\n\n        if self.config.device == \"cuda\" and torch.cuda.is_available():\n            pretrained_model = pretrained_model.cuda()\n            finetuned_models = [m.cuda() for m in finetuned_models]\n\n    with self.profile(\"merge model\"):\n        model = self.merge(pretrained_model, finetuned_models)\n\n    self.print_profile_summary()\n    if self.config.model_path is not None:\n        os.makedirs(os.path.dirname(self.config.model_path), exist_ok=True)\n        log.info(f\"Saving model to {self.config.model_path}\")\n        torch.save(model, self.config.model_path)\n    print_parameters(model)\n    return model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SingularProjectionMergingAlgorithm","title":"<code>SingularProjectionMergingAlgorithm</code>","text":"<p>               Bases: <code>ModelFusionAlgorithm</code>, <code>SimpleProfilerMixin</code></p> <p>A model fusion algorithm that projects parameter differences into the SVD subspace of a pretrained model.</p> <p>This algorithm is experimental and aims to investigate the location of task-specific knowledge.</p> Source code in <code>fusion_bench/method/smile_upscaling/singular_projection_merging.py</code> <pre><code>class SingularProjectionMergingAlgorithm(ModelFusionAlgorithm, SimpleProfilerMixin):\n    \"\"\"\n    A model fusion algorithm that projects parameter differences into the SVD subspace of a pretrained model.\n\n    This algorithm is experimental and aims to investigate the location of task-specific knowledge.\n    \"\"\"\n\n    @torch.no_grad()\n    def run(self, modelpool: ModelPool) -&gt; nn.Module:\n        \"\"\"\n        Run the singular projection merging algorithm on the given model pool.\n\n        Args:\n            modelpool (ModelPool): The pool of models to merge.\n\n        Returns:\n            nn.Module: The merged model.\n        \"\"\"\n        modelpool = to_modelpool(modelpool)\n\n        if self.config.model_path is not None and os.path.exists(\n            self.config.model_path\n        ):\n            log.info(f\"loading merged model from {self.config.model_path}\")\n            model = torch.load(self.config.model_path)\n\n        with self.profile(\"load pretrained model\"):\n            pretrained_model = modelpool.load_model(\"_pretrained_\").to(\n                self.config.device\n            )\n        with self.profile(\"load fine-tuned model\"):\n            finetuned_models = modelpool.load_model(modelpool.model_names[0]).to(\n                self.config.device\n            )\n\n        with self.profile(\"merge model\"):\n            model = self.merge(pretrained_model, finetuned_models)\n\n        if self.config.model_path is not None:\n            os.path.makedirs(os.path.dirname(self.config.model_path), exist_ok=True)\n            torch.save(model, self.config.model_path)\n\n        self.print_profile_summary()\n        return model\n\n    def merge(\n        self,\n        pretrained_model: nn.Module,\n        finetuned_model: nn.Module,\n        in_place: bool = True,\n    ) -&gt; nn.Module:\n        \"\"\"\n        Merges the pretrained model with the fine-tuned model by projecting parameter differences\n        into the SVD subspace of the pretrained model.\n\n        Args:\n            pretrained_model (nn.Module): The pretrained model.\n            finetuned_model (nn.Module): The fine-tuned model.\n            in_place (bool): If True, modifies the fine-tuned model in place. Otherwise, creates a copy.\n\n        Returns:\n            nn.Module: The merged model.\n        \"\"\"\n        if in_place:\n            model = finetuned_model\n        else:\n            model = deepcopy(finetuned_model)\n\n        for name, module in tqdm(\n            tuple(model.named_modules()),\n            \"Projection merging in SVD subspace of pretrained model\",\n        ):\n            if isinstance(module, nn.Linear):\n                name_list = name.split(\".\")\n                set_attr(\n                    model,\n                    name_list,\n                    self.projection_merge_linear(\n                        get_attr(pretrained_model, name_list),\n                        get_attr(finetuned_model, name_list),\n                        k=self.config.k,\n                    ),\n                )\n        return model\n\n    def projection_merge_linear(\n        self, pretrained_model: nn.Linear, finetuned_model: nn.Linear, k: int\n    ) -&gt; nn.Linear:\n        \"\"\"\n        Projects the parameter differences of linear layers into the SVD subspace of the pretrained model.\n\n        Args:\n            pretrained_model (nn.Linear): The linear layer of the pretrained model.\n            finetuned_model (nn.Linear): The linear layer of the fine-tuned model.\n            k (int): The number of singular values to keep. If negative, it is determined based on the sum of singular values.\n\n        Returns:\n            nn.Linear: The merged linear layer with projected parameter differences.\n        \"\"\"\n        w = pretrained_model.weight\n        w_ft = finetuned_model.weight\n\n        u, s, v = svd(w, full_matrices=self.config.full_matrices)\n        if k &lt; 0:\n            # find the position where the sum of singular values is larger than 50% of the total sum\n            cumsum = s.cumsum(0)\n            k = (cumsum &lt; cumsum[-1] * 0.5).sum().item() + 1\n\n        if self.config.rank == \"low\":\n            u = u[:, :k]\n            s = s[:k]\n            v = v[:, :k]\n        else:\n            u = u[:, k:]\n            s = s[k:]\n            v = v[:, k:]\n\n        w_diff = w_ft - w\n        w_diff_proj = u.T @ w_diff @ v\n        w.data = w + u @ w_diff_proj @ v.T\n        if pretrained_model.bias is not None:\n            pretrained_model.bias.data = finetuned_model.bias.data\n        return pretrained_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SingularProjectionMergingAlgorithm.merge","title":"<code>merge(pretrained_model, finetuned_model, in_place=True)</code>","text":"<p>Merges the pretrained model with the fine-tuned model by projecting parameter differences into the SVD subspace of the pretrained model.</p> <p>Parameters:</p> <ul> <li> <code>pretrained_model</code>               (<code>Module</code>)           \u2013            <p>The pretrained model.</p> </li> <li> <code>finetuned_model</code>               (<code>Module</code>)           \u2013            <p>The fine-tuned model.</p> </li> <li> <code>in_place</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, modifies the fine-tuned model in place. Otherwise, creates a copy.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: The merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/smile_upscaling/singular_projection_merging.py</code> <pre><code>def merge(\n    self,\n    pretrained_model: nn.Module,\n    finetuned_model: nn.Module,\n    in_place: bool = True,\n) -&gt; nn.Module:\n    \"\"\"\n    Merges the pretrained model with the fine-tuned model by projecting parameter differences\n    into the SVD subspace of the pretrained model.\n\n    Args:\n        pretrained_model (nn.Module): The pretrained model.\n        finetuned_model (nn.Module): The fine-tuned model.\n        in_place (bool): If True, modifies the fine-tuned model in place. Otherwise, creates a copy.\n\n    Returns:\n        nn.Module: The merged model.\n    \"\"\"\n    if in_place:\n        model = finetuned_model\n    else:\n        model = deepcopy(finetuned_model)\n\n    for name, module in tqdm(\n        tuple(model.named_modules()),\n        \"Projection merging in SVD subspace of pretrained model\",\n    ):\n        if isinstance(module, nn.Linear):\n            name_list = name.split(\".\")\n            set_attr(\n                model,\n                name_list,\n                self.projection_merge_linear(\n                    get_attr(pretrained_model, name_list),\n                    get_attr(finetuned_model, name_list),\n                    k=self.config.k,\n                ),\n            )\n    return model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SingularProjectionMergingAlgorithm.projection_merge_linear","title":"<code>projection_merge_linear(pretrained_model, finetuned_model, k)</code>","text":"<p>Projects the parameter differences of linear layers into the SVD subspace of the pretrained model.</p> <p>Parameters:</p> <ul> <li> <code>pretrained_model</code>               (<code>Linear</code>)           \u2013            <p>The linear layer of the pretrained model.</p> </li> <li> <code>finetuned_model</code>               (<code>Linear</code>)           \u2013            <p>The linear layer of the fine-tuned model.</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>The number of singular values to keep. If negative, it is determined based on the sum of singular values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Linear</code>           \u2013            <p>nn.Linear: The merged linear layer with projected parameter differences.</p> </li> </ul> Source code in <code>fusion_bench/method/smile_upscaling/singular_projection_merging.py</code> <pre><code>def projection_merge_linear(\n    self, pretrained_model: nn.Linear, finetuned_model: nn.Linear, k: int\n) -&gt; nn.Linear:\n    \"\"\"\n    Projects the parameter differences of linear layers into the SVD subspace of the pretrained model.\n\n    Args:\n        pretrained_model (nn.Linear): The linear layer of the pretrained model.\n        finetuned_model (nn.Linear): The linear layer of the fine-tuned model.\n        k (int): The number of singular values to keep. If negative, it is determined based on the sum of singular values.\n\n    Returns:\n        nn.Linear: The merged linear layer with projected parameter differences.\n    \"\"\"\n    w = pretrained_model.weight\n    w_ft = finetuned_model.weight\n\n    u, s, v = svd(w, full_matrices=self.config.full_matrices)\n    if k &lt; 0:\n        # find the position where the sum of singular values is larger than 50% of the total sum\n        cumsum = s.cumsum(0)\n        k = (cumsum &lt; cumsum[-1] * 0.5).sum().item() + 1\n\n    if self.config.rank == \"low\":\n        u = u[:, :k]\n        s = s[:k]\n        v = v[:, :k]\n    else:\n        u = u[:, k:]\n        s = s[k:]\n        v = v[:, k:]\n\n    w_diff = w_ft - w\n    w_diff_proj = u.T @ w_diff @ v\n    w.data = w + u @ w_diff_proj @ v.T\n    if pretrained_model.bias is not None:\n        pretrained_model.bias.data = finetuned_model.bias.data\n    return pretrained_model\n</code></pre>"},{"location":"api/fusion_bench.method/mixing/#fusion_bench.method.SingularProjectionMergingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Run the singular projection merging algorithm on the given model pool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>ModelPool</code>)           \u2013            <p>The pool of models to merge.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: The merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/smile_upscaling/singular_projection_merging.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: ModelPool) -&gt; nn.Module:\n    \"\"\"\n    Run the singular projection merging algorithm on the given model pool.\n\n    Args:\n        modelpool (ModelPool): The pool of models to merge.\n\n    Returns:\n        nn.Module: The merged model.\n    \"\"\"\n    modelpool = to_modelpool(modelpool)\n\n    if self.config.model_path is not None and os.path.exists(\n        self.config.model_path\n    ):\n        log.info(f\"loading merged model from {self.config.model_path}\")\n        model = torch.load(self.config.model_path)\n\n    with self.profile(\"load pretrained model\"):\n        pretrained_model = modelpool.load_model(\"_pretrained_\").to(\n            self.config.device\n        )\n    with self.profile(\"load fine-tuned model\"):\n        finetuned_models = modelpool.load_model(modelpool.model_names[0]).to(\n            self.config.device\n        )\n\n    with self.profile(\"merge model\"):\n        model = self.merge(pretrained_model, finetuned_models)\n\n    if self.config.model_path is not None:\n        os.path.makedirs(os.path.dirname(self.config.model_path), exist_ok=True)\n        torch.save(model, self.config.model_path)\n\n    self.print_profile_summary()\n    return model\n</code></pre>"},{"location":"api/fusion_bench.method/training/","title":"Model Training/Fine-Tuning","text":""},{"location":"api/fusion_bench.method/training/#clip-vision-model-fine-tuning","title":"CLIP vision model fine-tuning","text":"<ul> <li>ImageClassificationFineTuningForCLIP: Fine-tuning clip vision encoder on image classification tasks.</li> <li>ContinualImageClassificationFineTuningForCLIP: Continual fine-tuning of clip vision encoder on image classification tasks.</li> </ul>"},{"location":"api/fusion_bench.method/training/#fusion_bench.method.ImageClassificationFineTuningForCLIP","title":"<code>ImageClassificationFineTuningForCLIP</code>","text":"<p>               Bases: <code>CLIPClassificationMixin</code>, <code>SimpleProfilerMixin</code>, <code>ModelFusionAlgorithm</code></p> <p>A class for fine-tuning CLIP models for image classification tasks.</p> Source code in <code>fusion_bench/method/classification/clip_finetune.py</code> <pre><code>class ImageClassificationFineTuningForCLIP(\n    CLIPClassificationMixin,\n    SimpleProfilerMixin,\n    ModelFusionAlgorithm,\n):\n    \"\"\"\n    A class for fine-tuning CLIP models for image classification tasks.\n    \"\"\"\n\n    def run(self, modelpool: CLIPVisionModelPool):\n        \"\"\"\n        Executes the fine-tuning process.\n\n        Args:\n            modelpool (CLIPVisionModelPool): The modelpool is responsible for loading the pre-trained model and training datasets.\n\n        Returns:\n            VisionModel: The fine-tuned vision model.\n        \"\"\"\n        self.modelpool = to_modelpool(modelpool)\n        config = self.config\n        self.log_hyperparams(config, filename=\"method_config.yaml\")\n        self.finetune_method = \"fine-tune\"\n\n        L.seed_everything(config.seed)\n\n        task_names = modelpool.train_dataset_names\n        with self.profile(\"setup model and optimizer\"):\n            processor, classifier, optimizer, lr_scheduler = self.setup_model()\n\n            if config.state_dict_load_path is not None:\n                self.fabric.load(\n                    config.state_dict_load_path,\n                    {\"vision_model\": classifier.clip_model.vision_model},\n                )\n                if config.skip_training:\n                    return classifier.clip_model.vision_model\n\n            self.setup_zero_shot_classification_head(\n                clip_processor=processor,\n                clip_model=classifier.clip_model,\n                task_names=task_names,\n            )\n\n            self.fabric.setup(classifier, optimizer)\n\n        with self.profile(\"setup data\"):\n            train_datasets = [\n                CLIPDataset(modelpool.load_train_dataset(task_name), processor)\n                for task_name in task_names\n            ]\n            train_dataloaders = [\n                DataLoader(\n                    dataset,\n                    shuffle=True,\n                    batch_size=config.batch_size,\n                    num_workers=config.num_workers,\n                )\n                for dataset in train_datasets\n            ]\n            train_dataloaders = self.fabric.setup_dataloaders(*train_dataloaders)\n            if not isinstance(train_dataloaders, (list, tuple)):\n                train_dataloaders = [train_dataloaders]\n            train_dataloader_iters = [\n                iter(InfiniteDataLoader(loader)) for loader in train_dataloaders\n            ]\n\n        # train\n        for step_idx in tqdm(\n            range(config.num_steps),\n            desc=self.finetune_method,\n            disable=not self.fabric.is_global_zero,\n            dynamic_ncols=True,\n        ):\n            optimizer.zero_grad()\n            loss = 0\n            for task, loader in zip(task_names, train_dataloader_iters):\n                with self.profile(\"data loading\"):\n                    batch = next(loader)\n                    images, labels = batch\n                with self.profile(\"forward\"):\n                    classifier.zeroshot_weights = self.zeroshot_weights[task]\n                    logits = classifier(images)\n                loss = loss + nn.functional.cross_entropy(logits, labels)\n\n            with self.profile(\"backward\"):\n                self.fabric.backward(loss)\n            with self.profile(\"optimizer step\"):\n                optimizer.step()\n                lr_scheduler.step()\n\n            metrics = {\"train/loss\": loss}\n\n            self.fabric.log_dict(metrics, step=step_idx)\n\n            if (step_idx + 1) % config.save_interval == 0:\n                save_path = os.path.join(\n                    self.log_dir, \"checkpoints\", f\"step={step_idx}.ckpt\"\n                )\n                self.save_model(classifier, save_path)\n\n        if config.state_dict_save_path is not None:\n            self.save_model(classifier, config.state_dict_save_path)\n        self.print_profile_summary()\n        return classifier.clip_model.vision_model\n\n    def save_model(\n        self,\n        model: HFCLIPClassifier | CLIPModel | CLIPVisionModel | CLIPVisionTransformer,\n        save_path: str,\n    ):\n        \"\"\"\n        Save the vision model to the specified path.\n\n        Args:\n            model (Union[HFCLIPClassifier, CLIPModel, CLIPVisionModel, CLIPVisionTransformer]): The model to save.\n            save_path (str): The path to save the model.\n        \"\"\"\n        if isinstance(model, HFCLIPClassifier):\n            vision_model = model.clip_model.vision_model\n        elif isinstance(model, CLIPModel):\n            vision_model = model.vision_model\n        elif isinstance(model, CLIPVisionModel):\n            vision_model = model.vision_model\n        elif isinstance(model, CLIPVisionTransformer):\n            vision_model = model\n        else:\n            raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n        save_dir = os.path.dirname(save_path)\n        if save_dir and not os.path.exists(save_dir):\n            os.makedirs(save_dir, exist_ok=True)\n        self.fabric.save(save_path, {\"vision_model\": vision_model})\n\n    def setup_model(self):\n        \"\"\"\n        Sets up the model, optimizer, and learning rate scheduler.\n\n        This method initializes the CLIP model, applies LoRA if specified, and configures the optimizer and learning rate scheduler.\n\n        Returns:\n            Tuple: A tuple containing the processor, classifier, optimizer, and learning rate scheduler.\n        \"\"\"\n        config = self.config\n        modelpool = self.modelpool\n\n        clip_model: CLIPModel = modelpool.load_clip_model(\"_pretrained_\")\n        processor = modelpool.load_processor()\n\n        self.finetune_method = \"full fine-tune\"\n        if config.use_lora or config.use_l_lora:\n            self.finetune_method = \"lora fine-tune\"\n            lora_config = LoraConfig(\n                **OmegaConf.to_container(\n                    config.lora_config, resolve=True, enum_to_str=True\n                )\n            )\n            clip_model.vision_model = get_peft_model(\n                clip_model.vision_model, lora_config\n            )\n\n            if config.use_l_lora:\n                # http://arxiv.org/abs/2310.04742\n                # Anke Tang et al. Parameter Efficient Multi-task Model Fusion with Partial Linearization. ICLR 2024.\n                self.finetune_method = \"l-lora fine-tune\"\n                print(\"Linearizing Lora Layers\")\n                linearize_lora_model_(clip_model.vision_model)\n\n        classifier = HFCLIPClassifier(clip_model, processor=processor)\n\n        if self.fabric.is_global_zero:\n            print(\"=== Model Summary (For Vision Model Only) ===\")\n            print_parameters(classifier.clip_model.vision_model)\n        # configure optimizers\n        optimizer = torch.optim.Adam(\n            [\n                p\n                for p in classifier.clip_model.vision_model.parameters()\n                if p.requires_grad\n            ],\n            lr=config.learning_rate,\n            weight_decay=config.weight_decay,\n        )\n        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer=optimizer, T_max=config.num_steps\n        )\n\n        return processor, classifier, optimizer, lr_scheduler\n</code></pre>"},{"location":"api/fusion_bench.method/training/#fusion_bench.method.ImageClassificationFineTuningForCLIP.run","title":"<code>run(modelpool)</code>","text":"<p>Executes the fine-tuning process.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>CLIPVisionModelPool</code>)           \u2013            <p>The modelpool is responsible for loading the pre-trained model and training datasets.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>VisionModel</code>          \u2013            <p>The fine-tuned vision model.</p> </li> </ul> Source code in <code>fusion_bench/method/classification/clip_finetune.py</code> <pre><code>def run(self, modelpool: CLIPVisionModelPool):\n    \"\"\"\n    Executes the fine-tuning process.\n\n    Args:\n        modelpool (CLIPVisionModelPool): The modelpool is responsible for loading the pre-trained model and training datasets.\n\n    Returns:\n        VisionModel: The fine-tuned vision model.\n    \"\"\"\n    self.modelpool = to_modelpool(modelpool)\n    config = self.config\n    self.log_hyperparams(config, filename=\"method_config.yaml\")\n    self.finetune_method = \"fine-tune\"\n\n    L.seed_everything(config.seed)\n\n    task_names = modelpool.train_dataset_names\n    with self.profile(\"setup model and optimizer\"):\n        processor, classifier, optimizer, lr_scheduler = self.setup_model()\n\n        if config.state_dict_load_path is not None:\n            self.fabric.load(\n                config.state_dict_load_path,\n                {\"vision_model\": classifier.clip_model.vision_model},\n            )\n            if config.skip_training:\n                return classifier.clip_model.vision_model\n\n        self.setup_zero_shot_classification_head(\n            clip_processor=processor,\n            clip_model=classifier.clip_model,\n            task_names=task_names,\n        )\n\n        self.fabric.setup(classifier, optimizer)\n\n    with self.profile(\"setup data\"):\n        train_datasets = [\n            CLIPDataset(modelpool.load_train_dataset(task_name), processor)\n            for task_name in task_names\n        ]\n        train_dataloaders = [\n            DataLoader(\n                dataset,\n                shuffle=True,\n                batch_size=config.batch_size,\n                num_workers=config.num_workers,\n            )\n            for dataset in train_datasets\n        ]\n        train_dataloaders = self.fabric.setup_dataloaders(*train_dataloaders)\n        if not isinstance(train_dataloaders, (list, tuple)):\n            train_dataloaders = [train_dataloaders]\n        train_dataloader_iters = [\n            iter(InfiniteDataLoader(loader)) for loader in train_dataloaders\n        ]\n\n    # train\n    for step_idx in tqdm(\n        range(config.num_steps),\n        desc=self.finetune_method,\n        disable=not self.fabric.is_global_zero,\n        dynamic_ncols=True,\n    ):\n        optimizer.zero_grad()\n        loss = 0\n        for task, loader in zip(task_names, train_dataloader_iters):\n            with self.profile(\"data loading\"):\n                batch = next(loader)\n                images, labels = batch\n            with self.profile(\"forward\"):\n                classifier.zeroshot_weights = self.zeroshot_weights[task]\n                logits = classifier(images)\n            loss = loss + nn.functional.cross_entropy(logits, labels)\n\n        with self.profile(\"backward\"):\n            self.fabric.backward(loss)\n        with self.profile(\"optimizer step\"):\n            optimizer.step()\n            lr_scheduler.step()\n\n        metrics = {\"train/loss\": loss}\n\n        self.fabric.log_dict(metrics, step=step_idx)\n\n        if (step_idx + 1) % config.save_interval == 0:\n            save_path = os.path.join(\n                self.log_dir, \"checkpoints\", f\"step={step_idx}.ckpt\"\n            )\n            self.save_model(classifier, save_path)\n\n    if config.state_dict_save_path is not None:\n        self.save_model(classifier, config.state_dict_save_path)\n    self.print_profile_summary()\n    return classifier.clip_model.vision_model\n</code></pre>"},{"location":"api/fusion_bench.method/training/#fusion_bench.method.ImageClassificationFineTuningForCLIP.save_model","title":"<code>save_model(model, save_path)</code>","text":"<p>Save the vision model to the specified path.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Union[HFCLIPClassifier, CLIPModel, CLIPVisionModel, CLIPVisionTransformer]</code>)           \u2013            <p>The model to save.</p> </li> <li> <code>save_path</code>               (<code>str</code>)           \u2013            <p>The path to save the model.</p> </li> </ul> Source code in <code>fusion_bench/method/classification/clip_finetune.py</code> <pre><code>def save_model(\n    self,\n    model: HFCLIPClassifier | CLIPModel | CLIPVisionModel | CLIPVisionTransformer,\n    save_path: str,\n):\n    \"\"\"\n    Save the vision model to the specified path.\n\n    Args:\n        model (Union[HFCLIPClassifier, CLIPModel, CLIPVisionModel, CLIPVisionTransformer]): The model to save.\n        save_path (str): The path to save the model.\n    \"\"\"\n    if isinstance(model, HFCLIPClassifier):\n        vision_model = model.clip_model.vision_model\n    elif isinstance(model, CLIPModel):\n        vision_model = model.vision_model\n    elif isinstance(model, CLIPVisionModel):\n        vision_model = model.vision_model\n    elif isinstance(model, CLIPVisionTransformer):\n        vision_model = model\n    else:\n        raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n    save_dir = os.path.dirname(save_path)\n    if save_dir and not os.path.exists(save_dir):\n        os.makedirs(save_dir, exist_ok=True)\n    self.fabric.save(save_path, {\"vision_model\": vision_model})\n</code></pre>"},{"location":"api/fusion_bench.method/training/#fusion_bench.method.ImageClassificationFineTuningForCLIP.setup_model","title":"<code>setup_model()</code>","text":"<p>Sets up the model, optimizer, and learning rate scheduler.</p> <p>This method initializes the CLIP model, applies LoRA if specified, and configures the optimizer and learning rate scheduler.</p> <p>Returns:</p> <ul> <li> <code>Tuple</code>          \u2013            <p>A tuple containing the processor, classifier, optimizer, and learning rate scheduler.</p> </li> </ul> Source code in <code>fusion_bench/method/classification/clip_finetune.py</code> <pre><code>def setup_model(self):\n    \"\"\"\n    Sets up the model, optimizer, and learning rate scheduler.\n\n    This method initializes the CLIP model, applies LoRA if specified, and configures the optimizer and learning rate scheduler.\n\n    Returns:\n        Tuple: A tuple containing the processor, classifier, optimizer, and learning rate scheduler.\n    \"\"\"\n    config = self.config\n    modelpool = self.modelpool\n\n    clip_model: CLIPModel = modelpool.load_clip_model(\"_pretrained_\")\n    processor = modelpool.load_processor()\n\n    self.finetune_method = \"full fine-tune\"\n    if config.use_lora or config.use_l_lora:\n        self.finetune_method = \"lora fine-tune\"\n        lora_config = LoraConfig(\n            **OmegaConf.to_container(\n                config.lora_config, resolve=True, enum_to_str=True\n            )\n        )\n        clip_model.vision_model = get_peft_model(\n            clip_model.vision_model, lora_config\n        )\n\n        if config.use_l_lora:\n            # http://arxiv.org/abs/2310.04742\n            # Anke Tang et al. Parameter Efficient Multi-task Model Fusion with Partial Linearization. ICLR 2024.\n            self.finetune_method = \"l-lora fine-tune\"\n            print(\"Linearizing Lora Layers\")\n            linearize_lora_model_(clip_model.vision_model)\n\n    classifier = HFCLIPClassifier(clip_model, processor=processor)\n\n    if self.fabric.is_global_zero:\n        print(\"=== Model Summary (For Vision Model Only) ===\")\n        print_parameters(classifier.clip_model.vision_model)\n    # configure optimizers\n    optimizer = torch.optim.Adam(\n        [\n            p\n            for p in classifier.clip_model.vision_model.parameters()\n            if p.requires_grad\n        ],\n        lr=config.learning_rate,\n        weight_decay=config.weight_decay,\n    )\n    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer=optimizer, T_max=config.num_steps\n    )\n\n    return processor, classifier, optimizer, lr_scheduler\n</code></pre>"},{"location":"api/fusion_bench.method/training/#fusion_bench.method.ContinualImageClassificationFineTuningForCLIP","title":"<code>ContinualImageClassificationFineTuningForCLIP</code>","text":"<p>               Bases: <code>CLIPClassificationMixin</code>, <code>SimpleProfilerMixin</code>, <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/classification/continual_clip_finetune.py</code> <pre><code>class ContinualImageClassificationFineTuningForCLIP(\n    CLIPClassificationMixin,\n    SimpleProfilerMixin,\n    BaseAlgorithm,\n):\n    # attributes to configuration keys mapping\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"seed\": \"seed\",\n        \"shuffle_order\": \"shuffle_order\",\n        \"learning_rate\": \"learning_rate\",\n        \"weight_decay\": \"weight_decay\",\n        \"num_steps\": \"num_steps\",\n        \"batch_size\": \"batch_size\",\n        \"num_workers\": \"num_workers\",\n        \"save_interval\": \"save_interval\",\n        \"state_dict_load_path\": \"state_dict_load_path\",\n        \"state_dict_save_path\": \"state_dict_save_path\",\n        \"skip_training\": \"skip_training\",\n        \"use_lora\": \"use_lora\",\n        \"lora_config\": \"lora_config\",\n    }\n\n    def __init__(\n        self,\n        seed: int = 42,\n        shuffle_order: bool = True,\n        learning_rate: float = 1e-5,\n        weight_decay: float = 0,\n        num_steps: int = 4000,\n        batch_size: int = 128,\n        num_workers: int = 16,\n        save_interval: int = 500,\n        state_dict_load_path: Optional[str] = None,\n        state_dict_save_path: Optional[str] = None,\n        skip_training: bool = False,\n        use_lora: bool = False,\n        lora_config: Optional[LoraConfig] = None,\n    ):\n        self.seed = seed\n        self.shuffle_order = shuffle_order\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.num_steps = num_steps\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.save_interval = save_interval\n        self.state_dict_load_path = state_dict_load_path\n        self.state_dict_save_path = state_dict_save_path\n        self.skip_training = skip_training\n        self.use_lora = use_lora\n        self.lora_config = lora_config\n\n    def run(self, modelpool: CLIPVisionModelPool):\n        self.modelpool = to_modelpool(modelpool)\n        config = self.config\n        self.log_hyperparams(config, filename=\"method_config.yaml\")\n        self.finetune_method = \"fine-tune\"\n\n        if self.seed is not None:\n            L.seed_everything(self.seed)\n        else:\n            seed_everything_by_time(self.fabric)\n\n        task_names = list(modelpool.train_dataset_names)\n        if self.shuffle_order:\n            random.shuffle(task_names)\n        if self.fabric.is_global_zero:\n            save_to_json(task_names, os.path.join(self.log_dir, \"task_names.json\"))\n\n        if self._program.taskpool is not None and isinstance(\n            self._program.taskpool, CLIPVisionModelTaskPool\n        ):\n            has_taskpool = True\n            taskpool = cast(CLIPVisionModelTaskPool, self._program.taskpool)\n            test_datasets = taskpool._test_datasets\n        else:\n            has_taskpool = False\n\n        with self.profile(\"setup model and optimizer\"):\n            processor, classifier, optimizer, lr_scheduler = self.setup_model()\n\n            if self.state_dict_load_path is not None:\n                self.fabric.load(\n                    self.state_dict_load_path,\n                    {\"vision_model\": classifier.clip_model.vision_model},\n                )\n                if self.skip_training:\n                    return classifier.clip_model.vision_model\n\n            self.setup_zero_shot_classification_head(\n                clip_processor=processor,\n                clip_model=classifier.clip_model,\n                task_names=task_names,\n            )\n\n            init_optimizer_state_dict = optimizer.state_dict()\n            init_lr_scheduler_state_dict = lr_scheduler.state_dict()\n            self.fabric.setup(classifier, optimizer)\n\n        with self.profile(\"setup data\"):\n            train_datasets = [\n                CLIPDataset(modelpool.load_train_dataset(task_name), processor)\n                for task_name in task_names\n            ]\n            train_dataloaders = [\n                DataLoader(\n                    dataset,\n                    shuffle=True,\n                    batch_size=self.batch_size,\n                    num_workers=self.num_workers,\n                )\n                for dataset in train_datasets\n            ]\n            train_dataloaders = self.fabric.setup_dataloaders(*train_dataloaders)\n            if not isinstance(train_dataloaders, (list, tuple)):\n                train_dataloaders = [train_dataloaders]\n            train_dataloader_iters = [\n                iter(InfiniteDataLoader(loader)) for loader in train_dataloaders\n            ]\n\n        # continual train\n        for task_idx, task_name in tqdm(\n            enumerate(task_names),\n            dynamic_ncols=True,\n            disable=not self.fabric.is_global_zero,\n        ):\n            train_dataloader_iter = train_dataloader_iters[task_idx]\n\n            # reset optimizer and lr scheduler\n            print(\"reset optimizer and lr scheduler\")\n            optimizer.load_state_dict(init_optimizer_state_dict)\n            lr_scheduler.load_state_dict(init_lr_scheduler_state_dict)\n\n            for step_idx in tqdm(\n                range(self.num_steps),\n                desc=f\"continual fine-tune on {task_name}\",\n                disable=not self.fabric.is_global_zero,\n                dynamic_ncols=True,\n                leave=False,\n            ):\n                optimizer.zero_grad()\n                loss = 0\n                with self.profile(\"data loading\"):\n                    batch = next(train_dataloader_iter)\n                    images, labels = batch\n                with self.profile(\"forward\"):\n                    classifier.zeroshot_weights = self.zeroshot_weights[task_name]\n                    logits = classifier(images)\n                    assert (\n                        labels.max() &lt; logits.shape[1]\n                    ), f\"for task {task_name}, labels.max() = {labels.max()}, logits.shape[1] = {logits.shape[1]}\"\n                loss = loss + nn.functional.cross_entropy(logits, labels)\n\n                with self.profile(\"backward\"):\n                    self.fabric.backward(loss)\n                with self.profile(\"optimizer step\"):\n                    optimizer.step()\n                    lr_scheduler.step()\n\n                metrics = {\"train/loss\": loss}\n                self.fabric.log_dict(metrics, step=step_idx)\n\n                if (step_idx + 1) % self.save_interval == 0:\n                    save_path = os.path.join(\n                        self.log_dir,\n                        \"checkpoints\",\n                        f\"task={task_idx}_step={step_idx}.ckpt\",\n                    )\n                    self.save_model(classifier, save_path)\n\n            if has_taskpool:\n                taskpool._is_setup = False\n                taskpool._test_datasets = DictConfig(\n                    {t: test_datasets[t] for t in task_names[: task_idx + 1]}\n                )\n                eval_report = taskpool.evaluate(\n                    deepcopy(classifier.clip_model.vision_model),\n                    name=task_name,\n                )\n                if self.fabric.is_global_zero:\n                    save_to_json(\n                        eval_report,\n                        os.path.join(self.log_dir, f\"results_{task_idx}.json\"),\n                    )\n\n        if self.state_dict_save_path is not None:\n            self.save_model(classifier, self.state_dict_save_path)\n        self.print_profile_summary()\n        return classifier.clip_model.vision_model\n\n    def save_model(\n        self,\n        model: HFCLIPClassifier | CLIPModel | CLIPVisionModel | CLIPVisionTransformer,\n        save_path: str,\n    ):\n        \"\"\"\n        Save the vision model to the specified path.\n\n        Args:\n            model (Union[HFCLIPClassifier, CLIPModel, CLIPVisionModel, CLIPVisionTransformer]): The model to save.\n            save_path (str): The path to save the model.\n        \"\"\"\n        if isinstance(model, HFCLIPClassifier):\n            vision_model = model.clip_model.vision_model\n        elif isinstance(model, CLIPModel):\n            vision_model = model.vision_model\n        elif isinstance(model, CLIPVisionModel):\n            vision_model = model.vision_model\n        elif isinstance(model, CLIPVisionTransformer):\n            vision_model = model\n        else:\n            raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n        save_dir = os.path.dirname(save_path)\n        if save_dir and not os.path.exists(save_dir):\n            os.makedirs(save_dir, exist_ok=True)\n        self.fabric.save(save_path, {\"vision_model\": vision_model})\n\n    def setup_model(self):\n        \"\"\"\n        Sets up the model, optimizer, and learning rate scheduler.\n\n        This method initializes the CLIP model, applies LoRA if specified, and configures the optimizer and learning rate scheduler.\n\n        Returns:\n            Tuple: A tuple containing the processor, classifier, optimizer, and learning rate scheduler.\n        \"\"\"\n        config = self.config\n        modelpool = self.modelpool\n\n        clip_model: CLIPModel = modelpool.load_clip_model(\"_pretrained_\")\n        processor = modelpool.load_processor()\n\n        self.finetune_method = \"full fine-tune\"\n        if self.use_lora:\n            self.finetune_method = \"lora fine-tune\"\n            lora_config = LoraConfig(\n                **OmegaConf.to_container(\n                    self.lora_config, resolve=True, enum_to_str=True\n                )\n            )\n            clip_model.vision_model = get_peft_model(\n                clip_model.vision_model, lora_config\n            )\n\n        classifier = HFCLIPClassifier(clip_model, processor=processor)\n\n        if self.fabric.is_global_zero:\n            print(\"=== Model Summary (For Vision Model Only) ===\")\n            print_parameters(classifier.clip_model.vision_model)\n        # configure optimizers\n        optimizer = torch.optim.Adam(\n            [\n                p\n                for p in classifier.clip_model.vision_model.parameters()\n                if p.requires_grad\n            ],\n            lr=self.learning_rate,\n            weight_decay=self.weight_decay,\n        )\n        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer=optimizer, T_max=self.num_steps\n        )\n\n        return processor, classifier, optimizer, lr_scheduler\n</code></pre>"},{"location":"api/fusion_bench.method/training/#fusion_bench.method.ContinualImageClassificationFineTuningForCLIP.save_model","title":"<code>save_model(model, save_path)</code>","text":"<p>Save the vision model to the specified path.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Union[HFCLIPClassifier, CLIPModel, CLIPVisionModel, CLIPVisionTransformer]</code>)           \u2013            <p>The model to save.</p> </li> <li> <code>save_path</code>               (<code>str</code>)           \u2013            <p>The path to save the model.</p> </li> </ul> Source code in <code>fusion_bench/method/classification/continual_clip_finetune.py</code> <pre><code>def save_model(\n    self,\n    model: HFCLIPClassifier | CLIPModel | CLIPVisionModel | CLIPVisionTransformer,\n    save_path: str,\n):\n    \"\"\"\n    Save the vision model to the specified path.\n\n    Args:\n        model (Union[HFCLIPClassifier, CLIPModel, CLIPVisionModel, CLIPVisionTransformer]): The model to save.\n        save_path (str): The path to save the model.\n    \"\"\"\n    if isinstance(model, HFCLIPClassifier):\n        vision_model = model.clip_model.vision_model\n    elif isinstance(model, CLIPModel):\n        vision_model = model.vision_model\n    elif isinstance(model, CLIPVisionModel):\n        vision_model = model.vision_model\n    elif isinstance(model, CLIPVisionTransformer):\n        vision_model = model\n    else:\n        raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n    save_dir = os.path.dirname(save_path)\n    if save_dir and not os.path.exists(save_dir):\n        os.makedirs(save_dir, exist_ok=True)\n    self.fabric.save(save_path, {\"vision_model\": vision_model})\n</code></pre>"},{"location":"api/fusion_bench.method/training/#fusion_bench.method.ContinualImageClassificationFineTuningForCLIP.setup_model","title":"<code>setup_model()</code>","text":"<p>Sets up the model, optimizer, and learning rate scheduler.</p> <p>This method initializes the CLIP model, applies LoRA if specified, and configures the optimizer and learning rate scheduler.</p> <p>Returns:</p> <ul> <li> <code>Tuple</code>          \u2013            <p>A tuple containing the processor, classifier, optimizer, and learning rate scheduler.</p> </li> </ul> Source code in <code>fusion_bench/method/classification/continual_clip_finetune.py</code> <pre><code>def setup_model(self):\n    \"\"\"\n    Sets up the model, optimizer, and learning rate scheduler.\n\n    This method initializes the CLIP model, applies LoRA if specified, and configures the optimizer and learning rate scheduler.\n\n    Returns:\n        Tuple: A tuple containing the processor, classifier, optimizer, and learning rate scheduler.\n    \"\"\"\n    config = self.config\n    modelpool = self.modelpool\n\n    clip_model: CLIPModel = modelpool.load_clip_model(\"_pretrained_\")\n    processor = modelpool.load_processor()\n\n    self.finetune_method = \"full fine-tune\"\n    if self.use_lora:\n        self.finetune_method = \"lora fine-tune\"\n        lora_config = LoraConfig(\n            **OmegaConf.to_container(\n                self.lora_config, resolve=True, enum_to_str=True\n            )\n        )\n        clip_model.vision_model = get_peft_model(\n            clip_model.vision_model, lora_config\n        )\n\n    classifier = HFCLIPClassifier(clip_model, processor=processor)\n\n    if self.fabric.is_global_zero:\n        print(\"=== Model Summary (For Vision Model Only) ===\")\n        print_parameters(classifier.clip_model.vision_model)\n    # configure optimizers\n    optimizer = torch.optim.Adam(\n        [\n            p\n            for p in classifier.clip_model.vision_model.parameters()\n            if p.requires_grad\n        ],\n        lr=self.learning_rate,\n        weight_decay=self.weight_decay,\n    )\n    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer=optimizer, T_max=self.num_steps\n    )\n\n    return processor, classifier, optimizer, lr_scheduler\n</code></pre>"},{"location":"api/fusion_bench.method/training/#llm-fine-tuning","title":"LLM Fine-tuning","text":""},{"location":"api/fusion_bench.method/training/#fusion_bench.method.FullFinetuneSFT","title":"<code>FullFinetuneSFT</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>FabricTrainingMixin</code></p> Source code in <code>fusion_bench/method/lm_finetune/fullfinetune_sft.py</code> <pre><code>class FullFinetuneSFT(BaseAlgorithm, FabricTrainingMixin):\n\n    model: Union[nn.Module, \"_FabricModule\", \"LlamaForCausalLM\"]\n    optimizer: Union[torch.optim.Optimizer, \"_FabricOptimizer\"]\n    train_dataloader: Union[DataLoader, \"_FabricDataLoader\"]\n    lr_scheduler: torch.optim.lr_scheduler.LRScheduler\n    _latest_saved_checkpoint_global_step: int = -1\n\n    def __init__(\n        self,\n        optimizer: DictConfig,\n        lr_scheduler: Optional[DictConfig],\n        dataloader_kwargs: DictConfig,\n        max_epochs: int,\n        max_steps: int = -1,\n        max_steps_per_epoch: int = -1,\n        lr_scheduler_interval: Literal[\"epoch\", \"step\"] = \"step\",\n        lr_scheduler_frequency: int = 1,\n        checkpoint_save_interval: Literal[\"epoch\", \"step\"] = \"epoch\",\n        checkpoint_save_frequency: int = 1,\n        accumulate_grad_batches: int = 1,\n        gradient_clip_val: Optional[float] = None,\n        gradient_clip_algorithm: Literal[\"value\", \"norm\"] = \"norm\",\n        save_optimizer_state: bool = False,\n        save_full_model: bool = False,\n        save_ckpt_type: Literal[\"lightning\", \"hf\"] = \"lightning\",\n        ckpt_path: Optional[str] = None,\n        max_length: int = 6144,\n        fix_token_embedding: bool = True,\n        **kwargs,\n    ):\n        \"\"\"\n        Class for full finetuning of a language model on given SFT datasets.\n\n        Args:\n            optimizer(DictConfig): Configuration for the optimizer.\n            lr_scheduler(DictConfig): Configuration for the learning rate scheduler.\n            dataloader_kwargs(DictConfig): Configuration for the dataloader, such as batch size, num_workers, etc.\n            max_epochs(int): Maximum number of epochs to train the model. If set to -1, the training will continue indefinitely or until max_steps is reached.\n            max_steps(int): Maximum number of steps to train the model. If set to -1, the training will continue indefinitely or until max_epochs is reached.\n            max_steps_per_epoch(int): Maximum number of steps to train the model in each epoch. If set to -1, the training will continue until the end of the epoch.\n            lr_scheduler_interval(str): Interval at which to run the learning rate scheduler. Available options: 'epoch', 'step'. If set to 'epoch', the scheduler will run at the end of each epoch. If set to 'step', the scheduler will run at the end of each step.\n            lr_scheduler_frequency(int): Frequency at which to run the learning rate scheduler. The scheduler will run every `lr_scheduler_frequency` epochs or steps, depending on the value of `lr_scheduler_interval`.\n            checkpoint_save_interval(str): Interval at which to save the model checkpoint. Available options: 'epoch', 'step'. If set to 'epoch', the model will be saved at the end of each epoch. If set to 'step', the model will be saved at the end of each step.\n            checkpoint_save_frequency(int): Frequency at which to save the model checkpoint. The model will be saved every `checkpoint_save_frequency` epochs or steps, depending on the value of `checkpoint_save_interval`.\n            accumulate_grad_batches(int): Number of batches to accumulate gradients across before updating the model parameters.\n            gradient_clip_val(float): Value to clip the gradients. If set to None, no gradient clipping will be applied.\n            gradient_clip_algorithm(str): Algorithm to use for gradient clipping. Available options: 'value', 'norm'. If set to 'value', the gradients will be clipped to the specified value. If set to 'norm', the gradients will be clipped to the specified norm.\n            save_optimizer_state(bool): Whether to save the optimizer and lr_scheduler state along with the model checkpoint.\n            save_full_model(bool): Whether to save the full model or only the trainable parameters in the model checkpoint.\n            save_ckpt_type (str): Type of checkpoint to save. Available options: 'lightning', 'hf'. If set to 'lightning', the checkpoint will be saved in the lightning format. If set to 'hf', the checkpoint will be saved in the huggingface format.\n            ckpt_path(str): Path to the checkpoint to load before training. If set to None, no checkpoint will be loaded.\n            max_length(int): Maximum input length to consider. If the input length exceeds this value, it will be truncated.\n            fix_token_embedding(bool): Whether to fix the token embeddings during training. If set to True, the token embeddings will not be updated during training.\n        \"\"\"\n        self._optimizer = optimizer\n        self._lr_scheduler = lr_scheduler\n        self.dataloader_kwargs = dataloader_kwargs\n        self.max_epochs = max_epochs\n        self.max_steps = max_steps\n        self.max_steps_per_epoch = max_steps_per_epoch\n        self.lr_scheduler_interval = lr_scheduler_interval\n        self.lr_scheduler_frequency = lr_scheduler_frequency\n        self.checkpoint_save_interval = checkpoint_save_interval\n        self.checkpoint_save_frequency = checkpoint_save_frequency\n        self.accumulate_grad_batches = accumulate_grad_batches\n        self.gradient_clip_val = gradient_clip_val\n        self.gradient_clip_algorithm = gradient_clip_algorithm\n        self.save_optimizer_state = save_optimizer_state\n        self.save_full_model = save_full_model\n        self.save_ckpt_type = save_ckpt_type\n        self.ckpt_path = ckpt_path\n        self.max_length = max_length\n        self.fix_token_embedding = fix_token_embedding\n        super().__init__(**kwargs)\n\n    def run(self, modelpool: CausalLMPool):\n        self.modelpool = modelpool\n        self.setup()\n        self.train(self.model, self.optimizer, self.lr_scheduler)\n        return self.model\n\n    def setup_model(self):\n        self.tokenizer = self.modelpool.load_tokenizer()\n        if self.tokenizer.pad_token_id is None:\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n\n        model = self.modelpool.load_pretrained_model()\n        self.model: \"LlamaForCausalLM\" = model\n\n        if self.fix_token_embedding:\n            self.model.model.embed_tokens.requires_grad_(False)\n\n        if self.fabric.strategy == \"fsdp\" or isinstance(\n            self.fabric.strategy, FSDPStrategy\n        ):\n            # https://github.com/Lightning-AI/pytorch-lightning/issues/19267\n            self.model.gradient_checkpointing_enable(\n                gradient_checkpointing_kwargs={\"use_reentrant\": True}\n            )\n            self.use_cache = False\n        else:\n            self.use_cache = True\n        self.model_dtype = get_dtype(self.model)\n\n    def configure_optimizer(self):\n        # compute expected total steps\n        self.compute_expected_total_steps(self.train_dataloader)\n\n        optimizer = instantiate(self._optimizer, self.model.parameters())\n        if self._lr_scheduler is not None:\n            for key, arg in self._lr_scheduler.items():\n                if arg == \"_T_max_\":\n                    log.info(\n                        f\"Setting key `{key}` of lr_scheduler configuration to {self.expected_total_steps}\"\n                    )\n                    self._lr_scheduler[key] = self.expected_total_steps\n            lr_scheduler: torch.optim.lr_scheduler.LRScheduler = instantiate(\n                self._lr_scheduler,\n                optimizer=optimizer,\n            )\n        else:\n            lr_scheduler = None\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n\n    def setup_data(self):\n        fabric = self.fabric\n        modelpool = self.modelpool\n        assert (\n            len(modelpool.train_dataset_names) &gt; 0\n        ), \"No training datasets found in modelpool.\"\n\n        train_datasets = [\n            modelpool.load_train_dataset(dataset_name)\n            for dataset_name in modelpool.train_dataset_names\n        ]\n        if len(train_datasets) &gt; 1:\n            train_dataset = ConcatDataset(train_datasets)\n        else:\n            train_dataset = train_datasets[0]\n\n        self.train_dataset = train_dataset\n        self.train_dataloader = DataLoader(\n            train_dataset,\n            **self.dataloader_kwargs,\n            shuffle=True,\n            collate_fn=functools.partial(\n                padded_collate_sft, pad_token_id=self.tokenizer.pad_token_id\n            ),\n        )\n        self.train_dataloader = fabric.setup_dataloaders(self.train_dataloader)\n\n    def setup(self):\n        fabric = self.fabric\n\n        self.setup_model()\n        self.setup_data()\n\n        optimizer = self.configure_optimizer()\n        optimizer, lr_scheduler = optimizer[\"optimizer\"], optimizer[\"lr_scheduler\"]\n\n        self.model, self.optimizer = fabric.setup(self.model, optimizer)\n        self.lr_scheduler = lr_scheduler\n\n    @override\n    def train_epoch(self, *args, **kwargs):\n        fabric = self.fabric\n\n        accumulated_loss = 0\n        for step_idx, batch in enumerate(\n            pbar := tqdm(\n                self.train_dataloader,\n                desc=\"Training Batches\",\n                dynamic_ncols=True,\n                leave=False,\n                disable=not fabric.is_global_zero,\n            )\n        ):\n            is_accumulating = (step_idx + 1) % self.accumulate_grad_batches != 0\n\n            if self.max_length &gt; 0 and batch[\"input_ids\"].shape[1] &gt; self.max_length:\n                log.warning(\n                    f\"Input length exceeds max_length: {batch['input_ids'].shape[1]} &gt; {self.max_length}. Truncating input.\"\n                )\n                batch[\"input_ids\"] = batch[\"input_ids\"][:, : self.max_length]\n                batch[\"attention_mask\"] = batch[\"attention_mask\"][:, : self.max_length]\n                batch[\"labels\"] = batch[\"labels\"][:, : self.max_length]\n\n            # disable gradient synchronization if accumulating gradients across steps for improved performance\n            with fabric.no_backward_sync(self.model, enabled=is_accumulating):\n                # use_cache=True is not compatible with gradient checkpointing, so we disable it here\n                output = self.model(\n                    input_ids=batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                    labels=batch[\"labels\"],\n                    use_cache=self.use_cache,\n                )\n                loss = output[\"loss\"] / self.accumulate_grad_batches\n\n                fabric.backward(loss)\n                accumulated_loss += loss.item()\n\n            if not is_accumulating:\n                self.clip_gradients_if_needed(self.model, self.optimizer)\n\n                # run lr_scheduler at the end of the step if interval is set to \"step\"\n                if (\n                    self.lr_scheduler_interval == \"step\"\n                    and (self.global_step_idx + 1) % self.lr_scheduler_frequency == 0\n                ):\n                    self.lr_scheduler.step()\n\n                # update the model parameters and zero the gradients\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n\n                metrics = {\n                    \"train/loss\": accumulated_loss,\n                    \"train/epoch_idx\": self.epoch_idx,\n                    \"train/lr\": self.optimizer.param_groups[0][\"lr\"],\n                }\n                fabric.log_dict(metrics, step=self.global_step_idx)\n                pbar.set_postfix(metrics)\n\n                # save the model at the end of the step if interval is set to \"step\" and frequency is met\n                self.conditional_checkpoint_save(stage=\"end_of_step\")\n\n                # break if max_steps_per_epoch is set, and exit epoch\n                if (\n                    self.max_steps_per_epoch &gt; 0\n                    and step_idx + 1 &gt;= self.max_steps_per_epoch\n                ):\n                    break\n                # break if max_steps is set, and exit training\n                if self.max_steps &gt; 0 and self.global_step_idx &gt;= self.max_steps - 1:\n                    self.is_training = False\n                    break\n\n                self.global_step_idx += 1\n                accumulated_loss = 0\n\n    def save_checkpoint(\n        self,\n        path: Union[str, Path],\n        save_optimizer_state: Optional[bool] = None,\n        overwrite: bool = False,\n    ):\n        if not overwrite and os.path.exists(path):\n            return log.warning(f\"Checkpoint already exists at {path}. Skipping save.\")\n\n        fabric = self.fabric\n\n        if self.save_ckpt_type == \"lightning\":\n            state = {\"model\": self.model}\n\n            # save the optimizer and lr_scheduler state if needed\n            if self.save_optimizer_state and save_optimizer_state is not False:\n                state.update(\n                    {\n                        \"optimizer\": self.optimizer,\n                        \"lr_scheduler\": self.lr_scheduler,\n                        \"global_step_idx\": self.global_step_idx,\n                        \"epoch_idx\": self.epoch_idx,\n                    }\n                )\n\n            trainable_param_names = set(\n                name\n                for name, param in self.model.state_dict(keep_vars=True).items()\n                if param.requires_grad\n            )\n            filter = (\n                None\n                if self.save_full_model\n                else {\"model\": lambda k, p: k in trainable_param_names}\n            )\n\n            fabric.save(path, state=state, filter=filter)\n        else:\n            self.model.save_pretrained(path, is_main_process=fabric.is_global_zero)\n\n        self._latest_saved_checkpoint_global_step = self.global_step_idx\n\n    def load_checkpoint(self, path: Union[str, Path]):\n        fabric = self.fabric\n\n        state = {\"model\": self.model}\n\n        # save the optimizer and lr_scheduler state if needed\n        if self.save_optimizer_state:\n            state.update(\n                {\n                    \"optimizer\": self.optimizer,\n                    \"lr_scheduler\": self.lr_scheduler,\n                }\n            )\n\n        fabric.load(path, state)\n</code></pre>"},{"location":"api/fusion_bench.method/training/#fusion_bench.method.FullFinetuneSFT.__init__","title":"<code>__init__(optimizer, lr_scheduler, dataloader_kwargs, max_epochs, max_steps=-1, max_steps_per_epoch=-1, lr_scheduler_interval='step', lr_scheduler_frequency=1, checkpoint_save_interval='epoch', checkpoint_save_frequency=1, accumulate_grad_batches=1, gradient_clip_val=None, gradient_clip_algorithm='norm', save_optimizer_state=False, save_full_model=False, save_ckpt_type='lightning', ckpt_path=None, max_length=6144, fix_token_embedding=True, **kwargs)</code>","text":"<p>Class for full finetuning of a language model on given SFT datasets.</p> <p>Parameters:</p> <ul> <li> <code>optimizer</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration for the optimizer.</p> </li> <li> <code>lr_scheduler</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration for the learning rate scheduler.</p> </li> <li> <code>dataloader_kwargs</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration for the dataloader, such as batch size, num_workers, etc.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>)           \u2013            <p>Maximum number of epochs to train the model. If set to -1, the training will continue indefinitely or until max_steps is reached.</p> </li> <li> <code>max_steps</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Maximum number of steps to train the model. If set to -1, the training will continue indefinitely or until max_epochs is reached.</p> </li> <li> <code>max_steps_per_epoch</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Maximum number of steps to train the model in each epoch. If set to -1, the training will continue until the end of the epoch.</p> </li> <li> <code>lr_scheduler_interval</code>               (<code>str</code>, default:                   <code>'step'</code> )           \u2013            <p>Interval at which to run the learning rate scheduler. Available options: 'epoch', 'step'. If set to 'epoch', the scheduler will run at the end of each epoch. If set to 'step', the scheduler will run at the end of each step.</p> </li> <li> <code>lr_scheduler_frequency</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Frequency at which to run the learning rate scheduler. The scheduler will run every <code>lr_scheduler_frequency</code> epochs or steps, depending on the value of <code>lr_scheduler_interval</code>.</p> </li> <li> <code>checkpoint_save_interval</code>               (<code>str</code>, default:                   <code>'epoch'</code> )           \u2013            <p>Interval at which to save the model checkpoint. Available options: 'epoch', 'step'. If set to 'epoch', the model will be saved at the end of each epoch. If set to 'step', the model will be saved at the end of each step.</p> </li> <li> <code>checkpoint_save_frequency</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Frequency at which to save the model checkpoint. The model will be saved every <code>checkpoint_save_frequency</code> epochs or steps, depending on the value of <code>checkpoint_save_interval</code>.</p> </li> <li> <code>accumulate_grad_batches</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of batches to accumulate gradients across before updating the model parameters.</p> </li> <li> <code>gradient_clip_val</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Value to clip the gradients. If set to None, no gradient clipping will be applied.</p> </li> <li> <code>gradient_clip_algorithm</code>               (<code>str</code>, default:                   <code>'norm'</code> )           \u2013            <p>Algorithm to use for gradient clipping. Available options: 'value', 'norm'. If set to 'value', the gradients will be clipped to the specified value. If set to 'norm', the gradients will be clipped to the specified norm.</p> </li> <li> <code>save_optimizer_state</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the optimizer and lr_scheduler state along with the model checkpoint.</p> </li> <li> <code>save_full_model</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the full model or only the trainable parameters in the model checkpoint.</p> </li> <li> <code>save_ckpt_type</code>               (<code>str</code>, default:                   <code>'lightning'</code> )           \u2013            <p>Type of checkpoint to save. Available options: 'lightning', 'hf'. If set to 'lightning', the checkpoint will be saved in the lightning format. If set to 'hf', the checkpoint will be saved in the huggingface format.</p> </li> <li> <code>ckpt_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the checkpoint to load before training. If set to None, no checkpoint will be loaded.</p> </li> <li> <code>max_length</code>               (<code>int</code>, default:                   <code>6144</code> )           \u2013            <p>Maximum input length to consider. If the input length exceeds this value, it will be truncated.</p> </li> <li> <code>fix_token_embedding</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to fix the token embeddings during training. If set to True, the token embeddings will not be updated during training.</p> </li> </ul> Source code in <code>fusion_bench/method/lm_finetune/fullfinetune_sft.py</code> <pre><code>def __init__(\n    self,\n    optimizer: DictConfig,\n    lr_scheduler: Optional[DictConfig],\n    dataloader_kwargs: DictConfig,\n    max_epochs: int,\n    max_steps: int = -1,\n    max_steps_per_epoch: int = -1,\n    lr_scheduler_interval: Literal[\"epoch\", \"step\"] = \"step\",\n    lr_scheduler_frequency: int = 1,\n    checkpoint_save_interval: Literal[\"epoch\", \"step\"] = \"epoch\",\n    checkpoint_save_frequency: int = 1,\n    accumulate_grad_batches: int = 1,\n    gradient_clip_val: Optional[float] = None,\n    gradient_clip_algorithm: Literal[\"value\", \"norm\"] = \"norm\",\n    save_optimizer_state: bool = False,\n    save_full_model: bool = False,\n    save_ckpt_type: Literal[\"lightning\", \"hf\"] = \"lightning\",\n    ckpt_path: Optional[str] = None,\n    max_length: int = 6144,\n    fix_token_embedding: bool = True,\n    **kwargs,\n):\n    \"\"\"\n    Class for full finetuning of a language model on given SFT datasets.\n\n    Args:\n        optimizer(DictConfig): Configuration for the optimizer.\n        lr_scheduler(DictConfig): Configuration for the learning rate scheduler.\n        dataloader_kwargs(DictConfig): Configuration for the dataloader, such as batch size, num_workers, etc.\n        max_epochs(int): Maximum number of epochs to train the model. If set to -1, the training will continue indefinitely or until max_steps is reached.\n        max_steps(int): Maximum number of steps to train the model. If set to -1, the training will continue indefinitely or until max_epochs is reached.\n        max_steps_per_epoch(int): Maximum number of steps to train the model in each epoch. If set to -1, the training will continue until the end of the epoch.\n        lr_scheduler_interval(str): Interval at which to run the learning rate scheduler. Available options: 'epoch', 'step'. If set to 'epoch', the scheduler will run at the end of each epoch. If set to 'step', the scheduler will run at the end of each step.\n        lr_scheduler_frequency(int): Frequency at which to run the learning rate scheduler. The scheduler will run every `lr_scheduler_frequency` epochs or steps, depending on the value of `lr_scheduler_interval`.\n        checkpoint_save_interval(str): Interval at which to save the model checkpoint. Available options: 'epoch', 'step'. If set to 'epoch', the model will be saved at the end of each epoch. If set to 'step', the model will be saved at the end of each step.\n        checkpoint_save_frequency(int): Frequency at which to save the model checkpoint. The model will be saved every `checkpoint_save_frequency` epochs or steps, depending on the value of `checkpoint_save_interval`.\n        accumulate_grad_batches(int): Number of batches to accumulate gradients across before updating the model parameters.\n        gradient_clip_val(float): Value to clip the gradients. If set to None, no gradient clipping will be applied.\n        gradient_clip_algorithm(str): Algorithm to use for gradient clipping. Available options: 'value', 'norm'. If set to 'value', the gradients will be clipped to the specified value. If set to 'norm', the gradients will be clipped to the specified norm.\n        save_optimizer_state(bool): Whether to save the optimizer and lr_scheduler state along with the model checkpoint.\n        save_full_model(bool): Whether to save the full model or only the trainable parameters in the model checkpoint.\n        save_ckpt_type (str): Type of checkpoint to save. Available options: 'lightning', 'hf'. If set to 'lightning', the checkpoint will be saved in the lightning format. If set to 'hf', the checkpoint will be saved in the huggingface format.\n        ckpt_path(str): Path to the checkpoint to load before training. If set to None, no checkpoint will be loaded.\n        max_length(int): Maximum input length to consider. If the input length exceeds this value, it will be truncated.\n        fix_token_embedding(bool): Whether to fix the token embeddings during training. If set to True, the token embeddings will not be updated during training.\n    \"\"\"\n    self._optimizer = optimizer\n    self._lr_scheduler = lr_scheduler\n    self.dataloader_kwargs = dataloader_kwargs\n    self.max_epochs = max_epochs\n    self.max_steps = max_steps\n    self.max_steps_per_epoch = max_steps_per_epoch\n    self.lr_scheduler_interval = lr_scheduler_interval\n    self.lr_scheduler_frequency = lr_scheduler_frequency\n    self.checkpoint_save_interval = checkpoint_save_interval\n    self.checkpoint_save_frequency = checkpoint_save_frequency\n    self.accumulate_grad_batches = accumulate_grad_batches\n    self.gradient_clip_val = gradient_clip_val\n    self.gradient_clip_algorithm = gradient_clip_algorithm\n    self.save_optimizer_state = save_optimizer_state\n    self.save_full_model = save_full_model\n    self.save_ckpt_type = save_ckpt_type\n    self.ckpt_path = ckpt_path\n    self.max_length = max_length\n    self.fix_token_embedding = fix_token_embedding\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/training/#fusion_bench.method.PeftFinetuneSFT","title":"<code>PeftFinetuneSFT</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>FabricTrainingMixin</code></p> Source code in <code>fusion_bench/method/lm_finetune/peftfinetune_sft.py</code> <pre><code>class PeftFinetuneSFT(BaseAlgorithm, FabricTrainingMixin):\n\n    model: Union[\n        nn.Module, \"_FabricModule\", \"LlamaForCausalLM\", PeftModel, peft.LoraModel\n    ]\n    optimizer: Union[torch.optim.Optimizer, \"_FabricOptimizer\"]\n    train_dataloader: Union[DataLoader, \"_FabricDataLoader\"]\n    lr_scheduler: torch.optim.lr_scheduler.LRScheduler\n    _latest_saved_checkpoint_global_step: int = -1\n\n    def __init__(\n        self,\n        optimizer: DictConfig,\n        lr_scheduler: Optional[DictConfig],\n        peft_config: DictConfig,\n        dataloader_kwargs: DictConfig,\n        adapter_name: str = \"default\",\n        merge_and_unload: bool = False,\n        max_epochs: int = 1,\n        max_steps: int = -1,\n        max_steps_per_epoch: int = -1,\n        lr_scheduler_interval: Literal[\"epoch\", \"step\"] = \"step\",\n        lr_scheduler_frequency: int = 1,\n        checkpoint_save_interval: Literal[\"epoch\", \"step\"] = \"epoch\",\n        checkpoint_save_frequency: int = 1,\n        accumulate_grad_batches: int = 1,\n        gradient_clip_val: Optional[float] = None,\n        gradient_clip_algorithm: Literal[\"value\", \"norm\"] = \"norm\",\n        save_optimizer_state: bool = False,\n        save_full_model: bool = False,\n        save_ckpt_type: Literal[\"lightning\", \"peft\"] = \"peft\",\n        ckpt_path: Optional[str] = None,\n        max_length: int = 6144,\n        **kwargs,\n    ):\n        \"\"\"\n        Class for full finetuning of a language model on given SFT datasets.\n\n        Args:\n            optimizer(DictConfig): Configuration for the optimizer.\n            lr_scheduler(DictConfig): Configuration for the learning rate scheduler.\n            peft_config(DictConfig): Configuration for the PEFT model.\n            dataloader_kwargs(DictConfig): Configuration for the dataloader, such as batch size, num_workers, etc.\n            adapter_name(str): Name of the adapter to use for the PEFT model.\n            merge_and_unload(bool): Whether to merge and unload the model after training.\n            max_epochs(int): Maximum number of epochs to train the model. If set to -1, the training will continue indefinitely or until max_steps is reached.\n            max_steps(int): Maximum number of steps to train the model. If set to -1, the training will continue indefinitely or until max_epochs is reached.\n            max_steps_per_epoch(int): Maximum number of steps to train the model in each epoch. If set to -1, the training will continue until the end of the epoch.\n            lr_scheduler_interval(str): Interval at which to run the learning rate scheduler. Available options: 'epoch', 'step'. If set to 'epoch', the scheduler will run at the end of each epoch. If set to 'step', the scheduler will run at the end of each step.\n            lr_scheduler_frequency(int): Frequency at which to run the learning rate scheduler. The scheduler will run every `lr_scheduler_frequency` epochs or steps, depending on the value of `lr_scheduler_interval`.\n            checkpoint_save_interval(str): Interval at which to save the model checkpoint. Available options: 'epoch', 'step'. If set to 'epoch', the model will be saved at the end of each epoch. If set to 'step', the model will be saved at the end of each step.\n            checkpoint_save_frequency(int): Frequency at which to save the model checkpoint. The model will be saved every `checkpoint_save_frequency` epochs or steps, depending on the value of `checkpoint_save_interval`.\n            accumulate_grad_batches(int): Number of batches to accumulate gradients across before updating the model parameters.\n            gradient_clip_val(float): Value to clip the gradients. If set to None, no gradient clipping will be applied.\n            gradient_clip_algorithm(str): Algorithm to use for gradient clipping. Available options: 'value', 'norm'. If set to 'value', the gradients will be clipped to the specified value. If set to 'norm', the gradients will be clipped to the specified norm.\n            save_optimizer_state(bool): Whether to save the optimizer and lr_scheduler state along with the model checkpoint.\n            save_full_model(bool): Whether to save the full model or only the trainable parameters in the model checkpoint.\n            save_ckpt_type(str): Type of checkpoint to save. Available options: 'lightning', 'peft'. If set to 'lightning', the model will be saved using the Lightning checkpointing mechanism. If set to 'peft', the model will be saved using the PEFT checkpointing mechanism.\n            ckpt_path(str): Path to the checkpoint to load before training. If set to None, no checkpoint will be loaded.\n        \"\"\"\n        self._optimizer = optimizer\n        self._lr_scheduler = lr_scheduler\n        self._peft_config = peft_config\n        self.dataloader_kwargs = dataloader_kwargs\n        self.adapter_name = adapter_name\n        self.merge_and_unload = merge_and_unload\n        self.max_epochs = max_epochs\n        self.max_steps = max_steps\n        self.max_steps_per_epoch = max_steps_per_epoch\n        self.lr_scheduler_interval = lr_scheduler_interval\n        self.lr_scheduler_frequency = lr_scheduler_frequency\n        self.checkpoint_save_interval = checkpoint_save_interval\n        self.checkpoint_save_frequency = checkpoint_save_frequency\n        self.accumulate_grad_batches = accumulate_grad_batches\n        self.gradient_clip_val = gradient_clip_val\n        self.gradient_clip_algorithm = gradient_clip_algorithm\n        self.save_optimizer_state = save_optimizer_state\n        self.save_full_model = save_full_model\n        self.save_ckpt_type = save_ckpt_type\n        self.ckpt_path = ckpt_path\n        self.max_length = max_length\n        super().__init__(**kwargs)\n\n    def run(self, modelpool: CausalLMPool):\n        self.modelpool = modelpool\n        self.setup()\n        self.train(self.model, self.optimizer, self.lr_scheduler)\n\n        if self.merge_and_unload:\n            self.model = self.model.merge_and_unload()\n        return self.model\n\n    def setup_model(self):\n        # https://github.com/Lightning-AI/litgpt/blob/main/litgpt/finetune/lora.py\n        self.tokenizer = self.modelpool.load_tokenizer()\n        if self.tokenizer.pad_token_id is None:\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n\n        model = self.modelpool.load_pretrained_model()\n\n        # get the PEFT model\n        peft_config = instantiate(self._peft_config, _convert_=\"all\")\n        peft_config.save_pretrained(os.path.join(self.log_dir, \"peft_config\"))\n        peft_model = get_peft_model(model, peft_config, self.adapter_name)\n        peft_model.print_trainable_parameters()\n\n        self.model = peft_model\n\n        if self.fabric.strategy == \"fsdp\" or isinstance(\n            self.fabric.strategy, FSDPStrategy\n        ):\n            # https://github.com/Lightning-AI/pytorch-lightning/issues/19267\n            self.model.gradient_checkpointing_enable(\n                gradient_checkpointing_kwargs={\"use_reentrant\": True}\n            )\n            self.use_cache = False\n        else:\n            self.use_cache = True\n\n        self.model_dtype = get_dtype(self.model)\n        self.model = self.model.to(dtype=self.model_dtype)\n\n    def configure_optimizer(self):\n        # compute expected total steps\n        self.compute_expected_total_steps(self.train_dataloader)\n\n        optimizer = instantiate(self._optimizer, self.model.parameters())\n        if self._lr_scheduler is not None:\n            for key, arg in self._lr_scheduler.items():\n                if arg == \"_T_max_\":\n                    log.info(\n                        f\"Setting key `{key}` of lr_scheduler configuration to {self.expected_total_steps}\"\n                    )\n                    self._lr_scheduler[key] = self.expected_total_steps\n            lr_scheduler: torch.optim.lr_scheduler.LRScheduler = instantiate(\n                self._lr_scheduler,\n                optimizer=optimizer,\n            )\n        else:\n            lr_scheduler = None\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n\n    def setup_data(self):\n        fabric = self.fabric\n        modelpool = self.modelpool\n        assert (\n            len(modelpool.train_dataset_names) &gt; 0\n        ), \"No training datasets found in modelpool.\"\n\n        train_datasets = [\n            modelpool.load_train_dataset(dataset_name)\n            for dataset_name in modelpool.train_dataset_names\n        ]\n        if len(train_datasets) &gt; 1:\n            train_dataset = ConcatDataset(train_datasets)\n        else:\n            train_dataset = train_datasets[0]\n\n        self.train_dataset = train_dataset\n        self.train_dataloader = DataLoader(\n            train_dataset,\n            **self.dataloader_kwargs,\n            shuffle=True,\n            collate_fn=functools.partial(\n                padded_collate_sft, pad_token_id=self.tokenizer.pad_token_id\n            ),\n        )\n        self.train_dataloader = fabric.setup_dataloaders(self.train_dataloader)\n\n    def setup(self):\n        fabric = self.fabric\n\n        self.setup_model()\n        self.setup_data()\n\n        optimizer = self.configure_optimizer()\n        optimizer, lr_scheduler = optimizer[\"optimizer\"], optimizer[\"lr_scheduler\"]\n\n        self.model = self.fabric.setup_module(self.model)\n        self.optimizer = self.fabric.setup_optimizers(optimizer)\n        self.lr_scheduler = lr_scheduler\n\n    @override\n    def train_epoch(self, *args, **kwargs):\n        fabric = self.fabric\n\n        accumulated_loss = 0\n        for step_idx, batch in enumerate(\n            pbar := tqdm(\n                self.train_dataloader,\n                desc=\"Training Batches\",\n                dynamic_ncols=True,\n                leave=False,\n                disable=not fabric.is_global_zero,\n            )\n        ):\n            is_accumulating = (step_idx + 1) % self.accumulate_grad_batches != 0\n\n            if self.max_length &gt; 0 and batch[\"input_ids\"].shape[1] &gt; self.max_length:\n                log.warning(\n                    f\"Input length exceeds max_length: {batch['input_ids'].shape[1]} &gt; {self.max_length}. Truncating input.\"\n                )\n                batch[\"input_ids\"] = batch[\"input_ids\"][:, : self.max_length]\n                batch[\"attention_mask\"] = batch[\"attention_mask\"][:, : self.max_length]\n                batch[\"labels\"] = batch[\"labels\"][:, : self.max_length]\n\n            # disable gradient synchronization if accumulating gradients across steps for improved performance\n            with fabric.no_backward_sync(self.model, enabled=is_accumulating):\n                # use_cache=True is not compatible with gradient checkpointing, so we disable it here\n                output = self.model(\n                    input_ids=batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                    labels=batch[\"labels\"],\n                    use_cache=self.use_cache,\n                )\n                loss = output[\"loss\"] / self.accumulate_grad_batches\n\n                fabric.backward(loss)\n                accumulated_loss += loss.item()\n\n            if not is_accumulating:\n                self.clip_gradients_if_needed(self.model, self.optimizer)\n\n                # run lr_scheduler at the end of the step if interval is set to \"step\"\n                if (\n                    self.lr_scheduler_interval == \"step\"\n                    and (self.global_step_idx + 1) % self.lr_scheduler_frequency == 0\n                ):\n                    self.lr_scheduler.step()\n\n                # update the model parameters and zero the gradients\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n\n                metrics = {\n                    \"train/loss\": accumulated_loss,\n                    \"train/epoch_idx\": self.epoch_idx,\n                    \"train/lr\": self.optimizer.param_groups[0][\"lr\"],\n                }\n                fabric.log_dict(metrics, step=self.global_step_idx)\n                pbar.set_postfix(metrics)\n\n                # save the model at the end of the step if interval is set to \"step\" and frequency is met\n                self.conditional_checkpoint_save(stage=\"end_of_step\")\n\n                # break if max_steps_per_epoch is set, and exit epoch\n                if (\n                    self.max_steps_per_epoch &gt; 0\n                    and step_idx + 1 &gt;= self.max_steps_per_epoch\n                ):\n                    break\n                # break if max_steps is set, and exit training\n                if self.max_steps &gt; 0 and self.global_step_idx &gt;= self.max_steps - 1:\n                    self.is_training = False\n                    break\n\n                self.global_step_idx += 1\n                accumulated_loss = 0\n\n    def save_checkpoint(\n        self,\n        path: Union[str, Path],\n        save_optimizer_state: Optional[bool] = None,\n        overwrite: bool = False,\n    ):\n        if not overwrite and os.path.exists(path):\n            return log.warning(f\"Checkpoint already exists at {path}. Skipping save.\")\n\n        fabric = self.fabric\n        if self.save_ckpt_type == \"lightning\":\n            state = {\"model\": self.model}\n\n            # save the optimizer and lr_scheduler state if needed\n            if self.save_optimizer_state and save_optimizer_state is not False:\n                state.update(\n                    {\n                        \"optimizer\": self.optimizer,\n                        \"lr_scheduler\": self.lr_scheduler,\n                        \"global_step_idx\": self.global_step_idx,\n                        \"epoch_idx\": self.epoch_idx,\n                    }\n                )\n            trainable_param_names = set(\n                name\n                for name, param in self.model.state_dict(keep_vars=True).items()\n                if param.requires_grad\n            )\n            filter = (\n                None\n                if self.save_full_model\n                else {\"model\": lambda k, p: k in trainable_param_names}\n            )\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            fabric.save(path, state=state, filter=filter)\n        elif self.save_ckpt_type == \"peft\":\n            self.model.save_pretrained(path, is_main_process=fabric.is_global_zero)\n        else:\n            raise ValueError(\n                f\"Unknown save_ckpt_type: {self.save_ckpt_type}. Available options: 'lightning', 'peft'\"\n            )\n        self._latest_saved_checkpoint_global_step = self.global_step_idx\n\n    def load_checkpoint(self, path: Union[str, Path]):\n        fabric = self.fabric\n\n        state = {\"model\": self.model}\n\n        # save the optimizer and lr_scheduler state if needed\n        if self.save_optimizer_state:\n            state.update(\n                {\n                    \"optimizer\": self.optimizer,\n                    \"lr_scheduler\": self.lr_scheduler,\n                }\n            )\n\n        fabric.load(path, state)\n</code></pre>"},{"location":"api/fusion_bench.method/training/#fusion_bench.method.PeftFinetuneSFT.__init__","title":"<code>__init__(optimizer, lr_scheduler, peft_config, dataloader_kwargs, adapter_name='default', merge_and_unload=False, max_epochs=1, max_steps=-1, max_steps_per_epoch=-1, lr_scheduler_interval='step', lr_scheduler_frequency=1, checkpoint_save_interval='epoch', checkpoint_save_frequency=1, accumulate_grad_batches=1, gradient_clip_val=None, gradient_clip_algorithm='norm', save_optimizer_state=False, save_full_model=False, save_ckpt_type='peft', ckpt_path=None, max_length=6144, **kwargs)</code>","text":"<p>Class for full finetuning of a language model on given SFT datasets.</p> <p>Parameters:</p> <ul> <li> <code>optimizer</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration for the optimizer.</p> </li> <li> <code>lr_scheduler</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration for the learning rate scheduler.</p> </li> <li> <code>peft_config</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration for the PEFT model.</p> </li> <li> <code>dataloader_kwargs</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration for the dataloader, such as batch size, num_workers, etc.</p> </li> <li> <code>adapter_name</code>               (<code>str</code>, default:                   <code>'default'</code> )           \u2013            <p>Name of the adapter to use for the PEFT model.</p> </li> <li> <code>merge_and_unload</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to merge and unload the model after training.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Maximum number of epochs to train the model. If set to -1, the training will continue indefinitely or until max_steps is reached.</p> </li> <li> <code>max_steps</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Maximum number of steps to train the model. If set to -1, the training will continue indefinitely or until max_epochs is reached.</p> </li> <li> <code>max_steps_per_epoch</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Maximum number of steps to train the model in each epoch. If set to -1, the training will continue until the end of the epoch.</p> </li> <li> <code>lr_scheduler_interval</code>               (<code>str</code>, default:                   <code>'step'</code> )           \u2013            <p>Interval at which to run the learning rate scheduler. Available options: 'epoch', 'step'. If set to 'epoch', the scheduler will run at the end of each epoch. If set to 'step', the scheduler will run at the end of each step.</p> </li> <li> <code>lr_scheduler_frequency</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Frequency at which to run the learning rate scheduler. The scheduler will run every <code>lr_scheduler_frequency</code> epochs or steps, depending on the value of <code>lr_scheduler_interval</code>.</p> </li> <li> <code>checkpoint_save_interval</code>               (<code>str</code>, default:                   <code>'epoch'</code> )           \u2013            <p>Interval at which to save the model checkpoint. Available options: 'epoch', 'step'. If set to 'epoch', the model will be saved at the end of each epoch. If set to 'step', the model will be saved at the end of each step.</p> </li> <li> <code>checkpoint_save_frequency</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Frequency at which to save the model checkpoint. The model will be saved every <code>checkpoint_save_frequency</code> epochs or steps, depending on the value of <code>checkpoint_save_interval</code>.</p> </li> <li> <code>accumulate_grad_batches</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of batches to accumulate gradients across before updating the model parameters.</p> </li> <li> <code>gradient_clip_val</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Value to clip the gradients. If set to None, no gradient clipping will be applied.</p> </li> <li> <code>gradient_clip_algorithm</code>               (<code>str</code>, default:                   <code>'norm'</code> )           \u2013            <p>Algorithm to use for gradient clipping. Available options: 'value', 'norm'. If set to 'value', the gradients will be clipped to the specified value. If set to 'norm', the gradients will be clipped to the specified norm.</p> </li> <li> <code>save_optimizer_state</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the optimizer and lr_scheduler state along with the model checkpoint.</p> </li> <li> <code>save_full_model</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the full model or only the trainable parameters in the model checkpoint.</p> </li> <li> <code>save_ckpt_type</code>               (<code>str</code>, default:                   <code>'peft'</code> )           \u2013            <p>Type of checkpoint to save. Available options: 'lightning', 'peft'. If set to 'lightning', the model will be saved using the Lightning checkpointing mechanism. If set to 'peft', the model will be saved using the PEFT checkpointing mechanism.</p> </li> <li> <code>ckpt_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the checkpoint to load before training. If set to None, no checkpoint will be loaded.</p> </li> </ul> Source code in <code>fusion_bench/method/lm_finetune/peftfinetune_sft.py</code> <pre><code>def __init__(\n    self,\n    optimizer: DictConfig,\n    lr_scheduler: Optional[DictConfig],\n    peft_config: DictConfig,\n    dataloader_kwargs: DictConfig,\n    adapter_name: str = \"default\",\n    merge_and_unload: bool = False,\n    max_epochs: int = 1,\n    max_steps: int = -1,\n    max_steps_per_epoch: int = -1,\n    lr_scheduler_interval: Literal[\"epoch\", \"step\"] = \"step\",\n    lr_scheduler_frequency: int = 1,\n    checkpoint_save_interval: Literal[\"epoch\", \"step\"] = \"epoch\",\n    checkpoint_save_frequency: int = 1,\n    accumulate_grad_batches: int = 1,\n    gradient_clip_val: Optional[float] = None,\n    gradient_clip_algorithm: Literal[\"value\", \"norm\"] = \"norm\",\n    save_optimizer_state: bool = False,\n    save_full_model: bool = False,\n    save_ckpt_type: Literal[\"lightning\", \"peft\"] = \"peft\",\n    ckpt_path: Optional[str] = None,\n    max_length: int = 6144,\n    **kwargs,\n):\n    \"\"\"\n    Class for full finetuning of a language model on given SFT datasets.\n\n    Args:\n        optimizer(DictConfig): Configuration for the optimizer.\n        lr_scheduler(DictConfig): Configuration for the learning rate scheduler.\n        peft_config(DictConfig): Configuration for the PEFT model.\n        dataloader_kwargs(DictConfig): Configuration for the dataloader, such as batch size, num_workers, etc.\n        adapter_name(str): Name of the adapter to use for the PEFT model.\n        merge_and_unload(bool): Whether to merge and unload the model after training.\n        max_epochs(int): Maximum number of epochs to train the model. If set to -1, the training will continue indefinitely or until max_steps is reached.\n        max_steps(int): Maximum number of steps to train the model. If set to -1, the training will continue indefinitely or until max_epochs is reached.\n        max_steps_per_epoch(int): Maximum number of steps to train the model in each epoch. If set to -1, the training will continue until the end of the epoch.\n        lr_scheduler_interval(str): Interval at which to run the learning rate scheduler. Available options: 'epoch', 'step'. If set to 'epoch', the scheduler will run at the end of each epoch. If set to 'step', the scheduler will run at the end of each step.\n        lr_scheduler_frequency(int): Frequency at which to run the learning rate scheduler. The scheduler will run every `lr_scheduler_frequency` epochs or steps, depending on the value of `lr_scheduler_interval`.\n        checkpoint_save_interval(str): Interval at which to save the model checkpoint. Available options: 'epoch', 'step'. If set to 'epoch', the model will be saved at the end of each epoch. If set to 'step', the model will be saved at the end of each step.\n        checkpoint_save_frequency(int): Frequency at which to save the model checkpoint. The model will be saved every `checkpoint_save_frequency` epochs or steps, depending on the value of `checkpoint_save_interval`.\n        accumulate_grad_batches(int): Number of batches to accumulate gradients across before updating the model parameters.\n        gradient_clip_val(float): Value to clip the gradients. If set to None, no gradient clipping will be applied.\n        gradient_clip_algorithm(str): Algorithm to use for gradient clipping. Available options: 'value', 'norm'. If set to 'value', the gradients will be clipped to the specified value. If set to 'norm', the gradients will be clipped to the specified norm.\n        save_optimizer_state(bool): Whether to save the optimizer and lr_scheduler state along with the model checkpoint.\n        save_full_model(bool): Whether to save the full model or only the trainable parameters in the model checkpoint.\n        save_ckpt_type(str): Type of checkpoint to save. Available options: 'lightning', 'peft'. If set to 'lightning', the model will be saved using the Lightning checkpointing mechanism. If set to 'peft', the model will be saved using the PEFT checkpointing mechanism.\n        ckpt_path(str): Path to the checkpoint to load before training. If set to None, no checkpoint will be loaded.\n    \"\"\"\n    self._optimizer = optimizer\n    self._lr_scheduler = lr_scheduler\n    self._peft_config = peft_config\n    self.dataloader_kwargs = dataloader_kwargs\n    self.adapter_name = adapter_name\n    self.merge_and_unload = merge_and_unload\n    self.max_epochs = max_epochs\n    self.max_steps = max_steps\n    self.max_steps_per_epoch = max_steps_per_epoch\n    self.lr_scheduler_interval = lr_scheduler_interval\n    self.lr_scheduler_frequency = lr_scheduler_frequency\n    self.checkpoint_save_interval = checkpoint_save_interval\n    self.checkpoint_save_frequency = checkpoint_save_frequency\n    self.accumulate_grad_batches = accumulate_grad_batches\n    self.gradient_clip_val = gradient_clip_val\n    self.gradient_clip_algorithm = gradient_clip_algorithm\n    self.save_optimizer_state = save_optimizer_state\n    self.save_full_model = save_full_model\n    self.save_ckpt_type = save_ckpt_type\n    self.ckpt_path = ckpt_path\n    self.max_length = max_length\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/training/#reward-modeling","title":"Reward Modeling","text":""},{"location":"api/fusion_bench.method/training/#fusion_bench.method.BradleyTerryRewardModeling","title":"<code>BradleyTerryRewardModeling</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>FabricTrainingMixin</code></p> Source code in <code>fusion_bench/method/lm_finetune/bradley_terry_rm.py</code> <pre><code>class BradleyTerryRewardModeling(BaseAlgorithm, FabricTrainingMixin):\n\n    model: Union[nn.Module, \"_FabricModule\", \"LlamaForSequenceClassification\"]\n    optimizer: Union[torch.optim.Optimizer, \"_FabricOptimizer\"]\n    train_dataloader: Union[DataLoader, \"_FabricDataLoader\"]\n    lr_scheduler: torch.optim.lr_scheduler.LRScheduler\n\n    def __init__(\n        self,\n        optimizer: DictConfig,\n        lr_scheduler: Optional[DictConfig],\n        dataloader_kwargs: DictConfig,\n        max_epochs: int,\n        max_steps: int = -1,\n        max_steps_per_epoch: int = -1,\n        lr_scheduler_interval: Literal[\"epoch\", \"step\"] = \"step\",\n        lr_scheduler_frequency: int = 1,\n        checkpoint_save_interval: Literal[\"epoch\", \"step\"] = \"epoch\",\n        checkpoint_save_frequency: int = 1,\n        accumulate_grad_batches: int = 1,\n        gradient_clip_val: Optional[float] = None,\n        gradient_clip_algorithm: Literal[\"value\", \"norm\"] = \"norm\",\n        save_optimizer_state: bool = False,\n        save_full_model: bool = False,\n        save_ckpt_type: Literal[\"lightning\", \"hf\"] = \"lightning\",\n        ckpt_path: Optional[str] = None,\n        max_length: int = 6144,\n        fix_token_embedding: bool = True,\n        **kwargs,\n    ):\n        \"\"\"\n        Class for reward modeling using Bradley-Terry model.\n\n        Args:\n            optimizer(DictConfig): Configuration for the optimizer.\n            lr_scheduler(DictConfig): Configuration for the learning rate scheduler.\n            dataloader_kwargs(DictConfig): Configuration for the dataloader, such as batch size, num_workers, etc.\n            max_epochs(int): Maximum number of epochs to train the model. If set to -1, the training will continue indefinitely or until max_steps is reached.\n            max_steps(int): Maximum number of steps to train the model. If set to -1, the training will continue indefinitely or until max_epochs is reached.\n            max_steps_per_epoch(int): Maximum number of steps to train the model in each epoch. If set to -1, the training will continue until the end of the epoch.\n            lr_scheduler_interval(str): Interval at which to run the learning rate scheduler. Available options: 'epoch', 'step'. If set to 'epoch', the scheduler will run at the end of each epoch. If set to 'step', the scheduler will run at the end of each step.\n            lr_scheduler_frequency(int): Frequency at which to run the learning rate scheduler. The scheduler will run every `lr_scheduler_frequency` epochs or steps, depending on the value of `lr_scheduler_interval`.\n            checkpoint_save_interval(str): Interval at which to save the model checkpoint. Available options: 'epoch', 'step'. If set to 'epoch', the model will be saved at the end of each epoch. If set to 'step', the model will be saved at the end of each step.\n            checkpoint_save_frequency(int): Frequency at which to save the model checkpoint. The model will be saved every `checkpoint_save_frequency` epochs or steps, depending on the value of `checkpoint_save_interval`.\n            accumulate_grad_batches(int): Number of batches to accumulate gradients across before updating the model parameters.\n            gradient_clip_val(float): Value to clip the gradients. If set to None, no gradient clipping will be applied.\n            gradient_clip_algorithm(str): Algorithm to use for gradient clipping. Available options: 'value', 'norm'. If set to 'value', the gradients will be clipped to the specified value. If set to 'norm', the gradients will be clipped to the specified norm.\n            save_optimizer_state(bool): Whether to save the optimizer and lr_scheduler state along with the model checkpoint.\n            save_full_model(bool): Whether to save the full model or only the trainable parameters in the model checkpoint.\n            save_ckpt_type (str): Type of checkpoint to save. Available options: 'lightning', 'hf'. If set to 'lightning', the checkpoint will be saved in the lightning format. If set to 'hf', the checkpoint will be saved in the huggingface format.\n            ckpt_path(str): Path to the checkpoint to load before training. If set to None, no checkpoint will be loaded.\n            max_length(int): Maximum input length to consider. If the input length exceeds this value, it will be truncated.\n            fix_token_embedding(bool): Whether to fix the token embeddings during training. If set to True, the token embeddings will not be updated during training.\n        \"\"\"\n        self._optimizer = optimizer\n        self._lr_scheduler = lr_scheduler\n        self.dataloader_kwargs = dataloader_kwargs\n        self.max_epochs = max_epochs\n        self.max_steps = max_steps\n        self.max_steps_per_epoch = max_steps_per_epoch\n        self.lr_scheduler_interval = lr_scheduler_interval\n        self.lr_scheduler_frequency = lr_scheduler_frequency\n        self.checkpoint_save_interval = checkpoint_save_interval\n        self.checkpoint_save_frequency = checkpoint_save_frequency\n        self.accumulate_grad_batches = accumulate_grad_batches\n        self.gradient_clip_val = gradient_clip_val\n        self.gradient_clip_algorithm = gradient_clip_algorithm\n        self.save_optimizer_state = save_optimizer_state\n        self.save_full_model = save_full_model\n        self.save_ckpt_type = save_ckpt_type\n        self.ckpt_path = ckpt_path\n        self.max_length = max_length\n        self.fix_token_embedding = fix_token_embedding\n        super().__init__(**kwargs)\n\n    def run(self, modelpool: SequenceClassificationModelPool):\n        self.modelpool = modelpool\n        self.setup()\n        self.train(self.model, self.optimizer, self.lr_scheduler)\n        return self.model\n\n    def setup_model(self):\n        self.tokenizer = self.modelpool.load_tokenizer()\n        if self.tokenizer.pad_token_id is None:\n            self.tokenizer.pad_token_id = (\n                self.tokenizer.eos_token_id\n            )  #! make sure eos_token_id only show up at the end of the sequence\n\n        model = self.modelpool.load_pretrained_model()\n        self.model: \"LlamaForSequenceClassification\" = model\n\n        if model.config.pad_token_id is None:\n            model.config.pad_token_id = self.tokenizer.pad_token_id\n\n        if self.fix_token_embedding:\n            self.model.model.embed_tokens.requires_grad_(False)\n\n        if self.fabric.strategy == \"fsdp\" or isinstance(\n            self.fabric.strategy, FSDPStrategy\n        ):\n            # https://github.com/Lightning-AI/pytorch-lightning/issues/19267\n            self.model.gradient_checkpointing_enable(\n                gradient_checkpointing_kwargs={\"use_reentrant\": True}\n            )\n            self.use_cache = False\n        else:\n            self.use_cache = True\n        self.model_dtype = get_dtype(self.model)\n\n    def setup_data(self):\n        fabric = self.fabric\n        modelpool = self.modelpool\n        assert (\n            len(modelpool.train_dataset_names) &gt; 0\n        ), \"No training datasets found in modelpool.\"\n\n        train_datasets = [\n            modelpool.load_train_dataset(dataset_name)\n            for dataset_name in modelpool.train_dataset_names\n        ]\n        if len(train_datasets) &gt; 1:\n            train_dataset = ConcatDataset(train_datasets)\n        else:\n            train_dataset = train_datasets[0]\n\n        self.train_dataset = train_dataset\n        self.train_dataloader = DataLoader(\n            train_dataset,\n            **self.dataloader_kwargs,\n            shuffle=True,\n            collate_fn=functools.partial(\n                bradley_terry_rm_collate,\n                pad_token_id=self.tokenizer.pad_token_id,\n            ),  # NOTE: different from SFT, uses bradley_terry_rm_collate\n        )\n        self.train_dataloader = fabric.setup_dataloaders(self.train_dataloader)\n\n    def configure_optimizer(self):\n        # compute expected total steps\n        self.compute_expected_total_steps(self.train_dataloader)\n\n        optimizer = instantiate(self._optimizer, self.model.parameters())\n        if self._lr_scheduler is not None:\n            for key, arg in self._lr_scheduler.items():\n                if arg == \"_T_max_\":\n                    log.info(\n                        f\"Setting key `{key}` of lr_scheduler configuration to {self.expected_total_steps}\"\n                    )\n                    self._lr_scheduler[key] = self.expected_total_steps\n            lr_scheduler: torch.optim.lr_scheduler.LRScheduler = instantiate(\n                self._lr_scheduler,\n                optimizer=optimizer,\n            )\n        else:\n            lr_scheduler = None\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n\n    def setup(self):\n        fabric = self.fabric\n\n        self.setup_model()\n        self.setup_data()\n\n        optimizer = self.configure_optimizer()\n        optimizer, lr_scheduler = optimizer[\"optimizer\"], optimizer[\"lr_scheduler\"]\n\n        self.model, self.optimizer = fabric.setup(self.model, optimizer)\n        self.lr_scheduler = lr_scheduler\n\n    def compute_loss(self, batch: Dict[str, Union[Tensor, Any]]) -&gt; Dict[str, Tensor]:\n        \"\"\"\n        Maximize the likelihood of the winner over the loser using the Bradley-Terry model.\n\n        Args:\n            batch (Dict[str, Union[Tensor, Any]]): A dictionary containing the input token ids and attention masks for the winner and loser.\n        \"\"\"\n        batch_size = batch[\"input_ids\"].size(0)\n        assert batch_size % 2 == 0, \"Batch size must be even.\"\n\n        outputs = self.model(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            use_cache=self.use_cache,\n        )\n\n        rewards = outputs[0]\n        chosen_reward = rewards[: batch_size // 2]\n        rejected_rewards = rewards[batch_size // 2 :]\n        loss = -torch.log(torch.sigmoid(chosen_reward - rejected_rewards)).mean()\n\n        return {\n            \"chosen_reward\": chosen_reward,\n            \"rejected_reward\": rejected_rewards,\n            \"loss\": loss,\n        }\n\n    @override\n    def train_epoch(self, *args, **kwargs):\n        fabric = self.fabric\n\n        accumulated_loss = 0\n        accumulated_chosen_reward = 0\n        accumulated_rejected_reward = 0\n        for step_idx, batch in enumerate(\n            pbar := tqdm(\n                self.train_dataloader,\n                desc=\"Training Batches\",\n                dynamic_ncols=True,\n                leave=False,\n                disable=not fabric.is_global_zero,\n            )\n        ):\n            is_accumulating = (step_idx + 1) % self.accumulate_grad_batches != 0\n\n            if self.max_length &gt; 0 and batch[\"input_ids\"].shape[1] &gt; self.max_length:\n                log.warning(\n                    f\"Input length exceeds max_length: {batch['input_ids'].shape[1]} &gt; {self.max_length}. Truncating input.\"\n                )\n                batch[\"input_ids\"] = batch[\"input_ids\"][:, -self.max_length :]\n                batch[\"attention_mask\"] = batch[\"attention_mask\"][:, -self.max_length :]\n\n            # disable gradient synchronization if accumulating gradients across steps for improved performance\n            with fabric.no_backward_sync(self.model, enabled=is_accumulating):\n                # use_cache=True is not compatible with gradient checkpointing, so we disable it here\n                output = self.compute_loss(batch)\n                loss = output[\"loss\"] / self.accumulate_grad_batches\n\n                fabric.backward(loss)\n\n                accumulated_loss += loss.item()\n                accumulated_chosen_reward += output[\"chosen_reward\"].mean().item()\n                accumulated_rejected_reward += output[\"rejected_reward\"].mean().item()\n\n            # 1. update the model parameters if not accumulating gradients\n            # 2. step the lr_scheduler if interval is set to \"step\" and frequency is met\n            # 3. save the model if interval is set to \"step\" and frequency is met\n            # 4. log metrics\n            # 5. increase the global step index\n            if not is_accumulating:\n                self.clip_gradients_if_needed(self.model, self.optimizer)\n\n                # run lr_scheduler at the end of the step if interval is set to \"step\"\n                if (\n                    self.lr_scheduler_interval == \"step\"\n                    and (self.global_step_idx + 1) % self.lr_scheduler_frequency == 0\n                ):\n                    self.lr_scheduler.step()\n\n                # update the model parameters and zero the gradients\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n\n                metrics = {\n                    \"train/loss\": accumulated_loss,\n                    \"train/chosen_reward\": accumulated_chosen_reward\n                    / self.accumulate_grad_batches,\n                    \"train/rejected_reward\": accumulated_rejected_reward\n                    / self.accumulate_grad_batches,\n                    \"train/epoch_idx\": self.epoch_idx,\n                    \"train/lr\": self.optimizer.param_groups[0][\"lr\"],\n                }\n                metrics[\"train/chosen_reward-rejected_reward\"] = (\n                    metrics[\"train/chosen_reward\"] - metrics[\"train/rejected_reward\"]\n                )\n\n                fabric.log_dict(metrics, step=self.global_step_idx)\n                pbar.set_postfix(metrics)\n\n                # save the model at the end of the step if interval is set to \"step\" and frequency is met\n                self.conditional_checkpoint_save(stage=\"end_of_step\")\n\n                # break if max_steps_per_epoch is set, and exit epoch\n                if (\n                    self.max_steps_per_epoch &gt; 0\n                    and step_idx + 1 &gt;= self.max_steps_per_epoch\n                ):\n                    break\n                # break if max_steps is set, and exit training\n                if self.max_steps &gt; 0 and self.global_step_idx &gt;= self.max_steps - 1:\n                    self.is_training = False\n                    break\n\n                self.global_step_idx += 1\n                accumulated_loss = 0\n                accumulated_chosen_reward = 0\n                accumulated_rejected_reward = 0\n\n    def save_checkpoint(\n        self,\n        path: Union[str, Path],\n        save_optimizer_state: Optional[bool] = None,\n        overwrite: bool = False,\n    ):\n        if not overwrite and os.path.exists(path):\n            return log.warning(f\"Checkpoint already exists at {path}. Skipping save.\")\n\n        fabric = self.fabric\n\n        if self.save_ckpt_type == \"lightning\":\n            state = {\"model\": self.model}\n\n            # save the optimizer and lr_scheduler state if needed\n            if self.save_optimizer_state and save_optimizer_state is not False:\n                state.update(\n                    {\n                        \"optimizer\": self.optimizer,\n                        \"lr_scheduler\": self.lr_scheduler,\n                        \"global_step_idx\": self.global_step_idx,\n                        \"epoch_idx\": self.epoch_idx,\n                    }\n                )\n\n            trainable_param_names = set(\n                name\n                for name, param in self.model.state_dict(keep_vars=True).items()\n                if param.requires_grad\n            )\n            filter = (\n                None\n                if self.save_full_model\n                else {\"model\": lambda k, p: k in trainable_param_names}\n            )\n\n            fabric.save(path, state=state, filter=filter)\n        else:\n            self.model.save_pretrained(path, is_main_process=fabric.is_global_zero)\n\n        self._latest_saved_checkpoint_global_step = self.global_step_idx\n\n    def load_checkpoint(self, path: Union[str, Path]):\n        fabric = self.fabric\n\n        state = {\"model\": self.model}\n\n        # save the optimizer and lr_scheduler state if needed\n        if self.save_optimizer_state:\n            state.update(\n                {\n                    \"optimizer\": self.optimizer,\n                    \"lr_scheduler\": self.lr_scheduler,\n                }\n            )\n\n        fabric.load(path, state)\n</code></pre>"},{"location":"api/fusion_bench.method/training/#fusion_bench.method.BradleyTerryRewardModeling.__init__","title":"<code>__init__(optimizer, lr_scheduler, dataloader_kwargs, max_epochs, max_steps=-1, max_steps_per_epoch=-1, lr_scheduler_interval='step', lr_scheduler_frequency=1, checkpoint_save_interval='epoch', checkpoint_save_frequency=1, accumulate_grad_batches=1, gradient_clip_val=None, gradient_clip_algorithm='norm', save_optimizer_state=False, save_full_model=False, save_ckpt_type='lightning', ckpt_path=None, max_length=6144, fix_token_embedding=True, **kwargs)</code>","text":"<p>Class for reward modeling using Bradley-Terry model.</p> <p>Parameters:</p> <ul> <li> <code>optimizer</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration for the optimizer.</p> </li> <li> <code>lr_scheduler</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration for the learning rate scheduler.</p> </li> <li> <code>dataloader_kwargs</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration for the dataloader, such as batch size, num_workers, etc.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>)           \u2013            <p>Maximum number of epochs to train the model. If set to -1, the training will continue indefinitely or until max_steps is reached.</p> </li> <li> <code>max_steps</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Maximum number of steps to train the model. If set to -1, the training will continue indefinitely or until max_epochs is reached.</p> </li> <li> <code>max_steps_per_epoch</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Maximum number of steps to train the model in each epoch. If set to -1, the training will continue until the end of the epoch.</p> </li> <li> <code>lr_scheduler_interval</code>               (<code>str</code>, default:                   <code>'step'</code> )           \u2013            <p>Interval at which to run the learning rate scheduler. Available options: 'epoch', 'step'. If set to 'epoch', the scheduler will run at the end of each epoch. If set to 'step', the scheduler will run at the end of each step.</p> </li> <li> <code>lr_scheduler_frequency</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Frequency at which to run the learning rate scheduler. The scheduler will run every <code>lr_scheduler_frequency</code> epochs or steps, depending on the value of <code>lr_scheduler_interval</code>.</p> </li> <li> <code>checkpoint_save_interval</code>               (<code>str</code>, default:                   <code>'epoch'</code> )           \u2013            <p>Interval at which to save the model checkpoint. Available options: 'epoch', 'step'. If set to 'epoch', the model will be saved at the end of each epoch. If set to 'step', the model will be saved at the end of each step.</p> </li> <li> <code>checkpoint_save_frequency</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Frequency at which to save the model checkpoint. The model will be saved every <code>checkpoint_save_frequency</code> epochs or steps, depending on the value of <code>checkpoint_save_interval</code>.</p> </li> <li> <code>accumulate_grad_batches</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of batches to accumulate gradients across before updating the model parameters.</p> </li> <li> <code>gradient_clip_val</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Value to clip the gradients. If set to None, no gradient clipping will be applied.</p> </li> <li> <code>gradient_clip_algorithm</code>               (<code>str</code>, default:                   <code>'norm'</code> )           \u2013            <p>Algorithm to use for gradient clipping. Available options: 'value', 'norm'. If set to 'value', the gradients will be clipped to the specified value. If set to 'norm', the gradients will be clipped to the specified norm.</p> </li> <li> <code>save_optimizer_state</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the optimizer and lr_scheduler state along with the model checkpoint.</p> </li> <li> <code>save_full_model</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the full model or only the trainable parameters in the model checkpoint.</p> </li> <li> <code>save_ckpt_type</code>               (<code>str</code>, default:                   <code>'lightning'</code> )           \u2013            <p>Type of checkpoint to save. Available options: 'lightning', 'hf'. If set to 'lightning', the checkpoint will be saved in the lightning format. If set to 'hf', the checkpoint will be saved in the huggingface format.</p> </li> <li> <code>ckpt_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the checkpoint to load before training. If set to None, no checkpoint will be loaded.</p> </li> <li> <code>max_length</code>               (<code>int</code>, default:                   <code>6144</code> )           \u2013            <p>Maximum input length to consider. If the input length exceeds this value, it will be truncated.</p> </li> <li> <code>fix_token_embedding</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to fix the token embeddings during training. If set to True, the token embeddings will not be updated during training.</p> </li> </ul> Source code in <code>fusion_bench/method/lm_finetune/bradley_terry_rm.py</code> <pre><code>def __init__(\n    self,\n    optimizer: DictConfig,\n    lr_scheduler: Optional[DictConfig],\n    dataloader_kwargs: DictConfig,\n    max_epochs: int,\n    max_steps: int = -1,\n    max_steps_per_epoch: int = -1,\n    lr_scheduler_interval: Literal[\"epoch\", \"step\"] = \"step\",\n    lr_scheduler_frequency: int = 1,\n    checkpoint_save_interval: Literal[\"epoch\", \"step\"] = \"epoch\",\n    checkpoint_save_frequency: int = 1,\n    accumulate_grad_batches: int = 1,\n    gradient_clip_val: Optional[float] = None,\n    gradient_clip_algorithm: Literal[\"value\", \"norm\"] = \"norm\",\n    save_optimizer_state: bool = False,\n    save_full_model: bool = False,\n    save_ckpt_type: Literal[\"lightning\", \"hf\"] = \"lightning\",\n    ckpt_path: Optional[str] = None,\n    max_length: int = 6144,\n    fix_token_embedding: bool = True,\n    **kwargs,\n):\n    \"\"\"\n    Class for reward modeling using Bradley-Terry model.\n\n    Args:\n        optimizer(DictConfig): Configuration for the optimizer.\n        lr_scheduler(DictConfig): Configuration for the learning rate scheduler.\n        dataloader_kwargs(DictConfig): Configuration for the dataloader, such as batch size, num_workers, etc.\n        max_epochs(int): Maximum number of epochs to train the model. If set to -1, the training will continue indefinitely or until max_steps is reached.\n        max_steps(int): Maximum number of steps to train the model. If set to -1, the training will continue indefinitely or until max_epochs is reached.\n        max_steps_per_epoch(int): Maximum number of steps to train the model in each epoch. If set to -1, the training will continue until the end of the epoch.\n        lr_scheduler_interval(str): Interval at which to run the learning rate scheduler. Available options: 'epoch', 'step'. If set to 'epoch', the scheduler will run at the end of each epoch. If set to 'step', the scheduler will run at the end of each step.\n        lr_scheduler_frequency(int): Frequency at which to run the learning rate scheduler. The scheduler will run every `lr_scheduler_frequency` epochs or steps, depending on the value of `lr_scheduler_interval`.\n        checkpoint_save_interval(str): Interval at which to save the model checkpoint. Available options: 'epoch', 'step'. If set to 'epoch', the model will be saved at the end of each epoch. If set to 'step', the model will be saved at the end of each step.\n        checkpoint_save_frequency(int): Frequency at which to save the model checkpoint. The model will be saved every `checkpoint_save_frequency` epochs or steps, depending on the value of `checkpoint_save_interval`.\n        accumulate_grad_batches(int): Number of batches to accumulate gradients across before updating the model parameters.\n        gradient_clip_val(float): Value to clip the gradients. If set to None, no gradient clipping will be applied.\n        gradient_clip_algorithm(str): Algorithm to use for gradient clipping. Available options: 'value', 'norm'. If set to 'value', the gradients will be clipped to the specified value. If set to 'norm', the gradients will be clipped to the specified norm.\n        save_optimizer_state(bool): Whether to save the optimizer and lr_scheduler state along with the model checkpoint.\n        save_full_model(bool): Whether to save the full model or only the trainable parameters in the model checkpoint.\n        save_ckpt_type (str): Type of checkpoint to save. Available options: 'lightning', 'hf'. If set to 'lightning', the checkpoint will be saved in the lightning format. If set to 'hf', the checkpoint will be saved in the huggingface format.\n        ckpt_path(str): Path to the checkpoint to load before training. If set to None, no checkpoint will be loaded.\n        max_length(int): Maximum input length to consider. If the input length exceeds this value, it will be truncated.\n        fix_token_embedding(bool): Whether to fix the token embeddings during training. If set to True, the token embeddings will not be updated during training.\n    \"\"\"\n    self._optimizer = optimizer\n    self._lr_scheduler = lr_scheduler\n    self.dataloader_kwargs = dataloader_kwargs\n    self.max_epochs = max_epochs\n    self.max_steps = max_steps\n    self.max_steps_per_epoch = max_steps_per_epoch\n    self.lr_scheduler_interval = lr_scheduler_interval\n    self.lr_scheduler_frequency = lr_scheduler_frequency\n    self.checkpoint_save_interval = checkpoint_save_interval\n    self.checkpoint_save_frequency = checkpoint_save_frequency\n    self.accumulate_grad_batches = accumulate_grad_batches\n    self.gradient_clip_val = gradient_clip_val\n    self.gradient_clip_algorithm = gradient_clip_algorithm\n    self.save_optimizer_state = save_optimizer_state\n    self.save_full_model = save_full_model\n    self.save_ckpt_type = save_ckpt_type\n    self.ckpt_path = ckpt_path\n    self.max_length = max_length\n    self.fix_token_embedding = fix_token_embedding\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fusion_bench.method/training/#fusion_bench.method.BradleyTerryRewardModeling.compute_loss","title":"<code>compute_loss(batch)</code>","text":"<p>Maximize the likelihood of the winner over the loser using the Bradley-Terry model.</p> <p>Parameters:</p> <ul> <li> <code>batch</code>               (<code>Dict[str, Union[Tensor, Any]]</code>)           \u2013            <p>A dictionary containing the input token ids and attention masks for the winner and loser.</p> </li> </ul> Source code in <code>fusion_bench/method/lm_finetune/bradley_terry_rm.py</code> <pre><code>def compute_loss(self, batch: Dict[str, Union[Tensor, Any]]) -&gt; Dict[str, Tensor]:\n    \"\"\"\n    Maximize the likelihood of the winner over the loser using the Bradley-Terry model.\n\n    Args:\n        batch (Dict[str, Union[Tensor, Any]]): A dictionary containing the input token ids and attention masks for the winner and loser.\n    \"\"\"\n    batch_size = batch[\"input_ids\"].size(0)\n    assert batch_size % 2 == 0, \"Batch size must be even.\"\n\n    outputs = self.model(\n        input_ids=batch[\"input_ids\"],\n        attention_mask=batch[\"attention_mask\"],\n        use_cache=self.use_cache,\n    )\n\n    rewards = outputs[0]\n    chosen_reward = rewards[: batch_size // 2]\n    rejected_rewards = rewards[batch_size // 2 :]\n    loss = -torch.log(torch.sigmoid(chosen_reward - rejected_rewards)).mean()\n\n    return {\n        \"chosen_reward\": chosen_reward,\n        \"rejected_reward\": rejected_rewards,\n        \"loss\": loss,\n    }\n</code></pre>"},{"location":"api/fusion_bench.method/utility/","title":"Utility Classes","text":""},{"location":"api/fusion_bench.method/utility/#debugging-purpose","title":"Debugging Purpose","text":"<ul> <li>DummyAlgorithm: A dummy algorithm for testing purposes.</li> </ul>"},{"location":"api/fusion_bench.method/utility/#fusion_bench.method.DummyAlgorithm","title":"<code>DummyAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/dummy.py</code> <pre><code>class DummyAlgorithm(BaseAlgorithm):\n    def run(self, modelpool: BaseModelPool):\n        \"\"\"\n        This method returns the pretrained model from the model pool.\n        If the pretrained model is not available, it returns the first model from the model pool.\n\n        Args:\n            modelpool (BaseModelPool): The pool of models to fuse.\n\n        Raises:\n            AssertionError: If the model is not found in the model pool.\n        \"\"\"\n        if isinstance(modelpool, nn.Module):\n            return modelpool\n        elif not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n\n        model = modelpool.load_pretrained_or_first_model()\n\n        assert model is not None, \"Model is not found in the model pool.\"\n        return model\n</code></pre>"},{"location":"api/fusion_bench.method/utility/#fusion_bench.method.DummyAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>This method returns the pretrained model from the model pool. If the pretrained model is not available, it returns the first model from the model pool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool</code>)           \u2013            <p>The pool of models to fuse.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AssertionError</code>             \u2013            <p>If the model is not found in the model pool.</p> </li> </ul> Source code in <code>fusion_bench/method/dummy.py</code> <pre><code>def run(self, modelpool: BaseModelPool):\n    \"\"\"\n    This method returns the pretrained model from the model pool.\n    If the pretrained model is not available, it returns the first model from the model pool.\n\n    Args:\n        modelpool (BaseModelPool): The pool of models to fuse.\n\n    Raises:\n        AssertionError: If the model is not found in the model pool.\n    \"\"\"\n    if isinstance(modelpool, nn.Module):\n        return modelpool\n    elif not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(modelpool)\n\n    model = modelpool.load_pretrained_or_first_model()\n\n    assert model is not None, \"Model is not found in the model pool.\"\n    return model\n</code></pre>"},{"location":"api/fusion_bench.method/utility/#analysis-purpose","title":"Analysis Purpose","text":"<ul> <li>TaskVectorCosSimilarity: Computes the cosine similarity between task vectors.</li> <li>TaskVectorViolinPlot: Generates a violin plot for task vector distributions.</li> </ul>"},{"location":"api/fusion_bench.method/utility/#fusion_bench.method.TaskVectorCosSimilarity","title":"<code>TaskVectorCosSimilarity</code>","text":"<p>               Bases: <code>LightningFabricMixin</code>, <code>BaseAlgorithm</code></p> <p>Computes and analyzes cosine similarity between task vectors of models in a model pool.</p> <p>This algorithm extracts task vectors from fine-tuned models by computing the difference between their parameters and a pretrained base model. It then calculates the pairwise cosine similarity between all task vectors to understand the relationships and overlap between different tasks.</p> The task vector for a model is defined as <p>task_vector = finetuned_model_params - pretrained_model_params</p> <p>Parameters:</p> <ul> <li> <code>plot_heatmap</code>               (<code>bool</code>)           \u2013            <p>Whether to generate and save a heatmap visualization</p> </li> <li> <code>trainable_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only consider trainable parameters when computing task vectors. Defaults to True.</p> </li> <li> <code>max_points_per_model</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of parameters to sample per model for memory efficiency. If None, uses all parameters.</p> </li> <li> <code>output_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory to save outputs. If None, uses the fabric logger directory.</p> </li> </ul> Outputs <ul> <li>task_vector_cos_similarity.csv: Pairwise cosine similarity matrix</li> <li>task_vector_cos_similarity.pdf: Heatmap visualization (if plot_heatmap=True)</li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>The pretrained model from the model pool.</p> </li> </ul> Example <pre><code>&gt;&gt;&gt; algorithm = TaskVectorCosSimilarity(\n...     plot_heatmap=True,\n...     trainable_only=True,\n...     output_path=\"/path/to/outputs\"\n... )\n&gt;&gt;&gt; result = algorithm.run(modelpool)\n</code></pre> Source code in <code>fusion_bench/method/analysis/task_vector_cos_similarity.py</code> <pre><code>@auto_register_config\nclass TaskVectorCosSimilarity(\n    LightningFabricMixin,\n    BaseAlgorithm,\n):\n    \"\"\"\n    Computes and analyzes cosine similarity between task vectors of models in a model pool.\n\n    This algorithm extracts task vectors from fine-tuned models by computing the difference\n    between their parameters and a pretrained base model. It then calculates the pairwise\n    cosine similarity between all task vectors to understand the relationships and overlap\n    between different tasks.\n\n    The task vector for a model is defined as:\n        task_vector = finetuned_model_params - pretrained_model_params\n\n    Args:\n        plot_heatmap (bool): Whether to generate and save a heatmap visualization\n        trainable_only (bool, optional): If True, only consider trainable parameters\n            when computing task vectors. Defaults to True.\n        max_points_per_model (int, optional): Maximum number of parameters to sample\n            per model for memory efficiency. If None, uses all parameters.\n        output_path (str, optional): Directory to save outputs. If None, uses the\n            fabric logger directory.\n\n    Outputs:\n        - task_vector_cos_similarity.csv: Pairwise cosine similarity matrix\n        - task_vector_cos_similarity.pdf: Heatmap visualization (if plot_heatmap=True)\n\n    Returns:\n        The pretrained model from the model pool.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; algorithm = TaskVectorCosSimilarity(\n        ...     plot_heatmap=True,\n        ...     trainable_only=True,\n        ...     output_path=\"/path/to/outputs\"\n        ... )\n        &gt;&gt;&gt; result = algorithm.run(modelpool)\n        ```\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"_output_path\": \"output_path\",\n    }\n\n    def __init__(\n        self,\n        plot_heatmap: bool,\n        trainable_only: bool = True,\n        max_points_per_model: Optional[int] = None,\n        output_path: Optional[str] = None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self._output_path = output_path\n\n    @property\n    def output_path(self):\n        if self._output_path is None:\n            return self.fabric.logger.log_dir\n        else:\n            return self._output_path\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool):\n        \"\"\"\n        Execute the task vector cosine similarity analysis.\n\n        This method:\n        1. Loads the pretrained base model from the model pool\n        2. Computes task vectors for each fine-tuned model\n        3. Calculates pairwise cosine similarities between all task vectors\n        4. Saves the similarity matrix as a CSV file\n        5. Optionally generates and saves a heatmap visualization\n\n        Args:\n            modelpool (BaseModelPool): Pool containing pretrained and fine-tuned models\n\n        Returns:\n            nn.Module: The pretrained model from the model pool\n        \"\"\"\n        pretrained_model = modelpool.load_pretrained_model()\n\n        task_vectors = []\n        for name, finetuned_model in tqdm(\n            modelpool.named_models(), total=len(modelpool)\n        ):\n            print(f\"computing task vectors for {name}\")\n            task_vectors.append(\n                self.get_task_vector(pretrained_model, finetuned_model).to(\n                    torch.float64\n                )\n            )\n        task_vectors = torch.stack(task_vectors, dim=0)\n\n        cos_sim_matrix = torch.zeros(\n            len(modelpool), len(modelpool), dtype=torch.float64\n        )\n        for i in range(len(modelpool)):\n            for j in range(i, len(modelpool)):\n                assert task_vectors[i].size() == task_vectors[j].size()\n                cos_sim_matrix[i, j] = torch.nn.functional.cosine_similarity(\n                    task_vectors[i], task_vectors[j], dim=0\n                )\n                cos_sim_matrix[j, i] = cos_sim_matrix[i, j]\n\n        # convert the matrix to a pandas DataFrame\n        cos_sim_df = pd.DataFrame(\n            cos_sim_matrix.numpy(),\n            index=modelpool.model_names,\n            columns=modelpool.model_names,\n        )\n\n        print(cos_sim_df)\n        if self.output_path is not None:\n            os.makedirs(self.output_path, exist_ok=True)\n            cos_sim_df.to_csv(\n                os.path.join(self.output_path, \"task_vector_cos_similarity.csv\")\n            )\n\n        if self.plot_heatmap:\n            self._plot_heatmap(cos_sim_df)\n\n        return pretrained_model\n\n    def _plot_heatmap(self, data: pd.DataFrame):\n        \"\"\"\n        Generate and save a heatmap visualization of the cosine similarity matrix.\n\n        Creates a color-coded heatmap showing pairwise cosine similarities between\n        task vectors. The heatmap is saved as a PDF file in the output directory.\n\n        Args:\n            data (pd.DataFrame): Symmetric matrix of cosine similarities between\n                task vectors, with model names as both index and columns.\n\n        Returns:\n            None\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        # Create a heatmap using seaborn\n        plt.figure()\n        sns.heatmap(\n            data,\n            annot=True,\n            fmt=\".2f\",\n            cmap=\"GnBu\",\n        )\n\n        # Add title and labels with increased font size\n        plt.title(\"Heatmap of Cos Similarities\", fontsize=14)\n        # plt.xlabel(\"Task\", fontsize=14)\n        # plt.ylabel(\"Task\", fontsize=14)\n        plt.xticks(rotation=45)\n        plt.yticks(rotation=45)\n\n        # Show plot\n        plt.savefig(\n            os.path.join(self.output_path, \"task_vector_cos_similarity.pdf\"),\n            bbox_inches=\"tight\",\n        )\n        plt.close()\n\n    def get_task_vector(\n        self, pretrained_model: nn.Module, finetuned_model: nn.Module\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the task vector for a fine-tuned model.\n\n        The task vector represents the parameter changes from pretraining to\n        fine-tuning and is computed as:\n            task_vector = finetuned_params - pretrained_params\n\n        Args:\n            pretrained_model (nn.Module): The base pretrained model\n            finetuned_model (nn.Module): The fine-tuned model for a specific task\n\n        Returns:\n            torch.Tensor: Flattened task vector containing parameter differences.\n                If max_points_per_model is set, the vector may be downsampled.\n\n        Note:\n            - Converts parameters to float64 for numerical precision\n            - Supports optional downsampling for memory efficiency\n            - Uses only trainable parameters if trainable_only=True\n        \"\"\"\n        task_vector = state_dict_sub(\n            self.get_state_dict(finetuned_model),\n            self.get_state_dict(pretrained_model),\n        )\n        task_vector = state_dict_to_vector(task_vector)\n\n        task_vector = task_vector.cpu().float().numpy()\n        # downsample if necessary\n        if (\n            self.max_points_per_model is not None\n            and self.max_points_per_model &gt; 0\n            and task_vector.shape[0] &gt; self.max_points_per_model\n        ):\n            log.info(\n                f\"Downsampling task vectors to {self.max_points_per_model} points.\"\n            )\n            indices = np.random.choice(\n                task_vector.shape[0], self.max_points_per_model, replace=False\n            )\n            task_vector = task_vector[indices].copy()\n\n        task_vector = torch.from_numpy(task_vector)\n        return task_vector\n\n    def get_state_dict(self, model: nn.Module):\n        \"\"\"\n        Extract the state dictionary from a model.\n\n        Args:\n            model (nn.Module): The model to extract parameters from\n\n        Returns:\n            Dict[str, torch.Tensor]: State dictionary containing model parameters.\n                Returns only trainable parameters if trainable_only=True,\n                otherwise returns all parameters.\n        \"\"\"\n        if self.trainable_only:\n            return trainable_state_dict(model)\n        else:\n            return model.state_dict()\n</code></pre>"},{"location":"api/fusion_bench.method/utility/#fusion_bench.method.TaskVectorCosSimilarity.get_state_dict","title":"<code>get_state_dict(model)</code>","text":"<p>Extract the state dictionary from a model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to extract parameters from</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Dict[str, torch.Tensor]: State dictionary containing model parameters. Returns only trainable parameters if trainable_only=True, otherwise returns all parameters.</p> </li> </ul> Source code in <code>fusion_bench/method/analysis/task_vector_cos_similarity.py</code> <pre><code>def get_state_dict(self, model: nn.Module):\n    \"\"\"\n    Extract the state dictionary from a model.\n\n    Args:\n        model (nn.Module): The model to extract parameters from\n\n    Returns:\n        Dict[str, torch.Tensor]: State dictionary containing model parameters.\n            Returns only trainable parameters if trainable_only=True,\n            otherwise returns all parameters.\n    \"\"\"\n    if self.trainable_only:\n        return trainable_state_dict(model)\n    else:\n        return model.state_dict()\n</code></pre>"},{"location":"api/fusion_bench.method/utility/#fusion_bench.method.TaskVectorCosSimilarity.get_task_vector","title":"<code>get_task_vector(pretrained_model, finetuned_model)</code>","text":"<p>Compute the task vector for a fine-tuned model.</p> <p>The task vector represents the parameter changes from pretraining to fine-tuning and is computed as:     task_vector = finetuned_params - pretrained_params</p> <p>Parameters:</p> <ul> <li> <code>pretrained_model</code>               (<code>Module</code>)           \u2013            <p>The base pretrained model</p> </li> <li> <code>finetuned_model</code>               (<code>Module</code>)           \u2013            <p>The fine-tuned model for a specific task</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>torch.Tensor: Flattened task vector containing parameter differences. If max_points_per_model is set, the vector may be downsampled.</p> </li> </ul> Note <ul> <li>Converts parameters to float64 for numerical precision</li> <li>Supports optional downsampling for memory efficiency</li> <li>Uses only trainable parameters if trainable_only=True</li> </ul> Source code in <code>fusion_bench/method/analysis/task_vector_cos_similarity.py</code> <pre><code>def get_task_vector(\n    self, pretrained_model: nn.Module, finetuned_model: nn.Module\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the task vector for a fine-tuned model.\n\n    The task vector represents the parameter changes from pretraining to\n    fine-tuning and is computed as:\n        task_vector = finetuned_params - pretrained_params\n\n    Args:\n        pretrained_model (nn.Module): The base pretrained model\n        finetuned_model (nn.Module): The fine-tuned model for a specific task\n\n    Returns:\n        torch.Tensor: Flattened task vector containing parameter differences.\n            If max_points_per_model is set, the vector may be downsampled.\n\n    Note:\n        - Converts parameters to float64 for numerical precision\n        - Supports optional downsampling for memory efficiency\n        - Uses only trainable parameters if trainable_only=True\n    \"\"\"\n    task_vector = state_dict_sub(\n        self.get_state_dict(finetuned_model),\n        self.get_state_dict(pretrained_model),\n    )\n    task_vector = state_dict_to_vector(task_vector)\n\n    task_vector = task_vector.cpu().float().numpy()\n    # downsample if necessary\n    if (\n        self.max_points_per_model is not None\n        and self.max_points_per_model &gt; 0\n        and task_vector.shape[0] &gt; self.max_points_per_model\n    ):\n        log.info(\n            f\"Downsampling task vectors to {self.max_points_per_model} points.\"\n        )\n        indices = np.random.choice(\n            task_vector.shape[0], self.max_points_per_model, replace=False\n        )\n        task_vector = task_vector[indices].copy()\n\n    task_vector = torch.from_numpy(task_vector)\n    return task_vector\n</code></pre>"},{"location":"api/fusion_bench.method/utility/#fusion_bench.method.TaskVectorCosSimilarity.run","title":"<code>run(modelpool)</code>","text":"<p>Execute the task vector cosine similarity analysis.</p> <p>This method: 1. Loads the pretrained base model from the model pool 2. Computes task vectors for each fine-tuned model 3. Calculates pairwise cosine similarities between all task vectors 4. Saves the similarity matrix as a CSV file 5. Optionally generates and saves a heatmap visualization</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool</code>)           \u2013            <p>Pool containing pretrained and fine-tuned models</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The pretrained model from the model pool</p> </li> </ul> Source code in <code>fusion_bench/method/analysis/task_vector_cos_similarity.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool):\n    \"\"\"\n    Execute the task vector cosine similarity analysis.\n\n    This method:\n    1. Loads the pretrained base model from the model pool\n    2. Computes task vectors for each fine-tuned model\n    3. Calculates pairwise cosine similarities between all task vectors\n    4. Saves the similarity matrix as a CSV file\n    5. Optionally generates and saves a heatmap visualization\n\n    Args:\n        modelpool (BaseModelPool): Pool containing pretrained and fine-tuned models\n\n    Returns:\n        nn.Module: The pretrained model from the model pool\n    \"\"\"\n    pretrained_model = modelpool.load_pretrained_model()\n\n    task_vectors = []\n    for name, finetuned_model in tqdm(\n        modelpool.named_models(), total=len(modelpool)\n    ):\n        print(f\"computing task vectors for {name}\")\n        task_vectors.append(\n            self.get_task_vector(pretrained_model, finetuned_model).to(\n                torch.float64\n            )\n        )\n    task_vectors = torch.stack(task_vectors, dim=0)\n\n    cos_sim_matrix = torch.zeros(\n        len(modelpool), len(modelpool), dtype=torch.float64\n    )\n    for i in range(len(modelpool)):\n        for j in range(i, len(modelpool)):\n            assert task_vectors[i].size() == task_vectors[j].size()\n            cos_sim_matrix[i, j] = torch.nn.functional.cosine_similarity(\n                task_vectors[i], task_vectors[j], dim=0\n            )\n            cos_sim_matrix[j, i] = cos_sim_matrix[i, j]\n\n    # convert the matrix to a pandas DataFrame\n    cos_sim_df = pd.DataFrame(\n        cos_sim_matrix.numpy(),\n        index=modelpool.model_names,\n        columns=modelpool.model_names,\n    )\n\n    print(cos_sim_df)\n    if self.output_path is not None:\n        os.makedirs(self.output_path, exist_ok=True)\n        cos_sim_df.to_csv(\n            os.path.join(self.output_path, \"task_vector_cos_similarity.csv\")\n        )\n\n    if self.plot_heatmap:\n        self._plot_heatmap(cos_sim_df)\n\n    return pretrained_model\n</code></pre>"},{"location":"api/fusion_bench.method/utility/#fusion_bench.method.TaskVectorViolinPlot","title":"<code>TaskVectorViolinPlot</code>","text":"<p>               Bases: <code>LightningFabricMixin</code>, <code>SimpleProfilerMixin</code>, <code>BaseAlgorithm</code></p> <p>Creates violin plots to visualize the distribution of task vector values across models.</p> <p>This class implements the task vector visualization technique described in: \"Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging\" by L. Shen, A. Tang, E. Yang et al. (https://arxiv.org/abs/2410.21804)</p> <p>Task vectors represent the parameter differences between fine-tuned models and their pretrained base model, computed as:     task_vector = finetuned_params - pretrained_params</p> <p>The algorithm generates two types of violin plots: 1. Distribution of raw task vector values (positive and negative) 2. Distribution of absolute task vector values</p> <p>Parameters:</p> <ul> <li> <code>trainable_only</code>               (<code>bool</code>)           \u2013            <p>If True, only consider trainable parameters when computing task vectors. If False, use all parameters.</p> </li> <li> <code>max_points_per_model</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Maximum number of parameters to sample per model for memory efficiency. If None or 0, uses all parameters. Defaults to 1000.</p> </li> <li> <code>fig_kwargs</code>               (<code>dict</code>)           \u2013            <p>Dictionary of keyword arguments to pass to matplotlib.pyplot.subplots. Common options include: - figsize: Tuple of (width, height) in inches - dpi: Dots per inch for resolution - facecolor: Figure background color Defaults to None.</p> </li> <li> <code>output_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory to save the violin plots. If None, uses the fabric logger's log directory. Defaults to None.</p> </li> </ul> Outputs <ul> <li>task_vector_violin.pdf: Violin plot of raw task vector value distributions</li> <li>task_vector_violin_abs.pdf: Violin plot of absolute task vector value distributions</li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>The pretrained model from the model pool.</p> </li> </ul> Example <pre><code>plotter = TaskVectorViolinPlot(\n    trainable_only=True,\n    max_points_per_model=5000,\n    fig_kwargs={'figsize': (12, 8), 'dpi': 300},\n    output_path='./analysis_plots'\n)\npretrained_model = plotter.run(modelpool)\n</code></pre> Note <p>This visualization is particularly useful for understanding: - How different tasks affect model parameters - The magnitude and distribution of parameter changes - Similarities and differences between task adaptations</p> Source code in <code>fusion_bench/method/analysis/task_vector_violin_plot.py</code> <pre><code>@auto_register_config\nclass TaskVectorViolinPlot(\n    LightningFabricMixin,\n    SimpleProfilerMixin,\n    BaseAlgorithm,\n):\n    \"\"\"\n    Creates violin plots to visualize the distribution of task vector values across models.\n\n    This class implements the task vector visualization technique described in:\n    \"Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging\"\n    by L. Shen, A. Tang, E. Yang et al. (https://arxiv.org/abs/2410.21804)\n\n    Task vectors represent the parameter differences between fine-tuned models and their\n    pretrained base model, computed as:\n        task_vector = finetuned_params - pretrained_params\n\n    The algorithm generates two types of violin plots:\n    1. Distribution of raw task vector values (positive and negative)\n    2. Distribution of absolute task vector values\n\n    Args:\n        trainable_only (bool): If True, only consider trainable parameters when computing\n            task vectors. If False, use all parameters.\n        max_points_per_model (int, optional): Maximum number of parameters to sample\n            per model for memory efficiency. If None or 0, uses all parameters.\n            Defaults to 1000.\n        fig_kwargs (dict, optional): Dictionary of keyword arguments to pass to\n            matplotlib.pyplot.subplots. Common options include:\n            - figsize: Tuple of (width, height) in inches\n            - dpi: Dots per inch for resolution\n            - facecolor: Figure background color\n            Defaults to None.\n        output_path (str, optional): Directory to save the violin plots. If None,\n            uses the fabric logger's log directory. Defaults to None.\n\n    Outputs:\n        - task_vector_violin.pdf: Violin plot of raw task vector value distributions\n        - task_vector_violin_abs.pdf: Violin plot of absolute task vector value distributions\n\n    Returns:\n        The pretrained model from the model pool.\n\n    Example:\n        ```python\n        plotter = TaskVectorViolinPlot(\n            trainable_only=True,\n            max_points_per_model=5000,\n            fig_kwargs={'figsize': (12, 8), 'dpi': 300},\n            output_path='./analysis_plots'\n        )\n        pretrained_model = plotter.run(modelpool)\n        ```\n\n    Note:\n        This visualization is particularly useful for understanding:\n        - How different tasks affect model parameters\n        - The magnitude and distribution of parameter changes\n        - Similarities and differences between task adaptations\n    \"\"\"\n\n    # config_mapping is a mapping from the attributes to the key in the configuration files\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"_output_path\": \"output_path\",\n    }\n\n    def __init__(\n        self,\n        trainable_only: bool,\n        max_points_per_model: Optional[int] = 1000,\n        fig_kwawrgs=None,\n        output_path: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the TaskVectorViolinPlot analyzer.\n\n        Args:\n            trainable_only (bool): Whether to consider only trainable parameters when\n                computing task vectors. Set to True to focus on learnable parameters,\n                False to include all parameters including frozen ones.\n            max_points_per_model (int, optional): Maximum number of parameter values\n                to sample per model for visualization. Useful for large models to\n                manage memory usage and plot clarity. Set to None or 0 to use all\n                parameters. Defaults to 1000.\n            fig_kwargs (dict, optional): Keyword arguments passed to matplotlib's\n                subplots function for plot customization. Examples:\n                - {'figsize': (10, 6)} for plot dimensions\n                - {'dpi': 300} for high resolution\n                - {'facecolor': 'white'} for background color\n                Defaults to None (uses matplotlib defaults).\n            output_path (str, optional): Directory path where violin plots will be saved.\n                If None, uses the fabric logger's log directory. The directory will be\n                created if it doesn't exist. Defaults to None.\n            **kwargs: Additional keyword arguments passed to parent classes.\n\n        Note:\n            The parameter name 'fig_kwawrgs' appears to be a typo for 'fig_kwargs'.\n            This should be corrected in the parameter name for consistency.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._output_path = output_path\n\n    @property\n    def output_path(self):\n        if self._output_path is None:\n            return self.fabric.logger.log_dir\n        else:\n            return self._output_path\n\n    def run(self, modelpool: BaseModelPool):\n        \"\"\"\n        Execute the task vector violin plot analysis and visualization.\n\n        This method implements the core algorithm that:\n        1. Loads the pretrained base model from the model pool\n        2. Computes task vectors for each fine-tuned model (parameter differences)\n        3. Creates two violin plots showing the distribution of task vector values:\n           - Raw values plot: Shows positive and negative parameter changes\n           - Absolute values plot: Shows magnitude of parameter changes\n        4. Saves both plots as PDF files in the output directory\n\n        The visualization technique follows the approach described in:\n        \"Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging\"\n\n        Args:\n            modelpool (BaseModelPool): Pool containing both a pretrained model and\n                fine-tuned models. Must have `has_pretrained=True`.\n\n        Returns:\n            nn.Module: The pretrained model loaded from the model pool.\n\n        Raises:\n            AssertionError: If the model pool doesn't contain a pretrained model.\n\n        Side Effects:\n            - Creates output directory if it doesn't exist\n            - Saves 'task_vector_violin.pdf' (raw values distribution)\n            - Saves 'task_vector_violin_abs.pdf' (absolute values distribution)\n            - Prints progress information during task vector computation\n\n        Example Output Files:\n            - task_vector_violin.pdf: Shows how parameters change (+ and -)\n            - task_vector_violin_abs.pdf: Shows magnitude of parameter changes\n        \"\"\"\n        assert modelpool.has_pretrained\n        pretrained_model = modelpool.load_pretrained_model()\n\n        # Compute task vectors for each fine-tuned model\n        with torch.no_grad(), timeit_context(\"Computing task vectors\"):\n            task_vectors: Dict[str, NDArray] = {}\n            for name, finetuned_model in tqdm(\n                modelpool.named_models(), total=len(modelpool)\n            ):\n                print(f\"computing task vectors for {name}\")\n                task_vectors[name] = self.get_task_vector(\n                    pretrained_model, finetuned_model\n                )\n\n        # === Create violin plot ===\n        fig, ax = plt.subplots(\n            1, 1, **self.fig_kwargs if self.fig_kwargs is not None else {}\n        )\n        fig = cast(plt.Figure, fig)\n        ax = cast(plt.Axes, ax)\n\n        # Prepare data for plotting\n        data = [values for values in task_vectors.values()]\n        labels = list(task_vectors.keys())\n\n        # Create violin plot using seaborn\n        with timeit_context(\"ploting\"):\n            sns.violinplot(data=data, ax=ax)\n\n        # Customize plot\n        ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n        ax.set_ylabel(\"Task Vector Values\")\n        ax.set_title(\"Distribution of Task Vector Values\")\n\n        # Adjust layout to prevent label cutoff and save plot\n        plt.tight_layout()\n        os.makedirs(self.output_path, exist_ok=True)\n        output_file = f\"{self.output_path}/task_vector_violin.pdf\"\n        plt.savefig(output_file, bbox_inches=\"tight\")\n        plt.close(fig)\n\n        # === Create violin plot (Abs values) ===\n        fig, ax = plt.subplots(\n            1, 1, **self.fig_kwargs if self.fig_kwargs is not None else {}\n        )\n        fig = cast(plt.Figure, fig)\n        ax = cast(plt.Axes, ax)\n\n        # Prepare data for plotting\n        data = [np.abs(values) for values in task_vectors.values()]\n        labels = list(task_vectors.keys())\n\n        # Create violin plot using seaborn\n        with timeit_context(\"ploting abs value plot\"):\n            sns.violinplot(data=data, ax=ax)\n\n        # Customize plot\n        ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n        ax.set_ylabel(\"The Absolute Values\")\n        ax.set_title(\"Distribution of Task Vector Absolute Values\")\n\n        # Adjust layout to prevent label cutoff and save plot\n        plt.tight_layout()\n        os.makedirs(self.output_path, exist_ok=True)\n        output_file = f\"{self.output_path}/task_vector_violin_abs.pdf\"\n        plt.savefig(output_file, bbox_inches=\"tight\")\n        plt.close(fig)\n\n        return pretrained_model\n\n    def get_task_vector(self, pretrained_model, finetuned_model):\n        \"\"\"\n        Compute the task vector representing parameter changes from pretraining to fine-tuning.\n\n        The task vector quantifies how model parameters have changed during task-specific\n        fine-tuning and is computed as:\n            task_vector = finetuned_params - pretrained_params\n\n        Args:\n            pretrained_model (nn.Module): The base pretrained model\n            finetuned_model (nn.Module): The fine-tuned model for a specific task\n\n        Returns:\n            np.ndarray: Flattened numpy array containing parameter differences.\n                If max_points_per_model is set, the array may be randomly downsampled\n                for memory efficiency and visualization clarity.\n\n        Processing Steps:\n            1. Extract state dictionaries from both models\n            2. Compute parameter differences (subtraction)\n            3. Flatten to 1D vector\n            4. Convert to numpy array with float32 precision\n            5. Optionally downsample if max_points_per_model is specified\n\n        Note:\n            - Uses only trainable parameters if trainable_only=True\n            - Downsampling uses random sampling without replacement\n            - Preserves the relative distribution of parameter changes\n        \"\"\"\n        task_vector = state_dict_sub(\n            self.get_state_dict(finetuned_model),\n            self.get_state_dict(pretrained_model),\n        )\n        task_vector = state_dict_to_vector(task_vector)\n\n        task_vector = task_vector.cpu().float().numpy()\n        # downsample if necessary\n        if (\n            self.max_points_per_model is not None\n            and self.max_points_per_model &gt; 0\n            and task_vector.shape[0] &gt; self.max_points_per_model\n        ):\n            log.info(\n                f\"Downsampling task vectors to {self.max_points_per_model} points.\"\n            )\n            indices = np.random.choice(\n                task_vector.shape[0], self.max_points_per_model, replace=False\n            )\n            task_vector = task_vector[indices].copy()\n\n        return task_vector\n\n    def get_state_dict(self, model: nn.Module):\n        \"\"\"\n        Extract the state dictionary from a model based on parameter filtering settings.\n\n        Args:\n            model (nn.Module): The PyTorch model to extract parameters from\n\n        Returns:\n            Dict[str, torch.Tensor]: State dictionary containing model parameters.\n                If trainable_only=True, returns only parameters with requires_grad=True.\n                If trainable_only=False, returns all parameters including frozen ones.\n\n        Note:\n            This method respects the trainable_only configuration to focus analysis\n            on either learnable parameters or the complete parameter set depending\n            on the research question being addressed.\n        \"\"\"\n        if self.trainable_only:\n            return trainable_state_dict(model)\n        else:\n            return model.state_dict()\n</code></pre>"},{"location":"api/fusion_bench.method/utility/#fusion_bench.method.TaskVectorViolinPlot.__init__","title":"<code>__init__(trainable_only, max_points_per_model=1000, fig_kwawrgs=None, output_path=None, **kwargs)</code>","text":"<p>Initialize the TaskVectorViolinPlot analyzer.</p> <p>Parameters:</p> <ul> <li> <code>trainable_only</code>               (<code>bool</code>)           \u2013            <p>Whether to consider only trainable parameters when computing task vectors. Set to True to focus on learnable parameters, False to include all parameters including frozen ones.</p> </li> <li> <code>max_points_per_model</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Maximum number of parameter values to sample per model for visualization. Useful for large models to manage memory usage and plot clarity. Set to None or 0 to use all parameters. Defaults to 1000.</p> </li> <li> <code>fig_kwargs</code>               (<code>dict</code>)           \u2013            <p>Keyword arguments passed to matplotlib's subplots function for plot customization. Examples: - {'figsize': (10, 6)} for plot dimensions - {'dpi': 300} for high resolution - {'facecolor': 'white'} for background color Defaults to None (uses matplotlib defaults).</p> </li> <li> <code>output_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory path where violin plots will be saved. If None, uses the fabric logger's log directory. The directory will be created if it doesn't exist. Defaults to None.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments passed to parent classes.</p> </li> </ul> Note <p>The parameter name 'fig_kwawrgs' appears to be a typo for 'fig_kwargs'. This should be corrected in the parameter name for consistency.</p> Source code in <code>fusion_bench/method/analysis/task_vector_violin_plot.py</code> <pre><code>def __init__(\n    self,\n    trainable_only: bool,\n    max_points_per_model: Optional[int] = 1000,\n    fig_kwawrgs=None,\n    output_path: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the TaskVectorViolinPlot analyzer.\n\n    Args:\n        trainable_only (bool): Whether to consider only trainable parameters when\n            computing task vectors. Set to True to focus on learnable parameters,\n            False to include all parameters including frozen ones.\n        max_points_per_model (int, optional): Maximum number of parameter values\n            to sample per model for visualization. Useful for large models to\n            manage memory usage and plot clarity. Set to None or 0 to use all\n            parameters. Defaults to 1000.\n        fig_kwargs (dict, optional): Keyword arguments passed to matplotlib's\n            subplots function for plot customization. Examples:\n            - {'figsize': (10, 6)} for plot dimensions\n            - {'dpi': 300} for high resolution\n            - {'facecolor': 'white'} for background color\n            Defaults to None (uses matplotlib defaults).\n        output_path (str, optional): Directory path where violin plots will be saved.\n            If None, uses the fabric logger's log directory. The directory will be\n            created if it doesn't exist. Defaults to None.\n        **kwargs: Additional keyword arguments passed to parent classes.\n\n    Note:\n        The parameter name 'fig_kwawrgs' appears to be a typo for 'fig_kwargs'.\n        This should be corrected in the parameter name for consistency.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._output_path = output_path\n</code></pre>"},{"location":"api/fusion_bench.method/utility/#fusion_bench.method.TaskVectorViolinPlot.get_state_dict","title":"<code>get_state_dict(model)</code>","text":"<p>Extract the state dictionary from a model based on parameter filtering settings.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The PyTorch model to extract parameters from</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Dict[str, torch.Tensor]: State dictionary containing model parameters. If trainable_only=True, returns only parameters with requires_grad=True. If trainable_only=False, returns all parameters including frozen ones.</p> </li> </ul> Note <p>This method respects the trainable_only configuration to focus analysis on either learnable parameters or the complete parameter set depending on the research question being addressed.</p> Source code in <code>fusion_bench/method/analysis/task_vector_violin_plot.py</code> <pre><code>def get_state_dict(self, model: nn.Module):\n    \"\"\"\n    Extract the state dictionary from a model based on parameter filtering settings.\n\n    Args:\n        model (nn.Module): The PyTorch model to extract parameters from\n\n    Returns:\n        Dict[str, torch.Tensor]: State dictionary containing model parameters.\n            If trainable_only=True, returns only parameters with requires_grad=True.\n            If trainable_only=False, returns all parameters including frozen ones.\n\n    Note:\n        This method respects the trainable_only configuration to focus analysis\n        on either learnable parameters or the complete parameter set depending\n        on the research question being addressed.\n    \"\"\"\n    if self.trainable_only:\n        return trainable_state_dict(model)\n    else:\n        return model.state_dict()\n</code></pre>"},{"location":"api/fusion_bench.method/utility/#fusion_bench.method.TaskVectorViolinPlot.get_task_vector","title":"<code>get_task_vector(pretrained_model, finetuned_model)</code>","text":"<p>Compute the task vector representing parameter changes from pretraining to fine-tuning.</p> <p>The task vector quantifies how model parameters have changed during task-specific fine-tuning and is computed as:     task_vector = finetuned_params - pretrained_params</p> <p>Parameters:</p> <ul> <li> <code>pretrained_model</code>               (<code>Module</code>)           \u2013            <p>The base pretrained model</p> </li> <li> <code>finetuned_model</code>               (<code>Module</code>)           \u2013            <p>The fine-tuned model for a specific task</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>np.ndarray: Flattened numpy array containing parameter differences. If max_points_per_model is set, the array may be randomly downsampled for memory efficiency and visualization clarity.</p> </li> </ul> Processing Steps <ol> <li>Extract state dictionaries from both models</li> <li>Compute parameter differences (subtraction)</li> <li>Flatten to 1D vector</li> <li>Convert to numpy array with float32 precision</li> <li>Optionally downsample if max_points_per_model is specified</li> </ol> Note <ul> <li>Uses only trainable parameters if trainable_only=True</li> <li>Downsampling uses random sampling without replacement</li> <li>Preserves the relative distribution of parameter changes</li> </ul> Source code in <code>fusion_bench/method/analysis/task_vector_violin_plot.py</code> <pre><code>def get_task_vector(self, pretrained_model, finetuned_model):\n    \"\"\"\n    Compute the task vector representing parameter changes from pretraining to fine-tuning.\n\n    The task vector quantifies how model parameters have changed during task-specific\n    fine-tuning and is computed as:\n        task_vector = finetuned_params - pretrained_params\n\n    Args:\n        pretrained_model (nn.Module): The base pretrained model\n        finetuned_model (nn.Module): The fine-tuned model for a specific task\n\n    Returns:\n        np.ndarray: Flattened numpy array containing parameter differences.\n            If max_points_per_model is set, the array may be randomly downsampled\n            for memory efficiency and visualization clarity.\n\n    Processing Steps:\n        1. Extract state dictionaries from both models\n        2. Compute parameter differences (subtraction)\n        3. Flatten to 1D vector\n        4. Convert to numpy array with float32 precision\n        5. Optionally downsample if max_points_per_model is specified\n\n    Note:\n        - Uses only trainable parameters if trainable_only=True\n        - Downsampling uses random sampling without replacement\n        - Preserves the relative distribution of parameter changes\n    \"\"\"\n    task_vector = state_dict_sub(\n        self.get_state_dict(finetuned_model),\n        self.get_state_dict(pretrained_model),\n    )\n    task_vector = state_dict_to_vector(task_vector)\n\n    task_vector = task_vector.cpu().float().numpy()\n    # downsample if necessary\n    if (\n        self.max_points_per_model is not None\n        and self.max_points_per_model &gt; 0\n        and task_vector.shape[0] &gt; self.max_points_per_model\n    ):\n        log.info(\n            f\"Downsampling task vectors to {self.max_points_per_model} points.\"\n        )\n        indices = np.random.choice(\n            task_vector.shape[0], self.max_points_per_model, replace=False\n        )\n        task_vector = task_vector[indices].copy()\n\n    return task_vector\n</code></pre>"},{"location":"api/fusion_bench.method/utility/#fusion_bench.method.TaskVectorViolinPlot.run","title":"<code>run(modelpool)</code>","text":"<p>Execute the task vector violin plot analysis and visualization.</p> <p>This method implements the core algorithm that: 1. Loads the pretrained base model from the model pool 2. Computes task vectors for each fine-tuned model (parameter differences) 3. Creates two violin plots showing the distribution of task vector values:    - Raw values plot: Shows positive and negative parameter changes    - Absolute values plot: Shows magnitude of parameter changes 4. Saves both plots as PDF files in the output directory</p> <p>The visualization technique follows the approach described in: \"Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging\"</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>BaseModelPool</code>)           \u2013            <p>Pool containing both a pretrained model and fine-tuned models. Must have <code>has_pretrained=True</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The pretrained model loaded from the model pool.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AssertionError</code>             \u2013            <p>If the model pool doesn't contain a pretrained model.</p> </li> </ul> Side Effects <ul> <li>Creates output directory if it doesn't exist</li> <li>Saves 'task_vector_violin.pdf' (raw values distribution)</li> <li>Saves 'task_vector_violin_abs.pdf' (absolute values distribution)</li> <li>Prints progress information during task vector computation</li> </ul> Example Output Files <ul> <li>task_vector_violin.pdf: Shows how parameters change (+ and -)</li> <li>task_vector_violin_abs.pdf: Shows magnitude of parameter changes</li> </ul> Source code in <code>fusion_bench/method/analysis/task_vector_violin_plot.py</code> <pre><code>def run(self, modelpool: BaseModelPool):\n    \"\"\"\n    Execute the task vector violin plot analysis and visualization.\n\n    This method implements the core algorithm that:\n    1. Loads the pretrained base model from the model pool\n    2. Computes task vectors for each fine-tuned model (parameter differences)\n    3. Creates two violin plots showing the distribution of task vector values:\n       - Raw values plot: Shows positive and negative parameter changes\n       - Absolute values plot: Shows magnitude of parameter changes\n    4. Saves both plots as PDF files in the output directory\n\n    The visualization technique follows the approach described in:\n    \"Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging\"\n\n    Args:\n        modelpool (BaseModelPool): Pool containing both a pretrained model and\n            fine-tuned models. Must have `has_pretrained=True`.\n\n    Returns:\n        nn.Module: The pretrained model loaded from the model pool.\n\n    Raises:\n        AssertionError: If the model pool doesn't contain a pretrained model.\n\n    Side Effects:\n        - Creates output directory if it doesn't exist\n        - Saves 'task_vector_violin.pdf' (raw values distribution)\n        - Saves 'task_vector_violin_abs.pdf' (absolute values distribution)\n        - Prints progress information during task vector computation\n\n    Example Output Files:\n        - task_vector_violin.pdf: Shows how parameters change (+ and -)\n        - task_vector_violin_abs.pdf: Shows magnitude of parameter changes\n    \"\"\"\n    assert modelpool.has_pretrained\n    pretrained_model = modelpool.load_pretrained_model()\n\n    # Compute task vectors for each fine-tuned model\n    with torch.no_grad(), timeit_context(\"Computing task vectors\"):\n        task_vectors: Dict[str, NDArray] = {}\n        for name, finetuned_model in tqdm(\n            modelpool.named_models(), total=len(modelpool)\n        ):\n            print(f\"computing task vectors for {name}\")\n            task_vectors[name] = self.get_task_vector(\n                pretrained_model, finetuned_model\n            )\n\n    # === Create violin plot ===\n    fig, ax = plt.subplots(\n        1, 1, **self.fig_kwargs if self.fig_kwargs is not None else {}\n    )\n    fig = cast(plt.Figure, fig)\n    ax = cast(plt.Axes, ax)\n\n    # Prepare data for plotting\n    data = [values for values in task_vectors.values()]\n    labels = list(task_vectors.keys())\n\n    # Create violin plot using seaborn\n    with timeit_context(\"ploting\"):\n        sns.violinplot(data=data, ax=ax)\n\n    # Customize plot\n    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n    ax.set_ylabel(\"Task Vector Values\")\n    ax.set_title(\"Distribution of Task Vector Values\")\n\n    # Adjust layout to prevent label cutoff and save plot\n    plt.tight_layout()\n    os.makedirs(self.output_path, exist_ok=True)\n    output_file = f\"{self.output_path}/task_vector_violin.pdf\"\n    plt.savefig(output_file, bbox_inches=\"tight\")\n    plt.close(fig)\n\n    # === Create violin plot (Abs values) ===\n    fig, ax = plt.subplots(\n        1, 1, **self.fig_kwargs if self.fig_kwargs is not None else {}\n    )\n    fig = cast(plt.Figure, fig)\n    ax = cast(plt.Axes, ax)\n\n    # Prepare data for plotting\n    data = [np.abs(values) for values in task_vectors.values()]\n    labels = list(task_vectors.keys())\n\n    # Create violin plot using seaborn\n    with timeit_context(\"ploting abs value plot\"):\n        sns.violinplot(data=data, ax=ax)\n\n    # Customize plot\n    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n    ax.set_ylabel(\"The Absolute Values\")\n    ax.set_title(\"Distribution of Task Vector Absolute Values\")\n\n    # Adjust layout to prevent label cutoff and save plot\n    plt.tight_layout()\n    os.makedirs(self.output_path, exist_ok=True)\n    output_file = f\"{self.output_path}/task_vector_violin_abs.pdf\"\n    plt.savefig(output_file, bbox_inches=\"tight\")\n    plt.close(fig)\n\n    return pretrained_model\n</code></pre>"},{"location":"api/fusion_bench.utils/","title":"fusion_bench.utils","text":"<p>The <code>fusion_bench.utils</code> module provides a comprehensive collection of utility functions and classes that support the core functionality of FusionBench. These utilities handle common tasks such as model management, data processing, caching, logging, and system interactions, making it easier to build robust model fusion workflows.</p>"},{"location":"api/fusion_bench.utils/#topics","title":"Topics","text":"<ul> <li> Logging</li> <li> Profiling</li> <li> Caching</li> <li> Data</li> <li> Model</li> <li> Pytorch</li> <li> Filesystem</li> <li> ModelScope</li> <li> Package Management</li> </ul>"},{"location":"api/fusion_bench.utils/caching/","title":"Caching Utilities","text":""},{"location":"api/fusion_bench.utils/caching/#fusion_bench.utils.cache_utils","title":"<code>fusion_bench.utils.cache_utils</code>","text":""},{"location":"api/fusion_bench.utils/caching/#fusion_bench.utils.cache_utils.cache_to_disk","title":"<code>cache_to_disk(file_path)</code>","text":"<p>A decorator to cache the result of a function to a file. If the file exists, the result is loaded from the file. Otherwise, the function is executed and the result is saved to the file.</p> <p>deprecated</p> <p>This function is deprecated. Use <code>cache_with_joblib</code> instead for better caching capabilities including automatic cache invalidation, better object handling, and memory efficiency.</p>"},{"location":"api/fusion_bench.utils/caching/#fusion_bench.utils.cache_utils.cache_to_disk--example-usage","title":"Example usage","text":"<pre><code>@cache_to_disk(\"path_to_file.pkl\")\ndef some_function(*args: Any, **kwargs: Any) -&gt; Any:\n    # Function implementation\n    return \"some result\"\n</code></pre> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str</code>)           \u2013            <p>The path to the file where the result should be cached.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Callable</code> (              <code>Callable</code> )          \u2013            <p>The decorated function.</p> </li> </ul> Source code in <code>fusion_bench/utils/cache_utils.py</code> <pre><code>def cache_to_disk(file_path: Union[str, Path]) -&gt; Callable:\n    \"\"\"\n    A decorator to cache the result of a function to a file. If the file exists,\n    the result is loaded from the file. Otherwise, the function is executed and\n    the result is saved to the file.\n\n    !!! warning \"deprecated\"\n        This function is deprecated. Use `cache_with_joblib` instead for better\n        caching capabilities including automatic cache invalidation, better object\n        handling, and memory efficiency.\n\n    ## Example usage\n\n    ```python\n    @cache_to_disk(\"path_to_file.pkl\")\n    def some_function(*args: Any, **kwargs: Any) -&gt; Any:\n        # Function implementation\n        return \"some result\"\n    ```\n\n    Args:\n        file_path (str): The path to the file where the result should be cached.\n\n    Returns:\n        Callable: The decorated function.\n    \"\"\"\n    warnings.warn(\n        \"cache_to_disk is deprecated. Use cache_with_joblib instead for better \"\n        \"caching capabilities including automatic cache invalidation, better object \"\n        \"handling, and memory efficiency.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n    assert isinstance(file_path, Path)\n\n    def decorator(func: Callable) -&gt; Callable:\n        @wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n            if os.path.exists(file_path):\n                log.info(\n                    f\"Loading cached result of {func.__name__} from {file_path}\",\n                    stacklevel=2,\n                )\n                with open(file_path, \"rb\") as f:\n                    return pickle.load(f)\n            else:\n                result = func(*args, **kwargs)\n                file_path.parent.mkdir(parents=True, exist_ok=True)\n                with open(file_path, \"wb\") as f:\n                    pickle.dump(result, f)\n                return result\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/fusion_bench.utils/caching/#fusion_bench.utils.cache_utils.cache_with_joblib","title":"<code>cache_with_joblib(cache_dir=None, verbose=0)</code>","text":"<p>A decorator to cache the result of a function using joblib.Memory. This provides more advanced caching capabilities compared to cache_to_disk, including: - Automatic cache invalidation when function arguments change - Better handling of numpy arrays and other complex objects - Memory-efficient storage - Optional verbose output for cache hits/misses</p>"},{"location":"api/fusion_bench.utils/caching/#fusion_bench.utils.cache_utils.cache_with_joblib--example-usage","title":"Example usage","text":"<pre><code>@cache_with_joblib(\"./cache\", verbose=1)\ndef expensive_computation(x: int, y: str) -&gt; Any:\n    # Function implementation\n    return complex_result\n\n# Or with default settings:\n@cache_with_joblib()\ndef another_function(x: int) -&gt; int:\n    return x * 2\n</code></pre> <p>Parameters:</p> <ul> <li> <code>cache_dir</code>               (<code>Union[str, Path]</code>, default:                   <code>None</code> )           \u2013            <p>The directory where cache files should be stored. If <code>None</code>, a default directory <code>outputs/cache</code> will be used.</p> </li> <li> <code>verbose</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Verbosity level for joblib.Memory (0=silent, 1=basic, 2++=verbose).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Callable</code> (              <code>Callable</code> )          \u2013            <p>A decorator function that can be applied to functions.</p> </li> </ul> Source code in <code>fusion_bench/utils/cache_utils.py</code> <pre><code>def cache_with_joblib(\n    cache_dir: Union[str, Path] = None,\n    verbose: int = 0,\n) -&gt; Callable:\n    \"\"\"\n    A decorator to cache the result of a function using joblib.Memory. This provides\n    more advanced caching capabilities compared to cache_to_disk, including:\n    - Automatic cache invalidation when function arguments change\n    - Better handling of numpy arrays and other complex objects\n    - Memory-efficient storage\n    - Optional verbose output for cache hits/misses\n\n    ## Example usage\n\n    ```python\n    @cache_with_joblib(\"./cache\", verbose=1)\n    def expensive_computation(x: int, y: str) -&gt; Any:\n        # Function implementation\n        return complex_result\n\n    # Or with default settings:\n    @cache_with_joblib()\n    def another_function(x: int) -&gt; int:\n        return x * 2\n    ```\n\n    Args:\n        cache_dir (Union[str, Path]): The directory where cache files should be stored.\n            If `None`, a default directory `outputs/cache` will be used.\n        verbose (int): Verbosity level for joblib.Memory (0=silent, 1=basic, 2++=verbose).\n\n    Returns:\n        Callable: A decorator function that can be applied to functions.\n    \"\"\"\n\n    if cache_dir is None:\n        cache_dir = DEFAULT_CACHE_DIR\n\n    if isinstance(cache_dir, str):\n        cache_dir = Path(cache_dir)\n    assert isinstance(cache_dir, Path)\n\n    # Create the cache directory if it doesn't exist\n    cache_dir.mkdir(parents=True, exist_ok=True)\n\n    # Create a Memory object for this function\n    memory = Memory(location=cache_dir, verbose=verbose)\n\n    def decorator(func: Callable) -&gt; Callable:\n        nonlocal memory\n\n        # Create the cached version of the function\n        cached_func = memory.cache(func)\n\n        @wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n            return cached_func(*args, **kwargs)\n\n        # Expose useful methods from joblib.Memory\n        if not (\n            hasattr(cached_func, \"clear\")\n            or hasattr(cached_func, \"call\")\n            or hasattr(cached_func, \"check_call_in_cache\")\n        ):\n            wrapper.clear = cached_func.clear\n            wrapper.call = cached_func.call\n            wrapper.check_call_in_cache = cached_func.check_call_in_cache\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/fusion_bench.utils/data/","title":"Data Utilities","text":""},{"location":"api/fusion_bench.utils/data/#dataset-manipulation","title":"Dataset Manipulation","text":""},{"location":"api/fusion_bench.utils/data/#fusion_bench.utils.data","title":"<code>fusion_bench.utils.data</code>","text":""},{"location":"api/fusion_bench.utils/data/#fusion_bench.utils.data.InfiniteDataLoader","title":"<code>InfiniteDataLoader</code>","text":"<p>A wrapper class for DataLoader to create an infinite data loader. This is useful in case we are only interested in the number of steps and not the number of epochs.</p> <p>This class wraps a DataLoader and provides an iterator that resets when the end of the dataset is reached, creating an infinite loop.</p> <p>Attributes:</p> <ul> <li> <code>data_loader</code>               (<code>DataLoader</code>)           \u2013            <p>The DataLoader to wrap.</p> </li> <li> <code>data_iter</code>               (<code>iterator</code>)           \u2013            <p>An iterator over the DataLoader.</p> </li> </ul> Source code in <code>fusion_bench/utils/data.py</code> <pre><code>class InfiniteDataLoader:\n    \"\"\"\n    A wrapper class for DataLoader to create an infinite data loader.\n    This is useful in case we are only interested in the number of steps and not the number of epochs.\n\n    This class wraps a DataLoader and provides an iterator that resets\n    when the end of the dataset is reached, creating an infinite loop.\n\n    Attributes:\n        data_loader (DataLoader): The DataLoader to wrap.\n        data_iter (iterator): An iterator over the DataLoader.\n    \"\"\"\n\n    def __init__(self, data_loader: DataLoader):\n        self.data_loader = data_loader\n        self.data_iter = iter(data_loader)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        try:\n            data = next(self.data_iter)\n        except StopIteration:\n            self.data_iter = iter(self.data_loader)  # Reset the data loader\n            data = next(self.data_iter)\n        return data\n</code></pre>"},{"location":"api/fusion_bench.utils/data/#fusion_bench.utils.data.load_tensor_from_file","title":"<code>load_tensor_from_file(file_path, device=None)</code>","text":"<p>Loads a tensor from a file, which can be either a .pt, .pth or .np file. If the file is not one of these formats, it will try to load it as a pickle file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str</code>)           \u2013            <p>The path to the file to load.</p> </li> <li> <code>device</code>               (<code>Optional[Union[str, device]]</code>, default:                   <code>None</code> )           \u2013            <p>The device to move the tensor to. By default the tensor is loaded on the CPU.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>torch.Tensor: The tensor loaded from the file.</p> </li> </ul> Source code in <code>fusion_bench/utils/data.py</code> <pre><code>def load_tensor_from_file(\n    file_path: Union[str, Path], device: Optional[Union[str, torch.device]] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Loads a tensor from a file, which can be either a .pt, .pth or .np file.\n    If the file is not one of these formats, it will try to load it as a pickle file.\n\n    Args:\n        file_path (str): The path to the file to load.\n        device: The device to move the tensor to. By default the tensor is loaded on the CPU.\n\n    Returns:\n        torch.Tensor: The tensor loaded from the file.\n    \"\"\"\n    if file_path.endswith(\".np\"):\n        tensor = torch.from_numpy(np.load(file_path)).detach_()\n    if file_path.endswith((\".pt\", \".pth\")):\n        tensor = torch.load(file_path, map_location=\"cpu\").detach_()\n    else:\n        try:\n            tensor = pickle.load(open(file_path, \"rb\"))\n        except Exception:\n            raise ValueError(f\"Unsupported file format: {file_path}\")\n\n    # Move tensor to device\n    assert isinstance(tensor, torch.Tensor), f\"Expected tensor, got {type(tensor)}\"\n    if device is not None:\n        tensor = tensor.to(device=device)\n    return tensor\n</code></pre>"},{"location":"api/fusion_bench.utils/data/#fusion_bench.utils.data.train_validation_split","title":"<code>train_validation_split(dataset, validation_fraction=0.1, validation_size=None, random_seed=None, return_split='both')</code>","text":"<p>Split a dataset into a training and validation set.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The dataset to split.</p> </li> <li> <code>validation_fraction</code>               (<code>Optional[float]</code>, default:                   <code>0.1</code> )           \u2013            <p>The fraction of the dataset to use for validation.</p> </li> <li> <code>validation_size</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The number of samples to use for validation. <code>validation_fraction</code> must be set to <code>None</code> if this is provided.</p> </li> <li> <code>random_seed</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The random seed to use for reproducibility.</p> </li> <li> <code>return_split</code>               (<code>Literal['all', 'train', 'val']</code>, default:                   <code>'both'</code> )           \u2013            <p>The split to return.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Tuple[Dataset, Dataset], Dataset]</code>           \u2013            <p>Tuple[Dataset, Dataset]: The training and validation datasets.</p> </li> </ul> Source code in <code>fusion_bench/utils/data.py</code> <pre><code>def train_validation_split(\n    dataset: Dataset,\n    validation_fraction: Optional[float] = 0.1,\n    validation_size: Optional[int] = None,\n    random_seed: Optional[int] = None,\n    return_split: Literal[\"all\", \"train\", \"val\"] = \"both\",\n) -&gt; Union[Tuple[Dataset, Dataset], Dataset]:\n    \"\"\"\n    Split a dataset into a training and validation set.\n\n    Args:\n        dataset (Dataset): The dataset to split.\n        validation_fraction (Optional[float]): The fraction of the dataset to use for validation.\n        validation_size (Optional[int]): The number of samples to use for validation. `validation_fraction` must be set to `None` if this is provided.\n        random_seed (Optional[int]): The random seed to use for reproducibility.\n        return_split (Literal[\"all\", \"train\", \"val\"]): The split to return.\n\n    Returns:\n        Tuple[Dataset, Dataset]: The training and validation datasets.\n    \"\"\"\n    # Check the input arguments\n    assert (\n        validation_fraction is None or validation_size is None\n    ), \"Only one of validation_fraction and validation_size can be provided\"\n    assert (\n        validation_fraction is not None or validation_size is not None\n    ), \"Either validation_fraction or validation_size must be provided\"\n\n    # Compute the number of samples for training and validation\n    num_samples = len(dataset)\n    if validation_size is None:\n        assert (\n            0 &lt; validation_fraction &lt; 1\n        ), \"Validation fraction must be between 0 and 1\"\n        num_validation_samples = int(num_samples * validation_fraction)\n        num_training_samples = num_samples - num_validation_samples\n    else:\n        assert (\n            validation_size &lt; num_samples\n        ), \"Validation size must be less than num_samples\"\n        num_validation_samples = validation_size\n        num_training_samples = num_samples - num_validation_samples\n\n    # Split the dataset\n    generator = (\n        torch.Generator().manual_seed(random_seed) if random_seed is not None else None\n    )\n    training_dataset, validation_dataset = torch.utils.data.random_split(\n        dataset, [num_training_samples, num_validation_samples], generator=generator\n    )\n\n    # return the split as requested\n    if return_split == \"all\":\n        return training_dataset, validation_dataset\n    elif return_split == \"train\":\n        return training_dataset\n    elif return_split == \"val\":\n        return validation_dataset\n    else:\n        raise ValueError(f\"Invalid return_split: {return_split}\")\n</code></pre>"},{"location":"api/fusion_bench.utils/data/#fusion_bench.utils.data.train_validation_test_split","title":"<code>train_validation_test_split(dataset, validation_fraction, test_fraction, random_seed=None, return_spilt='all')</code>","text":"<p>Split a dataset into a training, validation and test set.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The dataset to split.</p> </li> <li> <code>validation_fraction</code>               (<code>float</code>)           \u2013            <p>The fraction of the dataset to use for validation.</p> </li> <li> <code>test_fraction</code>               (<code>float</code>)           \u2013            <p>The fraction of the dataset to use for test.</p> </li> <li> <code>random_seed</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The random seed to use for reproducibility.</p> </li> <li> <code>return_spilt</code>               (<code>Literal['all', 'train', 'val', 'test']</code>, default:                   <code>'all'</code> )           \u2013            <p>The split to return.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Tuple[Dataset, Dataset, Dataset], Dataset]</code>           \u2013            <p>Tuple[Dataset, Dataset, Dataset]: The training, validation and test datasets.</p> </li> </ul> Source code in <code>fusion_bench/utils/data.py</code> <pre><code>def train_validation_test_split(\n    dataset: Dataset,\n    validation_fraction: float,\n    test_fraction: float,\n    random_seed: Optional[int] = None,\n    return_spilt: Literal[\"all\", \"train\", \"val\", \"test\"] = \"all\",\n) -&gt; Union[Tuple[Dataset, Dataset, Dataset], Dataset]:\n    \"\"\"\n    Split a dataset into a training, validation and test set.\n\n    Args:\n        dataset (Dataset): The dataset to split.\n        validation_fraction (float): The fraction of the dataset to use for validation.\n        test_fraction (float): The fraction of the dataset to use for test.\n        random_seed (Optional[int]): The random seed to use for reproducibility.\n        return_spilt (Literal[\"all\", \"train\", \"val\", \"test\"]): The split to return.\n\n    Returns:\n        Tuple[Dataset, Dataset, Dataset]: The training, validation and test datasets.\n    \"\"\"\n    num_samples = len(dataset)\n    assert 0 &lt; validation_fraction &lt; 1, \"Validation fraction must be between 0 and 1\"\n    assert 0 &lt; test_fraction &lt; 1, \"Test fraction must be between 0 and 1\"\n    generaotr = (\n        torch.Generator().manual_seed(random_seed) if random_seed is not None else None\n    )\n\n    num_validation_samples = int(num_samples * validation_fraction)\n    num_test_samples = int(num_samples * test_fraction)\n    num_training_samples = num_samples - num_validation_samples - num_test_samples\n    training_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(\n        dataset,\n        [num_training_samples, num_validation_samples, num_test_samples],\n        generator=generaotr,\n    )\n\n    # return the split as requested\n    if return_spilt == \"all\":\n        return training_dataset, validation_dataset, test_dataset\n    elif return_spilt == \"train\":\n        return training_dataset\n    elif return_spilt == \"val\":\n        return validation_dataset\n    elif return_spilt == \"test\":\n        return test_dataset\n    else:\n        raise ValueError(f\"Invalid return_split: {return_spilt}\")\n</code></pre>"},{"location":"api/fusion_bench.utils/data/#json-importexport","title":"Json Import/Export","text":""},{"location":"api/fusion_bench.utils/data/#fusion_bench.utils.json","title":"<code>fusion_bench.utils.json</code>","text":""},{"location":"api/fusion_bench.utils/data/#fusion_bench.utils.json.load_from_json","title":"<code>load_from_json(path)</code>","text":"<p>load an object from a json file</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>the path to load the object</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>Union[dict, list]</code> )          \u2013            <p>the loaded object</p> </li> </ul> Source code in <code>fusion_bench/utils/json.py</code> <pre><code>def load_from_json(path: Union[str, Path]) -&gt; Union[dict, list]:\n    \"\"\"load an object from a json file\n\n    Args:\n        path (Union[str, Path]): the path to load the object\n\n    Returns:\n        dict: the loaded object\n    \"\"\"\n    with open(path, \"r\") as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/fusion_bench.utils/data/#fusion_bench.utils.json.print_json","title":"<code>print_json(j, indent='  ', verbose=False, print_type=True)</code>","text":"<p>print an overview of json file</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; print_json(open('path_to_json', 'r'))\n</code></pre> <p>Parameters:</p> <ul> <li> <code>j</code>               (<code>dict</code>)           \u2013            <p>loaded json file</p> </li> <li> <code>indent</code>               (<code>str</code>, default:                   <code>'  '</code> )           \u2013            <p>Defaults to '  '.</p> </li> </ul> Source code in <code>fusion_bench/utils/json.py</code> <pre><code>def print_json(j: dict, indent=\"  \", verbose: bool = False, print_type: bool = True):\n    R\"\"\"print an overview of json file\n\n    Examples:\n        &gt;&gt;&gt; print_json(open('path_to_json', 'r'))\n\n    Args:\n        j (dict): loaded json file\n        indent (str, optional): Defaults to '  '.\n    \"\"\"\n\n    def _print_json(j: dict, level):\n        def _sprint(s):\n            return indent * level + s\n\n        for k in j.keys():\n            if isinstance(j[k], dict):\n                print(_sprint(k) + \":\")\n                _print_json(j[k], level + 1)\n            elif _is_list_of_dict(j[k]):\n                if verbose:\n                    print(_sprint(k) + \": [\")\n                    for i in range(len(j[k]) - 1):\n                        _print_json(j[k][0], level + 2)\n                        print(_sprint(f\"{indent},\"))\n                    _print_json(j[k][-1], level + 2)\n                    print(_sprint(f\"{indent}]\"))\n                else:\n                    print(_sprint(k) + \": [\")\n                    _print_json(j[k][0], level + 2)\n                    print(_sprint(f\"{indent}] ... {len(j[k]) - 1} more\"))\n            else:\n                if print_type:\n                    print(f\"{_sprint(k)}: {_sprint_json_entry(j[k])}\")\n                else:\n                    print(f\"{_sprint(k)}: {j[k]}\")\n\n    _print_json(j, level=0)\n</code></pre>"},{"location":"api/fusion_bench.utils/data/#fusion_bench.utils.json.save_to_json","title":"<code>save_to_json(obj, path)</code>","text":"<p>save an object to a json file</p> <p>Parameters:</p> <ul> <li> <code>obj</code>               (<code>Any</code>)           \u2013            <p>the object to save</p> </li> <li> <code>path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>the path to save the object</p> </li> </ul> Source code in <code>fusion_bench/utils/json.py</code> <pre><code>def save_to_json(obj, path: Union[str, Path]):\n    \"\"\"\n    save an object to a json file\n\n    Args:\n        obj (Any): the object to save\n        path (Union[str, Path]): the path to save the object\n    \"\"\"\n    with open(path, \"w\") as f:\n        json.dump(obj, f)\n</code></pre>"},{"location":"api/fusion_bench.utils/data/#tensorboard-data-import","title":"TensorBoard Data Import","text":""},{"location":"api/fusion_bench.utils/data/#fusion_bench.utils.tensorboard","title":"<code>fusion_bench.utils.tensorboard</code>","text":"<p>functions deal with tensorboard logs.</p>"},{"location":"api/fusion_bench.utils/data/#fusion_bench.utils.tensorboard.parse_tensorboard_as_dict","title":"<code>parse_tensorboard_as_dict(path, scalars)</code>","text":"<p>returns a dictionary of pandas dataframes for each requested scalar.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>A file path to a directory containing tf events files, or a single        tf events file. The accumulator will load events from this path.</p> </li> <li> <code>scalars</code>               (<code>Iterable[str]</code>)           \u2013            <p>scalars</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Dict[str, pandas.DataFrame]: a dictionary of pandas dataframes for each requested scalar</p> </li> </ul> Source code in <code>fusion_bench/utils/tensorboard.py</code> <pre><code>def parse_tensorboard_as_dict(path: str, scalars: Iterable[str]):\n    \"\"\"\n    returns a dictionary of pandas dataframes for each requested scalar.\n\n    Args:\n        path(str): A file path to a directory containing tf events files, or a single\n                   tf events file. The accumulator will load events from this path.\n        scalars:   scalars\n\n    Returns:\n        Dict[str, pandas.DataFrame]: a dictionary of pandas dataframes for each requested scalar\n    \"\"\"\n    ea = event_accumulator.EventAccumulator(\n        path,\n        size_guidance={event_accumulator.SCALARS: 0},\n    )\n    _absorb_print = ea.Reload()\n    # make sure the scalars are in the event accumulator tags\n    assert all(\n        s in ea.Tags()[\"scalars\"] for s in scalars\n    ), \"some scalars were not found in the event accumulator\"\n    return {k: pd.DataFrame(ea.Scalars(k)) for k in scalars}\n</code></pre>"},{"location":"api/fusion_bench.utils/data/#fusion_bench.utils.tensorboard.parse_tensorboard_as_list","title":"<code>parse_tensorboard_as_list(path, scalars)</code>","text":"<p>returns a list of pandas dataframes for each requested scalar.</p> <p>see also: func:<code>parse_tensorboard_as_dict</code></p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>A file path to a directory containing tf events files, or a single        tf events file. The accumulator will load events from this path.</p> </li> <li> <code>scalars</code>               (<code>Iterable[str]</code>)           \u2013            <p>scalars</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>List[pandas.DataFrame]: a list of pandas dataframes for each requested scalar.</p> </li> </ul> Source code in <code>fusion_bench/utils/tensorboard.py</code> <pre><code>def parse_tensorboard_as_list(path: str, scalars: Iterable[str]):\n    \"\"\"\n    returns a list of pandas dataframes for each requested scalar.\n\n    see also: :py:func:`parse_tensorboard_as_dict`\n\n    Args:\n        path(str): A file path to a directory containing tf events files, or a single\n                   tf events file. The accumulator will load events from this path.\n        scalars:   scalars\n\n    Returns:\n        List[pandas.DataFrame]: a list of pandas dataframes for each requested scalar.\n    \"\"\"\n    d = parse_tensorboard_as_dict(path, scalars)\n    return [d[s] for s in scalars]\n</code></pre>"},{"location":"api/fusion_bench.utils/filesystem/","title":"FileSystem Utilities","text":""},{"location":"api/fusion_bench.utils/filesystem/#fusion_bench.utils.path","title":"<code>fusion_bench.utils.path</code>","text":""},{"location":"api/fusion_bench.utils/filesystem/#fusion_bench.utils.path.create_symlink","title":"<code>create_symlink(src_dir, dst_dir, link_name=None)</code>","text":"<p>Creates a symbolic link from src_dir to dst_dir.</p> <p>Parameters:</p> <ul> <li> <code>src_dir</code>               (<code>str</code>)           \u2013            <p>The source directory to link to.</p> </li> <li> <code>dst_dir</code>               (<code>str</code>)           \u2013            <p>The destination directory where the symlink will be created.</p> </li> <li> <code>link_name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The name of the symlink. If None, uses the basename of src_dir.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>OSError</code>             \u2013            <p>If the symbolic link creation fails.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If src_dir does not exist or is not a directory.</p> </li> </ul> Source code in <code>fusion_bench/utils/path.py</code> <pre><code>@rank_zero_only\ndef create_symlink(src_dir: str, dst_dir: str, link_name: str = None):\n    \"\"\"\n    Creates a symbolic link from src_dir to dst_dir.\n\n    Args:\n        src_dir (str): The source directory to link to.\n        dst_dir (str): The destination directory where the symlink will be created.\n        link_name (str, optional): The name of the symlink. If None, uses the basename of src_dir.\n\n    Raises:\n        OSError: If the symbolic link creation fails.\n        ValueError: If src_dir does not exist or is not a directory.\n    \"\"\"\n    if not os.path.exists(src_dir):\n        raise ValueError(f\"Source directory does not exist: {src_dir}\")\n\n    if not os.path.isdir(src_dir):\n        raise ValueError(f\"Source path is not a directory: {src_dir}\")\n\n    # Avoid creating symlink if source and destination are the same\n    if os.path.abspath(src_dir) == os.path.abspath(dst_dir):\n        log.warning(\n            \"Source and destination directories are the same, skipping symlink creation\"\n        )\n        return\n\n    # Create destination directory if it doesn't exist\n    os.makedirs(dst_dir, exist_ok=True)\n\n    # Determine link name\n    if link_name is None:\n        link_name = os.path.basename(src_dir)\n\n    link_path = os.path.join(dst_dir, link_name)\n    # if the link already exists, skip\n    if os.path.exists(link_path):\n        log.warning(f\"Symbolic link already exists, skipping: {link_path}\")\n        return\n\n    try:\n        # if the system is windows, use the `mklink` command in \"CMD\" to create the symlink\n        if os.name == \"nt\":\n            os.system(\n                f\"mklink /J {os.path.abspath(link_path)} {os.path.abspath(src_dir)}\"\n            )\n        else:\n            os.symlink(\n                src_dir,\n                link_path,\n                target_is_directory=True,\n            )\n        log.info(f\"Created symbolic link: {link_path} -&gt; {src_dir}\")\n    except OSError as e:\n        log.warning(f\"Failed to create symbolic link: {e}\")\n        raise\n</code></pre>"},{"location":"api/fusion_bench.utils/filesystem/#fusion_bench.utils.path.listdir_fullpath","title":"<code>listdir_fullpath(dir)</code>","text":"<p>list directory <code>dir</code>, return fullpaths</p> <p>Parameters:</p> <ul> <li> <code>dir</code>               (<code>str</code>)           \u2013            <p>directory name</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: a list of fullpaths</p> </li> </ul> Source code in <code>fusion_bench/utils/path.py</code> <pre><code>def listdir_fullpath(dir: str) -&gt; List[str]:\n    \"\"\"list directory `dir`, return fullpaths\n\n    Args:\n        dir (str): directory name\n\n    Returns:\n        List[str]: a list of fullpaths\n    \"\"\"\n    assert os.path.isdir(dir), \"Argument 'dir' must be a Directory\"\n    names = os.listdir(dir)\n    return [os.path.join(dir, name) for name in names]\n</code></pre>"},{"location":"api/fusion_bench.utils/logging/","title":"Logging Utilities","text":""},{"location":"api/fusion_bench.utils/logging/#fusion_bench.utils.rich_utils","title":"<code>fusion_bench.utils.rich_utils</code>","text":""},{"location":"api/fusion_bench.utils/logging/#fusion_bench.utils.rich_utils.display_available_styles","title":"<code>display_available_styles()</code>","text":"<p>Display all available styles in a grid.</p> Source code in <code>fusion_bench/utils/rich_utils.py</code> <pre><code>def display_available_styles():\n    \"\"\"Display all available styles in a grid.\"\"\"\n    console = Console()\n    style_samples = [\n        Panel(f\"Style: {style}\", expand=False, border_style=style)\n        for style in AVAILABLE_STYLES\n    ]\n    console.print(Columns(style_samples, equal=True, expand=False))\n</code></pre>"},{"location":"api/fusion_bench.utils/logging/#fusion_bench.utils.rich_utils.enforce_tags","title":"<code>enforce_tags(cfg, save_to_file=False)</code>","text":"<p>Prompts user to input tags from command line if no tags are provided in config.</p> <p>:param cfg: A DictConfig composed by Hydra. :param save_to_file: Whether to export tags to the hydra output folder. Default is <code>False</code>.</p> Source code in <code>fusion_bench/utils/rich_utils.py</code> <pre><code>@rank_zero_only\ndef enforce_tags(cfg: DictConfig, save_to_file: bool = False) -&gt; None:\n    \"\"\"Prompts user to input tags from command line if no tags are provided in config.\n\n    :param cfg: A DictConfig composed by Hydra.\n    :param save_to_file: Whether to export tags to the hydra output folder. Default is ``False``.\n    \"\"\"\n    if not cfg.get(\"tags\"):\n        if \"id\" in HydraConfig().cfg.hydra.job:\n            raise ValueError(\"Specify tags before launching a multirun!\")\n\n        log.warning(\"No tags provided in config. Prompting user to input tags...\")\n        tags = Prompt.ask(\"Enter a list of comma separated tags\", default=\"dev\")\n        tags = [t.strip() for t in tags.split(\",\") if t != \"\"]\n\n        with open_dict(cfg):\n            cfg.tags = tags\n\n        log.info(f\"Tags: {cfg.tags}\")\n\n    if save_to_file:\n        with open(Path(cfg.paths.output_dir, \"tags.log\"), \"w\") as file:\n            rich.print(cfg.tags, file=file)\n</code></pre>"},{"location":"api/fusion_bench.utils/logging/#fusion_bench.utils.rich_utils.print_bordered","title":"<code>print_bordered(message, title=None, style='blue', code_style=None)</code>","text":"<p>Print a message with a colored border.</p> <p>Args: message (str): The message to print. title (str, optional): The title of the panel. Defaults to None. style (str, optional): The color style for the border. Defaults to \"cyan\". code_style (str, optional): The syntax highlighting style if the message is code.                             Set to None for plain text. Defaults to \"python\".</p> Source code in <code>fusion_bench/utils/rich_utils.py</code> <pre><code>def print_bordered(message, title=None, style=\"blue\", code_style=None):\n    \"\"\"\n    Print a message with a colored border.\n\n    Args:\n    message (str): The message to print.\n    title (str, optional): The title of the panel. Defaults to None.\n    style (str, optional): The color style for the border. Defaults to \"cyan\".\n    code_style (str, optional): The syntax highlighting style if the message is code.\n                                Set to None for plain text. Defaults to \"python\".\n    \"\"\"\n    if code_style:\n        content = Syntax(message, code_style, theme=\"monokai\", word_wrap=True)\n    else:\n        content = Text(message)\n\n    panel = Panel(content, title=title, border_style=style)\n    print(panel)\n</code></pre>"},{"location":"api/fusion_bench.utils/logging/#fusion_bench.utils.rich_utils.print_config_tree","title":"<code>print_config_tree(cfg, print_order=('data', 'model', 'callbacks', 'logger', 'trainer', 'paths', 'extras'), resolve=False, save_to_file=False)</code>","text":"<p>Prints the contents of a DictConfig as a tree structure using the Rich library.</p> <p>:param cfg: A DictConfig composed by Hydra. :param print_order: Determines in what order config components are printed. Default is <code>(\"data\", \"model\", \"callbacks\", \"logger\", \"trainer\", \"paths\", \"extras\")</code>. :param resolve: Whether to resolve reference fields of DictConfig. Default is <code>False</code>. :param save_to_file: Whether to export config to the hydra output folder. Default is <code>False</code>.</p> Source code in <code>fusion_bench/utils/rich_utils.py</code> <pre><code>@rank_zero_only\ndef print_config_tree(\n    cfg: DictConfig,\n    print_order: Sequence[str] = (\n        \"data\",\n        \"model\",\n        \"callbacks\",\n        \"logger\",\n        \"trainer\",\n        \"paths\",\n        \"extras\",\n    ),\n    resolve: bool = False,\n    save_to_file: bool = False,\n) -&gt; None:\n    \"\"\"Prints the contents of a DictConfig as a tree structure using the Rich library.\n\n    :param cfg: A DictConfig composed by Hydra.\n    :param print_order: Determines in what order config components are printed. Default is ``(\"data\", \"model\",\n    \"callbacks\", \"logger\", \"trainer\", \"paths\", \"extras\")``.\n    :param resolve: Whether to resolve reference fields of DictConfig. Default is ``False``.\n    :param save_to_file: Whether to export config to the hydra output folder. Default is ``False``.\n    \"\"\"\n    style = \"tree\"\n    tree = rich.tree.Tree(\"CONFIG\", style=style, guide_style=style)\n\n    queue = []\n\n    # add fields from `print_order` to queue\n    for field in print_order:\n        (\n            queue.append(field)\n            if field in cfg\n            else log.warning(\n                f\"Field '{field}' not found in config. Skipping '{field}' config printing...\"\n            )\n        )\n\n    # add all the other fields to queue (not specified in `print_order`)\n    for field in cfg:\n        if field not in queue:\n            queue.append(field)\n\n    # generate config tree from queue\n    for field in queue:\n        branch = tree.add(field, style=style, guide_style=style)\n\n        config_group = cfg[field]\n        if isinstance(config_group, DictConfig):\n            branch_content = OmegaConf.to_yaml(config_group, resolve=resolve)\n        else:\n            branch_content = str(config_group)\n\n        branch.add(rich.syntax.Syntax(branch_content, \"yaml\"))\n\n    # print config tree\n    rich.print(tree)\n\n    # save config tree to file\n    if save_to_file:\n        with open(Path(cfg.paths.output_dir, \"config_tree.log\"), \"w\") as file:\n            rich.print(tree, file=file)\n</code></pre>"},{"location":"api/fusion_bench.utils/logging/#fusion_bench.utils.rich_utils.setup_colorlogging","title":"<code>setup_colorlogging(force=False, level=logging.INFO, **kwargs)</code>","text":"<p>Sets up color logging for the application.</p> Source code in <code>fusion_bench/utils/rich_utils.py</code> <pre><code>def setup_colorlogging(\n    force=False,\n    level=logging.INFO,\n    **kwargs,\n):\n    \"\"\"\n    Sets up color logging for the application.\n    \"\"\"\n    FORMAT = \"%(message)s\"\n\n    logging.basicConfig(\n        level=level,\n        format=FORMAT,\n        datefmt=\"[%X]\",\n        handlers=[RichHandler()],\n        force=force,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/fusion_bench.utils/logging/#fusion_bench.utils.pylogger","title":"<code>fusion_bench.utils.pylogger</code>","text":""},{"location":"api/fusion_bench.utils/logging/#fusion_bench.utils.pylogger.RankZeroLogger","title":"<code>RankZeroLogger</code>","text":"<p>               Bases: <code>Logger</code></p> <p>A logger that logs only on rank zero and works just like logging.Logger</p> Source code in <code>fusion_bench/utils/pylogger.py</code> <pre><code>class RankZeroLogger(logging.Logger):\n    \"\"\"A logger that logs only on rank zero and works just like logging.Logger\"\"\"\n\n    @rank_zero_only\n    def _log(self, *args, **kwargs):\n        if \"stacklevel\" in kwargs:\n            kwargs[\"stacklevel\"] += 1\n        else:\n            kwargs[\"stacklevel\"] = 2\n        return super()._log(*args, **kwargs)\n\n    def is_global_zero(self):\n        return rank_zero_only.rank == 0\n</code></pre>"},{"location":"api/fusion_bench.utils/logging/#fusion_bench.utils.pylogger.RankedLogger","title":"<code>RankedLogger</code>","text":"<p>               Bases: <code>LoggerAdapter</code></p> <p>A multi-GPU-friendly python command line logger.</p> Source code in <code>fusion_bench/utils/pylogger.py</code> <pre><code>class RankedLogger(logging.LoggerAdapter):\n    \"\"\"A multi-GPU-friendly python command line logger.\"\"\"\n\n    def __init__(\n        self,\n        name: str = __name__,\n        rank_zero_only: bool = False,\n        extra: Optional[Mapping[str, object]] = None,\n    ) -&gt; None:\n        \"\"\"Initializes a multi-GPU-friendly python command line logger that logs on all processes\n        with their rank prefixed in the log message.\n\n        :param name: The name of the logger. Default is ``__name__``.\n        :param rank_zero_only: Whether to force all logs to only occur on the rank zero process. Default is `False`.\n        :param extra: (Optional) A dict-like object which provides contextual information. See `logging.LoggerAdapter`.\n        \"\"\"\n        logger = logging.getLogger(name)\n        super().__init__(logger=logger, extra=extra)\n        self.rank_zero_only = rank_zero_only\n\n    def log(\n        self, level: int, msg: str, rank: Optional[int] = None, *args, **kwargs\n    ) -&gt; None:\n        \"\"\"Delegate a log call to the underlying logger, after prefixing its message with the rank\n        of the process it's being logged from. If `'rank'` is provided, then the log will only\n        occur on that rank/process.\n\n        :param level: The level to log at. Look at `logging.__init__.py` for more information.\n        :param msg: The message to log.\n        :param rank: The rank to log at.\n        :param args: Additional args to pass to the underlying logging function.\n        :param kwargs: Any additional keyword args to pass to the underlying logging function.\n        \"\"\"\n        if self.isEnabledFor(level):\n            msg, kwargs = self.process(msg, kwargs)\n            current_rank = getattr(rank_zero_only, \"rank\", None)\n            if current_rank is None:\n                raise RuntimeError(\n                    \"The `rank_zero_only.rank` needs to be set before use\"\n                )\n            msg = rank_prefixed_message(msg, current_rank)\n            if self.rank_zero_only:\n                if current_rank == 0:\n                    self.logger.log(level, msg, *args, **kwargs)\n            else:\n                if rank is None:\n                    self.logger.log(level, msg, *args, **kwargs)\n                elif current_rank == rank:\n                    self.logger.log(level, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/fusion_bench.utils/logging/#fusion_bench.utils.pylogger.RankedLogger.__init__","title":"<code>__init__(name=__name__, rank_zero_only=False, extra=None)</code>","text":"<p>Initializes a multi-GPU-friendly python command line logger that logs on all processes with their rank prefixed in the log message.</p> <p>:param name: The name of the logger. Default is <code>__name__</code>. :param rank_zero_only: Whether to force all logs to only occur on the rank zero process. Default is <code>False</code>. :param extra: (Optional) A dict-like object which provides contextual information. See <code>logging.LoggerAdapter</code>.</p> Source code in <code>fusion_bench/utils/pylogger.py</code> <pre><code>def __init__(\n    self,\n    name: str = __name__,\n    rank_zero_only: bool = False,\n    extra: Optional[Mapping[str, object]] = None,\n) -&gt; None:\n    \"\"\"Initializes a multi-GPU-friendly python command line logger that logs on all processes\n    with their rank prefixed in the log message.\n\n    :param name: The name of the logger. Default is ``__name__``.\n    :param rank_zero_only: Whether to force all logs to only occur on the rank zero process. Default is `False`.\n    :param extra: (Optional) A dict-like object which provides contextual information. See `logging.LoggerAdapter`.\n    \"\"\"\n    logger = logging.getLogger(name)\n    super().__init__(logger=logger, extra=extra)\n    self.rank_zero_only = rank_zero_only\n</code></pre>"},{"location":"api/fusion_bench.utils/logging/#fusion_bench.utils.pylogger.RankedLogger.log","title":"<code>log(level, msg, rank=None, *args, **kwargs)</code>","text":"<p>Delegate a log call to the underlying logger, after prefixing its message with the rank of the process it's being logged from. If <code>'rank'</code> is provided, then the log will only occur on that rank/process.</p> <p>:param level: The level to log at. Look at <code>logging.__init__.py</code> for more information. :param msg: The message to log. :param rank: The rank to log at. :param args: Additional args to pass to the underlying logging function. :param kwargs: Any additional keyword args to pass to the underlying logging function.</p> Source code in <code>fusion_bench/utils/pylogger.py</code> <pre><code>def log(\n    self, level: int, msg: str, rank: Optional[int] = None, *args, **kwargs\n) -&gt; None:\n    \"\"\"Delegate a log call to the underlying logger, after prefixing its message with the rank\n    of the process it's being logged from. If `'rank'` is provided, then the log will only\n    occur on that rank/process.\n\n    :param level: The level to log at. Look at `logging.__init__.py` for more information.\n    :param msg: The message to log.\n    :param rank: The rank to log at.\n    :param args: Additional args to pass to the underlying logging function.\n    :param kwargs: Any additional keyword args to pass to the underlying logging function.\n    \"\"\"\n    if self.isEnabledFor(level):\n        msg, kwargs = self.process(msg, kwargs)\n        current_rank = getattr(rank_zero_only, \"rank\", None)\n        if current_rank is None:\n            raise RuntimeError(\n                \"The `rank_zero_only.rank` needs to be set before use\"\n            )\n        msg = rank_prefixed_message(msg, current_rank)\n        if self.rank_zero_only:\n            if current_rank == 0:\n                self.logger.log(level, msg, *args, **kwargs)\n        else:\n            if rank is None:\n                self.logger.log(level, msg, *args, **kwargs)\n            elif current_rank == rank:\n                self.logger.log(level, msg, *args, **kwargs)\n</code></pre>"},{"location":"api/fusion_bench.utils/logging/#fusion_bench.utils.pylogger.get_rankzero_logger","title":"<code>get_rankzero_logger(name=None)</code>","text":"<p>Return a logger with the specified name, creating it if necessary.</p> <p>If no name is specified, return the root logger.</p> Source code in <code>fusion_bench/utils/pylogger.py</code> <pre><code>def get_rankzero_logger(name=None):\n    \"\"\"\n    Return a logger with the specified name, creating it if necessary.\n\n    If no name is specified, return the root logger.\n    \"\"\"\n    if not name or isinstance(name, str) and name == logging.root.name:\n        return logging.root\n    return RankZeroLogger.manager.getLogger(name)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/","title":"Model Utilities","text":""},{"location":"api/fusion_bench.utils/model/#type-definitions","title":"Type Definitions","text":""},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.type","title":"<code>fusion_bench.utils.type</code>","text":""},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.type.StateDictType","title":"<code>StateDictType = Dict[str, Tensor]</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.type.BoolStateDictType","title":"<code>BoolStateDictType = Dict[str, torch.BoolTensor]</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.type.TorchModelType","title":"<code>TorchModelType = TypeVar('TorchModelType', bound=(nn.Module))</code>  <code>module-attribute</code>","text":""},{"location":"api/fusion_bench.utils/model/#parameter-count-and-manipulation","title":"Parameter Count and Manipulation","text":""},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.parameters","title":"<code>fusion_bench.utils.parameters</code>","text":""},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.parameters.check_parameters_all_equal","title":"<code>check_parameters_all_equal(list_of_param_names)</code>","text":"<p>Checks if all models have the same parameters.</p> <p>This function takes a list of parameter names or state dictionaries from different models. It checks if all models have the same parameters by comparing the parameter names. If any model has different parameters, it raises a ValueError with the differing parameters.</p> <p>Parameters:</p> <ul> <li> <code>list_of_param_names</code>               (<code>List[Union[StateDict, List[str]]]</code>)           \u2013            <p>A list of parameter names or state dictionaries.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If any model has different parameters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>fusion_bench/utils/parameters.py</code> <pre><code>def check_parameters_all_equal(\n    list_of_param_names: List[Union[StateDictType, nn.Module, List[str]]],\n) -&gt; None:\n    \"\"\"\n    Checks if all models have the same parameters.\n\n    This function takes a list of parameter names or state dictionaries from different models.\n    It checks if all models have the same parameters by comparing the parameter names.\n    If any model has different parameters, it raises a ValueError with the differing parameters.\n\n    Args:\n        list_of_param_names (List[Union[StateDict, List[str]]]): A list of parameter names or state dictionaries.\n\n    Raises:\n        ValueError: If any model has different parameters.\n\n    Returns:\n        None\n    \"\"\"\n    if isinstance(list_of_param_names[0], Mapping):\n        list_of_param_names = [list(i.keys()) for i in list_of_param_names]\n    elif isinstance(list_of_param_names[0], nn.Module):\n        list_of_param_names = [list(i.state_dict().keys()) for i in list_of_param_names]\n    else:\n        parameter_names = set(list_of_param_names[0])\n\n        if len(list_of_param_names) &gt;= 2:\n            # raise ValueError(\"Number of models is less than 2.\")\n            for names in list_of_param_names[1:]:\n                current_parameterNames = set(names)\n                if current_parameterNames != parameter_names:\n                    raise ValueError(\n                        \"Differing parameter names in models. \"\n                        f\"The different parameters are {parameter_names.symmetric_difference(current_parameterNames)}\"\n                    )\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.parameters.count_parameters","title":"<code>count_parameters(module, non_zero_only=False)</code>","text":"<p>Counts the number of trainable and total parameters in a PyTorch model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The PyTorch model for which to count parameters.</p> </li> <li> <code>non_zero_only</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only non-zero parameters are counted. If False, all parameters are counted. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code> (              <code>tuple[int, int]</code> )          \u2013            <p>A tuple containing the number of trainable parameters and the total number of parameters.</p> </li> </ul> <p>Examples:</p> <pre><code># Count the parameters\ntrainable_params, all_params = count_parameters(model)\n</code></pre> Source code in <code>fusion_bench/utils/parameters.py</code> <pre><code>@torch.no_grad()\ndef count_parameters(module: nn.Module, non_zero_only: bool = False) -&gt; tuple[int, int]:\n    \"\"\"\n    Counts the number of trainable and total parameters in a PyTorch model.\n\n    Args:\n        model (nn.Module): The PyTorch model for which to count parameters.\n        non_zero_only (bool, optional): If True, only non-zero parameters are counted. If False, all parameters are counted. Defaults to False.\n\n    Returns:\n        tuple: A tuple containing the number of trainable parameters and the total number of parameters.\n\n    Examples:\n        ```python\n        # Count the parameters\n        trainable_params, all_params = count_parameters(model)\n        ```\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n\n    for name, param in module.named_parameters():\n        # count the number of parameters\n        num_params = _numel(param, non_zero_only)\n\n        # accumulate the number of trainable and total parameters\n        all_param += num_params\n        if param.requires_grad:\n            trainable_params += num_params\n\n    return trainable_params, all_param\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.parameters.get_parameter_statistics","title":"<code>get_parameter_statistics(module_or_state_dict, model_wise=False)</code>","text":"<p>Get statistics of the parameters in a PyTorch model or state dictionary.</p> <p>Parameters:</p> <ul> <li> <code>module_or_state_dict</code>               (<code>Union[Module, StateDictType]</code>)           \u2013            <p>The PyTorch model for which to get parameter statistics.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>A dictionary containing the mean, standard deviation, min, and max of the parameters.</p> </li> </ul> Source code in <code>fusion_bench/utils/parameters.py</code> <pre><code>@torch.no_grad()\ndef get_parameter_statistics(\n    module_or_state_dict: Union[nn.Module, StateDictType],\n    model_wise: bool = False,\n) -&gt; dict:\n    \"\"\"\n    Get statistics of the parameters in a PyTorch model or state dictionary.\n\n    Args:\n        module_or_state_dict (Union[nn.Module, StateDictType]): The PyTorch model for which to get parameter statistics.\n\n    Returns:\n        dict: A dictionary containing the mean, standard deviation, min, and max of the parameters.\n    \"\"\"\n    stats = {}\n    if isinstance(module_or_state_dict, nn.Module):\n        state_dict = module_or_state_dict.state_dict()\n    else:\n        state_dict = module_or_state_dict\n\n    if model_wise:\n        # if model-wise, return the statistics for the entire model\n        state_dict = {\"model\": state_dict_to_vector(state_dict)}\n\n    for name, param in state_dict.items():\n        stats[name] = {\n            \"mean\": param.data.mean().item(),\n            \"std\": param.data.std().item(),\n            \"min\": param.data.min().item(),\n            \"max\": param.data.max().item(),\n        }\n\n    return stats\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.parameters.get_parameter_summary","title":"<code>get_parameter_summary(module_or_state_dict, non_zero_only=False)</code>","text":"<p>Get a summary of the parameters in a PyTorch model.</p> Source code in <code>fusion_bench/utils/parameters.py</code> <pre><code>@torch.no_grad()\ndef get_parameter_summary(\n    module_or_state_dict: Union[nn.Module, StateDictType], non_zero_only: bool = False\n) -&gt; dict:\n    \"\"\"\n    Get a summary of the parameters in a PyTorch model.\n    \"\"\"\n    if isinstance(module_or_state_dict, nn.Module):\n        state_dict = module_or_state_dict.state_dict(keep_vars=True)\n    else:\n        state_dict = module_or_state_dict\n\n    trainable_params = 0\n    all_param = 0\n    bytes = 0\n\n    for name, param in state_dict.items():\n        # count the number of parameters\n        num_params = _numel(param, non_zero_only)\n        bytes += _numel(param, non_zero_only=False) * param.element_size()\n\n        # accumulate the number of trainable and total parameters\n        all_param += num_params\n        if param.requires_grad:\n            trainable_params += num_params\n\n    return {\n        \"trainable_params\": trainable_params,\n        \"all_param\": all_param,\n        \"bytes\": bytes,\n    }\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.parameters.human_readable","title":"<code>human_readable(num)</code>","text":"<p>Converts a number into a human-readable string with appropriate magnitude suffix.</p> <p>Examples:</p> <pre><code>print(human_readable(1500))\n# Output: '1.50K'\nprint(human_readable(1500000))\n# Output: '1.50M'\n</code></pre> <p>Parameters:</p> <ul> <li> <code>num</code>               (<code>int</code>)           \u2013            <p>The number to convert.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The human-readable string representation of the number.</p> </li> </ul> Source code in <code>fusion_bench/utils/parameters.py</code> <pre><code>def human_readable(num: int) -&gt; str:\n    \"\"\"\n    Converts a number into a human-readable string with appropriate magnitude suffix.\n\n    Examples:\n        ```python\n        print(human_readable(1500))\n        # Output: '1.50K'\n        print(human_readable(1500000))\n        # Output: '1.50M'\n        ```\n\n    Args:\n        num (int): The number to convert.\n\n    Returns:\n        str: The human-readable string representation of the number.\n    \"\"\"\n    if num &lt; 1000 and isinstance(num, int):\n        return str(num)\n    magnitude = 0\n    while abs(num) &gt;= 1000:\n        magnitude += 1\n        num /= 1000.0\n    return \"%.2f%s\" % (num, [\"\", \"K\", \"M\", \"B\", \"T\", \"P\"][magnitude])\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.parameters.print_parameters","title":"<code>print_parameters(module, is_human_readable=True, print_fn=print, non_zero_only=False)</code>","text":"<p>Prints the number of trainable and total parameters in a PyTorch model.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The PyTorch model for which to print parameters.</p> </li> <li> <code>human_readable</code>               (<code>bool</code>)           \u2013            <p>If True, the parameter counts are converted to a human-readable format (e.g., '1.5M' instead of '1500000'). Defaults to True.</p> </li> <li> <code>print_fn</code>               (<code>Callable</code>, default:                   <code>print</code> )           \u2013            <p>Function used to print the message.</p> </li> <li> <code>non_zero_only</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only non-zero elements are counted. If False, all elements are counted. Defaults to False.</p> </li> </ul> Prints <p>The number of trainable parameters, the total number of parameters, and the percentage of trainable parameters in the model.</p> Source code in <code>fusion_bench/utils/parameters.py</code> <pre><code>def print_parameters(\n    module: nn.Module,\n    is_human_readable: bool = True,\n    print_fn=print,\n    non_zero_only: bool = False,\n):\n    \"\"\"\n    Prints the number of trainable and total parameters in a PyTorch model.\n\n    Args:\n        module (nn.Module): The PyTorch model for which to print parameters.\n        human_readable (bool, optional): If True, the parameter counts are converted to a human-readable format (e.g., '1.5M' instead of '1500000'). Defaults to True.\n        print_fn (Callable): Function used to print the message.\n        non_zero_only (bool, optional): If True, only non-zero elements are counted. If False, all elements are counted. Defaults to False.\n\n    Prints:\n        The number of trainable parameters, the total number of parameters, and the percentage of trainable parameters in the model.\n    \"\"\"\n    trainable_params, all_param = count_parameters(module, non_zero_only=non_zero_only)\n    trainable_ratio = 100 * trainable_params / all_param\n    if is_human_readable:\n        trainable_params = human_readable(trainable_params)\n        all_param = human_readable(all_param)\n\n    print_fn(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {trainable_ratio:.4f}\"\n    )\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.parameters.state_dict_to_vector","title":"<code>state_dict_to_vector(state_dict, remove_keys=None)</code>","text":"<p>Convert a state dictionary to a vector.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>Union[dict[str, Tensor], Module]</code>)           \u2013            <p>The state dictionary to convert.</p> </li> <li> <code>remove_keys</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>List of keys to remove from the state dictionary. Defaults to [].</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>torch.Tensor: The converted vector.</p> </li> </ul> Source code in <code>fusion_bench/utils/parameters.py</code> <pre><code>def state_dict_to_vector(\n    state_dict: Union[StateDictType, nn.Module],\n    remove_keys: Optional[List[str]] = None,\n):\n    \"\"\"\n    Convert a state dictionary to a vector.\n\n    Args:\n        state_dict (Union[dict[str, torch.Tensor], nn.Module]): The state dictionary to convert.\n        remove_keys (list, optional): List of keys to remove from the state dictionary. Defaults to [].\n\n    Returns:\n        torch.Tensor: The converted vector.\n    \"\"\"\n    remove_keys = remove_keys if remove_keys is not None else []\n\n    if isinstance(state_dict, nn.Module):\n        shared_state_dict = state_dict.state_dict()\n    else:\n        shared_state_dict = copy.copy(state_dict)\n\n    # remove the keys to be removed\n    for key in remove_keys:\n        if key in shared_state_dict:\n            del shared_state_dict[key]\n\n    # sort the reference dict\n    sorted_shared_state_dict = OrderedDict(sorted(shared_state_dict.items()))\n\n    vector = nn.utils.parameters_to_vector(\n        [value.reshape(-1) for key, value in sorted_shared_state_dict.items()]\n    )\n    return vector\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.parameters.trainable_state_dict","title":"<code>trainable_state_dict(module, prefix='', keep_vars=False)</code>","text":"<p>Returns the state dictionary of the module containing only the trainable parameters.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The neural network module.</p> </li> <li> <code>prefix</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The prefix to add to the parameter names. Defaults to \"\".</p> </li> <li> <code>keep_vars</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the parameters are not detached. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>Dict[str, Tensor]: A dictionary containing the names and values of the trainable parameters.</p> </li> </ul> Source code in <code>fusion_bench/utils/parameters.py</code> <pre><code>def trainable_state_dict(\n    module: nn.Module,\n    prefix: str = \"\",\n    keep_vars: bool = False,\n) -&gt; StateDictType:\n    \"\"\"\n    Returns the state dictionary of the module containing only the trainable parameters.\n\n    Args:\n        module (nn.Module): The neural network module.\n        prefix (str, optional): The prefix to add to the parameter names. Defaults to \"\".\n        keep_vars (bool, optional): If True, the parameters are not detached. Defaults to False.\n\n    Returns:\n        Dict[str, Tensor]: A dictionary containing the names and values of the trainable parameters.\n    \"\"\"\n    state_dict = {\n        prefix + name: param if keep_vars else param.detach()\n        for name, param in module.named_parameters()\n        if param.requires_grad\n    }\n    return state_dict\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.parameters.vector_to_state_dict","title":"<code>vector_to_state_dict(vector, state_dict, remove_keys=None)</code>","text":"<p>Convert a vector to a state dictionary.</p> <p>Parameters:</p> <ul> <li> <code>vector</code>               (<code>Tensor</code>)           \u2013            <p>The vector to convert.</p> </li> <li> <code>state_dict</code>               (<code>Union[dict[str, Tensor], Module]</code>)           \u2013            <p>The reference state dictionary to define the order of the vector.</p> </li> <li> <code>remove_keys</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>List of keys to remove from the reference state dictionary. Defaults to [].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>Dict[str, Tensor]</code> )          \u2013            <p>The converted state dictionary.</p> </li> </ul> Source code in <code>fusion_bench/utils/parameters.py</code> <pre><code>def vector_to_state_dict(\n    vector: torch.Tensor,\n    state_dict: Union[StateDictType, nn.Module],\n    remove_keys: Optional[List[str]] = None,\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Convert a vector to a state dictionary.\n\n    Args:\n        vector (torch.Tensor): The vector to convert.\n        state_dict (Union[dict[str, torch.Tensor], nn.Module]): The reference state dictionary to define the order of the vector.\n        remove_keys (list, optional): List of keys to remove from the reference state dictionary. Defaults to [].\n\n    Returns:\n        dict: The converted state dictionary.\n    \"\"\"\n    remove_keys = remove_keys if remove_keys is not None else []\n\n    # create a reference dict to define the order of the vector\n    if isinstance(state_dict, nn.Module):\n        reference_dict = state_dict.state_dict()\n    else:\n        # shallow copy the state_dict\n        reference_dict = copy.copy(state_dict)\n\n    # remove the keys to be removed\n    for key in remove_keys:\n        if key in reference_dict:\n            del reference_dict[key]\n\n    # sort the reference dict\n    sorted_reference_dict = OrderedDict(sorted(reference_dict.items()))\n\n    # create a shared state dict using the reference dict\n    nn.utils.vector_to_parameters(vector, sorted_reference_dict.values())\n\n    # add back the encoder and decoder embedding weights.\n    if \"transformer.shared.weight\" in sorted_reference_dict:\n        for key in remove_keys:\n            sorted_reference_dict[key] = sorted_reference_dict[\n                \"transformer.shared.weight\"\n            ]\n    return sorted_reference_dict\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#state-dict-arithmetic","title":"State Dict Arithmetic","text":""},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic","title":"<code>fusion_bench.utils.state_dict_arithmetic</code>","text":""},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict","title":"<code>ArithmeticStateDict</code>","text":"<p>               Bases: <code>OrderedDict</code></p> <p>An OrderedDict subclass that supports arithmetic operations on state dictionaries.</p> <p>This class provides convenient operator overloading for common state dict operations like addition, subtraction, multiplication, and division, while maintaining all the functionality of OrderedDict.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sd1 = ArithmeticStateDict({'weight': torch.tensor([1.0, 2.0]), 'bias': torch.tensor([0.5])})\n&gt;&gt;&gt; sd2 = ArithmeticStateDict({'weight': torch.tensor([2.0, 3.0]), 'bias': torch.tensor([1.0])})\n&gt;&gt;&gt; result = sd1 + sd2  # Element-wise addition\n&gt;&gt;&gt; result = sd1 - sd2  # Element-wise subtraction\n&gt;&gt;&gt; result = sd1 * 2.0  # Scalar multiplication\n&gt;&gt;&gt; result = sd1 / 2.0  # Scalar division\n&gt;&gt;&gt; result = sd1 @ sd2  # Hadamard product\n</code></pre> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>class ArithmeticStateDict(OrderedDict):\n    \"\"\"\n    An OrderedDict subclass that supports arithmetic operations on state dictionaries.\n\n    This class provides convenient operator overloading for common state dict operations\n    like addition, subtraction, multiplication, and division, while maintaining all\n    the functionality of OrderedDict.\n\n    Examples:\n        &gt;&gt;&gt; sd1 = ArithmeticStateDict({'weight': torch.tensor([1.0, 2.0]), 'bias': torch.tensor([0.5])})\n        &gt;&gt;&gt; sd2 = ArithmeticStateDict({'weight': torch.tensor([2.0, 3.0]), 'bias': torch.tensor([1.0])})\n        &gt;&gt;&gt; result = sd1 + sd2  # Element-wise addition\n        &gt;&gt;&gt; result = sd1 - sd2  # Element-wise subtraction\n        &gt;&gt;&gt; result = sd1 * 2.0  # Scalar multiplication\n        &gt;&gt;&gt; result = sd1 / 2.0  # Scalar division\n        &gt;&gt;&gt; result = sd1 @ sd2  # Hadamard product\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize ArithmeticStateDict with the same interface as OrderedDict.\"\"\"\n        super().__init__(*args, **kwargs)\n\n    def __add__(\n        self, other: Union[\"ArithmeticStateDict\", StateDictType, Number]\n    ) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Element-wise addition with another state dict or scalar.\n\n        Args:\n            other: Another state dict to add or a scalar to add to all elements.\n\n        Returns:\n            A new ArithmeticStateDict with the element-wise sum.\n        \"\"\"\n        if isinstance(other, (int, float, Number)):\n            # Scalar addition\n            result_dict = state_dict_add_scalar(self, other)\n            return ArithmeticStateDict(result_dict)\n        elif isinstance(other, (dict, OrderedDict)):\n            # State dict addition\n            result_dict = state_dict_add(self, other, strict=True)\n            return ArithmeticStateDict(result_dict)\n        else:\n            raise TypeError(\n                f\"Cannot add ArithmeticStateDict with {type(other).__name__}\"\n            )\n\n    def __radd__(\n        self, other: Union[\"ArithmeticStateDict\", StateDictType, Number]\n    ) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Right addition (other + self).\n        Handles the case where sum() starts with 0 and scalar addition.\n        \"\"\"\n        if other == 0:  # sum() starts with 0 by default\n            return self\n        elif isinstance(other, (int, float, Number)):\n            # Scalar addition is commutative\n            return self.__add__(other)\n        elif isinstance(other, (dict, OrderedDict)):\n            return self.__add__(other)\n        else:\n            raise TypeError(\n                f\"Cannot add {type(other).__name__} with ArithmeticStateDict\"\n            )\n\n    def __sub__(\n        self, other: Union[\"ArithmeticStateDict\", StateDictType, Number]\n    ) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Element-wise subtraction with another state dict or scalar.\n\n        Args:\n            other: Another state dict to subtract or a scalar to subtract from all elements.\n\n        Returns:\n            A new ArithmeticStateDict with the element-wise difference.\n        \"\"\"\n        if isinstance(other, (int, float, Number)):\n            # Scalar subtraction: subtract scalar from all elements\n            result_dict = state_dict_add_scalar(self, -other)\n            return ArithmeticStateDict(result_dict)\n        elif isinstance(other, (dict, OrderedDict)):\n            # State dict subtraction\n            result_dict = state_dict_sub(self, other, strict=True)\n            return ArithmeticStateDict(result_dict)\n        else:\n            raise TypeError(\n                f\"Cannot subtract {type(other).__name__} from ArithmeticStateDict\"\n            )\n\n    def __rsub__(\n        self, other: Union[\"ArithmeticStateDict\", StateDictType, Number]\n    ) -&gt; \"ArithmeticStateDict\":\n        \"\"\"Right subtraction (other - self).\"\"\"\n        if isinstance(other, (int, float, Number)):\n            # Scalar - ArithmeticStateDict: subtract each element from scalar\n            result = ArithmeticStateDict()\n            for key, tensor in self.items():\n                result[key] = other - tensor\n            return result\n        elif isinstance(other, (dict, OrderedDict)):\n            result_dict = state_dict_sub(other, self, strict=True)\n            return ArithmeticStateDict(result_dict)\n        else:\n            raise TypeError(\n                f\"Cannot subtract ArithmeticStateDict from {type(other).__name__}\"\n            )\n\n    def __mul__(\n        self, scalar: Union[Number, \"ArithmeticStateDict\", StateDictType]\n    ) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Scalar multiplication or Hadamard product.\n\n        Args:\n            scalar: A scalar value for element-wise multiplication, or another state dict\n                   for Hadamard product.\n\n        Returns:\n            A new ArithmeticStateDict with the result.\n        \"\"\"\n        if isinstance(scalar, (int, float, Number)):\n            result_dict = state_dict_mul(self, scalar)\n            return ArithmeticStateDict(result_dict)\n        elif isinstance(scalar, (dict, OrderedDict)):\n            # Hadamard product for dict-like objects\n            result_dict = state_dict_hadamard_product(self, scalar)\n            return ArithmeticStateDict(result_dict)\n        else:\n            raise TypeError(\n                f\"Cannot multiply ArithmeticStateDict with {type(scalar).__name__}\"\n            )\n\n    def __rmul__(\n        self, scalar: Union[Number, \"ArithmeticStateDict\", StateDictType]\n    ) -&gt; \"ArithmeticStateDict\":\n        \"\"\"Right multiplication (scalar * self).\"\"\"\n        return self.__mul__(scalar)\n\n    def __truediv__(self, scalar: Number) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Scalar division.\n\n        Args:\n            scalar: A scalar value to divide by.\n\n        Returns:\n            A new ArithmeticStateDict with each element divided by scalar.\n\n        Raises:\n            ZeroDivisionError: If scalar is zero.\n            TypeError: If scalar is not a number.\n        \"\"\"\n        if not isinstance(scalar, (int, float, Number)):\n            raise TypeError(\n                f\"Cannot divide ArithmeticStateDict by {type(scalar).__name__}\"\n            )\n\n        result_dict = state_dict_div(self, scalar)\n        return ArithmeticStateDict(result_dict)\n\n    def __pow__(self, exponent: Number) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Element-wise power operation.\n\n        Args:\n            exponent: The exponent to raise each element to.\n\n        Returns:\n            A new ArithmeticStateDict with each element raised to the power.\n        \"\"\"\n        if not isinstance(exponent, (int, float, Number)):\n            raise TypeError(\n                f\"Cannot raise ArithmeticStateDict to power of {type(exponent).__name__}\"\n            )\n\n        result_dict = state_dict_power(self, exponent)\n        return ArithmeticStateDict(result_dict)\n\n    def __matmul__(\n        self, other: Union[\"ArithmeticStateDict\", StateDictType]\n    ) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Hadamard product (element-wise multiplication) using @ operator.\n\n        Args:\n            other: Another state dict for element-wise multiplication.\n\n        Returns:\n            A new ArithmeticStateDict with the Hadamard product.\n        \"\"\"\n        if not isinstance(other, (dict, OrderedDict)):\n            raise TypeError(\n                f\"Cannot compute Hadamard product with {type(other).__name__}\"\n            )\n\n        result_dict = state_dict_hadamard_product(self, other)\n        return ArithmeticStateDict(result_dict)\n\n    def __rmatmul__(\n        self, other: Union[\"ArithmeticStateDict\", StateDictType]\n    ) -&gt; \"ArithmeticStateDict\":\n        \"\"\"Right matrix multiplication (other @ self).\"\"\"\n        return self.__matmul__(other)\n\n    def __iadd__(\n        self, other: Union[\"ArithmeticStateDict\", StateDictType, Number]\n    ) -&gt; \"ArithmeticStateDict\":\n        \"\"\"In-place addition.\"\"\"\n        if isinstance(other, (int, float, Number)):\n            # Scalar addition\n            for key in self:\n                self[key] = self[key] + other\n        elif isinstance(other, (dict, OrderedDict)):\n            # State dict addition\n            for key in self:\n                if key in other:\n                    self[key] = self[key] + other[key]\n        else:\n            raise TypeError(f\"Cannot add {type(other).__name__} to ArithmeticStateDict\")\n        return self\n\n    def __isub__(\n        self, other: Union[\"ArithmeticStateDict\", StateDictType, Number]\n    ) -&gt; \"ArithmeticStateDict\":\n        \"\"\"In-place subtraction.\"\"\"\n        if isinstance(other, (int, float, Number)):\n            # Scalar subtraction\n            for key in self:\n                self[key] = self[key] - other\n        elif isinstance(other, (dict, OrderedDict)):\n            # State dict subtraction\n            for key in self:\n                if key in other:\n                    self[key] = self[key] - other[key]\n        else:\n            raise TypeError(\n                f\"Cannot subtract {type(other).__name__} from ArithmeticStateDict\"\n            )\n        return self\n\n    def __imul__(\n        self, scalar: Union[Number, \"ArithmeticStateDict\", StateDictType]\n    ) -&gt; \"ArithmeticStateDict\":\n        \"\"\"In-place multiplication.\"\"\"\n        if isinstance(scalar, (int, float, Number)):\n            for key in self:\n                self[key] = self[key] * scalar\n        elif isinstance(scalar, (dict, OrderedDict)):\n            for key in self:\n                if key in scalar:\n                    self[key] = self[key] * scalar[key]\n        else:\n            raise TypeError(\n                f\"Cannot multiply ArithmeticStateDict with {type(scalar).__name__}\"\n            )\n        return self\n\n    def __itruediv__(self, scalar: Number) -&gt; \"ArithmeticStateDict\":\n        \"\"\"In-place division.\"\"\"\n        if not isinstance(scalar, (int, float, Number)):\n            raise TypeError(\n                f\"Cannot divide ArithmeticStateDict by {type(scalar).__name__}\"\n            )\n        if scalar == 0:\n            raise ZeroDivisionError(\"Cannot divide by zero\")\n\n        for key in self:\n            self[key] = self[key] / scalar\n        return self\n\n    def __ipow__(self, exponent: Number) -&gt; \"ArithmeticStateDict\":\n        \"\"\"In-place power operation.\"\"\"\n        if not isinstance(exponent, (int, float, Number)):\n            raise TypeError(\n                f\"Cannot raise ArithmeticStateDict to power of {type(exponent).__name__}\"\n            )\n\n        for key in self:\n            self[key] = self[key] ** exponent\n        return self\n\n    def abs(self) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Element-wise absolute value.\n\n        Returns:\n            A new ArithmeticStateDict with absolute values.\n        \"\"\"\n        result = ArithmeticStateDict()\n        for key, tensor in self.items():\n            result[key] = torch.abs(tensor)\n        return result\n\n    def sqrt(self) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Element-wise square root.\n\n        Returns:\n            A new ArithmeticStateDict with square roots.\n        \"\"\"\n        result = ArithmeticStateDict()\n        for key, tensor in self.items():\n            result[key] = torch.sqrt(tensor)\n        return result\n\n    def sum(self) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Sum with other ArithmeticStateDicts using the + operator.\n\n        Args:\n            *others: Other ArithmeticStateDicts to sum with.\n\n        Returns:\n            A new ArithmeticStateDict with the sum.\n        \"\"\"\n        # This is used for when sum() is called on a list of ArithmeticStateDicts\n        return self\n\n    def to_device(\n        self,\n        device: Union[torch.device, str],\n        copy: bool = False,\n        inplace: bool = False,\n    ) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Move all tensors to the specified device.\n\n        Args:\n            device: Target device.\n            copy: Whether to force a copy.\n            inplace: Whether to modify in place.\n\n        Returns:\n            ArithmeticStateDict with tensors on the target device.\n        \"\"\"\n        if inplace:\n            for key, tensor in self.items():\n                self[key] = tensor.to(device, non_blocking=True, copy=copy)\n            return self\n        else:\n            result = ArithmeticStateDict()\n            for key, tensor in self.items():\n                result[key] = tensor.to(device, non_blocking=True, copy=copy)\n            return result\n\n    def clone(self) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Create a deep copy with cloned tensors.\n\n        Returns:\n            A new ArithmeticStateDict with cloned tensors.\n        \"\"\"\n        result = ArithmeticStateDict()\n        for key, tensor in self.items():\n            result[key] = tensor.clone()\n        return result\n\n    def detach(self) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Detach all tensors from the computation graph.\n\n        Returns:\n            A new ArithmeticStateDict with detached tensors.\n        \"\"\"\n        result = ArithmeticStateDict()\n        for key, tensor in self.items():\n            result[key] = tensor.detach()\n        return result\n\n    def num_params(self) -&gt; int:\n        \"\"\"\n        Calculate the total number of parameters.\n\n        Returns:\n            Total number of parameters in all tensors.\n        \"\"\"\n        return sum(tensor.numel() for tensor in self.values())\n\n    @classmethod\n    def from_state_dict(cls, state_dict: StateDictType) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Create an ArithmeticStateDict from a regular state dict.\n\n        Args:\n            state_dict: A regular state dictionary.\n\n        Returns:\n            A new ArithmeticStateDict with the same data.\n        \"\"\"\n        return cls(state_dict)\n\n    @classmethod\n    def weighted_sum(\n        cls,\n        state_dicts: List[Union[\"ArithmeticStateDict\", StateDictType]],\n        weights: List[float],\n    ) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Compute a weighted sum of multiple state dicts.\n\n        Args:\n            state_dicts: List of state dicts to combine.\n            weights: List of weights for the combination.\n\n        Returns:\n            A new ArithmeticStateDict with the weighted sum.\n        \"\"\"\n        result_dict = state_dict_weighted_sum(state_dicts, weights)\n        return cls(result_dict)\n\n    @classmethod\n    def average(\n        cls, state_dicts: List[Union[\"ArithmeticStateDict\", StateDictType]]\n    ) -&gt; \"ArithmeticStateDict\":\n        \"\"\"\n        Compute the average of multiple state dicts.\n\n        Args:\n            state_dicts: List of state dicts to average.\n\n        Returns:\n            A new ArithmeticStateDict with the average.\n        \"\"\"\n        result_dict = state_dict_avg(state_dicts)\n        return cls(result_dict)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__add__","title":"<code>__add__(other)</code>","text":"<p>Element-wise addition with another state dict or scalar.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[ArithmeticStateDict, StateDictType, Number]</code>)           \u2013            <p>Another state dict to add or a scalar to add to all elements.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>A new ArithmeticStateDict with the element-wise sum.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __add__(\n    self, other: Union[\"ArithmeticStateDict\", StateDictType, Number]\n) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Element-wise addition with another state dict or scalar.\n\n    Args:\n        other: Another state dict to add or a scalar to add to all elements.\n\n    Returns:\n        A new ArithmeticStateDict with the element-wise sum.\n    \"\"\"\n    if isinstance(other, (int, float, Number)):\n        # Scalar addition\n        result_dict = state_dict_add_scalar(self, other)\n        return ArithmeticStateDict(result_dict)\n    elif isinstance(other, (dict, OrderedDict)):\n        # State dict addition\n        result_dict = state_dict_add(self, other, strict=True)\n        return ArithmeticStateDict(result_dict)\n    else:\n        raise TypeError(\n            f\"Cannot add ArithmeticStateDict with {type(other).__name__}\"\n        )\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__iadd__","title":"<code>__iadd__(other)</code>","text":"<p>In-place addition.</p> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __iadd__(\n    self, other: Union[\"ArithmeticStateDict\", StateDictType, Number]\n) -&gt; \"ArithmeticStateDict\":\n    \"\"\"In-place addition.\"\"\"\n    if isinstance(other, (int, float, Number)):\n        # Scalar addition\n        for key in self:\n            self[key] = self[key] + other\n    elif isinstance(other, (dict, OrderedDict)):\n        # State dict addition\n        for key in self:\n            if key in other:\n                self[key] = self[key] + other[key]\n    else:\n        raise TypeError(f\"Cannot add {type(other).__name__} to ArithmeticStateDict\")\n    return self\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__imul__","title":"<code>__imul__(scalar)</code>","text":"<p>In-place multiplication.</p> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __imul__(\n    self, scalar: Union[Number, \"ArithmeticStateDict\", StateDictType]\n) -&gt; \"ArithmeticStateDict\":\n    \"\"\"In-place multiplication.\"\"\"\n    if isinstance(scalar, (int, float, Number)):\n        for key in self:\n            self[key] = self[key] * scalar\n    elif isinstance(scalar, (dict, OrderedDict)):\n        for key in self:\n            if key in scalar:\n                self[key] = self[key] * scalar[key]\n    else:\n        raise TypeError(\n            f\"Cannot multiply ArithmeticStateDict with {type(scalar).__name__}\"\n        )\n    return self\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize ArithmeticStateDict with the same interface as OrderedDict.</p> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initialize ArithmeticStateDict with the same interface as OrderedDict.\"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__ipow__","title":"<code>__ipow__(exponent)</code>","text":"<p>In-place power operation.</p> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __ipow__(self, exponent: Number) -&gt; \"ArithmeticStateDict\":\n    \"\"\"In-place power operation.\"\"\"\n    if not isinstance(exponent, (int, float, Number)):\n        raise TypeError(\n            f\"Cannot raise ArithmeticStateDict to power of {type(exponent).__name__}\"\n        )\n\n    for key in self:\n        self[key] = self[key] ** exponent\n    return self\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__isub__","title":"<code>__isub__(other)</code>","text":"<p>In-place subtraction.</p> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __isub__(\n    self, other: Union[\"ArithmeticStateDict\", StateDictType, Number]\n) -&gt; \"ArithmeticStateDict\":\n    \"\"\"In-place subtraction.\"\"\"\n    if isinstance(other, (int, float, Number)):\n        # Scalar subtraction\n        for key in self:\n            self[key] = self[key] - other\n    elif isinstance(other, (dict, OrderedDict)):\n        # State dict subtraction\n        for key in self:\n            if key in other:\n                self[key] = self[key] - other[key]\n    else:\n        raise TypeError(\n            f\"Cannot subtract {type(other).__name__} from ArithmeticStateDict\"\n        )\n    return self\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__itruediv__","title":"<code>__itruediv__(scalar)</code>","text":"<p>In-place division.</p> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __itruediv__(self, scalar: Number) -&gt; \"ArithmeticStateDict\":\n    \"\"\"In-place division.\"\"\"\n    if not isinstance(scalar, (int, float, Number)):\n        raise TypeError(\n            f\"Cannot divide ArithmeticStateDict by {type(scalar).__name__}\"\n        )\n    if scalar == 0:\n        raise ZeroDivisionError(\"Cannot divide by zero\")\n\n    for key in self:\n        self[key] = self[key] / scalar\n    return self\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__matmul__","title":"<code>__matmul__(other)</code>","text":"<p>Hadamard product (element-wise multiplication) using @ operator.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[ArithmeticStateDict, StateDictType]</code>)           \u2013            <p>Another state dict for element-wise multiplication.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>A new ArithmeticStateDict with the Hadamard product.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __matmul__(\n    self, other: Union[\"ArithmeticStateDict\", StateDictType]\n) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Hadamard product (element-wise multiplication) using @ operator.\n\n    Args:\n        other: Another state dict for element-wise multiplication.\n\n    Returns:\n        A new ArithmeticStateDict with the Hadamard product.\n    \"\"\"\n    if not isinstance(other, (dict, OrderedDict)):\n        raise TypeError(\n            f\"Cannot compute Hadamard product with {type(other).__name__}\"\n        )\n\n    result_dict = state_dict_hadamard_product(self, other)\n    return ArithmeticStateDict(result_dict)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__mul__","title":"<code>__mul__(scalar)</code>","text":"<p>Scalar multiplication or Hadamard product.</p> <p>Parameters:</p> <ul> <li> <code>scalar</code>               (<code>Union[Number, ArithmeticStateDict, StateDictType]</code>)           \u2013            <p>A scalar value for element-wise multiplication, or another state dict    for Hadamard product.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>A new ArithmeticStateDict with the result.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __mul__(\n    self, scalar: Union[Number, \"ArithmeticStateDict\", StateDictType]\n) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Scalar multiplication or Hadamard product.\n\n    Args:\n        scalar: A scalar value for element-wise multiplication, or another state dict\n               for Hadamard product.\n\n    Returns:\n        A new ArithmeticStateDict with the result.\n    \"\"\"\n    if isinstance(scalar, (int, float, Number)):\n        result_dict = state_dict_mul(self, scalar)\n        return ArithmeticStateDict(result_dict)\n    elif isinstance(scalar, (dict, OrderedDict)):\n        # Hadamard product for dict-like objects\n        result_dict = state_dict_hadamard_product(self, scalar)\n        return ArithmeticStateDict(result_dict)\n    else:\n        raise TypeError(\n            f\"Cannot multiply ArithmeticStateDict with {type(scalar).__name__}\"\n        )\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__pow__","title":"<code>__pow__(exponent)</code>","text":"<p>Element-wise power operation.</p> <p>Parameters:</p> <ul> <li> <code>exponent</code>               (<code>Number</code>)           \u2013            <p>The exponent to raise each element to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>A new ArithmeticStateDict with each element raised to the power.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __pow__(self, exponent: Number) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Element-wise power operation.\n\n    Args:\n        exponent: The exponent to raise each element to.\n\n    Returns:\n        A new ArithmeticStateDict with each element raised to the power.\n    \"\"\"\n    if not isinstance(exponent, (int, float, Number)):\n        raise TypeError(\n            f\"Cannot raise ArithmeticStateDict to power of {type(exponent).__name__}\"\n        )\n\n    result_dict = state_dict_power(self, exponent)\n    return ArithmeticStateDict(result_dict)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__radd__","title":"<code>__radd__(other)</code>","text":"<p>Right addition (other + self). Handles the case where sum() starts with 0 and scalar addition.</p> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __radd__(\n    self, other: Union[\"ArithmeticStateDict\", StateDictType, Number]\n) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Right addition (other + self).\n    Handles the case where sum() starts with 0 and scalar addition.\n    \"\"\"\n    if other == 0:  # sum() starts with 0 by default\n        return self\n    elif isinstance(other, (int, float, Number)):\n        # Scalar addition is commutative\n        return self.__add__(other)\n    elif isinstance(other, (dict, OrderedDict)):\n        return self.__add__(other)\n    else:\n        raise TypeError(\n            f\"Cannot add {type(other).__name__} with ArithmeticStateDict\"\n        )\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__rmatmul__","title":"<code>__rmatmul__(other)</code>","text":"<p>Right matrix multiplication (other @ self).</p> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __rmatmul__(\n    self, other: Union[\"ArithmeticStateDict\", StateDictType]\n) -&gt; \"ArithmeticStateDict\":\n    \"\"\"Right matrix multiplication (other @ self).\"\"\"\n    return self.__matmul__(other)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__rmul__","title":"<code>__rmul__(scalar)</code>","text":"<p>Right multiplication (scalar * self).</p> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __rmul__(\n    self, scalar: Union[Number, \"ArithmeticStateDict\", StateDictType]\n) -&gt; \"ArithmeticStateDict\":\n    \"\"\"Right multiplication (scalar * self).\"\"\"\n    return self.__mul__(scalar)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__rsub__","title":"<code>__rsub__(other)</code>","text":"<p>Right subtraction (other - self).</p> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __rsub__(\n    self, other: Union[\"ArithmeticStateDict\", StateDictType, Number]\n) -&gt; \"ArithmeticStateDict\":\n    \"\"\"Right subtraction (other - self).\"\"\"\n    if isinstance(other, (int, float, Number)):\n        # Scalar - ArithmeticStateDict: subtract each element from scalar\n        result = ArithmeticStateDict()\n        for key, tensor in self.items():\n            result[key] = other - tensor\n        return result\n    elif isinstance(other, (dict, OrderedDict)):\n        result_dict = state_dict_sub(other, self, strict=True)\n        return ArithmeticStateDict(result_dict)\n    else:\n        raise TypeError(\n            f\"Cannot subtract ArithmeticStateDict from {type(other).__name__}\"\n        )\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__sub__","title":"<code>__sub__(other)</code>","text":"<p>Element-wise subtraction with another state dict or scalar.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[ArithmeticStateDict, StateDictType, Number]</code>)           \u2013            <p>Another state dict to subtract or a scalar to subtract from all elements.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>A new ArithmeticStateDict with the element-wise difference.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __sub__(\n    self, other: Union[\"ArithmeticStateDict\", StateDictType, Number]\n) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Element-wise subtraction with another state dict or scalar.\n\n    Args:\n        other: Another state dict to subtract or a scalar to subtract from all elements.\n\n    Returns:\n        A new ArithmeticStateDict with the element-wise difference.\n    \"\"\"\n    if isinstance(other, (int, float, Number)):\n        # Scalar subtraction: subtract scalar from all elements\n        result_dict = state_dict_add_scalar(self, -other)\n        return ArithmeticStateDict(result_dict)\n    elif isinstance(other, (dict, OrderedDict)):\n        # State dict subtraction\n        result_dict = state_dict_sub(self, other, strict=True)\n        return ArithmeticStateDict(result_dict)\n    else:\n        raise TypeError(\n            f\"Cannot subtract {type(other).__name__} from ArithmeticStateDict\"\n        )\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.__truediv__","title":"<code>__truediv__(scalar)</code>","text":"<p>Scalar division.</p> <p>Parameters:</p> <ul> <li> <code>scalar</code>               (<code>Number</code>)           \u2013            <p>A scalar value to divide by.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>A new ArithmeticStateDict with each element divided by scalar.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ZeroDivisionError</code>             \u2013            <p>If scalar is zero.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If scalar is not a number.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def __truediv__(self, scalar: Number) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Scalar division.\n\n    Args:\n        scalar: A scalar value to divide by.\n\n    Returns:\n        A new ArithmeticStateDict with each element divided by scalar.\n\n    Raises:\n        ZeroDivisionError: If scalar is zero.\n        TypeError: If scalar is not a number.\n    \"\"\"\n    if not isinstance(scalar, (int, float, Number)):\n        raise TypeError(\n            f\"Cannot divide ArithmeticStateDict by {type(scalar).__name__}\"\n        )\n\n    result_dict = state_dict_div(self, scalar)\n    return ArithmeticStateDict(result_dict)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.abs","title":"<code>abs()</code>","text":"<p>Element-wise absolute value.</p> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>A new ArithmeticStateDict with absolute values.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def abs(self) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Element-wise absolute value.\n\n    Returns:\n        A new ArithmeticStateDict with absolute values.\n    \"\"\"\n    result = ArithmeticStateDict()\n    for key, tensor in self.items():\n        result[key] = torch.abs(tensor)\n    return result\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.average","title":"<code>average(state_dicts)</code>  <code>classmethod</code>","text":"<p>Compute the average of multiple state dicts.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[Union[ArithmeticStateDict, StateDictType]]</code>)           \u2013            <p>List of state dicts to average.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>A new ArithmeticStateDict with the average.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>@classmethod\ndef average(\n    cls, state_dicts: List[Union[\"ArithmeticStateDict\", StateDictType]]\n) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Compute the average of multiple state dicts.\n\n    Args:\n        state_dicts: List of state dicts to average.\n\n    Returns:\n        A new ArithmeticStateDict with the average.\n    \"\"\"\n    result_dict = state_dict_avg(state_dicts)\n    return cls(result_dict)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.clone","title":"<code>clone()</code>","text":"<p>Create a deep copy with cloned tensors.</p> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>A new ArithmeticStateDict with cloned tensors.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def clone(self) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Create a deep copy with cloned tensors.\n\n    Returns:\n        A new ArithmeticStateDict with cloned tensors.\n    \"\"\"\n    result = ArithmeticStateDict()\n    for key, tensor in self.items():\n        result[key] = tensor.clone()\n    return result\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.detach","title":"<code>detach()</code>","text":"<p>Detach all tensors from the computation graph.</p> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>A new ArithmeticStateDict with detached tensors.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def detach(self) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Detach all tensors from the computation graph.\n\n    Returns:\n        A new ArithmeticStateDict with detached tensors.\n    \"\"\"\n    result = ArithmeticStateDict()\n    for key, tensor in self.items():\n        result[key] = tensor.detach()\n    return result\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.from_state_dict","title":"<code>from_state_dict(state_dict)</code>  <code>classmethod</code>","text":"<p>Create an ArithmeticStateDict from a regular state dict.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>StateDictType</code>)           \u2013            <p>A regular state dictionary.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>A new ArithmeticStateDict with the same data.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>@classmethod\ndef from_state_dict(cls, state_dict: StateDictType) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Create an ArithmeticStateDict from a regular state dict.\n\n    Args:\n        state_dict: A regular state dictionary.\n\n    Returns:\n        A new ArithmeticStateDict with the same data.\n    \"\"\"\n    return cls(state_dict)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.num_params","title":"<code>num_params()</code>","text":"<p>Calculate the total number of parameters.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Total number of parameters in all tensors.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def num_params(self) -&gt; int:\n    \"\"\"\n    Calculate the total number of parameters.\n\n    Returns:\n        Total number of parameters in all tensors.\n    \"\"\"\n    return sum(tensor.numel() for tensor in self.values())\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.sqrt","title":"<code>sqrt()</code>","text":"<p>Element-wise square root.</p> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>A new ArithmeticStateDict with square roots.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def sqrt(self) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Element-wise square root.\n\n    Returns:\n        A new ArithmeticStateDict with square roots.\n    \"\"\"\n    result = ArithmeticStateDict()\n    for key, tensor in self.items():\n        result[key] = torch.sqrt(tensor)\n    return result\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.sum","title":"<code>sum()</code>","text":"<p>Sum with other ArithmeticStateDicts using the + operator.</p> <p>Parameters:</p> <ul> <li> <code>*others</code>           \u2013            <p>Other ArithmeticStateDicts to sum with.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>A new ArithmeticStateDict with the sum.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def sum(self) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Sum with other ArithmeticStateDicts using the + operator.\n\n    Args:\n        *others: Other ArithmeticStateDicts to sum with.\n\n    Returns:\n        A new ArithmeticStateDict with the sum.\n    \"\"\"\n    # This is used for when sum() is called on a list of ArithmeticStateDicts\n    return self\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.to_device","title":"<code>to_device(device, copy=False, inplace=False)</code>","text":"<p>Move all tensors to the specified device.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>Union[device, str]</code>)           \u2013            <p>Target device.</p> </li> <li> <code>copy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force a copy.</p> </li> <li> <code>inplace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to modify in place.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>ArithmeticStateDict with tensors on the target device.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def to_device(\n    self,\n    device: Union[torch.device, str],\n    copy: bool = False,\n    inplace: bool = False,\n) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Move all tensors to the specified device.\n\n    Args:\n        device: Target device.\n        copy: Whether to force a copy.\n        inplace: Whether to modify in place.\n\n    Returns:\n        ArithmeticStateDict with tensors on the target device.\n    \"\"\"\n    if inplace:\n        for key, tensor in self.items():\n            self[key] = tensor.to(device, non_blocking=True, copy=copy)\n        return self\n    else:\n        result = ArithmeticStateDict()\n        for key, tensor in self.items():\n            result[key] = tensor.to(device, non_blocking=True, copy=copy)\n        return result\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.ArithmeticStateDict.weighted_sum","title":"<code>weighted_sum(state_dicts, weights)</code>  <code>classmethod</code>","text":"<p>Compute a weighted sum of multiple state dicts.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[Union[ArithmeticStateDict, StateDictType]]</code>)           \u2013            <p>List of state dicts to combine.</p> </li> <li> <code>weights</code>               (<code>List[float]</code>)           \u2013            <p>List of weights for the combination.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArithmeticStateDict</code>           \u2013            <p>A new ArithmeticStateDict with the weighted sum.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>@classmethod\ndef weighted_sum(\n    cls,\n    state_dicts: List[Union[\"ArithmeticStateDict\", StateDictType]],\n    weights: List[float],\n) -&gt; \"ArithmeticStateDict\":\n    \"\"\"\n    Compute a weighted sum of multiple state dicts.\n\n    Args:\n        state_dicts: List of state dicts to combine.\n        weights: List of weights for the combination.\n\n    Returns:\n        A new ArithmeticStateDict with the weighted sum.\n    \"\"\"\n    result_dict = state_dict_weighted_sum(state_dicts, weights)\n    return cls(result_dict)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.num_params_of_state_dict","title":"<code>num_params_of_state_dict(state_dict)</code>","text":"<p>Calculate the total number of parameters in a state dict.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>StateDictType</code>)           \u2013            <p>The state dict to count parameters in.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The total number of parameters in the state dict.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def num_params_of_state_dict(state_dict: StateDictType) -&gt; int:\n    \"\"\"\n    Calculate the total number of parameters in a state dict.\n\n    Args:\n        state_dict: The state dict to count parameters in.\n\n    Returns:\n        The total number of parameters in the state dict.\n    \"\"\"\n    return sum(tensor.numel() for tensor in state_dict.values())\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_add","title":"<code>state_dict_add(a, b, strict=True, device=None, show_pbar=False)</code>","text":"<p>Compute the element-wise sum of two state dicts.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>StateDictType</code>)           \u2013            <p>The first state dict.</p> </li> <li> <code>b</code>               (<code>StateDictType</code>)           \u2013            <p>The second state dict.</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to require exact key matching between state dicts.</p> </li> <li> <code>device</code>               (<code>Optional[Union[device, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional device to move the result tensors to.</p> </li> <li> <code>show_pbar</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to show a progress bar during computation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>A state dict containing the element-wise sums.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If strict=True and the state dicts have different parameters.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_add(\n    a: StateDictType,\n    b: StateDictType,\n    strict: bool = True,\n    device: Optional[Union[torch.device, str]] = None,\n    show_pbar: bool = False,\n) -&gt; StateDictType:\n    \"\"\"\n    Compute the element-wise sum of two state dicts.\n\n    Args:\n        a: The first state dict.\n        b: The second state dict.\n        strict: Whether to require exact key matching between state dicts.\n        device: Optional device to move the result tensors to.\n        show_pbar: Whether to show a progress bar during computation.\n\n    Returns:\n        A state dict containing the element-wise sums.\n\n    Raises:\n        ValueError: If strict=True and the state dicts have different parameters.\n    \"\"\"\n    result = OrderedDict()\n\n    if strict:\n        _validate_state_dict_same_keys([a, b])\n        keys_to_process = a.keys()\n    else:\n        keys_to_process = set(a.keys()) &amp; set(b.keys())\n\n    keys_iter = (\n        tqdm(keys_to_process, desc=\"Adding state dicts\")\n        if show_pbar\n        else keys_to_process\n    )\n\n    for key in keys_iter:\n        if key in b:  # This check is redundant when strict=True but harmless\n            result[key] = a[key] + b[key]\n\n    if device is not None:\n        result = state_dict_to_device(result, device)\n\n    return result\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_add_scalar","title":"<code>state_dict_add_scalar(state_dict, scalar)</code>","text":"<p>Add a scalar value to all parameters in a state dict.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>StateDictType</code>)           \u2013            <p>The state dict to modify.</p> </li> <li> <code>scalar</code>               (<code>Number</code>)           \u2013            <p>The scalar value to add to each parameter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>A new state dict with the scalar added to each parameter.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_add_scalar(state_dict: StateDictType, scalar: Number) -&gt; StateDictType:\n    \"\"\"\n    Add a scalar value to all parameters in a state dict.\n\n    Args:\n        state_dict: The state dict to modify.\n        scalar: The scalar value to add to each parameter.\n\n    Returns:\n        A new state dict with the scalar added to each parameter.\n    \"\"\"\n    return OrderedDict((key, tensor + scalar) for key, tensor in state_dict.items())\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_avg","title":"<code>state_dict_avg(state_dicts)</code>","text":"<p>Calculate the element-wise average of a list of state dicts.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[StateDictType]</code>)           \u2013            <p>List of state dicts to average.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>A state dict containing the averaged parameters.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the list is empty or state dicts have different keys.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_avg(state_dicts: List[StateDictType]) -&gt; StateDictType:\n    \"\"\"\n    Calculate the element-wise average of a list of state dicts.\n\n    Args:\n        state_dicts: List of state dicts to average.\n\n    Returns:\n        A state dict containing the averaged parameters.\n\n    Raises:\n        ValueError: If the list is empty or state dicts have different keys.\n    \"\"\"\n    _validate_state_dict_list_not_empty(state_dicts)\n    _validate_state_dict_same_keys(state_dicts)\n\n    num_state_dicts = len(state_dicts)\n    avg_state_dict = OrderedDict()\n\n    # Initialize with zeros_like for better performance\n    for key in state_dicts[0]:\n        avg_state_dict[key] = torch.zeros_like(state_dicts[0][key])\n\n    # Accumulate all state dicts\n    for state_dict in state_dicts:\n        for key in avg_state_dict:\n            avg_state_dict[key] += state_dict[key]\n\n    # Divide by number of state dicts\n    for key in avg_state_dict:\n        avg_state_dict[key] /= num_state_dicts\n\n    return avg_state_dict\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_binary_mask","title":"<code>state_dict_binary_mask(a, b, compare_fn='greater', strict=True, show_pbar=False)</code>","text":"<p>Create binary masks by comparing elements in two state dicts.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>StateDictType</code>)           \u2013            <p>The first state dict.</p> </li> <li> <code>b</code>               (<code>StateDictType</code>)           \u2013            <p>The second state dict.</p> </li> <li> <code>compare_fn</code>               (<code>Union[Literal['greater', 'less', 'equal', 'not_equal'], Callable[[Tensor, Tensor], BoolTensor]]</code>, default:                   <code>'greater'</code> )           \u2013            <p>Comparison function to use. Can be a string literal        (\"greater\", \"less\", \"equal\", \"not_equal\") or a callable        that takes two tensors and returns a boolean tensor.</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to require exact key matching between state dicts.</p> </li> <li> <code>show_pbar</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to show a progress bar during computation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BoolStateDictType</code>           \u2013            <p>A dictionary containing boolean masks based on the comparison.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If compare_fn is not a valid string or callable, or if strict=True        and the state dicts have different keys or incompatible tensor shapes.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If tensors have incompatible types.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_binary_mask(\n    a: StateDictType,\n    b: StateDictType,\n    compare_fn: Union[\n        Literal[\"greater\", \"less\", \"equal\", \"not_equal\"],\n        Callable[[Tensor, Tensor], torch.BoolTensor],\n    ] = \"greater\",\n    strict: bool = True,\n    show_pbar: bool = False,\n) -&gt; BoolStateDictType:\n    \"\"\"\n    Create binary masks by comparing elements in two state dicts.\n\n    Args:\n        a: The first state dict.\n        b: The second state dict.\n        compare_fn: Comparison function to use. Can be a string literal\n                   (\"greater\", \"less\", \"equal\", \"not_equal\") or a callable\n                   that takes two tensors and returns a boolean tensor.\n        strict: Whether to require exact key matching between state dicts.\n        show_pbar: Whether to show a progress bar during computation.\n\n    Returns:\n        A dictionary containing boolean masks based on the comparison.\n\n    Raises:\n        ValueError: If compare_fn is not a valid string or callable, or if strict=True\n                   and the state dicts have different keys or incompatible tensor shapes.\n        TypeError: If tensors have incompatible types.\n    \"\"\"\n    compare_fn_dict = {\n        \"greater\": lambda x, y: x &gt; y,\n        \"less\": lambda x, y: x &lt; y,\n        \"equal\": lambda x, y: x == y,\n        \"not_equal\": lambda x, y: x != y,\n    }\n\n    if isinstance(compare_fn, str):\n        if compare_fn not in compare_fn_dict:\n            raise ValueError(\n                f\"Invalid compare_fn string: {compare_fn}. Must be one of {list(compare_fn_dict.keys())}\"\n            )\n        compare_fn = compare_fn_dict[compare_fn]\n    elif not callable(compare_fn):\n        raise ValueError(\n            f\"compare_fn must be a string or a callable, but got {type(compare_fn)}\"\n        )\n\n    result = OrderedDict()\n\n    if strict:\n        _validate_state_dict_same_keys([a, b])\n        keys_to_process = a.keys()\n    else:\n        keys_to_process = set(a.keys()) &amp; set(b.keys())\n\n    keys_iter = (\n        tqdm(keys_to_process, desc=\"Creating binary masks\")\n        if show_pbar\n        else keys_to_process\n    )\n\n    for key in keys_iter:\n        result[key] = compare_fn(a[key], b[key])\n\n    return result\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_diff_abs","title":"<code>state_dict_diff_abs(a, b)</code>","text":"<p>Compute the element-wise absolute difference between two state dicts.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>StateDictType</code>)           \u2013            <p>The first state dict.</p> </li> <li> <code>b</code>               (<code>StateDictType</code>)           \u2013            <p>The second state dict.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>A state dict containing the absolute differences.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_diff_abs(a: StateDictType, b: StateDictType) -&gt; StateDictType:\n    \"\"\"\n    Compute the element-wise absolute difference between two state dicts.\n\n    Args:\n        a: The first state dict.\n        b: The second state dict.\n\n    Returns:\n        A state dict containing the absolute differences.\n    \"\"\"\n    diff = state_dict_sub(a, b)\n    return OrderedDict((key, tensor.abs()) for key, tensor in diff.items())\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_div","title":"<code>state_dict_div(state_dict, scalar, show_pbar=False)</code>","text":"<p>Divide all parameters in a state dict by a scalar.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>StateDictType</code>)           \u2013            <p>The state dict to divide.</p> </li> <li> <code>scalar</code>               (<code>float</code>)           \u2013            <p>The scalar value to divide each parameter by.</p> </li> <li> <code>show_pbar</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to show a progress bar during computation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>A new state dict with each parameter divided by the scalar.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ZeroDivisionError</code>             \u2013            <p>If scalar is zero.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_div(\n    state_dict: StateDictType, scalar: float, show_pbar: bool = False\n) -&gt; StateDictType:\n    \"\"\"\n    Divide all parameters in a state dict by a scalar.\n\n    Args:\n        state_dict: The state dict to divide.\n        scalar: The scalar value to divide each parameter by.\n        show_pbar: Whether to show a progress bar during computation.\n\n    Returns:\n        A new state dict with each parameter divided by the scalar.\n\n    Raises:\n        ZeroDivisionError: If scalar is zero.\n    \"\"\"\n    if scalar == 0:\n        raise ZeroDivisionError(\"Cannot divide state dict by zero\")\n\n    keys_iter = (\n        tqdm(state_dict.keys(), desc=\"Dividing state dict\")\n        if show_pbar\n        else state_dict.keys()\n    )\n    return OrderedDict((key, state_dict[key] / scalar) for key in keys_iter)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_flatten","title":"<code>state_dict_flatten(state_dict)</code>","text":"<p>Flatten all tensors in a state dict into a single 1D tensor.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>StateDictType</code>)           \u2013            <p>The state dict to flatten.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>A single flattened tensor containing all parameters.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_flatten(state_dict: StateDictType) -&gt; Tensor:\n    \"\"\"\n    Flatten all tensors in a state dict into a single 1D tensor.\n\n    Args:\n        state_dict: The state dict to flatten.\n\n    Returns:\n        A single flattened tensor containing all parameters.\n    \"\"\"\n    return torch.cat([tensor.flatten() for tensor in state_dict.values()])\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_hadamard_product","title":"<code>state_dict_hadamard_product(a, b)</code>","text":"<p>Compute the Hadamard product (element-wise multiplication) of two state dicts.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>StateDictType</code>)           \u2013            <p>The first state dict.</p> </li> <li> <code>b</code>               (<code>StateDictType</code>)           \u2013            <p>The second state dict.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>A state dict containing the element-wise products.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the state dicts have different keys or incompatible tensor shapes.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If tensors have incompatible types.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_hadamard_product(a: StateDictType, b: StateDictType) -&gt; StateDictType:\n    \"\"\"\n    Compute the Hadamard product (element-wise multiplication) of two state dicts.\n\n    Args:\n        a: The first state dict.\n        b: The second state dict.\n\n    Returns:\n        A state dict containing the element-wise products.\n\n    Raises:\n        ValueError: If the state dicts have different keys or incompatible tensor shapes.\n        TypeError: If tensors have incompatible types.\n    \"\"\"\n    _validate_state_dict_same_keys([a, b])\n    return OrderedDict((key, a[key] * b[key]) for key in a)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_interpolation","title":"<code>state_dict_interpolation(state_dicts, scalars)</code>","text":"<p>Interpolate between multiple state dicts using specified scalar weights.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[StateDictType]</code>)           \u2013            <p>List of state dicts to interpolate between.</p> </li> <li> <code>scalars</code>               (<code>List[float]</code>)           \u2013            <p>List of scalar weights for interpolation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>A state dict containing the interpolated parameters.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the lists have different lengths or are empty, or if state dicts have different keys.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_interpolation(\n    state_dicts: List[StateDictType], scalars: List[float]\n) -&gt; StateDictType:\n    \"\"\"\n    Interpolate between multiple state dicts using specified scalar weights.\n\n    Args:\n        state_dicts: List of state dicts to interpolate between.\n        scalars: List of scalar weights for interpolation.\n\n    Returns:\n        A state dict containing the interpolated parameters.\n\n    Raises:\n        ValueError: If the lists have different lengths or are empty, or if state dicts have different keys.\n    \"\"\"\n    _validate_state_dict_list_not_empty(state_dicts)\n    _validate_list_lengths_equal(state_dicts, scalars, \"state_dicts\", \"scalars\")\n    _validate_state_dict_same_keys(state_dicts)\n\n    interpolated_state_dict = OrderedDict()\n\n    # Initialize with zeros\n    for key in state_dicts[0]:\n        interpolated_state_dict[key] = torch.zeros_like(state_dicts[0][key])\n\n    # Accumulate weighted contributions\n    for state_dict, scalar in zip(state_dicts, scalars):\n        for key in interpolated_state_dict:\n            interpolated_state_dict[key] += scalar * state_dict[key]\n\n    return interpolated_state_dict\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_mul","title":"<code>state_dict_mul(state_dict, scalar)</code>","text":"<p>Multiply all parameters in a state dict by a scalar.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>StateDictType</code>)           \u2013            <p>The state dict to multiply.</p> </li> <li> <code>scalar</code>               (<code>float</code>)           \u2013            <p>The scalar value to multiply each parameter by.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>A new state dict with each parameter multiplied by the scalar.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_mul(state_dict: StateDictType, scalar: float) -&gt; StateDictType:\n    \"\"\"\n    Multiply all parameters in a state dict by a scalar.\n\n    Args:\n        state_dict: The state dict to multiply.\n        scalar: The scalar value to multiply each parameter by.\n\n    Returns:\n        A new state dict with each parameter multiplied by the scalar.\n    \"\"\"\n    return OrderedDict((key, scalar * tensor) for key, tensor in state_dict.items())\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_power","title":"<code>state_dict_power(state_dict, p)</code>","text":"<p>Raise all parameters in a state dict to a power.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>StateDictType</code>)           \u2013            <p>The state dict to raise to a power.</p> </li> <li> <code>p</code>               (<code>float</code>)           \u2013            <p>The exponent to raise each parameter to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>A new state dict with each parameter raised to the power p.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_power(state_dict: StateDictType, p: float) -&gt; StateDictType:\n    \"\"\"\n    Raise all parameters in a state dict to a power.\n\n    Args:\n        state_dict: The state dict to raise to a power.\n        p: The exponent to raise each parameter to.\n\n    Returns:\n        A new state dict with each parameter raised to the power p.\n    \"\"\"\n    return OrderedDict((key, tensor**p) for key, tensor in state_dict.items())\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_sub","title":"<code>state_dict_sub(a, b, strict=True, device=None)</code>","text":"<p>Compute the element-wise difference between two state dicts (a - b).</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>StateDictType</code>)           \u2013            <p>The first state dict (minuend).</p> </li> <li> <code>b</code>               (<code>StateDictType</code>)           \u2013            <p>The second state dict (subtrahend).</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to require exact key matching between state dicts.</p> </li> <li> <code>device</code>               (<code>Optional[Union[device, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional device to move the result tensors to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>A state dict containing the element-wise differences.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If strict=True and the state dicts have different keys or incompatible tensor shapes.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If tensors have incompatible types.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_sub(\n    a: StateDictType,\n    b: StateDictType,\n    strict: bool = True,\n    device: Optional[Union[torch.device, str]] = None,\n) -&gt; StateDictType:\n    \"\"\"\n    Compute the element-wise difference between two state dicts (a - b).\n\n    Args:\n        a: The first state dict (minuend).\n        b: The second state dict (subtrahend).\n        strict: Whether to require exact key matching between state dicts.\n        device: Optional device to move the result tensors to.\n\n    Returns:\n        A state dict containing the element-wise differences.\n\n    Raises:\n        ValueError: If strict=True and the state dicts have different keys or incompatible tensor shapes.\n        TypeError: If tensors have incompatible types.\n    \"\"\"\n    result = OrderedDict()\n\n    if strict:\n        _validate_state_dict_same_keys([a, b])\n        keys_to_process = a.keys()\n    else:\n        keys_to_process = set(a.keys()) &amp; set(b.keys())\n\n    for key in keys_to_process:\n        result_tensor = a[key] - b[key]\n        if device is not None:\n            result_tensor = result_tensor.to(device, non_blocking=True)\n        result[key] = result_tensor\n\n    return result\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_sum","title":"<code>state_dict_sum(state_dicts)</code>","text":"<p>Compute the element-wise sum of multiple state dicts.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[StateDictType]</code>)           \u2013            <p>List of state dicts to sum.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>A state dict containing the element-wise sums.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the list is empty or state dicts have different keys.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_sum(state_dicts: List[StateDictType]) -&gt; StateDictType:\n    \"\"\"\n    Compute the element-wise sum of multiple state dicts.\n\n    Args:\n        state_dicts: List of state dicts to sum.\n\n    Returns:\n        A state dict containing the element-wise sums.\n\n    Raises:\n        ValueError: If the list is empty or state dicts have different keys.\n    \"\"\"\n    _validate_state_dict_list_not_empty(state_dicts)\n    _validate_state_dict_same_keys(state_dicts)\n\n    sum_state_dict = OrderedDict()\n\n    # Initialize with zeros\n    for key in state_dicts[0]:\n        sum_state_dict[key] = torch.zeros_like(state_dicts[0][key])\n\n    # Accumulate all state dicts\n    for state_dict in state_dicts:\n        for key in sum_state_dict:\n            sum_state_dict[key] += state_dict[key]\n\n    return sum_state_dict\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_to_device","title":"<code>state_dict_to_device(state_dict, device, copy=False, inplace=False)</code>","text":"<p>Move state dict tensors to the specified device.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>StateDictType</code>)           \u2013            <p>The state dictionary to move.</p> </li> <li> <code>device</code>               (<code>Union[device, str]</code>)           \u2013            <p>Target device for the tensors.</p> </li> <li> <code>copy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force a copy even when the tensor is already on the target device.</p> </li> <li> <code>inplace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to modify the input state dict in place.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>State dict with tensors moved to the specified device.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_to_device(\n    state_dict: StateDictType,\n    device: Union[torch.device, str],\n    copy: bool = False,\n    inplace: bool = False,\n) -&gt; StateDictType:\n    \"\"\"\n    Move state dict tensors to the specified device.\n\n    Args:\n        state_dict: The state dictionary to move.\n        device: Target device for the tensors.\n        copy: Whether to force a copy even when the tensor is already on the target device.\n        inplace: Whether to modify the input state dict in place.\n\n    Returns:\n        State dict with tensors moved to the specified device.\n    \"\"\"\n    if inplace:\n        ret_state_dict = state_dict\n    else:\n        ret_state_dict = OrderedDict()\n\n    for key, tensor in state_dict.items():\n        ret_state_dict[key] = cast(Tensor, tensor).to(\n            device, non_blocking=True, copy=copy\n        )\n    return ret_state_dict\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dict_weighted_sum","title":"<code>state_dict_weighted_sum(state_dicts, weights, device=None)</code>","text":"<p>Compute the weighted sum of multiple state dicts.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[StateDictType]</code>)           \u2013            <p>List of state dicts to combine.</p> </li> <li> <code>weights</code>               (<code>List[float]</code>)           \u2013            <p>List of weights for the weighted sum.</p> </li> <li> <code>device</code>               (<code>Optional[Union[device, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional device to move the result tensors to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDictType</code>           \u2013            <p>A state dict containing the weighted sum of parameters.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the lists have different lengths or are empty, or if state dicts have different keys.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dict_weighted_sum(\n    state_dicts: List[StateDictType],\n    weights: List[float],\n    device: Optional[Union[torch.device, str]] = None,\n) -&gt; StateDictType:\n    \"\"\"\n    Compute the weighted sum of multiple state dicts.\n\n    Args:\n        state_dicts: List of state dicts to combine.\n        weights: List of weights for the weighted sum.\n        device: Optional device to move the result tensors to.\n\n    Returns:\n        A state dict containing the weighted sum of parameters.\n\n    Raises:\n        ValueError: If the lists have different lengths or are empty, or if state dicts have different keys.\n    \"\"\"\n    _validate_state_dict_list_not_empty(state_dicts)\n    _validate_list_lengths_equal(state_dicts, weights, \"state_dicts\", \"weights\")\n    _validate_state_dict_same_keys(state_dicts)\n\n    weighted_sum_state_dict = OrderedDict()\n\n    # Single pass initialization and computation for better performance\n    for key in state_dicts[0]:\n        # Get reference tensor and handle sparse tensors\n        ref_tensor = state_dicts[0][key]\n        is_sparse = ref_tensor.is_sparse if hasattr(ref_tensor, \"is_sparse\") else False\n\n        # Initialize result tensor\n        if is_sparse:\n            # For sparse tensors, start with zeros in dense format for efficient accumulation\n            result_tensor = torch.zeros_like(ref_tensor).to_dense()\n        else:\n            result_tensor = torch.zeros_like(ref_tensor)\n\n        # Accumulate weighted contributions in a single loop\n        for state_dict, weight in zip(state_dicts, weights):\n            tensor = state_dict[key]\n\n            # Optimize for common cases\n            if weight == 0.0:\n                continue  # Skip zero weights\n            elif weight == 1.0:\n                result_tensor += tensor  # Avoid multiplication for unit weights\n            else:\n                # Use in-place operations when possible for memory efficiency\n                if is_sparse and hasattr(tensor, \"is_sparse\") and tensor.is_sparse:\n                    result_tensor += weight * tensor.to_dense()\n                else:\n                    result_tensor += weight * tensor\n\n        # Move to target device if specified (do this once per tensor, not per operation)\n        if device is not None:\n            result_tensor = result_tensor.to(device, non_blocking=True)\n\n        # Convert back to sparse if original was sparse and result is suitable\n        if is_sparse and hasattr(result_tensor, \"to_sparse\"):\n            try:\n                # Only convert back to sparse if it would be memory efficient\n                # (i.e., if the result has sufficient sparsity)\n                if result_tensor.numel() &gt; 0:\n                    sparsity_ratio = (result_tensor == 0).float().mean().item()\n                    if sparsity_ratio &gt; 0.5:  # Convert back if &gt;50% zeros\n                        result_tensor = result_tensor.to_sparse()\n            except (RuntimeError, AttributeError):\n                # If conversion fails, keep as dense\n                pass\n\n        weighted_sum_state_dict[key] = result_tensor\n\n    return weighted_sum_state_dict\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.state_dict_arithmetic.state_dicts_check_keys","title":"<code>state_dicts_check_keys(state_dicts)</code>","text":"<p>Check that all state dictionaries have the same keys.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[StateDictType]</code>)           \u2013            <p>A list of state dictionaries to check.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the state dictionaries have different keys or the list is empty.</p> </li> </ul> Source code in <code>fusion_bench/utils/state_dict_arithmetic.py</code> <pre><code>def state_dicts_check_keys(state_dicts: List[StateDictType]) -&gt; None:\n    \"\"\"\n    Check that all state dictionaries have the same keys.\n\n    Args:\n        state_dicts: A list of state dictionaries to check.\n\n    Raises:\n        ValueError: If the state dictionaries have different keys or the list is empty.\n    \"\"\"\n    _validate_state_dict_list_not_empty(state_dicts)\n    _validate_state_dict_same_keys(state_dicts)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#lazy-model-loading","title":"Lazy Model Loading","text":""},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.lazy_state_dict.LazyStateDict","title":"<code>fusion_bench.utils.lazy_state_dict.LazyStateDict</code>","text":"<p>               Bases: <code>Mapping[str, Tensor]</code>, <code>Generic[TorchModelType]</code></p> <p>A dictionary-like object that lazily loads tensors from model checkpoints.</p> Source code in <code>fusion_bench/utils/lazy_state_dict.py</code> <pre><code>class LazyStateDict(Mapping[str, torch.Tensor], Generic[TorchModelType]):\n    \"\"\"\n    A dictionary-like object that lazily loads tensors from model checkpoints.\n    \"\"\"\n\n    _local_path: str\n    \"\"\"Local path to the checkpoint.\"\"\"\n    _state_dict_cache: Optional[Dict]\n    \"\"\"Cache for the state dict, if enabled.\"\"\"\n    _index_filename: Optional[str]\n    _checkpoint_files: Optional[List[str]]\n    _index: Optional[Dict[str, str]]\n    \"\"\"Mapping of parameter names to checkpoint files.\"\"\"\n\n    meta_module: TorchModelType = None\n    meta_module_class: Optional[Type[TorchModelType]] = None\n\n    def __init__(\n        self,\n        checkpoint: str,\n        meta_module_class: Optional[Type[TorchModelType]] = None,\n        meta_module: Optional[TorchModelType] = None,\n        cache_state_dict: bool = False,\n        torch_dtype: Optional[torch.dtype] = None,\n        device: str = \"cpu\",\n        hf_revision: Optional[str] = None,\n        hf_cache_dir: Optional[str] = None,\n        hf_proxies: Optional[Dict] = None,\n    ):\n        \"\"\"\n        Initialize LazyStateDict with a checkpoint path.\n\n        Args:\n            checkpoint (str): Path to the checkpoint file or directory.\n            meta_module_class (Type[nn.Module], optional): Class of the meta module to instantiate.\n            meta_module (nn.Module, optional): Pre-initialized meta module.\n            cache_state_dict (bool): Whether to cache the state dict in memory.\n            torch_dtype (torch.dtype, optional): The dtype to use for the tensors.\n            device (str): The device to load the tensors onto.\n            hf_revision (str, optional): The revision of the model to download from Hugging Face Hub.\n            hf_cache_dir (str, optional): The cache directory for Hugging Face models.\n            hf_proxies (Dict, optional): Proxies to use for downloading from Hugging Face Hub.\n        \"\"\"\n        self.cache_state_dict = cache_state_dict\n\n        # Validate that both meta_module_class and meta_module are not provided\n        if meta_module_class is not None and meta_module is not None:\n            raise ValueError(\n                \"Cannot provide both meta_module_class and meta_module, please provide only one.\"\n            )\n\n        self.meta_module_class = meta_module_class\n        if isinstance(self.meta_module_class, str):\n            self.meta_module_class = import_object(self.meta_module_class)\n        self.meta_module = meta_module\n\n        # Instantiate meta module if class provided\n        if self.meta_module_class is not None:\n            with init_empty_weights():\n                self.meta_module = self.meta_module_class.from_pretrained(\n                    checkpoint,\n                    torch_dtype=torch_dtype,\n                    revision=hf_revision,\n                    cache_dir=hf_cache_dir,\n                    proxies=hf_proxies,\n                )\n\n        # Store original checkpoint path and resolve to local path\n        self._checkpoint = checkpoint\n        self._local_path = resolve_checkpoint_path(\n            checkpoint,\n            hf_revision=hf_revision,\n            hf_cache_dir=hf_cache_dir,\n            hf_proxies=hf_proxies,\n        )\n\n        # Detect checkpoint file type and set up indexing\n        self._index, self._index_filename, self._checkpoint_files = (\n            self._resolve_checkpoint_files(self._local_path)\n        )\n\n        # Set up based on checkpoint type\n        if self._index is not None:\n            # if meta_module is provided, remove the keys that are not in the meta_module\n            if self.meta_module is not None:\n                meta_module_state_dict = self.meta_module.state_dict()\n                for key in tuple(self._index.keys()):\n                    if key not in meta_module_state_dict:\n                        self._index.pop(key)\n            if cache_state_dict:\n                self._state_dict_cache = {}\n            else:\n                self._state_dict_cache = None\n        elif len(self._checkpoint_files) == 1 and self._checkpoint_files[0].endswith(\n            SAFE_WEIGHTS_NAME\n        ):\n            # SafeTensors file: create index mapping all keys to this file\n            with safe_open(\n                self._checkpoint_files[0], framework=\"pt\", device=device\n            ) as f:\n                self._index = {key: self._checkpoint_files[0] for key in f.keys()}\n                if cache_state_dict:\n                    self._state_dict_cache = {}\n                else:\n                    self._state_dict_cache = None\n        elif len(self._checkpoint_files) == 1 and self._checkpoint_files[0].endswith(\n            WEIGHTS_NAME\n        ):\n            # PyTorch .bin file: load entire state dict immediately\n            log.info(f\"Loading full state dict from {WEIGHTS_NAME}\")\n            self._state_dict_cache = torch.load(self._checkpoint_files[0])\n            # if meta_module is provided, remove the keys that are not in the meta_module\n            if self.meta_module is not None:\n                meta_module_state_dict = self.meta_module.state_dict()\n                for key in tuple(self._state_dict_cache.keys()):\n                    if key not in meta_module_state_dict:\n                        self._state_dict_cache.pop(key)\n        else:\n            # Unsupported checkpoint format\n            raise ValueError(\n                f\"Cannot determine the type of checkpoint, please provide a checkpoint path to a file containing a whole state dict with file name {WEIGHTS_NAME} or {SAFE_WEIGHTS_NAME}, or the index of a sharded checkpoint ending with `.index.json`.\"\n            )\n\n        self._torch_dtype = parse_dtype(torch_dtype)\n        self._device = device\n\n    @property\n    def checkpoint(self) -&gt; str:\n        return self._checkpoint\n\n    @property\n    def config(self) -&gt; \"PretrainedConfig\":\n        return AutoConfig.from_pretrained(self._checkpoint)\n\n    @property\n    def dtype(self) -&gt; torch.dtype:\n        \"\"\"\n        `torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n        \"\"\"\n        if hasattr(self, \"_cached_dtype\"):\n            return self._cached_dtype\n\n        first_key = next(iter(self.keys()))\n        first_param = self[first_key]\n        self._cached_dtype = first_param.dtype\n        return self._cached_dtype\n\n    def state_dict(self, keep_vars: bool = False) -&gt; \"LazyStateDict\":\n        \"\"\"\n        Args:\n            keep_vars (bool): Ignored, as LazyStateDict does not support keep_vars. Just for compatibility.\n        \"\"\"\n        return deepcopy(self)\n\n    def _resolve_checkpoint_files(self, checkpoint: str):\n        \"\"\"\n        Detect and resolve checkpoint files based on the checkpoint path.\n\n        Handles single files, directories with state dict files, and sharded checkpoints.\n\n        Returns:\n            Tuple of (index_dict, index_filename, checkpoint_files)\n        \"\"\"\n        # Reference: https://huggingface.co/docs/accelerate/v0.17.1/en/usage_guides/big_modeling\n        checkpoint_files = None\n        index_filename = None\n        if os.path.isfile(checkpoint):\n            # Single file: check if it's an index or a state dict\n            if str(checkpoint).endswith(\".json\"):\n                index_filename = checkpoint\n            else:\n                checkpoint_files = [checkpoint]\n        elif os.path.isdir(checkpoint):\n            # check if the whole state dict is present\n            potential_state_bin = [\n                f for f in os.listdir(checkpoint) if f == WEIGHTS_NAME\n            ]\n            potential_state_safetensor = [\n                f for f in os.listdir(checkpoint) if f == SAFE_WEIGHTS_NAME\n            ]\n            if len(potential_state_bin) == 1:\n                checkpoint_files = [os.path.join(checkpoint, potential_state_bin[0])]\n            elif len(potential_state_safetensor) == 1:\n                checkpoint_files = [\n                    os.path.join(checkpoint, potential_state_safetensor[0])\n                ]\n            else:\n                # Check for sharded checkpoints\n                potential_index = [\n                    f for f in os.listdir(checkpoint) if f.endswith(\".index.json\")\n                ]\n                if len(potential_index) == 0:\n                    raise ValueError(\n                        f\"{checkpoint} is not a folder containing a `.index.json` file or a {WEIGHTS_NAME} or a {SAFE_WEIGHTS_NAME} file\"\n                    )\n                elif len(potential_index) == 1:\n                    index_filename = os.path.join(checkpoint, potential_index[0])\n                else:\n                    raise ValueError(\n                        f\"{checkpoint} containing more than one `.index.json` file, delete the irrelevant ones.\"\n                    )\n        else:\n            # Invalid checkpoint path\n            raise ValueError(\n                \"`checkpoint` should be the path to a file containing a whole state dict, or the index of a sharded \"\n                f\"checkpoint, or a folder containing a sharded checkpoint or the whole state dict, but got {checkpoint}.\"\n            )\n\n        # Load index file if present\n        if index_filename is not None:\n            checkpoint_folder = os.path.split(index_filename)[0]\n            with open(index_filename) as f:\n                index = json.loads(f.read())\n\n            # Extract weight_map if present (standard format)\n            if \"weight_map\" in index:\n                index = index[\"weight_map\"]\n            # Get list of unique checkpoint files\n            checkpoint_files = sorted(list(set(index.values())))\n            checkpoint_files = [\n                os.path.join(checkpoint_folder, f) for f in checkpoint_files\n            ]\n        else:\n            index = None\n        return index, index_filename, checkpoint_files\n\n    def _load_tensor_from_checkpoint_file(\n        self, checkpoint_file: str, key: str, update_cache: bool = True\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Load a tensor from the checkpoint file.\n        For safetensors, loads only the requested tensor.\n        For PyTorch files, loads the entire state dict on first access.\n        \"\"\"\n        if checkpoint_file.endswith(\".safetensors\"):\n            with safe_open(checkpoint_file, framework=\"pt\", device=self._device) as f:\n                tensor = f.get_tensor(key)\n                if self._torch_dtype is not None:\n                    tensor = tensor.to(self._torch_dtype)\n                if update_cache and self._state_dict_cache is not None:\n                    self._state_dict_cache[key] = tensor\n                return tensor\n        else:\n            # PyTorch .bin file: load entire state dict\n            state_dict = torch.load(checkpoint_file, map_location=self._device)\n            if update_cache:\n                if self._state_dict_cache is not None:\n                    self._state_dict_cache.update(state_dict)\n                else:\n                    log.warning(\n                        f\"Load full state dict from file {checkpoint_file}, but state dict cache is disabled.\"\n                    )\n            return state_dict[key]\n\n    def __getitem__(self, key: str) -&gt; torch.Tensor:\n        if self._state_dict_cache is not None and key in self._state_dict_cache:\n            return self._state_dict_cache[key]\n\n        if self._index is None:\n            if len(self._checkpoint_files) == 1 and os.path.isfile(\n                self._checkpoint_files[0]\n            ):\n                checkpoint_file = self._checkpoint_files[0]\n                tensor = self._load_tensor_from_checkpoint_file(\n                    checkpoint_file, key, update_cache=True\n                )\n                return tensor\n            else:\n                if len(self._checkpoint_files) &gt; 1:\n                    raise RuntimeError(\n                        \"Get multiple checkpoint files, but index is not provided.\"\n                    )\n                if not os.path.isfile(self._checkpoint_files[0]):\n                    raise FileNotFoundError(\n                        f\"Checkpoint file {self._checkpoint_files[0]} not found.\"\n                    )\n                raise RuntimeError(\"Unexpected error.\")\n        else:\n            if key not in self._index:\n                raise KeyError(f\"Key {key} not found in index.\")\n            checkpoint_file = os.path.join(self._local_path, self._index[key])\n            if not os.path.isfile(checkpoint_file):\n                raise FileNotFoundError(f\"Checkpoint file {checkpoint_file} not found.\")\n            tensor = self._load_tensor_from_checkpoint_file(\n                checkpoint_file, key, update_cache=True\n            )\n            return tensor\n\n    def pop(self, key: str):\n        assert key in list(\n            self.keys()\n        ), \"KeyError: Cannot pop a tensor for a key that does not exist in the LazyStateDict.\"\n        if self._state_dict_cache is not None and key in self._state_dict_cache:\n            if key in self._index:\n                self._index.pop(key)\n            return self._state_dict_cache.pop(key)\n        if key in self._index:\n            self._index.pop(key)\n        return None\n\n    def __setitem__(self, key: str, value: torch.Tensor) -&gt; None:\n        \"\"\"\n        Set a tensor in the LazyStateDict. This will update the state dict cache if it is enabled.\n        \"\"\"\n        assert key in list(\n            self.keys()\n        ), \"KeyError: Cannot set a tensor for a key that does not exist in the LazyStateDict.\"\n        if self._state_dict_cache is not None:\n            self._state_dict_cache[key] = value\n        else:\n            log.warning(\"State dict cache is disabled, initializing the cache.\")\n            self._state_dict_cache = {key: value}\n\n    def __contains__(self, key: str) -&gt; bool:\n        if self._state_dict_cache is not None and key in self._state_dict_cache:\n            return True\n        if self._index is not None and key in self._index:\n            return True\n        if len(self._checkpoint_files) == 1 and os.path.isfile(\n            self._checkpoint_files[0]\n        ):\n            try:\n                tensor = self._load_tensor_from_checkpoint_file(\n                    self._checkpoint_files[0], key, update_cache=False\n                )\n                return tensor is not None\n            except (KeyError, FileNotFoundError, RuntimeError, EOFError):\n                return False\n        return False\n\n    def __len__(self) -&gt; int:\n        if self._index is not None:\n            return len(self._index)\n        if len(self._checkpoint_files) == 1 and os.path.isfile(\n            self._checkpoint_files[0]\n        ):\n            checkpoint_file = self._checkpoint_files[0]\n            if checkpoint_file.endswith(\".safetensors\"):\n                with safe_open(checkpoint_file, framework=\"pt\", device=\"cpu\") as f:\n                    return len(tuple(f.keys()))\n            else:\n                return len(\n                    tuple(torch.load(checkpoint_file, map_location=\"cpu\").keys())\n                )\n        raise RuntimeError(\n            \"Unexpected error: cannot determine the number of keys in the state dict.\"\n        )\n\n    def __iter__(self) -&gt; Iterator[str]:\n        if self._index is not None:\n            return iter(self._index)\n        elif self._state_dict_cache is not None:\n            return iter(self._state_dict_cache)\n        else:\n            raise RuntimeError(\n                \"Unexpected error: cannot determine the keys in the state dict.\"\n            )\n\n    def keys(self) -&gt; Iterator[str]:\n        for key in self:\n            yield key\n\n    def values(self) -&gt; Iterator[torch.Tensor]:\n        for key in self:\n            yield self[key]\n\n    def items(self) -&gt; Iterator[Tuple[str, torch.Tensor]]:\n        for key in self:\n            yield key, self[key]\n\n    def __repr__(self) -&gt; str:\n        if self._index is not None:\n            return f\"{self.__class__.__name__}(keys={list(self.keys())})\"\n        else:\n            return (\n                f\"{self.__class__.__name__}(checkpoint_files={self._checkpoint_files})\"\n            )\n\n    def get_parameter(self, target: str) -&gt; torch.Tensor:\n        return self[target]\n\n    def get_submodule(self, target: str) -&gt; nn.Module:\n        if self.meta_module is not None:\n            module: nn.Module = deepcopy(self.meta_module.get_submodule(target))\n            module.to_empty(device=self._device)\n            state_dict = {}\n            for name, _ in module.named_parameters():\n                state_dict[name] = self[f\"{target}.{name}\"]\n            module.load_state_dict(state_dict)\n            return module\n        else:\n            raise RuntimeError(\n                \"Cannot get submodule because meta_module is not provided.\"\n            )\n\n    def load_state_dict(\n        self, state_dict: Mapping[str, torch.Tensor], strict: bool = True\n    ) -&gt; _IncompatibleKeys:\n        \"\"\"\n        Load a state dict into this LazyStateDict.\n        This method is only for compatibility with nn.Module and it overrides the cache of LazyStateDict.\n\n        Args:\n            state_dict (Dict[str, torch.Tensor]): The state dict to load.\n            strict (bool): Whether to enforce that all keys in the state dict are present in this LazyStateDict.\n        \"\"\"\n        if not isinstance(state_dict, Mapping):\n            raise TypeError(\n                f\"Expected state_dict to be dict-like, got {type(state_dict)}.\"\n            )\n\n        missing_keys: list[str] = []\n        unexpected_keys: list[str] = []\n        error_msgs: list[str] = []\n\n        log.warning(\n            \"Loading state dict into LazyStateDict is not recommended, as it may lead to unexpected behavior. \"\n            \"Use with caution.\"\n        )\n\n        # Check for unexpected keys in the provided state_dict\n        for key in state_dict:\n            if key not in self:\n                unexpected_keys.append(key)\n\n        # Check for missing keys that are expected in this LazyStateDict\n        for key in self.keys():\n            if key not in state_dict:\n                missing_keys.append(key)\n\n        # Handle strict mode\n        if strict:\n            if len(unexpected_keys) &gt; 0:\n                error_msgs.insert(\n                    0,\n                    \"Unexpected key(s) in state_dict: {}. \".format(\n                        \", \".join(f'\"{k}\"' for k in unexpected_keys)\n                    ),\n                )\n            if len(missing_keys) &gt; 0:\n                error_msgs.insert(\n                    0,\n                    \"Missing key(s) in state_dict: {}. \".format(\n                        \", \".join(f'\"{k}\"' for k in missing_keys)\n                    ),\n                )\n\n        if len(error_msgs) &gt; 0:\n            raise RuntimeError(\n                \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n                    self.__class__.__name__, \"\\n\\t\".join(error_msgs)\n                )\n            )\n\n        # Load the state dict values\n        for key, value in state_dict.items():\n            if key in self:  # Only set keys that exist in this LazyStateDict\n                self[key] = value\n\n        return _IncompatibleKeys(missing_keys, unexpected_keys)\n\n    def __getattr__(self, name: str):\n        if \"meta_module\" in self.__dict__:\n            meta_module = self.__dict__[\"meta_module\"]\n            if meta_module is not None:\n                if \"_parameters\" in meta_module.__dict__:\n                    if name in meta_module.__dict__[\"_parameters\"]:\n                        return self.get_parameter(name)\n                if \"_modules\" in meta_module.__dict__:\n                    if name in meta_module.__dict__[\"_modules\"]:\n                        return self.get_submodule(name)\n        raise AttributeError(\n            f\"'{type(self).__name__}' object has no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.lazy_state_dict.LazyStateDict.dtype","title":"<code>dtype</code>  <code>property</code>","text":"<p><code>torch.dtype</code>: The dtype of the module (assuming that all the module parameters have the same dtype).</p>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.lazy_state_dict.LazyStateDict.__init__","title":"<code>__init__(checkpoint, meta_module_class=None, meta_module=None, cache_state_dict=False, torch_dtype=None, device='cpu', hf_revision=None, hf_cache_dir=None, hf_proxies=None)</code>","text":"<p>Initialize LazyStateDict with a checkpoint path.</p> <p>Parameters:</p> <ul> <li> <code>checkpoint</code>               (<code>str</code>)           \u2013            <p>Path to the checkpoint file or directory.</p> </li> <li> <code>meta_module_class</code>               (<code>Type[Module]</code>, default:                   <code>None</code> )           \u2013            <p>Class of the meta module to instantiate.</p> </li> <li> <code>meta_module</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Pre-initialized meta module.</p> </li> <li> <code>cache_state_dict</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to cache the state dict in memory.</p> </li> <li> <code>torch_dtype</code>               (<code>dtype</code>, default:                   <code>None</code> )           \u2013            <p>The dtype to use for the tensors.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cpu'</code> )           \u2013            <p>The device to load the tensors onto.</p> </li> <li> <code>hf_revision</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The revision of the model to download from Hugging Face Hub.</p> </li> <li> <code>hf_cache_dir</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The cache directory for Hugging Face models.</p> </li> <li> <code>hf_proxies</code>               (<code>Dict</code>, default:                   <code>None</code> )           \u2013            <p>Proxies to use for downloading from Hugging Face Hub.</p> </li> </ul> Source code in <code>fusion_bench/utils/lazy_state_dict.py</code> <pre><code>def __init__(\n    self,\n    checkpoint: str,\n    meta_module_class: Optional[Type[TorchModelType]] = None,\n    meta_module: Optional[TorchModelType] = None,\n    cache_state_dict: bool = False,\n    torch_dtype: Optional[torch.dtype] = None,\n    device: str = \"cpu\",\n    hf_revision: Optional[str] = None,\n    hf_cache_dir: Optional[str] = None,\n    hf_proxies: Optional[Dict] = None,\n):\n    \"\"\"\n    Initialize LazyStateDict with a checkpoint path.\n\n    Args:\n        checkpoint (str): Path to the checkpoint file or directory.\n        meta_module_class (Type[nn.Module], optional): Class of the meta module to instantiate.\n        meta_module (nn.Module, optional): Pre-initialized meta module.\n        cache_state_dict (bool): Whether to cache the state dict in memory.\n        torch_dtype (torch.dtype, optional): The dtype to use for the tensors.\n        device (str): The device to load the tensors onto.\n        hf_revision (str, optional): The revision of the model to download from Hugging Face Hub.\n        hf_cache_dir (str, optional): The cache directory for Hugging Face models.\n        hf_proxies (Dict, optional): Proxies to use for downloading from Hugging Face Hub.\n    \"\"\"\n    self.cache_state_dict = cache_state_dict\n\n    # Validate that both meta_module_class and meta_module are not provided\n    if meta_module_class is not None and meta_module is not None:\n        raise ValueError(\n            \"Cannot provide both meta_module_class and meta_module, please provide only one.\"\n        )\n\n    self.meta_module_class = meta_module_class\n    if isinstance(self.meta_module_class, str):\n        self.meta_module_class = import_object(self.meta_module_class)\n    self.meta_module = meta_module\n\n    # Instantiate meta module if class provided\n    if self.meta_module_class is not None:\n        with init_empty_weights():\n            self.meta_module = self.meta_module_class.from_pretrained(\n                checkpoint,\n                torch_dtype=torch_dtype,\n                revision=hf_revision,\n                cache_dir=hf_cache_dir,\n                proxies=hf_proxies,\n            )\n\n    # Store original checkpoint path and resolve to local path\n    self._checkpoint = checkpoint\n    self._local_path = resolve_checkpoint_path(\n        checkpoint,\n        hf_revision=hf_revision,\n        hf_cache_dir=hf_cache_dir,\n        hf_proxies=hf_proxies,\n    )\n\n    # Detect checkpoint file type and set up indexing\n    self._index, self._index_filename, self._checkpoint_files = (\n        self._resolve_checkpoint_files(self._local_path)\n    )\n\n    # Set up based on checkpoint type\n    if self._index is not None:\n        # if meta_module is provided, remove the keys that are not in the meta_module\n        if self.meta_module is not None:\n            meta_module_state_dict = self.meta_module.state_dict()\n            for key in tuple(self._index.keys()):\n                if key not in meta_module_state_dict:\n                    self._index.pop(key)\n        if cache_state_dict:\n            self._state_dict_cache = {}\n        else:\n            self._state_dict_cache = None\n    elif len(self._checkpoint_files) == 1 and self._checkpoint_files[0].endswith(\n        SAFE_WEIGHTS_NAME\n    ):\n        # SafeTensors file: create index mapping all keys to this file\n        with safe_open(\n            self._checkpoint_files[0], framework=\"pt\", device=device\n        ) as f:\n            self._index = {key: self._checkpoint_files[0] for key in f.keys()}\n            if cache_state_dict:\n                self._state_dict_cache = {}\n            else:\n                self._state_dict_cache = None\n    elif len(self._checkpoint_files) == 1 and self._checkpoint_files[0].endswith(\n        WEIGHTS_NAME\n    ):\n        # PyTorch .bin file: load entire state dict immediately\n        log.info(f\"Loading full state dict from {WEIGHTS_NAME}\")\n        self._state_dict_cache = torch.load(self._checkpoint_files[0])\n        # if meta_module is provided, remove the keys that are not in the meta_module\n        if self.meta_module is not None:\n            meta_module_state_dict = self.meta_module.state_dict()\n            for key in tuple(self._state_dict_cache.keys()):\n                if key not in meta_module_state_dict:\n                    self._state_dict_cache.pop(key)\n    else:\n        # Unsupported checkpoint format\n        raise ValueError(\n            f\"Cannot determine the type of checkpoint, please provide a checkpoint path to a file containing a whole state dict with file name {WEIGHTS_NAME} or {SAFE_WEIGHTS_NAME}, or the index of a sharded checkpoint ending with `.index.json`.\"\n        )\n\n    self._torch_dtype = parse_dtype(torch_dtype)\n    self._device = device\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.lazy_state_dict.LazyStateDict.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set a tensor in the LazyStateDict. This will update the state dict cache if it is enabled.</p> Source code in <code>fusion_bench/utils/lazy_state_dict.py</code> <pre><code>def __setitem__(self, key: str, value: torch.Tensor) -&gt; None:\n    \"\"\"\n    Set a tensor in the LazyStateDict. This will update the state dict cache if it is enabled.\n    \"\"\"\n    assert key in list(\n        self.keys()\n    ), \"KeyError: Cannot set a tensor for a key that does not exist in the LazyStateDict.\"\n    if self._state_dict_cache is not None:\n        self._state_dict_cache[key] = value\n    else:\n        log.warning(\"State dict cache is disabled, initializing the cache.\")\n        self._state_dict_cache = {key: value}\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.lazy_state_dict.LazyStateDict.load_state_dict","title":"<code>load_state_dict(state_dict, strict=True)</code>","text":"<p>Load a state dict into this LazyStateDict. This method is only for compatibility with nn.Module and it overrides the cache of LazyStateDict.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>Dict[str, Tensor]</code>)           \u2013            <p>The state dict to load.</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to enforce that all keys in the state dict are present in this LazyStateDict.</p> </li> </ul> Source code in <code>fusion_bench/utils/lazy_state_dict.py</code> <pre><code>def load_state_dict(\n    self, state_dict: Mapping[str, torch.Tensor], strict: bool = True\n) -&gt; _IncompatibleKeys:\n    \"\"\"\n    Load a state dict into this LazyStateDict.\n    This method is only for compatibility with nn.Module and it overrides the cache of LazyStateDict.\n\n    Args:\n        state_dict (Dict[str, torch.Tensor]): The state dict to load.\n        strict (bool): Whether to enforce that all keys in the state dict are present in this LazyStateDict.\n    \"\"\"\n    if not isinstance(state_dict, Mapping):\n        raise TypeError(\n            f\"Expected state_dict to be dict-like, got {type(state_dict)}.\"\n        )\n\n    missing_keys: list[str] = []\n    unexpected_keys: list[str] = []\n    error_msgs: list[str] = []\n\n    log.warning(\n        \"Loading state dict into LazyStateDict is not recommended, as it may lead to unexpected behavior. \"\n        \"Use with caution.\"\n    )\n\n    # Check for unexpected keys in the provided state_dict\n    for key in state_dict:\n        if key not in self:\n            unexpected_keys.append(key)\n\n    # Check for missing keys that are expected in this LazyStateDict\n    for key in self.keys():\n        if key not in state_dict:\n            missing_keys.append(key)\n\n    # Handle strict mode\n    if strict:\n        if len(unexpected_keys) &gt; 0:\n            error_msgs.insert(\n                0,\n                \"Unexpected key(s) in state_dict: {}. \".format(\n                    \", \".join(f'\"{k}\"' for k in unexpected_keys)\n                ),\n            )\n        if len(missing_keys) &gt; 0:\n            error_msgs.insert(\n                0,\n                \"Missing key(s) in state_dict: {}. \".format(\n                    \", \".join(f'\"{k}\"' for k in missing_keys)\n                ),\n            )\n\n    if len(error_msgs) &gt; 0:\n        raise RuntimeError(\n            \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n                self.__class__.__name__, \"\\n\\t\".join(error_msgs)\n            )\n        )\n\n    # Load the state dict values\n    for key, value in state_dict.items():\n        if key in self:  # Only set keys that exist in this LazyStateDict\n            self[key] = value\n\n    return _IncompatibleKeys(missing_keys, unexpected_keys)\n</code></pre>"},{"location":"api/fusion_bench.utils/model/#fusion_bench.utils.lazy_state_dict.LazyStateDict.state_dict","title":"<code>state_dict(keep_vars=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>keep_vars</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Ignored, as LazyStateDict does not support keep_vars. Just for compatibility.</p> </li> </ul> Source code in <code>fusion_bench/utils/lazy_state_dict.py</code> <pre><code>def state_dict(self, keep_vars: bool = False) -&gt; \"LazyStateDict\":\n    \"\"\"\n    Args:\n        keep_vars (bool): Ignored, as LazyStateDict does not support keep_vars. Just for compatibility.\n    \"\"\"\n    return deepcopy(self)\n</code></pre>"},{"location":"api/fusion_bench.utils/modelscope/","title":"ModelScope Integration","text":""},{"location":"api/fusion_bench.utils/modelscope/#fusion_bench.utils.modelscope","title":"<code>fusion_bench.utils.modelscope</code>","text":""},{"location":"api/fusion_bench.utils/modelscope/#fusion_bench.utils.modelscope.load_dataset","title":"<code>load_dataset(name, split='train', platform='hf')</code>","text":"<p>Load a dataset from Hugging Face or ModelScope.</p> <p>Parameters:</p> <ul> <li> <code>platform</code>               (<code>Literal['hf', 'modelscope']</code>, default:                   <code>'hf'</code> )           \u2013            <p>The platform to load the dataset from.</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the dataset.</p> </li> <li> <code>split</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>The split of the dataset to load (default is \"train\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>          \u2013            <p>The loaded dataset.</p> </li> </ul> Source code in <code>fusion_bench/utils/modelscope.py</code> <pre><code>def load_dataset(\n    name: str,\n    split: str = \"train\",\n    platform: Literal[\"hf\", \"huggingface\", \"modelscope\"] = \"hf\",\n):\n    \"\"\"\n    Load a dataset from Hugging Face or ModelScope.\n\n    Args:\n        platform (Literal['hf', 'modelscope']): The platform to load the dataset from.\n        name (str): The name of the dataset.\n        split (str): The split of the dataset to load (default is \"train\").\n\n    Returns:\n        Dataset: The loaded dataset.\n    \"\"\"\n    validate_and_suggest_corrections(platform, AVAILABLE_PLATFORMS)\n    if platform == \"hf\" or platform == \"huggingface\":\n        return datasets_load_dataset(name, split=split)\n    elif platform == \"modelscope\":\n        dataset_dir = modelscope_snapshot_download(name, repo_type=\"dataset\")\n        return datasets_load_dataset(dataset_dir, split=split)\n    else:\n        _raise_unknown_platform_error(platform)\n</code></pre>"},{"location":"api/fusion_bench.utils/modelscope/#fusion_bench.utils.modelscope.resolve_file_path","title":"<code>resolve_file_path(repo_id, filename, repo_type='model', platform='hf', **kwargs)</code>","text":"<p>Resolve and download a specific file from a repository across multiple platforms.</p> <p>This function downloads a specific file from repositories hosted on various platforms including local paths, Hugging Face Hub, and ModelScope. It handles platform-specific URL prefixes and automatically determines the appropriate download method.</p> <p>Parameters:</p> <ul> <li> <code>repo_id</code>               (<code>str</code>)           \u2013            <p>Repository identifier. Can be: - Local directory path (file will be joined with this path if it exists) - Hugging Face model/dataset ID (e.g., \"bert-base-uncased\") - ModelScope model/dataset ID - URL-prefixed ID (e.g., \"hf://model-name\", \"modelscope://model-name\").   The prefix will override the platform argument.</p> </li> <li> <code>filename</code>               (<code>str</code>)           \u2013            <p>The specific file to download from the repository.</p> </li> <li> <code>repo_type</code>               (<code>Literal['model', 'dataset']</code>, default:                   <code>'model'</code> )           \u2013            <p>Type of repository. Defaults to \"model\". Used for ModelScope platform to determine the correct download function.</p> </li> <li> <code>platform</code>               (<code>Literal['hf', 'huggingface', 'modelscope']</code>, default:                   <code>'hf'</code> )           \u2013            <p>Platform to download from. Defaults to \"hf\". Options: - \"hf\" or \"huggingface\": Hugging Face Hub - \"modelscope\": ModelScope platform</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to the underlying download functions (e.g., cache_dir, force_download, use_auth_token).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Local path to the downloaded file.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unsupported repo_type is specified for ModelScope platform.</p> </li> <li> <code>ImportError</code>             \u2013            <p>If required dependencies for the specified platform are not installed.</p> </li> <li> <code>FileNotFoundError</code>             \u2013            <p>If the file cannot be found or downloaded.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Download config.json from a Hugging Face model\n&gt;&gt;&gt; resolve_file_path(\"bert-base-uncased\", \"config.json\")\n\"/home/user/.cache/huggingface/hub/models--bert-base-uncased/.../config.json\"\n</code></pre> <pre><code>&gt;&gt;&gt; # Download from ModelScope\n&gt;&gt;&gt; resolve_file_path(\n...     \"damo/nlp_bert_backbone_base_std\",\n...     \"pytorch_model.bin\",\n...     platform=\"modelscope\"\n... )\n\"/home/user/.cache/modelscope/hub/.../pytorch_model.bin\"\n</code></pre> <pre><code>&gt;&gt;&gt; # Local file path\n&gt;&gt;&gt; resolve_file_path(\"/path/to/local/model\", \"config.json\")\n\"/path/to/local/model/config.json\"\n</code></pre> <pre><code>&gt;&gt;&gt; # URL-prefixed repository\n&gt;&gt;&gt; resolve_file_path(\"hf://microsoft/DialoGPT-medium\", \"config.json\")\n\"/home/user/.cache/huggingface/hub/.../config.json\"\n</code></pre> <pre><code>&gt;&gt;&gt; # Download dataset file from ModelScope\n&gt;&gt;&gt; resolve_file_path(\n...     \"DAMO_NLP/jd\",\n...     \"train.json\",\n...     repo_type=\"dataset\",\n...     platform=\"modelscope\"\n... )\n\"/home/user/.cache/modelscope/datasets/.../train.json\"\n</code></pre> Source code in <code>fusion_bench/utils/modelscope.py</code> <pre><code>def resolve_file_path(\n    repo_id: str,\n    filename: str,\n    repo_type: Literal[\"model\", \"dataset\"] = \"model\",\n    platform: Literal[\"hf\", \"huggingface\", \"modelscope\"] = \"hf\",\n    **kwargs,\n) -&gt; str:\n    \"\"\"\n    Resolve and download a specific file from a repository across multiple platforms.\n\n    This function downloads a specific file from repositories hosted on various platforms\n    including local paths, Hugging Face Hub, and ModelScope. It handles platform-specific\n    URL prefixes and automatically determines the appropriate download method.\n\n    Args:\n        repo_id (str): Repository identifier. Can be:\n            - Local directory path (file will be joined with this path if it exists)\n            - Hugging Face model/dataset ID (e.g., \"bert-base-uncased\")\n            - ModelScope model/dataset ID\n            - URL-prefixed ID (e.g., \"hf://model-name\", \"modelscope://model-name\").\n              The prefix will override the platform argument.\n        filename (str): The specific file to download from the repository.\n        repo_type (Literal[\"model\", \"dataset\"], optional): Type of repository.\n            Defaults to \"model\". Used for ModelScope platform to determine the\n            correct download function.\n        platform (Literal[\"hf\", \"huggingface\", \"modelscope\"], optional):\n            Platform to download from. Defaults to \"hf\". Options:\n            - \"hf\" or \"huggingface\": Hugging Face Hub\n            - \"modelscope\": ModelScope platform\n        **kwargs: Additional arguments passed to the underlying download functions\n            (e.g., cache_dir, force_download, use_auth_token).\n\n    Returns:\n        str: Local path to the downloaded file.\n\n    Raises:\n        ValueError: If an unsupported repo_type is specified for ModelScope platform.\n        ImportError: If required dependencies for the specified platform are not installed.\n        FileNotFoundError: If the file cannot be found or downloaded.\n\n    Examples:\n        &gt;&gt;&gt; # Download config.json from a Hugging Face model\n        &gt;&gt;&gt; resolve_file_path(\"bert-base-uncased\", \"config.json\")\n        \"/home/user/.cache/huggingface/hub/models--bert-base-uncased/.../config.json\"\n\n        &gt;&gt;&gt; # Download from ModelScope\n        &gt;&gt;&gt; resolve_file_path(\n        ...     \"damo/nlp_bert_backbone_base_std\",\n        ...     \"pytorch_model.bin\",\n        ...     platform=\"modelscope\"\n        ... )\n        \"/home/user/.cache/modelscope/hub/.../pytorch_model.bin\"\n\n        &gt;&gt;&gt; # Local file path\n        &gt;&gt;&gt; resolve_file_path(\"/path/to/local/model\", \"config.json\")\n        \"/path/to/local/model/config.json\"\n\n        &gt;&gt;&gt; # URL-prefixed repository\n        &gt;&gt;&gt; resolve_file_path(\"hf://microsoft/DialoGPT-medium\", \"config.json\")\n        \"/home/user/.cache/huggingface/hub/.../config.json\"\n\n        &gt;&gt;&gt; # Download dataset file from ModelScope\n        &gt;&gt;&gt; resolve_file_path(\n        ...     \"DAMO_NLP/jd\",\n        ...     \"train.json\",\n        ...     repo_type=\"dataset\",\n        ...     platform=\"modelscope\"\n        ... )\n        \"/home/user/.cache/modelscope/datasets/.../train.json\"\n    \"\"\"\n    # If it's a HuggingFace Hub model id, download snapshot\n    if repo_id.startswith(\"hf://\") or repo_id.startswith(\"huggingface://\"):\n        repo_id = repo_id.replace(\"hf://\", \"\").replace(\"huggingface://\", \"\")\n        platform = \"hf\"\n    # If it's a ModelScope model id, download snapshot\n    elif repo_id.startswith(\"modelscope://\"):\n        repo_id = repo_id.replace(\"modelscope://\", \"\")\n        platform = \"modelscope\"\n\n    # If it's a local file or directory, return as is\n    if os.path.exists(repo_id):\n        return os.path.join(repo_id, filename)\n\n    if platform in [\"hf\", \"huggingface\"]:\n        return hf_hub_download(\n            repo_id=repo_id,\n            filename=filename,\n            repo_type=repo_type,\n            **kwargs,\n        )\n    elif platform == \"modelscope\":\n        if repo_type == \"model\":\n            return modelscope_model_file_download(\n                model_id=repo_id, file_path=filename, **kwargs\n            )\n        elif repo_type == \"dataset\":\n            return modelscope_dataset_file_download(\n                dataset_id=repo_id, file_path=filename, **kwargs\n            )\n        else:\n            raise ValueError(\n                f\"Unsupported repo_type: {repo_type}. Supported types are 'model' and 'dataset'.\"\n            )\n    else:\n        _raise_unknown_platform_error(platform)\n</code></pre>"},{"location":"api/fusion_bench.utils/modelscope/#fusion_bench.utils.modelscope.resolve_repo_path","title":"<code>resolve_repo_path(repo_id, repo_type='model', platform='hf', **kwargs)</code>","text":"<p>Resolve and download a repository from various platforms to a local path.</p> <p>This function handles multiple repository sources including local paths, Hugging Face, and ModelScope. It automatically downloads remote repositories to local cache and returns the local path for further use.</p> <p>Parameters:</p> <ul> <li> <code>repo_id</code>               (<code>str</code>)           \u2013            <p>Repository identifier. Can be: - Local file/directory path (returned as-is if exists) - Hugging Face model/dataset ID (e.g., \"bert-base-uncased\") - ModelScope model/dataset ID - URL-prefixed ID (e.g., \"hf://model-name\", \"modelscope://model-name\").   The prefix will override the platform argument.</p> </li> <li> <code>repo_type</code>               (<code>str</code>, default:                   <code>'model'</code> )           \u2013            <p>Type of repository to download. Defaults to \"model\". Common values include \"model\" and \"dataset\".</p> </li> <li> <code>platform</code>               (<code>Literal['hf', 'huggingface', 'modelscope']</code>, default:                   <code>'hf'</code> )           \u2013            <p>Platform to download from. Defaults to \"hf\". Options: - \"hf\" or \"huggingface\": Hugging Face Hub - \"modelscope\": ModelScope platform</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to the underlying download functions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>Local path to the repository (either existing local path or downloaded cache path).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If the repository cannot be found or downloaded from any platform.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If an unsupported platform is specified.</p> </li> <li> <code>ImportError</code>             \u2013            <p>If required dependencies for the specified platform are not installed.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Local path (returned as-is)\n&gt;&gt;&gt; resolve_repo_path(\"/path/to/local/model\")\n\"/path/to/local/model\"\n</code></pre> <pre><code>&gt;&gt;&gt; # Hugging Face model\n&gt;&gt;&gt; resolve_repo_path(\"bert-base-uncased\")\n\"/home/user/.cache/huggingface/hub/models--bert-base-uncased/...\"\n</code></pre> <pre><code>&gt;&gt;&gt; # ModelScope model with explicit platform\n&gt;&gt;&gt; resolve_repo_path(\"damo/nlp_bert_backbone_base_std\", platform=\"modelscope\")\n\"/home/user/.cache/modelscope/hub/damo/nlp_bert_backbone_base_std/...\"\n</code></pre> <pre><code>&gt;&gt;&gt; # URL-prefixed repository ID\n&gt;&gt;&gt; resolve_repo_path(\"hf://microsoft/DialoGPT-medium\")\n\"/home/user/.cache/huggingface/hub/models--microsoft--DialoGPT-medium/...\"\n</code></pre> Source code in <code>fusion_bench/utils/modelscope.py</code> <pre><code>def resolve_repo_path(\n    repo_id: str,\n    repo_type: Optional[str] = \"model\",\n    platform: Literal[\"hf\", \"huggingface\", \"modelscope\"] = \"hf\",\n    **kwargs,\n):\n    \"\"\"\n    Resolve and download a repository from various platforms to a local path.\n\n    This function handles multiple repository sources including local paths, Hugging Face,\n    and ModelScope. It automatically downloads remote repositories to local cache and\n    returns the local path for further use.\n\n    Args:\n        repo_id (str): Repository identifier. Can be:\n            - Local file/directory path (returned as-is if exists)\n            - Hugging Face model/dataset ID (e.g., \"bert-base-uncased\")\n            - ModelScope model/dataset ID\n            - URL-prefixed ID (e.g., \"hf://model-name\", \"modelscope://model-name\").\n              The prefix will override the platform argument.\n        repo_type (str, optional): Type of repository to download. Defaults to \"model\".\n            Common values include \"model\" and \"dataset\".\n        platform (Literal[\"hf\", \"huggingface\", \"modelscope\"], optional):\n            Platform to download from. Defaults to \"hf\". Options:\n            - \"hf\" or \"huggingface\": Hugging Face Hub\n            - \"modelscope\": ModelScope platform\n        **kwargs: Additional arguments passed to the underlying download functions.\n\n    Returns:\n        str: Local path to the repository (either existing local path or downloaded cache path).\n\n    Raises:\n        FileNotFoundError: If the repository cannot be found or downloaded from any platform.\n        ValueError: If an unsupported platform is specified.\n        ImportError: If required dependencies for the specified platform are not installed.\n\n    Examples:\n        &gt;&gt;&gt; # Local path (returned as-is)\n        &gt;&gt;&gt; resolve_repo_path(\"/path/to/local/model\")\n        \"/path/to/local/model\"\n\n        &gt;&gt;&gt; # Hugging Face model\n        &gt;&gt;&gt; resolve_repo_path(\"bert-base-uncased\")\n        \"/home/user/.cache/huggingface/hub/models--bert-base-uncased/...\"\n\n        &gt;&gt;&gt; # ModelScope model with explicit platform\n        &gt;&gt;&gt; resolve_repo_path(\"damo/nlp_bert_backbone_base_std\", platform=\"modelscope\")\n        \"/home/user/.cache/modelscope/hub/damo/nlp_bert_backbone_base_std/...\"\n\n        &gt;&gt;&gt; # URL-prefixed repository ID\n        &gt;&gt;&gt; resolve_repo_path(\"hf://microsoft/DialoGPT-medium\")\n        \"/home/user/.cache/huggingface/hub/models--microsoft--DialoGPT-medium/...\"\n    \"\"\"\n    # If it's a HuggingFace Hub model id, download snapshot\n    if repo_id.startswith(\"hf://\") or repo_id.startswith(\"huggingface://\"):\n        repo_id = repo_id.replace(\"hf://\", \"\").replace(\"huggingface://\", \"\")\n        platform = \"hf\"\n    # If it's a ModelScope model id, download snapshot\n    elif repo_id.startswith(\"modelscope://\"):\n        repo_id = repo_id.replace(\"modelscope://\", \"\")\n        platform = \"modelscope\"\n\n    # If it's a local file or directory, return as is\n    if os.path.exists(repo_id):\n        return repo_id\n\n    try:\n        validate_and_suggest_corrections(platform, AVAILABLE_PLATFORMS)\n        # This will download the model to the cache and return the local path\n        if platform in [\"hf\", \"huggingface\"]:\n            local_path = huggingface_snapshot_download(\n                repo_id=repo_id, repo_type=repo_type, **kwargs\n            )\n        elif platform == \"modelscope\":\n            local_path = modelscope_snapshot_download(\n                repo_id=repo_id, repo_type=repo_type, **kwargs\n            )\n        else:\n            _raise_unknown_platform_error(platform)\n        return local_path\n    except Exception as e:\n        raise FileNotFoundError(f\"Could not resolve checkpoint: {repo_id}. Error: {e}\")\n</code></pre>"},{"location":"api/fusion_bench.utils/package_management/","title":"Package Management","text":""},{"location":"api/fusion_bench.utils/package_management/#fusion_bench.utils.packages","title":"<code>fusion_bench.utils.packages</code>","text":""},{"location":"api/fusion_bench.utils/package_management/#fusion_bench.utils.packages.compare_versions","title":"<code>compare_versions(v1, v2)</code>","text":"<p>Compare two version strings. Returns -1 if v1 &lt; v2, 0 if v1 == v2, 1 if v1 &gt; v2</p> Source code in <code>fusion_bench/utils/packages.py</code> <pre><code>def compare_versions(v1: str, v2: str) -&gt; int:\n    \"\"\"Compare two version strings.\n    Returns -1 if v1 &lt; v2, 0 if v1 == v2, 1 if v1 &gt; v2\"\"\"\n\n    v1 = version.parse(v1)\n    v2 = version.parse(v2)\n    if v1 &lt; v2:\n        return -1\n    elif v1 &gt; v2:\n        return 1\n    else:\n        return 0\n</code></pre>"},{"location":"api/fusion_bench.utils/package_management/#fusion_bench.utils.packages.import_object","title":"<code>import_object(abs_obj_name)</code>","text":"<p>Imports a class from a module given the absolute class name.</p> <p>Parameters:</p> <ul> <li> <code>abs_obj_name</code>               (<code>str</code>)           \u2013            <p>The absolute name of the object to import.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The imported class.</p> </li> </ul> Source code in <code>fusion_bench/utils/packages.py</code> <pre><code>def import_object(abs_obj_name: str) -&gt; Any:\n    \"\"\"\n    Imports a class from a module given the absolute class name.\n\n    Args:\n        abs_obj_name (str): The absolute name of the object to import.\n\n    Returns:\n        The imported class.\n    \"\"\"\n    module_name, obj_name = abs_obj_name.rsplit(\".\", 1)\n    module = importlib.import_module(module_name)\n    return getattr(module, obj_name)\n</code></pre>"},{"location":"api/fusion_bench.utils/package_management/#fusion_bench.utils.lazy_imports","title":"<code>fusion_bench.utils.lazy_imports</code>","text":"<p>Lazy-Imports module.</p> <p>This is code taken from the <code>HuggingFace team &lt;https://huggingface.co/&gt;</code>. Many thanks to HuggingFace for <code>your consent &lt;https://github.com/huggingface/transformers/issues/12861#issuecomment-886712209&gt;</code> to publish it as a standalone package.</p>"},{"location":"api/fusion_bench.utils/package_management/#fusion_bench.utils.lazy_imports.LazyImporter","title":"<code>LazyImporter</code>","text":"<p>               Bases: <code>ModuleType</code></p> <p>Lazy importer for modules and their components.</p> <p>This class allows for lazy importing of modules, meaning modules are only imported when they are actually accessed. This can help reduce startup time and memory usage for large packages with many optional dependencies.</p> <p>Attributes:</p> <ul> <li> <code>_modules</code>               (<code>Set[str]</code>)           \u2013            <p>Set of module names available for import.</p> </li> <li> <code>_class_to_module</code>               (<code>Dict[str, str]</code>)           \u2013            <p>Mapping from class/function names to their module names.</p> </li> <li> <code>_objects</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Dictionary of extra objects to include in the module.</p> </li> <li> <code>_name</code>           \u2013            <p>Name of the module.</p> </li> <li> <code>_import_structure</code>           \u2013            <p>Dictionary mapping module names to lists of their exports.</p> </li> </ul> Source code in <code>fusion_bench/utils/lazy_imports.py</code> <pre><code>class LazyImporter(ModuleType):\n    \"\"\"Lazy importer for modules and their components.\n\n    This class allows for lazy importing of modules, meaning modules are only\n    imported when they are actually accessed. This can help reduce startup\n    time and memory usage for large packages with many optional dependencies.\n\n    Attributes:\n        _modules: Set of module names available for import.\n        _class_to_module: Mapping from class/function names to their module names.\n        _objects: Dictionary of extra objects to include in the module.\n        _name: Name of the module.\n        _import_structure: Dictionary mapping module names to lists of their exports.\n    \"\"\"\n\n    # Very heavily inspired by optuna.integration._IntegrationModule\n    # https://github.com/optuna/optuna/blob/master/optuna/integration/__init__.py\n    def __init__(\n        self,\n        name: str,\n        module_file: str,\n        import_structure: Dict[str, List[str]],\n        extra_objects: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the LazyImporter.\n\n        Args:\n            name: The name of the module.\n            module_file: Path to the module file.\n            import_structure: Dictionary mapping module names to lists of their exports.\n            extra_objects: Optional dictionary of extra objects to include.\n        \"\"\"\n        super().__init__(name)\n        self._modules: Set[str] = set(import_structure.keys())\n        self._class_to_module: Dict[str, str] = {}\n        for key, values in import_structure.items():\n            for value in values:\n                self._class_to_module[value] = key\n        # Needed for autocompletion in an IDE\n        self.__all__: List[str] = list(import_structure.keys()) + sum(\n            import_structure.values(), []\n        )\n        self.__file__ = module_file\n        self.__path__ = [os.path.dirname(module_file)]\n        self._objects: Dict[str, Any] = {} if extra_objects is None else extra_objects\n        self._name = name\n        self._import_structure = import_structure\n\n    # Needed for autocompletion in an IDE\n    def __dir__(self) -&gt; List[str]:\n        \"\"\"Return list of available attributes for autocompletion.\n\n        Returns:\n            List of all available attribute names.\n        \"\"\"\n        return super().__dir__() + self.__all__\n\n    def __getattr__(self, name: str) -&gt; Any:\n        \"\"\"Get attribute lazily, importing the module if necessary.\n\n        Args:\n            name: The name of the attribute to retrieve.\n\n        Returns:\n            The requested attribute.\n\n        Raises:\n            AttributeError: If the attribute is not found in any module.\n        \"\"\"\n        if name in self._objects:\n            return self._objects[name]\n        if name in self._modules:\n            value = self._get_module(name)\n        elif name in self._class_to_module:\n            module = self._get_module(self._class_to_module[name])\n            value = getattr(module, name)\n        else:\n            raise AttributeError(f\"module {self.__name__} has no attribute {name}\")\n\n        setattr(self, name, value)\n        return value\n\n    def _get_module(self, module_name: str) -&gt; ModuleType:\n        \"\"\"Import and return the specified module.\n\n        Args:\n            module_name: Name of the module to import.\n\n        Returns:\n            The imported module.\n        \"\"\"\n        return importlib.import_module(\".\" + module_name, self.__name__)\n\n    def __reduce__(self) -&gt; tuple:\n        \"\"\"Support for pickling the LazyImporter.\n\n        Returns:\n            Tuple containing the class and arguments needed to reconstruct the object.\n        \"\"\"\n        return (self.__class__, (self._name, self.__file__, self._import_structure))\n</code></pre>"},{"location":"api/fusion_bench.utils/package_management/#fusion_bench.utils.lazy_imports.LazyImporter.__dir__","title":"<code>__dir__()</code>","text":"<p>Return list of available attributes for autocompletion.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of all available attribute names.</p> </li> </ul> Source code in <code>fusion_bench/utils/lazy_imports.py</code> <pre><code>def __dir__(self) -&gt; List[str]:\n    \"\"\"Return list of available attributes for autocompletion.\n\n    Returns:\n        List of all available attribute names.\n    \"\"\"\n    return super().__dir__() + self.__all__\n</code></pre>"},{"location":"api/fusion_bench.utils/package_management/#fusion_bench.utils.lazy_imports.LazyImporter.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Get attribute lazily, importing the module if necessary.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the attribute to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The requested attribute.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AttributeError</code>             \u2013            <p>If the attribute is not found in any module.</p> </li> </ul> Source code in <code>fusion_bench/utils/lazy_imports.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Get attribute lazily, importing the module if necessary.\n\n    Args:\n        name: The name of the attribute to retrieve.\n\n    Returns:\n        The requested attribute.\n\n    Raises:\n        AttributeError: If the attribute is not found in any module.\n    \"\"\"\n    if name in self._objects:\n        return self._objects[name]\n    if name in self._modules:\n        value = self._get_module(name)\n    elif name in self._class_to_module:\n        module = self._get_module(self._class_to_module[name])\n        value = getattr(module, name)\n    else:\n        raise AttributeError(f\"module {self.__name__} has no attribute {name}\")\n\n    setattr(self, name, value)\n    return value\n</code></pre>"},{"location":"api/fusion_bench.utils/package_management/#fusion_bench.utils.lazy_imports.LazyImporter.__init__","title":"<code>__init__(name, module_file, import_structure, extra_objects=None)</code>","text":"<p>Initialize the LazyImporter.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the module.</p> </li> <li> <code>module_file</code>               (<code>str</code>)           \u2013            <p>Path to the module file.</p> </li> <li> <code>import_structure</code>               (<code>Dict[str, List[str]]</code>)           \u2013            <p>Dictionary mapping module names to lists of their exports.</p> </li> <li> <code>extra_objects</code>               (<code>Optional[Dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary of extra objects to include.</p> </li> </ul> Source code in <code>fusion_bench/utils/lazy_imports.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    module_file: str,\n    import_structure: Dict[str, List[str]],\n    extra_objects: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"Initialize the LazyImporter.\n\n    Args:\n        name: The name of the module.\n        module_file: Path to the module file.\n        import_structure: Dictionary mapping module names to lists of their exports.\n        extra_objects: Optional dictionary of extra objects to include.\n    \"\"\"\n    super().__init__(name)\n    self._modules: Set[str] = set(import_structure.keys())\n    self._class_to_module: Dict[str, str] = {}\n    for key, values in import_structure.items():\n        for value in values:\n            self._class_to_module[value] = key\n    # Needed for autocompletion in an IDE\n    self.__all__: List[str] = list(import_structure.keys()) + sum(\n        import_structure.values(), []\n    )\n    self.__file__ = module_file\n    self.__path__ = [os.path.dirname(module_file)]\n    self._objects: Dict[str, Any] = {} if extra_objects is None else extra_objects\n    self._name = name\n    self._import_structure = import_structure\n</code></pre>"},{"location":"api/fusion_bench.utils/package_management/#fusion_bench.utils.lazy_imports.LazyImporter.__reduce__","title":"<code>__reduce__()</code>","text":"<p>Support for pickling the LazyImporter.</p> <p>Returns:</p> <ul> <li> <code>tuple</code>           \u2013            <p>Tuple containing the class and arguments needed to reconstruct the object.</p> </li> </ul> Source code in <code>fusion_bench/utils/lazy_imports.py</code> <pre><code>def __reduce__(self) -&gt; tuple:\n    \"\"\"Support for pickling the LazyImporter.\n\n    Returns:\n        Tuple containing the class and arguments needed to reconstruct the object.\n    \"\"\"\n    return (self.__class__, (self._name, self.__file__, self._import_structure))\n</code></pre>"},{"location":"api/fusion_bench.utils/package_management/#fusion_bench.utils.lazy_imports.LazyPyModule","title":"<code>LazyPyModule</code>","text":"<p>               Bases: <code>ModuleType</code></p> <p>Module wrapper for lazy import.</p> <p>Adapted from Optuna: https://github.com/optuna/optuna/blob/1f92d496b0c4656645384e31539e4ee74992ff55/optuna/init.py</p> <p>This class wraps specified module and lazily import it when they are actually accessed. This can help reduce startup time and memory usage by deferring module imports until they are needed.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name of module to apply lazy import.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>_name</code>               (<code>str</code>)           \u2013            <p>The name of the module to be lazily imported.</p> </li> </ul> Source code in <code>fusion_bench/utils/lazy_imports.py</code> <pre><code>class LazyPyModule(ModuleType):\n    \"\"\"Module wrapper for lazy import.\n\n    Adapted from Optuna: https://github.com/optuna/optuna/blob/1f92d496b0c4656645384e31539e4ee74992ff55/optuna/__init__.py\n\n    This class wraps specified module and lazily import it when they are actually accessed.\n    This can help reduce startup time and memory usage by deferring module imports\n    until they are needed.\n\n    Args:\n        name: Name of module to apply lazy import.\n\n    Attributes:\n        _name: The name of the module to be lazily imported.\n    \"\"\"\n\n    def __init__(self, name: str) -&gt; None:\n        \"\"\"Initialize the LazyPyModule.\n\n        Args:\n            name: The name of the module to be lazily imported.\n        \"\"\"\n        super().__init__(name)\n        self._name: str = name\n\n    def _load(self) -&gt; ModuleType:\n        \"\"\"Load the actual module and update this object's dictionary.\n\n        Returns:\n            The loaded module.\n        \"\"\"\n        module = importlib.import_module(self._name)\n        self.__dict__.update(module.__dict__)\n        return module\n\n    def __getattr__(self, item: str) -&gt; Any:\n        \"\"\"Get attribute from the lazily loaded module.\n\n        Args:\n            item: The name of the attribute to retrieve.\n\n        Returns:\n            The requested attribute from the loaded module.\n        \"\"\"\n        return getattr(self._load(), item)\n</code></pre>"},{"location":"api/fusion_bench.utils/package_management/#fusion_bench.utils.lazy_imports.LazyPyModule.__getattr__","title":"<code>__getattr__(item)</code>","text":"<p>Get attribute from the lazily loaded module.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str</code>)           \u2013            <p>The name of the attribute to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The requested attribute from the loaded module.</p> </li> </ul> Source code in <code>fusion_bench/utils/lazy_imports.py</code> <pre><code>def __getattr__(self, item: str) -&gt; Any:\n    \"\"\"Get attribute from the lazily loaded module.\n\n    Args:\n        item: The name of the attribute to retrieve.\n\n    Returns:\n        The requested attribute from the loaded module.\n    \"\"\"\n    return getattr(self._load(), item)\n</code></pre>"},{"location":"api/fusion_bench.utils/package_management/#fusion_bench.utils.lazy_imports.LazyPyModule.__init__","title":"<code>__init__(name)</code>","text":"<p>Initialize the LazyPyModule.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the module to be lazily imported.</p> </li> </ul> Source code in <code>fusion_bench/utils/lazy_imports.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    \"\"\"Initialize the LazyPyModule.\n\n    Args:\n        name: The name of the module to be lazily imported.\n    \"\"\"\n    super().__init__(name)\n    self._name: str = name\n</code></pre>"},{"location":"api/fusion_bench.utils/profiling/","title":"Profiling Utilities","text":""},{"location":"api/fusion_bench.utils/profiling/#fusion_bench.utils.timer","title":"<code>fusion_bench.utils.timer</code>","text":""},{"location":"api/fusion_bench.utils/profiling/#fusion_bench.utils.timer.timeit_context","title":"<code>timeit_context</code>","text":"<p>A context manager for measuring and logging execution time of code blocks.</p> <p>This context manager provides precise timing measurements with automatic logging of elapsed time. It supports nested timing contexts with proper indentation for hierarchical timing analysis, making it ideal for profiling complex operations with multiple sub-components.</p> <p>Parameters:</p> <ul> <li> <code>msg</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Custom message to identify the timed code block. If provided, logs \"[BEGIN] {msg}\" at start and includes context in the final timing report. Defaults to None.</p> </li> <li> <code>loglevel</code>               (<code>int</code>, default:                   <code>INFO</code> )           \u2013            <p>Python logging level for output messages. Uses standard logging levels (DEBUG=10, INFO=20, WARNING=30, etc.). Defaults to logging.INFO.</p> </li> </ul> Example <p>Basic usage: <pre><code>with timeit_context(\"data loading\"):\n    data = load_large_dataset()\n# Logs: [BEGIN] data loading\n# Logs: [END]   Elapsed time: 2.34s\n</code></pre></p> <p>Nested timing: <pre><code>with timeit_context(\"model training\"):\n    with timeit_context(\"data preprocessing\"):\n        preprocess_data()\n    with timeit_context(\"forward pass\"):\n        model(data)\n# Output shows nested structure:\n# [BEGIN] model training\n#   [BEGIN] data preprocessing\n#   [END]   Elapsed time: 0.15s\n#   [BEGIN] forward pass\n#   [END]   Elapsed time: 0.89s\n# [END]   Elapsed time: 1.04s\n</code></pre></p> <p>Custom log level: <pre><code>with timeit_context(\"debug operation\", loglevel=logging.DEBUG):\n    debug_function()\n</code></pre></p> Source code in <code>fusion_bench/utils/timer.py</code> <pre><code>class timeit_context:\n    \"\"\"\n    A context manager for measuring and logging execution time of code blocks.\n\n    This context manager provides precise timing measurements with automatic logging\n    of elapsed time. It supports nested timing contexts with proper indentation\n    for hierarchical timing analysis, making it ideal for profiling complex\n    operations with multiple sub-components.\n\n    Args:\n        msg (str, optional): Custom message to identify the timed code block.\n            If provided, logs \"[BEGIN] {msg}\" at start and includes context\n            in the final timing report. Defaults to None.\n        loglevel (int, optional): Python logging level for output messages.\n            Uses standard logging levels (DEBUG=10, INFO=20, WARNING=30, etc.).\n            Defaults to logging.INFO.\n\n    Example:\n        Basic usage:\n        ```python\n        with timeit_context(\"data loading\"):\n            data = load_large_dataset()\n        # Logs: [BEGIN] data loading\n        # Logs: [END]   Elapsed time: 2.34s\n        ```\n\n        Nested timing:\n        ```python\n        with timeit_context(\"model training\"):\n            with timeit_context(\"data preprocessing\"):\n                preprocess_data()\n            with timeit_context(\"forward pass\"):\n                model(data)\n        # Output shows nested structure:\n        # [BEGIN] model training\n        #   [BEGIN] data preprocessing\n        #   [END]   Elapsed time: 0.15s\n        #   [BEGIN] forward pass\n        #   [END]   Elapsed time: 0.89s\n        # [END]   Elapsed time: 1.04s\n        ```\n\n        Custom log level:\n        ```python\n        with timeit_context(\"debug operation\", loglevel=logging.DEBUG):\n            debug_function()\n        ```\n    \"\"\"\n\n    nest_level = -1\n\n    def _log(self, msg):\n        \"\"\"\n        Internal method for logging messages with appropriate stack level.\n\n        This helper method ensures that log messages appear to originate from\n        the caller's code rather than from internal timer methods, providing\n        more useful debugging information.\n\n        Args:\n            msg (str): The message to log at the configured log level.\n        \"\"\"\n        log.log(self.loglevel, msg, stacklevel=3)\n\n    def __init__(self, msg: str = None, loglevel=logging.INFO) -&gt; None:\n        \"\"\"\n        Initialize a new timing context with optional message and log level.\n\n        Args:\n            msg (str, optional): Descriptive message for the timed operation.\n                If provided, will be included in the begin/end log messages\n                to help identify what is being timed. Defaults to None.\n            loglevel (int, optional): Python logging level for timer output.\n                Common values include:\n                - logging.DEBUG (10): Detailed debugging information\n                - logging.INFO (20): General information (default)\n                - logging.WARNING (30): Warning messages\n                - logging.ERROR (40): Error messages\n                Defaults to logging.INFO.\n        \"\"\"\n        self.loglevel = loglevel\n        self.msg = msg\n\n    def __enter__(self) -&gt; None:\n        \"\"\"\n        Enter the timing context and start the timer.\n\n        This method is automatically called when entering the 'with' statement.\n        It records the current timestamp, increments the nesting level for\n        proper log indentation, and optionally logs a begin message.\n\n        Returns:\n            None: This context manager doesn't return a value to the 'as' clause.\n                  All timing information is handled internally and logged automatically.\n        \"\"\"\n        self.start_time = time.time()\n        timeit_context.nest_level += 1\n        if self.msg is not None:\n            self._log(\"  \" * timeit_context.nest_level + \"[BEGIN] \" + str(self.msg))\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        \"\"\"\n        Exit the timing context and log the elapsed time.\n\n        This method is automatically called when exiting the 'with' statement,\n        whether through normal completion or exception. It calculates the total\n        elapsed time and logs the results with proper nesting indentation.\n\n        Args:\n            exc_type (type): Exception type if an exception occurred, None otherwise.\n            exc_val (Exception): Exception instance if an exception occurred, None otherwise.\n            exc_tb (traceback): Exception traceback if an exception occurred, None otherwise.\n\n        Returns:\n            None: Does not suppress exceptions (returns None/False implicitly).\n                  Any exceptions that occurred in the timed block will propagate normally.\n        \"\"\"\n        end_time = time.time()\n        elapsed_time = end_time - self.start_time\n        self._log(\n            \"  \" * timeit_context.nest_level\n            + \"[END]   \"\n            + str(f\"Elapsed time: {elapsed_time:.2f}s\")\n        )\n        timeit_context.nest_level -= 1\n</code></pre>"},{"location":"api/fusion_bench.utils/profiling/#fusion_bench.utils.timer.timeit_context.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter the timing context and start the timer.</p> <p>This method is automatically called when entering the 'with' statement. It records the current timestamp, increments the nesting level for proper log indentation, and optionally logs a begin message.</p> <p>Returns:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            <p>This context manager doesn't return a value to the 'as' clause.   All timing information is handled internally and logged automatically.</p> </li> </ul> Source code in <code>fusion_bench/utils/timer.py</code> <pre><code>def __enter__(self) -&gt; None:\n    \"\"\"\n    Enter the timing context and start the timer.\n\n    This method is automatically called when entering the 'with' statement.\n    It records the current timestamp, increments the nesting level for\n    proper log indentation, and optionally logs a begin message.\n\n    Returns:\n        None: This context manager doesn't return a value to the 'as' clause.\n              All timing information is handled internally and logged automatically.\n    \"\"\"\n    self.start_time = time.time()\n    timeit_context.nest_level += 1\n    if self.msg is not None:\n        self._log(\"  \" * timeit_context.nest_level + \"[BEGIN] \" + str(self.msg))\n</code></pre>"},{"location":"api/fusion_bench.utils/profiling/#fusion_bench.utils.timer.timeit_context.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Exit the timing context and log the elapsed time.</p> <p>This method is automatically called when exiting the 'with' statement, whether through normal completion or exception. It calculates the total elapsed time and logs the results with proper nesting indentation.</p> <p>Parameters:</p> <ul> <li> <code>exc_type</code>               (<code>type</code>)           \u2013            <p>Exception type if an exception occurred, None otherwise.</p> </li> <li> <code>exc_val</code>               (<code>Exception</code>)           \u2013            <p>Exception instance if an exception occurred, None otherwise.</p> </li> <li> <code>exc_tb</code>               (<code>traceback</code>)           \u2013            <p>Exception traceback if an exception occurred, None otherwise.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            <p>Does not suppress exceptions (returns None/False implicitly).   Any exceptions that occurred in the timed block will propagate normally.</p> </li> </ul> Source code in <code>fusion_bench/utils/timer.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n    \"\"\"\n    Exit the timing context and log the elapsed time.\n\n    This method is automatically called when exiting the 'with' statement,\n    whether through normal completion or exception. It calculates the total\n    elapsed time and logs the results with proper nesting indentation.\n\n    Args:\n        exc_type (type): Exception type if an exception occurred, None otherwise.\n        exc_val (Exception): Exception instance if an exception occurred, None otherwise.\n        exc_tb (traceback): Exception traceback if an exception occurred, None otherwise.\n\n    Returns:\n        None: Does not suppress exceptions (returns None/False implicitly).\n              Any exceptions that occurred in the timed block will propagate normally.\n    \"\"\"\n    end_time = time.time()\n    elapsed_time = end_time - self.start_time\n    self._log(\n        \"  \" * timeit_context.nest_level\n        + \"[END]   \"\n        + str(f\"Elapsed time: {elapsed_time:.2f}s\")\n    )\n    timeit_context.nest_level -= 1\n</code></pre>"},{"location":"api/fusion_bench.utils/profiling/#fusion_bench.utils.timer.timeit_context.__init__","title":"<code>__init__(msg=None, loglevel=logging.INFO)</code>","text":"<p>Initialize a new timing context with optional message and log level.</p> <p>Parameters:</p> <ul> <li> <code>msg</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Descriptive message for the timed operation. If provided, will be included in the begin/end log messages to help identify what is being timed. Defaults to None.</p> </li> <li> <code>loglevel</code>               (<code>int</code>, default:                   <code>INFO</code> )           \u2013            <p>Python logging level for timer output. Common values include: - logging.DEBUG (10): Detailed debugging information - logging.INFO (20): General information (default) - logging.WARNING (30): Warning messages - logging.ERROR (40): Error messages Defaults to logging.INFO.</p> </li> </ul> Source code in <code>fusion_bench/utils/timer.py</code> <pre><code>def __init__(self, msg: str = None, loglevel=logging.INFO) -&gt; None:\n    \"\"\"\n    Initialize a new timing context with optional message and log level.\n\n    Args:\n        msg (str, optional): Descriptive message for the timed operation.\n            If provided, will be included in the begin/end log messages\n            to help identify what is being timed. Defaults to None.\n        loglevel (int, optional): Python logging level for timer output.\n            Common values include:\n            - logging.DEBUG (10): Detailed debugging information\n            - logging.INFO (20): General information (default)\n            - logging.WARNING (30): Warning messages\n            - logging.ERROR (40): Error messages\n            Defaults to logging.INFO.\n    \"\"\"\n    self.loglevel = loglevel\n    self.msg = msg\n</code></pre>"},{"location":"api/fusion_bench.utils/torch/","title":"PyTorch Utilities","text":""},{"location":"api/fusion_bench.utils/torch/#device-management","title":"Device Management","text":""},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.devices","title":"<code>fusion_bench.utils.devices</code>","text":""},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.devices.cleanup_cuda","title":"<code>cleanup_cuda()</code>","text":"<p>Call gc collect, empty CUDA cache, and reset peak memory stats.</p> Source code in <code>fusion_bench/utils/devices.py</code> <pre><code>def cleanup_cuda():\n    \"\"\"\n    Call gc collect, empty CUDA cache, and reset peak memory stats.\n    \"\"\"\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n</code></pre>"},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.devices.clear_cuda_cache","title":"<code>clear_cuda_cache()</code>","text":"<p>Clears the CUDA memory cache to free up GPU memory. Works only if CUDA is available.</p> Source code in <code>fusion_bench/utils/devices.py</code> <pre><code>def clear_cuda_cache():\n    \"\"\"\n    Clears the CUDA memory cache to free up GPU memory.\n    Works only if CUDA is available.\n    \"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    else:\n        log.warning(\"CUDA is not available. No cache to clear.\")\n</code></pre>"},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.devices.get_current_device","title":"<code>get_current_device()</code>","text":"<p>Gets the current available device for PyTorch operations. This is used for distributed training.</p> <p>This function checks the availability of various types of devices in the following order: 1. XPU (Intel's AI accelerator) 2. NPU (Neural Processing Unit) 3. MPS (Metal Performance Shaders, for Apple devices) 4. CUDA (NVIDIA's GPU) 5. CPU (Central Processing Unit, used as a fallback)</p> <p>The function returns the first available device found in the above order. If none of the specialized devices are available, it defaults to the CPU.</p> <p>Returns:</p> <ul> <li> <code>device</code>           \u2013            <p>torch.device: The current available device for PyTorch operations.</p> </li> </ul> Environment Variables <p>LOCAL_RANK: This environment variable is used to specify the device index for multi-device setups.             If not set, it defaults to \"0\".</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; device = get_current_device()\n&gt;&gt;&gt; print(device)\nxpu:0  # or npu:0, mps:0, cuda:0, cpu depending on availability\n</code></pre> Source code in <code>fusion_bench/utils/devices.py</code> <pre><code>def get_current_device() -&gt; torch.device:\n    R\"\"\"\n    Gets the current available device for PyTorch operations.\n    This is used for distributed training.\n\n    This function checks the availability of various types of devices in the following order:\n    1. XPU (Intel's AI accelerator)\n    2. NPU (Neural Processing Unit)\n    3. MPS (Metal Performance Shaders, for Apple devices)\n    4. CUDA (NVIDIA's GPU)\n    5. CPU (Central Processing Unit, used as a fallback)\n\n    The function returns the first available device found in the above order. If none of the specialized devices\n    are available, it defaults to the CPU.\n\n    Returns:\n        torch.device: The current available device for PyTorch operations.\n\n    Environment Variables:\n        LOCAL_RANK: This environment variable is used to specify the device index for multi-device setups.\n                    If not set, it defaults to \"0\".\n\n    Example:\n\n        &gt;&gt;&gt; device = get_current_device()\n        &gt;&gt;&gt; print(device)\n        xpu:0  # or npu:0, mps:0, cuda:0, cpu depending on availability\n    \"\"\"\n\n    if is_torch_xpu_available():\n        device = \"xpu:{}\".format(os.environ.get(\"LOCAL_RANK\", \"0\"))\n    elif is_torch_npu_available():\n        device = \"npu:{}\".format(os.environ.get(\"LOCAL_RANK\", \"0\"))\n    elif is_torch_mps_available():\n        device = \"mps:{}\".format(os.environ.get(\"LOCAL_RANK\", \"0\"))\n    elif is_torch_cuda_available():\n        device = \"cuda:{}\".format(os.environ.get(\"LOCAL_RANK\", \"0\"))\n    else:\n        device = \"cpu\"\n\n    return torch.device(device)\n</code></pre>"},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.devices.get_device","title":"<code>get_device(obj)</code>","text":"<p>Get the device of a given object.</p> <p>Parameters:</p> <ul> <li> <code>obj</code>               (<code>Any</code>)           \u2013            <p>The object whose device is to be determined.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>device</code>           \u2013            <p>torch.device: The device of the given object.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the object type is not supported.</p> </li> </ul> Source code in <code>fusion_bench/utils/devices.py</code> <pre><code>def get_device(obj: Any) -&gt; torch.device:\n    \"\"\"\n    Get the device of a given object.\n\n    Args:\n        obj: The object whose device is to be determined.\n\n    Returns:\n        torch.device: The device of the given object.\n\n    Raises:\n        ValueError: If the object type is not supported.\n    \"\"\"\n    if isinstance(obj, torch.Tensor):\n        return obj.device\n    elif isinstance(obj, torch.nn.Module):\n        if hasattr(obj, \"device\"):\n            return obj.device\n        else:\n            return next(iter(obj.parameters())).device\n    elif isinstance(obj, torch.device):\n        return obj\n    else:\n        raise ValueError(f\"Unsupported object type: {type(obj)}\")\n</code></pre>"},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.devices.get_device_capabilities","title":"<code>get_device_capabilities(device)</code>","text":"<p>Get capabilities information for a given device.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device for which to get capabilities information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>A dictionary containing capabilities information for the given device.</p> </li> </ul> Source code in <code>fusion_bench/utils/devices.py</code> <pre><code>def get_device_capabilities(device: torch.device) -&gt; dict:\n    \"\"\"\n    Get capabilities information for a given device.\n\n    Args:\n        device (torch.device): The device for which to get capabilities information.\n\n    Returns:\n        dict: A dictionary containing capabilities information for the given device.\n    \"\"\"\n    if device.type == \"cuda\":\n        return {\n            \"name\": torch.cuda.get_device_name(device),\n            \"capability\": torch.cuda.get_device_capability(device),\n            \"total_memory\": torch.cuda.get_device_properties(device).total_memory,\n            \"multi_processor_count\": torch.cuda.get_device_properties(\n                device\n            ).multi_processor_count,\n        }\n    else:\n        raise ValueError(\n            f\"Capabilities information not available for device type: {device.type}\"\n        )\n</code></pre>"},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.devices.get_device_memory_info","title":"<code>get_device_memory_info(device, reset_stats=True)</code>","text":"<p>Get memory information for a given device.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device for which to get memory information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>A dictionary containing memory information for the given device.</p> </li> </ul> Source code in <code>fusion_bench/utils/devices.py</code> <pre><code>def get_device_memory_info(device: torch.device, reset_stats: bool = True) -&gt; dict:\n    \"\"\"\n    Get memory information for a given device.\n\n    Args:\n        device (torch.device): The device for which to get memory information.\n\n    Returns:\n        dict: A dictionary containing memory information for the given device.\n    \"\"\"\n    if device.type == \"cuda\":\n        total_memory = torch.cuda.get_device_properties(device).total_memory\n        reserved_memory = torch.cuda.memory_reserved(device)\n        allocated_memory = torch.cuda.memory_allocated(device)\n        peak_memory_active = torch.cuda.memory_stats(device).get(\n            \"active_bytes.all.peak\", 0\n        )\n        peak_mem_alloc = torch.cuda.max_memory_allocated(device)\n        peak_mem_reserved = torch.cuda.max_memory_reserved(device)\n\n        if reset_stats:\n            torch.cuda.reset_peak_memory_stats(device)\n\n        return {\n            \"total_memory\": total_memory,\n            \"reserved_memory\": reserved_memory,\n            \"allocated_memory\": allocated_memory,\n            \"peak_memory_active\": peak_memory_active,\n            \"peak_memory_allocated\": peak_mem_alloc,\n            \"peak_memory_reserved\": peak_mem_reserved,\n        }\n    else:\n        raise ValueError(\n            f\"Memory information not available for device type: {device.type}\"\n        )\n</code></pre>"},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.devices.num_devices","title":"<code>num_devices(devices)</code>","text":"<p>Return the number of devices.</p> <p>Parameters:</p> <ul> <li> <code>devices</code>               (<code>Union[int, List[int], str]</code>)           \u2013            <p><code>devices</code> can be a single int to specify the number of devices, or a list of device ids, e.g. [0, 1, 2, 3], or a str of device ids, e.g. \"0,1,2,3\" and \"[0, 1, 2]\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The number of devices.</p> </li> </ul> Source code in <code>fusion_bench/utils/devices.py</code> <pre><code>def num_devices(devices: Union[int, List[int], str]) -&gt; int:\n    \"\"\"\n    Return the number of devices.\n\n    Args:\n        devices: `devices` can be a single int to specify the number of devices, or a list of device ids, e.g. [0, 1, 2, 3], or a str of device ids, e.g. \"0,1,2,3\" and \"[0, 1, 2]\".\n\n    Returns:\n        The number of devices.\n    \"\"\"\n    if isinstance(devices, int):\n        return devices\n    elif isinstance(devices, str):\n        return len(devices.split(\",\"))\n    elif isinstance(devices, list):\n        return len(devices)\n    else:\n        raise TypeError(\n            f\"devices must be a single int or a list of ints, but got {type(devices)}\"\n        )\n</code></pre>"},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.devices.print_memory_usage","title":"<code>print_memory_usage(print_fn=print)</code>","text":"<p>Print the current GPU memory usage.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A string containing the allocated and cached memory in MB.</p> </li> </ul> Source code in <code>fusion_bench/utils/devices.py</code> <pre><code>def print_memory_usage(print_fn=print) -&gt; str:\n    \"\"\"\n    Print the current GPU memory usage.\n\n    Returns:\n        str: A string containing the allocated and cached memory in MB.\n    \"\"\"\n    allocated = torch.cuda.memory_allocated() / 1024**2  # \u8f6c\u6362\u4e3a MB\n    cached = torch.cuda.memory_reserved() / 1024**2  # \u8f6c\u6362\u4e3a MB\n    print_str = f\"Allocated Memory: {allocated:.2f} MB\\nCached Memory: {cached:.2f} MB\"\n    print_fn(print_str)\n    return print_str\n</code></pre>"},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.devices.to_device","title":"<code>to_device(obj, device, copy_on_move=False, **kwargs)</code>","text":"<p>Move a given object to the specified device.</p> <p>This function recursively moves tensors, modules, lists, tuples, and dictionaries to the specified device. For unsupported types, the object is returned as is.</p> <p>Parameters:</p> <ul> <li> <code>obj</code>               (<code>T</code>)           \u2013            <p>The object to be moved to the device. This can be a torch.Tensor, torch.nn.Module, list, tuple, or dict.</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The target device to move the object to. This can be <code>None</code>.</p> </li> <li> <code>copy_on_move</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force a copy operation when moving tensors to a different device. If True, tensors will be copied when moved to a different device (copy=True is passed to tensor.to()). If False (default), tensors are moved without forcing a copy operation, allowing PyTorch to optimize the operation. This parameter only affects torch.Tensor objects; modules and other types are unaffected. Defaults to False.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments to be passed to the <code>to</code> method of torch.Tensor or torch.nn.Module. For example, <code>non_blocking=True</code>, <code>dtype=torch.float16</code>. Note that if <code>copy_on_move=True</code>, the <code>copy</code> keyword argument will be automatically set and should not be provided manually.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>T</code>           \u2013            <p>The object moved to the specified device. The type of the returned object matches the type of the input object.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n&gt;&gt;&gt; to_device(tensor, torch.device('cuda'))\ntensor([1, 2, 3], device='cuda:0')\n\n&gt;&gt;&gt; model = torch.nn.Linear(2, 2)\n&gt;&gt;&gt; to_device(model, torch.device('cuda'))\nLinear(..., device='cuda:0')\n\n&gt;&gt;&gt; data = [torch.tensor([1, 2]), torch.tensor([3, 4])]\n&gt;&gt;&gt; to_device(data, torch.device('cuda'))\n[tensor([1, 2], device='cuda:0'), tensor([3, 4], device='cuda:0')]\n\n&gt;&gt;&gt; # Force copy when moving to different device\n&gt;&gt;&gt; tensor = torch.tensor([1, 2, 3], device='cpu')\n&gt;&gt;&gt; copied_tensor = to_device(tensor, torch.device('cuda'), copy_on_move=True)\n&gt;&gt;&gt; # tensor and copied_tensor will have different memory locations\n</code></pre> Source code in <code>fusion_bench/utils/devices.py</code> <pre><code>def to_device(\n    obj: T,\n    device: Optional[torch.device],\n    copy_on_move: bool = False,\n    **kwargs: Any,\n) -&gt; T:\n    \"\"\"\n    Move a given object to the specified device.\n\n    This function recursively moves tensors, modules, lists, tuples, and dictionaries to the specified device.\n    For unsupported types, the object is returned as is.\n\n    Args:\n        obj: The object to be moved to the device. This can be a torch.Tensor, torch.nn.Module, list, tuple, or dict.\n        device (torch.device): The target device to move the object to. This can be `None`.\n        copy_on_move (bool, optional): Whether to force a copy operation when moving tensors to a different device.\n            If True, tensors will be copied when moved to a different device (copy=True is passed to tensor.to()).\n            If False (default), tensors are moved without forcing a copy operation, allowing PyTorch to optimize\n            the operation. This parameter only affects torch.Tensor objects; modules and other types are unaffected.\n            Defaults to False.\n        **kwargs: Additional keyword arguments to be passed to the `to` method of torch.Tensor or torch.nn.Module.\n            For example, `non_blocking=True`, `dtype=torch.float16`. Note that if `copy_on_move=True`, the `copy`\n            keyword argument will be automatically set and should not be provided manually.\n\n    Returns:\n        The object moved to the specified device. The type of the returned object matches the type of the input object.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; tensor = torch.tensor([1, 2, 3])\n        &gt;&gt;&gt; to_device(tensor, torch.device('cuda'))\n        tensor([1, 2, 3], device='cuda:0')\n\n        &gt;&gt;&gt; model = torch.nn.Linear(2, 2)\n        &gt;&gt;&gt; to_device(model, torch.device('cuda'))\n        Linear(..., device='cuda:0')\n\n        &gt;&gt;&gt; data = [torch.tensor([1, 2]), torch.tensor([3, 4])]\n        &gt;&gt;&gt; to_device(data, torch.device('cuda'))\n        [tensor([1, 2], device='cuda:0'), tensor([3, 4], device='cuda:0')]\n\n        &gt;&gt;&gt; # Force copy when moving to different device\n        &gt;&gt;&gt; tensor = torch.tensor([1, 2, 3], device='cpu')\n        &gt;&gt;&gt; copied_tensor = to_device(tensor, torch.device('cuda'), copy_on_move=True)\n        &gt;&gt;&gt; # tensor and copied_tensor will have different memory locations\n        ```\n    \"\"\"\n    if isinstance(obj, torch.Tensor):\n        if copy_on_move:\n            if obj.device != torch.device(device):\n                kwargs[\"copy\"] = True\n        return obj.to(device, **kwargs)\n    elif isinstance(obj, torch.nn.Module):\n        return obj.to(device, **kwargs)\n    elif isinstance(obj, list):\n        return [to_device(o, device, **kwargs) for o in obj]\n    elif isinstance(obj, tuple):\n        return tuple(to_device(o, device, **kwargs) for o in obj)\n    elif isinstance(obj, dict):\n        return {key: to_device(value, device, **kwargs) for key, value in obj.items()}\n    else:\n        # the default behavior is to return the object as is\n        return obj\n</code></pre>"},{"location":"api/fusion_bench.utils/torch/#dtype","title":"Dtype","text":""},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.dtype","title":"<code>fusion_bench.utils.dtype</code>","text":""},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.dtype.get_dtype","title":"<code>get_dtype(obj)</code>","text":"<p>Get the data type (dtype) of a given object.</p> <p>Returns:</p> <ul> <li> <code>dtype</code>           \u2013            <p>torch.dtype: The data type of the given object.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the object type is not supported.</p> </li> </ul> Source code in <code>fusion_bench/utils/dtype.py</code> <pre><code>def get_dtype(obj) -&gt; torch.dtype:\n    \"\"\"\n    Get the data type (dtype) of a given object.\n\n    Returns:\n        torch.dtype: The data type of the given object.\n\n    Raises:\n        ValueError: If the object type is not supported.\n    \"\"\"\n    if isinstance(obj, torch.Tensor):\n        return obj.dtype\n    elif isinstance(obj, torch.nn.Module):\n        if hasattr(obj, \"dtype\"):\n            return obj.dtype\n        else:\n            return next(iter(obj.parameters())).dtype\n    elif isinstance(obj, (torch.device, str)):\n        return parse_dtype(obj)\n    else:\n        raise ValueError(f\"Unsupported object type: {type(obj)}\")\n</code></pre>"},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.dtype.infer_optim_dtype","title":"<code>infer_optim_dtype(model_dtype)</code>","text":"<p>Infers the optimal dtype according to the model_dtype and device compatibility.</p> Source code in <code>fusion_bench/utils/dtype.py</code> <pre><code>def infer_optim_dtype(model_dtype: \"torch.dtype\") -&gt; \"torch.dtype\":\n    r\"\"\"\n    Infers the optimal dtype according to the model_dtype and device compatibility.\n    \"\"\"\n    _is_fp16_available = is_torch_npu_available() or is_torch_cuda_available()\n    try:\n        _is_bf16_available = is_torch_bf16_gpu_available() or (\n            is_torch_npu_available() and torch.npu.is_bf16_supported()\n        )\n    except Exception:\n        _is_bf16_available = False\n\n    if _is_bf16_available and model_dtype == torch.bfloat16:\n        return torch.bfloat16\n    elif _is_fp16_available:\n        return torch.float16\n    else:\n        return torch.float32\n</code></pre>"},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.dtype.parse_dtype","title":"<code>parse_dtype(dtype)</code>","text":"<p>Parses a string representation of a data type and returns the corresponding torch.dtype.</p> <p>Parameters:</p> <ul> <li> <code>dtype</code>               (<code>Optional[str]</code>)           \u2013            <p>The string representation of the data type.                    Can be one of \"float32\", \"float\", \"float64\", \"double\",                    \"float16\", \"half\", \"bfloat16\", or \"bf16\".                    If None, returns None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[dtype]</code>           \u2013            <p>torch.dtype: The corresponding torch.dtype if the input is a valid string representation.          If the input is already a torch.dtype, it is returned as is.          If the input is None, returns None.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input string does not correspond to a supported data type.</p> </li> </ul> Source code in <code>fusion_bench/utils/dtype.py</code> <pre><code>def parse_dtype(dtype: Optional[str]) -&gt; Optional[torch.dtype]:\n    \"\"\"\n    Parses a string representation of a data type and returns the corresponding torch.dtype.\n\n    Args:\n        dtype (Optional[str]): The string representation of the data type.\n                               Can be one of \"float32\", \"float\", \"float64\", \"double\",\n                               \"float16\", \"half\", \"bfloat16\", or \"bf16\".\n                               If None, returns None.\n\n    Returns:\n        torch.dtype: The corresponding torch.dtype if the input is a valid string representation.\n                     If the input is already a torch.dtype, it is returned as is.\n                     If the input is None, returns None.\n\n    Raises:\n        ValueError: If the input string does not correspond to a supported data type.\n    \"\"\"\n    if isinstance(dtype, torch.dtype):\n        return dtype\n\n    if dtype is None:\n        return None\n\n    dtype = dtype.strip('\"')\n    if dtype not in PRECISION_STR_TO_DTYPE:\n        raise ValueError(f\"Unsupported dtype string: {dtype}\")\n\n    dtype = PRECISION_STR_TO_DTYPE[dtype]\n    return dtype\n</code></pre>"},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.dtype.set_default_dtype","title":"<code>set_default_dtype(dtype)</code>","text":"<p>Context manager to set torch's default dtype.</p> <p>Parameters:</p> <ul> <li> <code>dtype</code>               (<code>dtype</code>)           \u2013            <p>The desired default dtype inside the context manager.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ContextManager</code> (              <code>None</code> )          \u2013            <p>context manager for setting default dtype.</p> </li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; with set_default_dtype(torch.bfloat16):\n&gt;&gt;&gt;     x = torch.tensor([1, 2, 3])\n&gt;&gt;&gt;     x.dtype\ntorch.bfloat16\n</code></pre> Source code in <code>fusion_bench/utils/dtype.py</code> <pre><code>@contextlib.contextmanager\ndef set_default_dtype(dtype: torch.dtype) -&gt; Generator[None, None, None]:\n    \"\"\"\n    Context manager to set torch's default dtype.\n\n    Args:\n        dtype (torch.dtype): The desired default dtype inside the context manager.\n\n    Returns:\n        ContextManager: context manager for setting default dtype.\n\n    Example:\n\n        &gt;&gt;&gt; with set_default_dtype(torch.bfloat16):\n        &gt;&gt;&gt;     x = torch.tensor([1, 2, 3])\n        &gt;&gt;&gt;     x.dtype\n        torch.bfloat16\n\n\n    \"\"\"\n    old_dtype = torch.get_default_dtype()\n    torch.set_default_dtype(dtype)\n    try:\n        yield\n    finally:\n        torch.set_default_dtype(old_dtype)\n</code></pre>"},{"location":"api/fusion_bench.utils/torch/#fusion_bench.utils.dtype.validate_expected_param_dtype","title":"<code>validate_expected_param_dtype(named_params, dtype)</code>","text":"<p>Validates that all input parameters have the expected dtype.</p> <p>Parameters:</p> <ul> <li> <code>named_params</code>               (<code>Iterable[Tuple[str, Parameter]]</code>)           \u2013            <p>Iterable of named parameters.</p> </li> <li> <code>dtype</code>               (<code>dtype</code>)           \u2013            <p>Expected dtype.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If any parameter has a different dtype than <code>dtype</code>.</p> </li> </ul> Source code in <code>fusion_bench/utils/dtype.py</code> <pre><code>def validate_expected_param_dtype(\n    named_params: Iterable[Tuple[str, torch.nn.Parameter]], dtype: torch.dtype\n) -&gt; None:\n    \"\"\"\n    Validates that all input parameters have the expected dtype.\n\n    Args:\n        named_params (Iterable[Tuple[str, torch.nn.Parameter]]): Iterable of named parameters.\n        dtype (torch.dtype): Expected dtype.\n\n    Raises:\n        ValueError: If any parameter has a different dtype than `dtype`.\n    \"\"\"\n    for name, param in named_params:\n        if param.dtype != dtype:\n            raise ValueError(\n                f\"Parameter {name} has dtype {param.dtype}, but expected {dtype}\"\n            )\n</code></pre>"},{"location":"cli/fusion_bench/","title":"<code>fusion_bench</code>: The Command Line Interface for FusionBench","text":"<p><code>fusion_bench</code> is the command line interface for running model fusion benchmarks in the FusionBench project. It provides a flexible way to configure and execute various fusion algorithms on different model pools and evaluate them across multiple tasks.</p>"},{"location":"cli/fusion_bench/#details-and-options","title":"Details and Options","text":"<p><code>fusion_bench</code> takes a configuration file as input, which specifies the models, fusion method to be used, and the datasets to be evaluated. running <code>fusion_bench</code> is equivalent to running <code>python fusion_bench/scripts/cli.py</code>.</p> <pre><code>fusion_bench [--config-path CONFIG_PATH] [--config-name CONFIG_NAME] \\\n    OPTION_1=VALUE_1 OPTION_2=VALUE_2 ...\n\n# or equivalently\npython fusion_bench/scripts/cli.py [--config-path CONFIG_PATH] [--config-name CONFIG_NAME] \\\n    OPTION_1=VALUE_1 OPTION_2=VALUE_2 ...\n</code></pre> <p><code>fusion_bench</code> has the following options, <code>method</code>, <code>modelpool</code>, and <code>taskpool</code> are the most important ones among these options:</p>"},{"location":"cli/fusion_bench/#hydra-options","title":"Hydra options","text":"<ul> <li>--help, -h: Application's help. Print help message and exit.</li> </ul> <pre><code>fusion_bench --help\n</code></pre> <ul> <li>--hydra-help: Hydra's help.</li> <li>--version: Show Hydra's version and exit.</li> <li>--cfg, -c: Show config instead of running [job|hydra|all].   This is useful for debugging the configuration. However, this just prints plain text configuration without color highlighting or formatting.</li> </ul> <pre><code>fusion_bench --cfg\n</code></pre> <p>Or equivalently with the following options:</p> <pre><code># this will print the configuration using rich library, which provides syntax highlighting and better formatting\nfusion_bench print_config=true dry_run=true\n</code></pre> <ul> <li>--resolve: Used in conjunction with --cfg, resolve config interpolations before printing.</li> <li>--package, -p: Config package to show. For example, when you only want to see the configuration for <code>method</code>.</li> </ul> <pre><code>fusion_bench --cfg job -p method\n</code></pre> <ul> <li>--info, -i: Print Hydra information [all|config|defaults|defaults-tree|plugins|searchpath]</li> <li>--config-path, -cp: Overrides the config_path specified in hydra.main(). The config_path is absolute or relative to the Python file declaring @hydra.main(). By default, the config path is the <code>config</code> or <code>fusion_bench_config</code> directory in the project root.</li> <li>--config-name, -cn: Overrides the config_name specified in hydra.main(). By default, the config name is <code>example_config</code> so <code>config/example_config.yaml</code> will be loaded. You can also specify another config name, for example:</li> </ul> <pre><code># this will load the config from `config/llama_weighted_average.yaml`\nfusion_bench --config-name llama_weighted_average.yaml\n</code></pre> <ul> <li>--config-dir, -cd: Adds an additional config dir to the config search path</li> <li>--multirun, -m: Run multiple jobs with the configured launcher and sweeper. For more information, see Hydra documentation.</li> <li>--experimental-rerun: Rerun a job from a previous config pickle</li> </ul>"},{"location":"cli/fusion_bench/#shell-completion","title":"Shell Completion","text":"<p>This is useful for tab completion in the shell. You can install shell completion for Bash, Fish, and Zsh.</p>  Screenshot of tab completion in the shell.  <ul> <li>--shell-completion, -sc: Install or Uninstall shell completion:</li> <li> <p>Bash - Install:</p> <pre><code>eval \"$(fusion_bench -sc install=bash)\"\n</code></pre> </li> <li> <p>Bash - Uninstall:</p> <pre><code>eval \"$(fusion_bench -sc uninstall=bash)\"\n</code></pre> </li> <li> <p>Fish - Install:</p> <pre><code>fusion_bench -sc install=fish | source\n</code></pre> </li> <li> <p>Fish - Uninstall:</p> <pre><code>fusion_bench -sc uninstall=fish | source\n</code></pre> </li> <li> <p>Zsh - Install:     Zsh is compatible with the Bash shell completion, see the documentation for details.</p> <pre><code>eval \"$(fusion_bench -sc install=bash)\"\n</code></pre> </li> <li> <p>Zsh - Uninstall:</p> <pre><code>eval \"$(fusion_bench -sc uninstall=bash)\"\n</code></pre> </li> </ul>"},{"location":"cli/fusion_bench/#application-options","title":"Application Options","text":"<ul> <li> <p>report_save_path: The path to save the report. If not specified or is <code>false</code>, the report will not be saved. The report will be saved as a JSON file. Default is <code>false</code>.     For example, to save the report to <code>outputs/report.json</code>:</p> <pre><code>fusion_bench report_save_path=outputs/report.json\n</code></pre> </li> <li> <p>print_config: Whether to print the configuration to the console. If not specified or is <code>false</code>, the configuration will not be printed. Default is <code>true</code>.     For example, to print the configuration:</p> <pre><code>fusion_bench print_config=true\n</code></pre> </li> <li> <p>dry_run: Perform a dry run.     This will only validate the configuration without running the actual code. Default is <code>false</code>.     For example, to perform a dry run and print the configuration:</p> <pre><code>fusion_bench dry_run=true print_config=true\n</code></pre> </li> <li> <p>merged_model_save_path: The path to save the merged model. If specified, the merged model will be saved to this path by calling <code>modelpool.save_model</code>.     For example, to save the merged model to <code>outputs/merged_model.pt</code>:</p> <pre><code>fusion_bench merged_model_save_path=outputs/merged_model.pt\n</code></pre> <p>Note that the behavior of <code>modelpool.save_model</code> depends on the implementation of the model pool. Take <code>AutoModelForCausalLMPool</code> as an example, it will save the model to the specified path as a directory containing the model configuration and safetensor files, i.e., calling <code>model.save_pretrained(merged_model_save_path)</code>.</p> Example of <code>modelpool.save_model</code> <p><code>ModelPool</code> is the base class for model pools. The <code>save_model</code> method is defined in the <code>ModelPool</code> class and can be overridden in the derived classes. For example, <code>AutoModelForCausalLMPool</code> overrides the <code>save_model</code> method to save the model using the <code>save_pretrained</code> method of the model. The following is an example of the <code>save_model</code> method in the <code>ModelPool</code> class and the <code>AutoModelForCausalLMPool</code> class.</p> <p>By default, FusionBench will call <code>modelpool.save_model(model, merged_model_save_path, **merged_model_save_kwargs)</code> if the options below are provided. That is, additional keyword arguments can be forwarded when supported by the ModelPool implementation.</p> </li> <li> <p>merged_model_save_kwargs: Extra keyword arguments forwarded to <code>modelpool.save_model</code> when saving the merged model. Provide a dict-like value.</p> <p>Example (CausalLMPool): save to a local directory and also save the tokenizer and avoid pushing to the hub.</p> <pre><code>fusion_bench -c job \\\n    merged_model_save_path=outputs/merged_model \\\n    merged_model_save_kwargs='{push_to_hub: false, save_tokenizer: true}' \\\n    method=linear/weighted_average_for_llama \\\n    modelpool=CausalLMPool/simle_mixtral_exp_v4 \\\n    taskpool=dummy\n</code></pre> <p>Note</p> <p>The exact set of supported kwargs is defined by the chosen ModelPool. For example, <code>CausalLMPool.save_model</code> accepts <code>push_to_hub</code>, <code>save_tokenizer</code>, etc.</p> </li> </ul>"},{"location":"cli/fusion_bench/#method-modelpool-and-taskpool-options","title":"method, modelpool and taskpool options","text":"<p>As mentioned earlier, <code>method</code>, <code>modelpool</code>, and <code>taskpool</code> are the most important options in <code>fusion_bench</code>. The basic usage is as follows:</p> <pre><code>fusion_bench method=&lt;METHOD&gt; modelpool=&lt;MODELPOOL&gt; taskpool=&lt;TASKPOOL&gt;\n</code></pre> <p>To override the default configuration, you can specify additional options as follows:</p> <pre><code>fusion_bench \\\n  method=&lt;METHOD&gt; \\\n    method.&lt;OPTION_1&gt;=&lt;VALUE_1&gt; \\\n      method.&lt;OPTION_1&gt;.&lt;SUBOPTION_1&gt;=&lt;VALUE_1_1&gt; \\\n      method.&lt;OPTION_1&gt;.&lt;SUBOPTION_2&gt;=&lt;VALUE_1_2&gt; \\\n    method.&lt;OPTION_2&gt;=&lt;VALUE_2&gt; \\\n  modelpool=&lt;MODELPOOL&gt; \\\n    ...\n  taskpool=&lt;TASKPOOL&gt; \\\n    ...\n</code></pre> <p>Paremeter Overrides: In the above example, <code>&lt;METHOD&gt;</code>, <code>&lt;MODELPOOL&gt;</code>, and <code>&lt;TASKPOOL&gt;</code> are the names of the method, model pool, and task pool, respectively. <code>&lt;OPTION_1&gt;</code>, <code>&lt;VALUE_1&gt;</code>, <code>&lt;SUBOPTION_1&gt;</code>, <code>&lt;VALUE_1_1&gt;</code>, etc., are the options and values for the method. In particular, the options for the method are prefixed with <code>method.</code>, e.g., <code>method.&lt;OPTION_1&gt;</code>. And the suboptions are prefixed with <code>method.&lt;OPTION_1&gt;.</code>, e.g., <code>method.&lt;OPTION_1&gt;.&lt;SUBOPTION_1&gt;</code>.</p>"},{"location":"cli/fusion_bench/#basic-examples","title":"Basic Examples","text":"<p>merge two CLIP models using task arithmetic:</p> <pre><code>fusion_bench method=task_arithmetic \\\n  modelpool=clip-vit-base-patch32_svhn_and_mnist \\\n  taskpool=clip-vit-base-patch32_svhn_and_mnist\n</code></pre> <p>The overall configuration is as follows:</p> <pre><code>method: # (1)!\n  ...\nmodelpool: # (2)!\n  ...\ntaskpool: # (3)!\n  ...\nfast_dev_run: false\nprint_config: true\nreport_save_path: false\n</code></pre> <ol> <li>Configuration for method, <code>fusion_bench.method.load_algorithm_from_config</code> checks the 'name' attribute of the configuration and returns an instance of the corresponding algorithm.</li> <li>Configuration for model pool, <code>fusion_bench.modelpool.load_modelpool_from_config</code> checks the 'type' attribute of the configuration and returns an instance of the corresponding model pool.</li> <li>Configuration for task pool, <code>fusion_bench.taskpool.load_taskpool_from_config</code> checks the 'type' attribute of the configuration and returns an instance of the corresponding task pool.</li> </ol> <p>merge multiple CLIP models using simple averaging:</p> <pre><code>fusion_bench method=simple_average modelpool=clip-vit-base-patch32_TA8.yaml taskpool=dummy\n</code></pre>"},{"location":"cli/fusion_bench/#running-in-offline-mode","title":"Running in Offline Mode","text":"<p>In the offline mode, the model pool will not download the models from the internet. Instead, it will use the models that are already downloaded to the local cache.</p> <p>To run <code>fusion_bench</code> in offline mode, you can run the following command before running <code>fusion_bench</code>:</p> <pre><code>source offline_mode.sh\n</code></pre> <p>Or set the environment variable according to the content of <code>offline_mode.sh</code>.</p>"},{"location":"cli/fusion_bench/#debugging-and-troubleshooting","title":"Debugging and Troubleshooting","text":"<p>During algorithm development, you may want to debug the code or inspect the configuration. Here are some tips for debugging and troubleshooting.</p>"},{"location":"cli/fusion_bench/#debugging-in-vscode","title":"Debugging in VSCode","text":"<p>Visual Studio Code (VSCode) is a popular code editor that supports debugging Python code with Python extension. To debug the code using VSCode, you can use the following configuration in your <code>.vscode/launch.json</code>:</p> <pre><code>{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"FusionBench with Arguments\",\n            \"type\": \"debugpy\",\n            \"request\": \"launch\",\n            \"module\": \"fusion_bench.scripts.cli\", // (1)!\n            \"args\": \"${command:pickArgs}\", // (2)!\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": true\n        }\n    ]\n}\n</code></pre> <ol> <li>The <code>module</code> field specifies the module to run. In this case, it is <code>fusion_bench.scripts.cli</code>. You can also specify the path to the script directly with <code>program</code> filed, e.g., <code>\"program\": ${workspaceFolder}/fusion_bench/scripts/cli.py</code>.</li> <li>The <code>args</code> field specifies the arguments to pass to the script. You can use <code>${command:pickArgs}</code> to pick the arguments interactively when you run the debugger. Or you can specify the arguments directly, e.g., <code>\"args\": [\"--config-name\", \"example_config\"]</code>.</li> </ol> <p>Once you have the configuration in your <code>launch.json</code>, you can start debugging by selecting the <code>FusionBench with Arguments</code> configuration and pressing <code>F5</code>.</p> <p></p>"},{"location":"cli/fusion_bench/#debugging-in-pycharm","title":"Debugging in PyCharm","text":"<p>Debugging in PyCharm with arguments needs to be configured in the <code>Run/Debug Configurations</code>.</p> <ol> <li>Click on the <code>Run</code> menu click <code>Edit Configurations...</code> </li> <li>Select <code>+</code> in top right corner and select <code>Python</code> </li> <li>Provide the name, absolute path of the script (<code>fusion_bench/scripts/cli.py</code>) or select the script by clicking three dots (green arrow), script parameters, and python interpreter. </li> </ol>"},{"location":"cli/fusion_bench_webui/","title":"FusionBench Command Generator WebUI","text":"<p>deprecated</p> <p>This tool is no longer actively maintained after <code>fusion-bench&gt;=0.2</code> and may be removed in future releases.</p> <p>FusionBench Command Generator is a user-friendly web interface for generating FusionBench commands based on configuration files. It provides an interactive way to select and customize FusionBench configurations, making it easier to run experiments with different settings.</p> FusionBench Command Generator WebUI"},{"location":"cli/fusion_bench_webui/#usage","title":"Usage","text":"<p>Run the program with the following command:</p> <pre><code>fusion_bench_webui [OPTIONS]\n</code></pre>"},{"location":"cli/fusion_bench_webui/#options","title":"Options","text":"<ul> <li><code>--config-path PATH</code>: Specify the path to the config directory. If not provided, the default FusionBench config path will be used.</li> <li><code>--print-tree</code>: Print the configuration tree structure before launching the web interface. Default is <code>False</code>.</li> <li><code>--bind-ip IP</code>: Specify the IP address to bind the web UI. Default is <code>127.0.0.1</code>.</li> <li><code>--port PORT</code>: Specify the port to run the web UI. Default is <code>7860</code>.</li> <li><code>--share</code>: Share the web UI. Default is <code>False</code>.</li> </ul> <p>The web interface consists of the following components:</p> <ol> <li>Root Config Dropdown: Select the base configuration file.</li> <li>Configuration Groups: Nested structure of configuration options, allowing you to customize settings for each group.</li> <li>Generated Command: Displays the generated FusionBench command based on your selections.</li> <li>Overall Configuration: Shows the complete configuration in YAML format.</li> </ol>"},{"location":"config/","title":"FusionBench Configuration","text":"<p>This directory contains configuration files for FusionBench.  These configurations are essential for setting up and managing various algorithms and their hyperparameters.</p>"},{"location":"config/#configuration-structure","title":"Configuration Structure","text":"<p>FusionBench employs a modular configuration system, which is divided into three primary groups:</p> <ol> <li>Method Configuration: Defines the fusion algorithm and its associated hyperparameters.</li> <li>Model Pool Configuration: Manages the models involved in the fusion process, including datasets, tokenizers, preprocessors, and other related resources.</li> <li>Task Pool Configuration: Specifies the tasks and their corresponding datasets used for evaluating the fused models.</li> </ol>"},{"location":"config/dataset/image_classification/","title":"Image Classification Dataset Configurations","text":"<p>This folder contains the dataset configuration for image classification tasks.</p> <ul> <li>Each dataset should have 'image' and 'label' columns.</li> <li>If a dataset has no test split, we will use the validation split as the test split and create the validation set from the training set.</li> </ul>"},{"location":"config/method/expert_sparsity/","title":"Index","text":"<p>Original repo: https://github.com/Lucky-Lance/Expert_Sparsity</p> <p>Reference:     Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models.     ACL 2024.     http://arxiv.org/abs/2402.14800</p>"},{"location":"config/model/clip-vit/","title":"Index","text":"<p>This folder contains the configuration for the CLIP-ViT models (managed by <code>fusion_bench.modelpool.CLIPVisionModelPool</code>).</p>"},{"location":"config/model/clip-vit/#expected-configuration","title":"Expected Configuration","text":""},{"location":"config/model/clip-vit/#detailed-configuration","title":"Detailed Configuration","text":"<pre><code>${name_of_model}:\n  _target_: ${function_to_load_model}\n  ... # arguments to pass to the function\n</code></pre> <p>For example, to load the pre-trained CLIP-ViT-B/16 model, you can use the following configuration:</p> <pre><code>_pretrained_: # `_pretrained_` is a special key in FusionBench that indicates the model is pre-trained\n  _target_: transformers.CLIPVisionModel.from_pretrained\n  pretrained_model_name_or_path: openai/clip-vit-base-patch16\n</code></pre> <p>In this case, calling <code>modelpool.load_model(\"_pretrained_\")</code> will return a <code>transformers.CLIPVisionModel</code> instance, which is equivalent to call <code>transformers.CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch16\")</code>.</p> <p>The detailed configuration is more flexible and can be used when you need to pass additional arguments to the <code>from_pretrained</code> function or call custom functions to load and preprocess the model.</p>"},{"location":"config/model/clip-vit/#simplified-configuration","title":"Simplified Configuration","text":"<pre><code>${name_of_model}: ${pretrained_model_name_or_path}\n</code></pre> <p>This is a simplified configuration that is equivalent to the detailed configuration.</p> <p>For example, to load the pre-trained CLIP-ViT-B/16 model, you can use the following configuration:</p> <pre><code>_pretrained_: openai/clip-vit-base-patch16\n</code></pre>"},{"location":"config/modelpool/OpenCLIPVisionModelPool/","title":"OpenCLIPVisionModelPool","text":"<p>This is a model pool for OpenCLIP Vision models.</p>"},{"location":"config/modelpool/OpenCLIPVisionModelPool/#usage","title":"Usage","text":"<p>By default, the model checkpoints are placed in the <code>.cache/task_vectors_checkpoints</code> directory.</p> <pre><code>.cache/\n\u251c\u2500\u2500 task_vectors_checkpoints/\n\u2502   \u251c\u2500\u2500 ViT-B-16\n\u2502   \u2502   \u251c\u2500\u2500 Cars/finetuned.pt\n\u2502   \u2502   \u251c\u2500\u2500 DTD/finetuned.pt\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 ViT-B-32\n\u2502   \u2502   \u251c\u2500\u2500 Cars/finetuned.pt\n\u2502   \u2502   \u251c\u2500\u2500 DTD/finetuned.pt\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 head_Cars.pt\n\u2502   \u251c\u2500\u2500 head_DTD.pt\n\u2502   \u251c\u2500\u2500 ...\n|   \u2514\u2500\u2500 zeroshot.pt\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"config/modelpool/OpenCLIPVisionModelPool/#model-configuration","title":"Model Configuration","text":"<p>The model pool supports several formats for model configuration:</p> <ol> <li>Direct Path (String): </li> <li>A string path to a model checkpoint in pickle format</li> <li> <p>Example: <code>\"path/to/model.pt\"</code></p> </li> <li> <p>Pickle Path Configuration:    <pre><code>model_name: \"ViT-B-16\"  # Name of the model\npickle_path: \"path/to/model.pt\"  # Path to pickle file\n</code></pre></p> </li> <li> <p>State Dict Configuration:    <pre><code>model_name: \"ViT-B-16\"  # Name of the model\nstate_dict_path: \"path/to/state_dict.pt\"  # Path to state dict file\n</code></pre></p> </li> <li> <p>Hydra Configuration:</p> </li> <li>Any configuration that can be instantiated using Hydra's <code>instantiate</code></li> </ol>"},{"location":"config/modelpool/OpenCLIPVisionModelPool/#classification-head-configuration","title":"Classification Head Configuration","text":"<p>The classification heads can be configured in two ways:</p> <ol> <li>Direct Path (String):</li> <li>A string path to a classification head checkpoint in pickle format</li> <li> <p>Example: <code>\"path/to/head.pt\"</code></p> </li> <li> <p>Hydra Configuration:</p> </li> <li>Any configuration that can be instantiated using Hydra's <code>instantiate</code></li> </ol>"},{"location":"config/modelpool/OpenCLIPVisionModelPool/#dataset-configuration","title":"Dataset Configuration","text":"<p>The model pool supports loading datasets in two ways:</p> <ol> <li>Direct Dataset Name (String):</li> <li>A string identifier that can be loaded using <code>datasets.load_dataset</code></li> <li> <p>Example: <code>\"cifar10\"</code></p> </li> <li> <p>Custom Configuration:</p> </li> <li>Any custom dataset configuration that can be handled by the parent class</li> </ol>"},{"location":"config/modelpool/OpenCLIPVisionModelPool/#example-configuration","title":"Example Configuration","text":"<p>Here's an example of a complete configuration:</p> <pre><code>models:\n  vit_b16:\n    model_name: \"ViT-B-16\"\n    pickle_path: \".cache/task_vectors_checkpoints/ViT-B-16/Cars/finetuned.pt\"\n  vit_b32:\n    model_name: \"ViT-B-32\"\n    state_dict_path: \".cache/task_vectors_checkpoints/ViT-B-32/DTD/finetuned.pt\"\n\nclassification_heads:\n  cars_head: \".cache/task_vectors_checkpoints/head_Cars.pt\"\n  dtd_head: \".cache/task_vectors_checkpoints/head_DTD.pt\"\n</code></pre>"},{"location":"get_started/","title":"Get Started with FusionBench","text":""},{"location":"get_started/#general-structure-of-fusionbench","title":"General Structure of FusionBench","text":"Framework of FusionBench <p>FusionBench is a pioneering project that provides a comprehensive benchmark for deep model fusion, facilitating the evaluation and comparison of various model fusion techniques. The project is meticulously designed to support rigorous analysis and experimentation in the field of model fusion, offering a versatile and modular codebase tailored for advanced research and development.</p> <p>The general structure of the FusionBench project can be visualized through its modular framework, which is divided into several key components:</p> <ol> <li>Fusion Algorithm: The core component where Model Fusion takes place. It integrates models from the Model Pool and adjusts them according to the specified fusion algorithms. The output is then evaluated for performance and effectiveness.</li> <li>Model Pool: A repository of various pre-trained models that can be accessed and utilized for fusion. This pool serves as the foundation for creating new, fused models by leveraging the strengths of each individual model.</li> <li>Task Pool: A collection of tasks that the fused models are evaluated on. These tasks help in assessing the practical applicability and robustness of the fused models.</li> <li>Models &amp; Warpers, Datasets, and Metrics: These underlying modules include:<ul> <li>Models &amp; Warpers: Tools and scripts for model loading, wrapping, and pre-processing.</li> <li>Datasets: The datasets used for training, validation, and testing the fused models.</li> <li>Metrics: The performance metrics used to evaluate the models, providing a comprehensive understanding of their capabilities.</li> </ul> </li> <li>YAML Configurations: Central to the project's modularity, YAML files are used to configure models, datasets, and metrics, allowing seamless customization and scalability.     This is based on the hydra framework, which allows for easy customization and scalability.      Read More</li> </ol> <p>By organizing these components into a structured and modular codebase, FusionBench ensures flexibility, ease of use, and scalability for researchers and developers. The project not only serves as a benchmark but also as a robust platform for innovation in the realm of deep model fusion.</p> <p>To summarize the key features of FusionBench:</p> <ul> <li>Comprehensive Benchmark: FusionBench provides a wide range of fusion algorithms, model pools, and tasks for thorough evaluation.</li> <li>Modular Design: The project is structured into separate modules for algorithms, model pools, and task pools, allowing easy extension and customization.</li> <li>Command-line Interface: A flexible CLI tool <code>fusion_bench</code> for running experiments with various configurations.</li> <li>Extensive Documentation: Detailed guides, API references, and examples to help users get started quickly.</li> </ul> <ul> <li> <p>Fusion Algorithm Module</p> <p>Implement the fusion algorithms. Receive the model pool and return the fused model.</p> <p> Read More</p> </li> <li> <p>Model Pool Module</p> <p>Manage the models, including large language models. Responsible for loading, preprocessing, and saving the models.</p> <p> Read More</p> </li> <li> <p>Task Pool Module</p> <p>Manage the tasks. Responsible for loading evaluation datasets and metrics, and evaluating the fused model.</p> <p> Read More</p> </li> </ul>"},{"location":"get_started/#why-hydra-for-configuration-management-and-argument-parsing","title":"Why Hydra for configuration management and argument parsing?","text":"<p>Hydra is a powerful framework for managing complex configurations in Python applications. It allows you to compose and override configurations easily, making it an ideal choice for projects like FusionBench that has a modular design and require flexible and dynamic configuration management.</p>"},{"location":"get_started/#the-fusionbench-command-line-interface","title":"The FusionBench Command Line Interface","text":"<p>The <code>fusion_bench</code> CLI is your primary tool for executing model fusion experiments and benchmarks. This powerful command-line interface serves as the entry point for all FusionBench operations, orchestrating the entire fusion pipeline from model loading to evaluation.</p> <p>The CLI operates on a configuration-driven approach, where you specify:</p> <ul> <li>Models to fuse: Define which pre-trained models from your model pool to combine</li> <li>Fusion algorithm: Choose the specific fusion technique (e.g., simple averaging, task arithmetic, AdaMerging)</li> <li>Evaluation datasets: Specify the tasks and datasets for assessing the fused model's performance</li> <li>Execution parameters: Configure runtime settings, device allocation, and output preferences</li> </ul> <p>To execute a fusion experiment, use the following command structure:</p> <pre><code>fusion_bench [--config-path CONFIG_PATH] [--config-name CONFIG_NAME] \\\n    OPTION_1=VALUE_1 OPTION_2=VALUE_2 ...\n</code></pre> <ul> <li><code>--config-path</code>: Specifies the directory containing your configuration files (optional if using default paths)</li> <li><code>--config-name</code>: Names the specific YAML configuration file to use (without the <code>.yaml</code> extension)</li> <li>Dynamic overrides: <code>OPTION_KEY=VALUE</code> pairs that allow you to modify configuration parameters on-the-fly</li> </ul> <p>Example Usage:</p> <pre><code># Run with a specific configuration\nfusion_bench --config-name custom_config\n\n# Override specific parameters\nfusion_bench --config-name custom_config method.alpha=0.5\n\n# Use a custom config path\nfusion_bench --config-path ./my_configs --config-name custom_fusion\n</code></pre>"},{"location":"get_started/#execution-pipeline","title":"Execution Pipeline","text":"<p>When you run <code>fusion_bench</code>, the system orchestrates a sophisticated pipeline that handles model fusion and evaluation. Here's what happens under the hood:</p> <ol> <li>Configuration Loading: The system loads and validates your YAML configuration</li> <li>Component Initialization: Model pools, task pools, and fusion algorithms are instantiated</li> <li>Model Fusion: The specified algorithm processes models from the pool to create a unified model</li> <li>Evaluation: The fused model is tested against the defined task datasets</li> <li>Reporting: Performance metrics and comparison results are generated</li> </ol> <p>The following pseudocode illustrates this process:</p> FusionBench Execution Pipeline<pre><code>class FusionBenchProgram:\n    def __init__(self, method, modelpool, taskpool):\n        \"\"\"Initialize the fusion program with core components.\"\"\"\n        self.method = method          # Fusion algorithm configuration\n        self.modelpool = modelpool    # Collection of models to fuse\n        self.taskpool = taskpool      # Evaluation tasks and datasets\n\n    def run(self):\n        \"\"\"Execute the complete fusion and evaluation pipeline.\"\"\"\n        # Step 1: Load and initialize components\n        algorithm = load_algorithm(self.method)\n        modelpool = load_modelpool(self.modelpool)\n        taskpool = load_taskpool(self.taskpool)\n\n        # Step 2: Execute fusion algorithm to create merged model\n        merged_model = algorithm.run(modelpool)\n\n        # Step 3: Evaluate the fused model on specified tasks\n        report = taskpool.evaluate(merged_model)\n\n        # Step 4: Generate performance reports and comparisons\n        return report\n\ndef main(cfg):\n    \"\"\"Main entry point for FusionBench execution.\"\"\"\n    program = FusionBenchProgram(\n        method=cfg.method,        # Algorithm configuration\n        modelpool=cfg.modelpool,  # Model collection settings\n        taskpool=cfg.taskpool,    # Evaluation task settings\n    )\n    results = program.run()\n    return results\n\nif __name__ == \"__main__\":\n    # Load configuration and execute fusion pipeline\n    main(cfg)\n</code></pre>"},{"location":"get_started/#understanding-the-workflow","title":"Understanding the Workflow","text":"<p>The execution follows a clear, linear progression. For comprehensive information on all available CLI options, configuration parameters, and advanced usage patterns, refer to the detailed CLI documentation.</p> <p>The following flowchart illustrates the complete FusionBench execution pipeline:</p>"},{"location":"get_started/#runtime-framework-lightning-fabric-integration","title":"Runtime Framework: \u26a1 Lightning Fabric Integration","text":"<p>By default, <code>fusion_bench</code> launches a <code>FabricModelFusionProgram</code> that orchestrates the entire fusion workflow, including model loading, fusion computation, and evaluation. This program is built on Lightning Fabric.</p>"},{"location":"get_started/#why-lightning-fabric","title":"Why Lightning Fabric?","text":"<p>Lightning Fabric serves as our runtime framework for several compelling reasons:</p> <p>Lightning Fabric is a lightweight and flexible framework that provides a unified interface for distributed training and device management. It simplifies the process of scaling models across multiple GPUs, TPUs, and other hardware accelerators, making it easier to develop and deploy complex machine learning applications.</p> <p>However, it is possible to implement the program without relying on Lightning Fabric. This would involve manually handling the complexities of distributed training, device management, and other aspects that Lightning Fabric abstracts away. While this approach may offer more control and customization, it also requires significantly more effort and expertise to implement effectively.</p>"},{"location":"get_started/advanced_examples/","title":"Advanced Examples","text":"<ul> <li> <p>Customize Program</p> <p>Learn how to modify the FusionBench program to suit your needs.</p> <p> Read More</p> </li> <li> <p>Hyperparameter Optimization with Optuna</p> <p>Learn how to use Optuna for hyperparameter optimization in FusionBench.</p> <p> Read More</p> </li> </ul>"},{"location":"get_started/advanced_examples/customize_program/","title":"Customize Program","text":"<p>This tutorial demonstrates how to create custom programs in FusionBench. While FusionBench is primarily designed for model fusion, programs are flexible execution units that can orchestrate any type of workflow - from simple greeting messages to complex data processing tasks, and of course, sophisticated model fusion experiments.</p>"},{"location":"get_started/advanced_examples/customize_program/#understanding-programs","title":"\ud83c\udfaf Understanding Programs","text":"<p>Programs in FusionBench serve as the orchestration layer that can:</p> <ul> <li>Execute Any Workflow: Not limited to model fusion - can be data processing, analysis, automation, etc.</li> <li>Handle Configuration: Parse and apply Hydra configurations for reproducible execution</li> <li>Manage Resources: Control logging, file I/O, and system resources</li> <li>Coordinate Components: When needed, manage interaction between algorithms, model pools, and task pools</li> <li>Process Results: Save outputs, generate reports, and handle workflow results</li> </ul> <p>Programs provide the infrastructure to run any configurable workflow using FusionBench's robust configuration system.</p> <p>The main program class is <code>FabricModelFusionProgram</code>, which integrates with PyTorch Lightning Fabric for scalable execution.</p>"},{"location":"get_started/advanced_examples/customize_program/#program-architecture","title":"\ud83c\udfd7\ufe0f Program Architecture","text":""},{"location":"get_started/advanced_examples/customize_program/#base-classes","title":"Base Classes","text":"<p>All programs inherit from <code>BaseHydraProgram</code>:</p> <pre><code>from fusion_bench.programs import BaseHydraProgram\n\nclass BaseHydraProgram(BaseYAMLSerializable):\n    \"\"\"\n    Abstract base class for all FusionBench programs that use Hydra configuration.\n    \"\"\"\n\n    @abstractmethod\n    def run(self):\n        \"\"\"Execute the main program workflow.\"\"\"\n        pass\n</code></pre>"},{"location":"get_started/advanced_examples/customize_program/#fabricmodelfusionprogram-structure","title":"FabricModelFusionProgram Structure","text":"<p>The main program class provides comprehensive functionality:</p> <pre><code>from fusion_bench.programs import FabricModelFusionProgram\nfrom fusion_bench.mixins import LightningFabricMixin\n\nclass FabricModelFusionProgram(\n    LightningFabricMixin,  # Provides Lightning Fabric integration\n    BaseHydraProgram,      # Provides Hydra configuration support\n):\n    # Core components\n    method: BaseAlgorithm      # The fusion algorithm\n    modelpool: BaseModelPool   # Collection of models to merge\n    taskpool: BaseTaskPool     # Evaluation tasks (optional)\n</code></pre>"},{"location":"get_started/advanced_examples/customize_program/#creating-custom-programs","title":"\ud83d\udd27 Creating Custom Programs","text":"<p>Custom programs don't have to be fusion programs! They can be any workflow that benefits from Hydra configuration management. Let's start with a simple example.</p>"},{"location":"get_started/advanced_examples/customize_program/#simple-greeting-program","title":"Simple Greeting Program","text":"<p>Here's a minimal example that just prints a greeting message:</p> <pre><code>import logging\nfrom typing import Optional\n\nfrom omegaconf import DictConfig\n\nfrom fusion_bench.programs import BaseHydraProgram\n\nlog = logging.getLogger(__name__)\n\n\nclass GreetingProgram(BaseHydraProgram):\n    \"\"\"\n    A simple program that greets users with a custom message.\n    \"\"\"\n\n    _config_mapping = BaseHydraProgram._config_mapping | {\n        \"message\": \"message\",\n        \"name\": \"name\",\n        \"repeat_count\": \"repeat_count\",\n    }\n\n    def __init__(\n        self,\n        message: str = \"Hello\",\n        name: str = \"World\",\n        repeat_count: int = 1,\n        **kwargs,\n    ):\n        self.message = message\n        self.name = name\n        self.repeat_count = repeat_count\n        super().__init__(**kwargs)\n\n    def run(self):\n        \"\"\"Execute the greeting workflow.\"\"\"\n        log.info(\"Starting greeting program\")\n\n        # Create the greeting\n        greeting = f\"{self.message}, {self.name}!\"\n\n        # Print the greeting multiple times\n        for i in range(self.repeat_count):\n            if self.repeat_count &gt; 1:\n                print(f\"[{i+1}/{self.repeat_count}] {greeting}\")\n            else:\n                print(greeting)\n\n        log.info(\"Greeting program completed\")\n        return greeting\n</code></pre> <p>Usage Configuration:</p> config/_get_started/greeting_program.yaml<pre><code>_target_: fusion_bench._get_started.greeting_program.GreetingProgram\nmessage: \"Welcome to FusionBench\"\nname: \"Developer\"\nrepeat_count: 3\n</code></pre> <p>Command Line Usage:</p> <pre><code>fusion_bench \\\n    --config-path $PWD/config/_get_started \\\n    --config-name greeting_program \\\n    message=\"Hello there\" \\\n    name=\"FusionBench User\"\n</code></pre> <p>This program will output:</p> <pre><code>[INFO] - Starting greeting program\n[1/3] Hello there, FusionBench User!\n[2/3] Hello there, FusionBench User!\n[3/3] Hello there, FusionBench User!\n[INFO] - Greeting program completed\n</code></pre>"},{"location":"get_started/advanced_examples/customize_program/#basic-custom-program-template","title":"Basic Custom Program Template","text":"<p>Here's a template for creating your own program:</p> <pre><code>import logging\nfrom typing import Optional, Dict, Any\nfrom omegaconf import DictConfig\nfrom torch import nn\n\nfrom fusion_bench.programs import BaseHydraProgram\nfrom fusion_bench.method import BaseAlgorithm\nfrom fusion_bench.modelpool import BaseModelPool\nfrom fusion_bench.taskpool import BaseTaskPool\nfrom fusion_bench.utils import instantiate\n\nlog = logging.getLogger(__name__)\n\n\nclass CustomFusionProgram(BaseHydraProgram):\n    \"\"\"\n    Custom program for specialized fusion workflows.\n    \"\"\"\n\n    _config_mapping = BaseHydraProgram._config_mapping | {\n        \"_method\": \"method\",\n        \"_modelpool\": \"modelpool\", \n        \"_taskpool\": \"taskpool\",\n        \"custom_param\": \"custom_param\",\n    }\n\n    def __init__(\n        self,\n        method: DictConfig,\n        modelpool: DictConfig,\n        taskpool: Optional[DictConfig] = None,\n        custom_param: str = \"default_value\",\n        **kwargs\n    ):\n        self._method = method\n        self._modelpool = modelpool\n        self._taskpool = taskpool\n        self.custom_param = custom_param\n        super().__init__(**kwargs)\n\n    def run(self):\n        \"\"\"Execute the custom fusion workflow.\"\"\"\n        log.info(\"Starting custom fusion program\")\n\n        # 1. Load components\n        self.method = instantiate(self._method)\n        self.modelpool = instantiate(self._modelpool)\n\n        if self._taskpool is not None:\n            self.taskpool = instantiate(self._taskpool)\n\n        # 2. Custom pre-processing\n        self._preprocess_models()\n\n        # 3. Execute fusion\n        merged_model = self.method.run(self.modelpool)\n\n        # 4. Custom post-processing\n        merged_model = self._postprocess_model(merged_model)\n\n        # 5. Evaluate if taskpool is available\n        if hasattr(self, 'taskpool') and self.taskpool is not None:\n            report = self.taskpool.evaluate(merged_model)\n            self._save_report(report)\n\n        return merged_model\n\n    def _preprocess_models(self):\n        \"\"\"Custom preprocessing of models before fusion.\"\"\"\n        log.info(\"Preprocessing models...\")\n        # Add your custom preprocessing logic here\n        pass\n\n    def _postprocess_model(self, merged_model: nn.Module) -&gt; nn.Module:\n        \"\"\"Custom postprocessing of the merged model.\"\"\"\n        log.info(\"Postprocessing merged model...\")\n        # Add your custom postprocessing logic here\n        return merged_model\n\n    def _save_report(self, report: Dict[str, Any]):\n        \"\"\"Save evaluation report with custom formatting.\"\"\"\n        log.info(\"Saving evaluation report...\")\n        # Add custom report saving logic here\n        pass\n</code></pre>"},{"location":"get_started/advanced_examples/customize_program/#usage-examples","title":"\ud83d\ude80 Usage Examples","text":""},{"location":"get_started/advanced_examples/customize_program/#command-line-usage","title":"Command Line Usage","text":"<pre><code># Use custom program\nfusion_bench \\\n    --config-path my_configs \\\n    --config-name custom_fusion \\\n    program.custom_param=\"new_value\"\n</code></pre>"},{"location":"get_started/advanced_examples/customize_program/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from omegaconf import DictConfig\nfrom my_package import CustomFusionProgram\n\n# Create configuration\nconfig = DictConfig({\n    \"_target_\": \"mypackage.CustomProgram\"\n    \"method\": {...},\n    \"modelpool\": {...},\n})\n\n# Instantiate and run program\nprogram = CustomFusionProgram(**config)\nresult = program.run()\n</code></pre>"},{"location":"get_started/advanced_examples/hyperparameter_optimization_with_optuna/","title":"Hyperparameter Optimization with Optuna","text":"<p>This guide demonstrates how to use Optuna for hyperparameter optimization in FusionBench. We'll walk through optimizing the scaling factor for the Task Arithmetic algorithm using automated hyperparameter search.</p>"},{"location":"get_started/advanced_examples/hyperparameter_optimization_with_optuna/#overview","title":"Overview","text":"<p>Hyperparameter optimization is crucial for achieving optimal model fusion performance. Instead of manually testing different parameter combinations, Optuna provides intelligent search strategies to find the best hyperparameters efficiently.</p> <p>The example shows how to optimize the <code>scaling_factor</code> parameter of the <code>TaskArithmeticAlgorithm</code> across multiple CLIP models.</p>"},{"location":"get_started/advanced_examples/hyperparameter_optimization_with_optuna/#prerequisites","title":"Prerequisites","text":"<p>Install Optuna if you haven't already:</p> <pre><code>pip install optuna\npip install optuna-dashboard\n</code></pre>"},{"location":"get_started/advanced_examples/hyperparameter_optimization_with_optuna/#implementation","title":"Implementation","text":"<p>The complete implementation can be found in examples/hyperparam_search/task_arithmetic.py:</p>"},{"location":"get_started/advanced_examples/hyperparameter_optimization_with_optuna/#1-setup-and-configuration","title":"1. Setup and Configuration","text":"<pre><code>import os\nimport lightning as L\nimport optuna\nfrom hydra import compose, initialize\n\nfrom fusion_bench import instantiate\nfrom fusion_bench.method import TaskArithmeticAlgorithm\nfrom fusion_bench.modelpool import CLIPVisionModelPool\nfrom fusion_bench.scripts.cli import _get_default_config_path\nfrom fusion_bench.taskpool import CLIPVisionModelTaskPool\n\n# Initialize Lightning Fabric for efficient computation\nfabric = L.Fabric(accelerator=\"auto\", devices=1)\n\n# Load configuration using Hydra\nwith initialize(\n    version_base=None,\n    config_path=os.path.relpath(\n        _get_default_config_path(), start=os.path.dirname(__file__)\n    ),\n):\n    cfg = compose(\n        config_name=\"fabric_model_fusion\",\n        overrides=[\n            \"modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8\",\n            \"taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\",\n        ],\n    )\n    modelpool: CLIPVisionModelPool = instantiate(cfg.modelpool)\n    taskpool: CLIPVisionModelTaskPool = instantiate(cfg.taskpool)\n    taskpool._fabric_instance = fabric\n</code></pre>"},{"location":"get_started/advanced_examples/hyperparameter_optimization_with_optuna/#2-define-the-objective-function","title":"2. Define the Objective Function","text":"<p>The objective function evaluates model performance for a given set of hyperparameters:</p> <pre><code>def average_accuracy(trial: optuna.Trial) -&gt; float:\n    # Suggest a scaling factor value between 0.0 and 1.0\n    scaling_factor = trial.suggest_float(\"x\", 0.0, 1.0)\n\n    # Create algorithm with the suggested hyperparameter\n    algorithm = TaskArithmeticAlgorithm(scaling_factor=scaling_factor)\n\n    # Run model fusion\n    merged_model = algorithm.run(modelpool)\n\n    # Evaluate the merged model\n    report = taskpool.evaluate(merged_model)\n\n    # Return the metric to optimize (average accuracy across tasks)\n    return report[\"average\"][\"accuracy\"]\n</code></pre>"},{"location":"get_started/advanced_examples/hyperparameter_optimization_with_optuna/#3-run-optimization-study","title":"3. Run Optimization Study","text":"<pre><code># Create an Optuna study with SQLite storage for persistence\nstudy = optuna.create_study(\n    storage=\"sqlite:///hyperparam_search.db\",\n    study_name=\"arithmetic_task_on_eight_clip_models\",\n    direction=optuna.study.StudyDirection.MAXIMIZE,\n)\n\n# Optimize for 20 trials\nstudy.optimize(average_accuracy, n_trials=20)\n\n# Print the best results\nprint(f\"Best value: {study.best_value} (params: {study.best_params})\")\n</code></pre> <p>The example uses SQLite storage (<code>sqlite:///hyperparam_search.db</code>) to persist optimization results. This allows you to:</p> <ul> <li>Resume interrupted studies</li> <li>Analyze results later</li> <li>Share studies across different runs</li> </ul>"},{"location":"get_started/advanced_examples/hyperparameter_optimization_with_optuna/#running-the-example","title":"Running the Example","text":"<p>Execute the hyperparameter optimization:</p> <pre><code>cd examples/hyperparam_search\npython task_arithmetic.py\n</code></pre> <p>Launch the Optuna dashboard to visualize optimization results:</p> <pre><code>optuna-dashboard sqlite:///hyperparam_search.db\n</code></pre> <p>This opens a web interface (typically at <code>http://localhost:8080</code>) where you can:</p> <ul> <li>View optimization history and parameter importance</li> <li>Analyze trial performance with interactive plots</li> <li>Compare different hyperparameter combinations</li> <li>Monitor convergence and identify optimal regions</li> </ul> <p>The dashboard provides insights into how different scaling factors affect model performance, helping you understand the hyperparameter landscape.</p>"},{"location":"get_started/basic_examples/","title":"Basic Examples","text":""},{"location":"get_started/basic_examples/#get-started-with-fusionbench-cli","title":"Get Started with FusionBench CLI","text":"<ul> <li> <p>Structured Configs</p> <p>Learn how to build structured configuration files and group configurations effectively in FusionBench.</p> <p> Read More</p> </li> <li> <p>CLIP Simple Average</p> <p>Merge clip vision models using simple average.</p> <p> Read More</p> </li> <li> <p>CLIP Task Arithmetic</p> <p>Merge CLIP vision models using task arithmetic, allowing you to adjust the scaling factor as a hyperparameter.</p> <p> Read More</p> </li> <li> <p>Evaluate Single CLIP Model</p> <p>Evaluate the performance of a single CLIP model on image classification tasks.</p> <p> Read More</p> </li> <li> <p>Merge Large Language Models</p> <p>Merge large language models using SLERP.</p> <p> Read More</p> </li> </ul>"},{"location":"get_started/basic_examples/#fusionbench-as-a-package","title":"FusionBench as a Package","text":"<ul> <li> <p>Import and Use Merging Algorithms</p> <p>Learn how to import and use different merging algorithms in FusionBench.</p> <p> Read More</p> </li> <li> <p>Parallel Ensemble</p> <p>Learn how to create an ensemble from multiple CLIP vision models and inference in parallel using FusionBench.</p> <p> Read More</p> </li> </ul>"},{"location":"get_started/basic_examples/clip_simple_average/","title":"CLIP Simple Average","text":"<p>This tutorial demonstrates how to merge the vision encoders of CLIP (Contrastive Language-Image Pre-training) models using the Simple Average algorithm - a straightforward, hyperparameter-free approach to model fusion that combines multiple task-specific models into a single unified model.</p> <p>Mathematically, the Simple Average algorithm can be expressed as:</p> \\[ \\theta_{merged} = \\frac{1}{N} \\sum_{i=1}^{N} \\theta_{i} \\] <p>where \\( \\theta_{merged} \\) is the set of parameters for the merged model, \\( N \\) is the number of source models, and \\( \\theta_{i} \\) are the parameters of the individual models.</p> <p>This method works especially well for large models that have been fine-tuned on distinct downstream tasks<sup>1</sup>, or for models trained on the same task but with varying hyperparameter settings such as learning rate or batch size<sup>2</sup><sup>3</sup>.</p>"},{"location":"get_started/basic_examples/clip_simple_average/#standalone-yaml-configuration","title":"\ud83d\udd27 Standalone YAML Configuration","text":"<p>The example uses the following standalone configuration file that demonstrates merging CLIP models fine-tuned on different image classification datasets:</p> config/_get_started/clip_simple_average.yaml<pre><code>_target_: fusion_bench.programs.FabricModelFusionProgram # (1)!\n_recursive_: false\nmethod: # (2)!\n  _target_: fusion_bench.method.SimpleAverageAlgorithm\nmodelpool: # (3)!\n  _target_: fusion_bench.modelpool.CLIPVisionModelPool\n  models:\n    _pretrained_: openai/clip-vit-base-patch32\n    sun397: tanganke/clip-vit-base-patch32_sun397\n    stanford-cars: tanganke/clip-vit-base-patch32_stanford-cars\ntaskpool: # (4)!\n  _target_: fusion_bench.taskpool.CLIPVisionModelTaskPool\n  test_datasets:\n    sun397:\n      _target_: datasets.load_dataset\n      path: tanganke/sun397\n      split: test\n    stanford-cars:\n      _target_: datasets.load_dataset\n      path: tanganke/stanford_cars\n      split: test\n  clip_model: openai/clip-vit-base-patch32\n  processor: openai/clip-vit-base-patch32\n</code></pre> <ol> <li>This is the program to handle the model fusion workflow.</li> <li>This is the method config to perform model fusion.</li> <li>This is the model pool config containing the base and fine-tuned models.</li> <li>This is the task pool config defining evaluation datasets.</li> </ol>"},{"location":"get_started/basic_examples/clip_simple_average/#configuration-breakdown","title":"Configuration Breakdown","text":"<ul> <li>Program: This is the top level configuration that specifies the program to run.      Here we specify the main program as <code>FabricModelFusionProgram</code> which handles the model fusion workflow.</li> <li>Method: This is the method config to perform model fusion.      Here we specify the method as <code>SimpleAverageAlgorithm</code>, which performs model fusion.</li> <li>Model Pool: This is the model pool config containing the base and fine-tuned models.      In this example, it contains two fine-tuned models:<ul> <li><code>sun397</code>: Fine-tuned on SUN397 scene recognition dataset</li> <li><code>stanford-cars</code>: Fine-tuned on Stanford Cars dataset</li> </ul> </li> <li>Task Pool: A task pool object is responsible for evaluating the merged model's performance. In this example, we specify t</li> </ul>"},{"location":"get_started/basic_examples/clip_simple_average/#running-the-example","title":"\ud83d\ude80 Running the Example","text":"<p>Execute the model merging process with the following command:</p> <pre><code>fusion_bench --config-path $PWD/config/_get_started --config-name clip_simple_average\n</code></pre> <p>This command will:</p> <ol> <li>Load the specified CLIP models from the model pool</li> <li>Apply the Simple Average algorithm to merge their parameters</li> <li>Evaluate the merged model on the specified test datasets</li> <li>Generate performance reports comparing the merged model against individual models</li> </ol>"},{"location":"get_started/basic_examples/clip_simple_average/#key-learning-points","title":"\ud83c\udf93 Key Learning Points","text":"<p>This example teaches you:</p> <ol> <li>Basic Configuration: How to structure a FusionBench configuration file</li> <li>Model Pool Setup: How to specify multiple models for merging</li> <li>Task Pool Configuration: How to define evaluation datasets</li> <li>Simple Execution: How to run model merging with a single command</li> </ol>"},{"location":"get_started/basic_examples/clip_simple_average/#debugging-configuration-vs-code","title":"\ud83d\udc1b Debugging Configuration (VS Code)","text":".vscode/launch.json<pre><code>{\n    \"name\": \"clip_simple_average\",\n    \"type\": \"debugpy\",\n    \"request\": \"launch\",\n    \"module\": \"fusion_bench.scripts.cli\",\n    \"args\": [\n        \"--config-path\",\n        \"${workspaceFolder}/config/_get_started\",\n        \"--config-name\",\n        \"clip_simple_average\"\n    ],\n    \"console\": \"integratedTerminal\",\n    \"justMyCode\": true,\n    \"env\": {\n        \"HYDRA_FULL_ERROR\": \"1\"\n    }\n}\n</code></pre> <ol> <li> <p>P. Yadav et al., \u201cWhat Matters for Model Merging at Scale?,\u201d Oct. 04, 2024, arXiv: arXiv:2410.03617. Accessed: Oct. 11, 2024. Available: http://arxiv.org/abs/2410.03617 \u21a9</p> </li> <li> <p>M. Wortsman et al., \u201cModel soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,\u201d July 01, 2022, arXiv: arXiv:2203.05482. Accessed: May 08, 2023. Available: http://arxiv.org/abs/2203.05482 \u21a9</p> </li> <li> <p>A. Chegini et al., \u201cModel Soup for Better RLHF: Weight Space Averaging to Improve Alignment in LLMs\u201d.\u00a0\u21a9</p> </li> </ol>"},{"location":"get_started/basic_examples/clip_task_arithmetic/","title":"CLIP Task Arithmetic","text":"<p>This tutorial demonstrates how to merge CLIP (Contrastive Language-Image Pre-training) models using the Task Arithmetic algorithm <sup>1</sup> - a powerful model fusion technique that combines multiple task-specific models by manipulating their \"task vectors\" with configurable scaling factors.</p> <p>Task Arithmetic is an advanced model fusion technique that operates on the concept of task vectors - the directional differences between a fine-tuned model and its pretrained base model. This approach provides more fine-grained control over the fusion process compared to simple averaging.</p> <p>Mathematically, Task Arithmetic can be expressed as:</p> <p>Step 1: Compute Task Vectors</p> \\[ \\tau_i = \\theta_i - \\theta_0 \\] <p>Step 2: Scale and Combine Task Vectors</p> \\[ \\theta_{merged} = \\theta_0 + \\lambda \\sum_{i=1}^{N} \\tau_i \\] <p>where:</p> <ul> <li>\\( \\theta_{merged} \\) is the final merged model parameters</li> <li>\\( \\theta_0 \\) is the pretrained base model parameters  </li> <li>\\( \\theta_i \\) are the fine-tuned model parameters</li> <li>\\( \\tau_i \\) are the task vectors (learned adaptations)</li> <li>\\( \\lambda \\) is the scaling factor that controls the strength of task vector influence</li> <li>\\( N \\) is the number of task-specific models</li> </ul>"},{"location":"get_started/basic_examples/clip_task_arithmetic/#standalone-yaml-configuration","title":"\ud83d\udd27 Standalone YAML Configuration","text":"<p>The example uses the following configuration that demonstrates merging CLIP models with task arithmetic on image classification datasets:</p> config/_get_started/clip_task_arithmetic.yaml<pre><code>_target_: fusion_bench.programs.FabricModelFusionProgram\n_recursive_: false\nmethod:\n  _target_: fusion_bench.method.TaskArithmeticAlgorithm\n  scaling_factor: 0.7\nmodelpool:\n  _target_: fusion_bench.modelpool.CLIPVisionModelPool\n  models:\n    _pretrained_: openai/clip-vit-base-patch32\n    sun397: tanganke/clip-vit-base-patch32_sun397\n    stanford-cars: tanganke/clip-vit-base-patch32_stanford-cars\ntaskpool:\n  _target_: fusion_bench.taskpool.CLIPVisionModelTaskPool\n  test_datasets:\n    sun397:\n      _target_: datasets.load_dataset\n      path: tanganke/sun397\n      split: test\n    stanford-cars:\n      _target_: datasets.load_dataset\n      path: tanganke/stanford_cars\n      split: test\n  clip_model: openai/clip-vit-base-patch32\n  processor: openai/clip-vit-base-patch32\n</code></pre> <ol> <li>Program Configuration: Specifies <code>FabricModelFusionProgram</code> to handle the fusion workflow</li> <li> <p>Method Configuration: Uses <code>TaskArithmeticAlgorithm</code> with a scaling factor, whose default value is set as 0.7. The option names in the configuration file are the same as those in the code.</p> <p><code>TaskArithmeticAlgorithm.__init__()</code></p> <p>Initializes the TaskArithmeticAlgorithm with the given scaling factor.</p> <p>Parameters:</p> <ul> <li> <code>scaling_factor</code>               (<code>int</code>)           \u2013            <p>The factor by which the task vectors will be scaled before merging.</p> </li> </ul> Source code in <code>fusion_bench/method/task_arithmetic/task_arithmetic.py</code> <pre><code>def __init__(self, scaling_factor: int, **kwargs):\n    \"\"\"\n    Initializes the TaskArithmeticAlgorithm with the given scaling factor.\n\n    Args:\n        scaling_factor (int): The factor by which the task vectors will be scaled before merging.\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre> </li> <li> <p>Model Pool: Contains the base pretrained model and fine-tuned variants</p> </li> <li>Task Pool: Defines evaluation datasets for performance assessment</li> </ol>"},{"location":"get_started/basic_examples/clip_task_arithmetic/#running-the-example","title":"\ud83d\ude80 Running the Example","text":"<p>Execute the task arithmetic fusion with the following command:</p> <pre><code>fusion_bench --config-path $PWD/config/_get_started --config-name clip_task_arithmetic\n</code></pre>"},{"location":"get_started/basic_examples/clip_task_arithmetic/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>You can experiment with different scaling factors by overriding the configuration:</p> <pre><code># More conservative fusion (less task-specific influence)\nfusion_bench --config-path $PWD/config/_get_started --config-name clip_task_arithmetic \\\n    method.scale_factor=0.5\n\n# More aggressive fusion (stronger task-specific influence)  \nfusion_bench --config-path $PWD/config/_get_started --config-name clip_task_arithmetic \\\n    method.scale_factor=1.0\n</code></pre>"},{"location":"get_started/basic_examples/clip_task_arithmetic/#debugging-configuration-vs-code","title":"\ud83d\udc1b Debugging Configuration (VS Code)","text":".vscode/launch.json<pre><code>{\n    \"name\": \"clip_task_arithmetic\",\n    \"type\": \"debugpy\",\n    \"request\": \"launch\",\n    \"module\": \"fusion_bench.scripts.cli\",\n    \"args\": [\n        \"--config-path\",\n        \"${workspaceFolder}/config/_get_started\",\n        \"--config-name\",\n        \"clip_task_arithmetic\"\n    ],\n    \"console\": \"integratedTerminal\",\n    \"justMyCode\": true,\n    \"env\": {\n        \"HYDRA_FULL_ERROR\": \"1\"\n    }\n}\n</code></pre> <ol> <li> <p>G. Ilharco et al., \u201cEditing Models with Task Arithmetic,\u201d Mar. 31, 2023, arXiv: arXiv:2212.04089. doi: 10.48550/arXiv.2212.04089.\u00a0\u21a9</p> </li> </ol>"},{"location":"get_started/basic_examples/evaluate_single_clip_model/","title":"Evaluate a Single CLIP Model","text":"<p>This tutorial demonstrates how to evaluate a single CLIP (Contrastive Language-Image Pre-training) model on multiple downstream vision tasks using FusionBench CLI.  This serves as a baseline for understanding model performance before applying fusion techniques.</p> <p>This example utilizes the <code>DummyAlgorithm</code>, a specialized class designed for single model evaluation. It returns the pretrained model as-is, or the first available model if <code>_pretrained_</code> is not present, without applying any modifications.</p>"},{"location":"get_started/basic_examples/evaluate_single_clip_model/#standalone-yaml-configuration","title":"\ud83d\udd27 Standalone YAML Configuration","text":"<p>The example uses the following configuration that evaluates a pretrained CLIP model on multiple image classification datasets:</p> config/_get_started/clip_evaluate_single_model.yaml<pre><code>_target_: fusion_bench.programs.FabricModelFusionProgram\n_recursive_: false\nmethod:\n  _target_: fusion_bench.method.DummyAlgorithm\nmodelpool:\n  _target_: fusion_bench.modelpool.CLIPVisionModelPool\n  models:\n    _pretrained_: openai/clip-vit-base-patch32\ntaskpool:\n  _target_: fusion_bench.taskpool.CLIPVisionModelTaskPool\n  test_datasets:\n    sun397:\n      _target_: datasets.load_dataset\n      path: tanganke/sun397\n      split: test\n    stanford-cars:\n      _target_: datasets.load_dataset\n      path: tanganke/stanford_cars\n      split: test\n  clip_model: openai/clip-vit-base-patch32\n  processor: openai/clip-vit-base-patch32\n</code></pre> <ol> <li>Program Configuration: Specifies <code>FabricModelFusionProgram</code> to handle the evaluation workflow</li> <li>Method Configuration: Uses <code>DummyAlgorithm</code> which passes through the input model unchanged</li> <li>Model Pool: Contains only the base pretrained CLIP model (<code>openai/clip-vit-base-patch32</code>).     <pre><code>models={'_pretrained_': 'openai/clip-vit-base-patch32'}\n</code></pre></li> <li>Task Pool: Defines evaluation datasets and specifies the CLIP model and processor for inference.     In this examples:     <pre><code>test_datasets = {\n    'sun397': ...,\n    'stanford-cars': ...,\n}\n</code></pre></li> </ol>"},{"location":"get_started/basic_examples/evaluate_single_clip_model/#running-the-example","title":"\ud83d\ude80 Running the Example","text":"<p>Execute the model evaluation with the following command:</p> <pre><code>fusion_bench \\\n    --config-path $PWD/config/_get_started \\\n    --config-name clip_evaluate_single_model\n</code></pre> <p>Or override the model path via pass <code>modelpool.models._pretrained_=&lt;new_model_path&gt;</code>:</p> <pre><code>fusion_bench \\\n    --config-path $PWD/config/_get_started \\\n    --config-name clip_evaluate_single_model \\\n    modelpool.models._pretrained_=&lt;new_model_path&gt;\n</code></pre>"},{"location":"get_started/basic_examples/evaluate_single_clip_model/#debugging-configuration-vs-code","title":"\ud83d\udc1b Debugging Configuration (VS Code)","text":".vscode/launch.json<pre><code>{\n    \"name\": \"clip_evaluate_single_model\",\n    \"type\": \"debugpy\",\n    \"request\": \"launch\",\n    \"module\": \"fusion_bench.scripts.cli\",\n    \"args\": [\n        \"--config-path\",\n        \"${workspaceFolder}/config/_get_started\",\n        \"--config-name\",\n        \"clip_evaluate_single_model\"\n    ],\n    \"console\": \"integratedTerminal\",\n    \"justMyCode\": true,\n    \"env\": {\n        \"HYDRA_FULL_ERROR\": \"1\"\n    }\n}\n</code></pre>"},{"location":"get_started/basic_examples/import_and_use_merging_algorithms/","title":"Import and Use Merging Algorithms","text":"<p>This tutorial demonstrates how to import and use different model merging algorithms from FusionBench as a Python package. You'll learn how to programmatically create model pools, apply various fusion algorithms, and obtain merged models without using the CLI interface.</p>"},{"location":"get_started/basic_examples/import_and_use_merging_algorithms/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"get_started/basic_examples/import_and_use_merging_algorithms/#creating-a-model-pool","title":"Creating a Model Pool","text":"<p>First, let's create a simple model pool with multiple models that we want to merge:</p> <pre><code>from torch import nn\nfrom fusion_bench.modelpool import BaseModelPool\n\ndef create_mlp(in_features: int, hidden_units: int, out_features: int):\n    \"\"\"Create a simple multi-layer perceptron.\"\"\"\n    return nn.Sequential(\n        nn.Linear(in_features, hidden_units),\n        nn.ReLU(),\n        nn.Linear(hidden_units, out_features)\n    )\n\n# Create multiple models with the same architecture\nmodels = {\n    \"model_1\": create_mlp(768, 3072, 768),\n    \"model_2\": create_mlp(768, 3072, 768),\n    \"model_3\": create_mlp(768, 3072, 768)\n}\n\n# Create a model pool\nmodel_pool = BaseModelPool(models)\n</code></pre> <p>The simplest approach is to use the Simple Average algorithm, which averages the parameters of all models:</p> <pre><code>from fusion_bench.method import SimpleAverageAlgorithm\n\n# Initialize the algorithm\nalgorithm = SimpleAverageAlgorithm()\n\n# Merge the models\nmerged_model = algorithm.run(model_pool)\n\nprint(f\"Successfully merged {len(models)} models!\")\n</code></pre>"},{"location":"get_started/basic_examples/import_and_use_merging_algorithms/#more-examples","title":"\ud83d\udca1 More Examples","text":"<p>FusionBench provides various merging algorithms. Here are some commonly used ones:</p>"},{"location":"get_started/basic_examples/import_and_use_merging_algorithms/#1-simple-average","title":"1. Simple Average","text":"<p>Averages all model parameters equally - no hyperparameters needed:</p> <pre><code>from fusion_bench.method import SimpleAverageAlgorithm\n\nalgorithm = SimpleAverageAlgorithm()\nmerged_model = algorithm.run(model_pool)\n</code></pre>"},{"location":"get_started/basic_examples/import_and_use_merging_algorithms/#2-weighted-average","title":"2. Weighted Average","text":"<p>Allows you to assign different weights to each model:</p> <pre><code>from fusion_bench.method import WeightedAverageAlgorithm\n\n# Define weights for each model (must sum to 1.0)\nweights = [0.5, 0.3, 0.2]\n\nalgorithm = WeightedAverageAlgorithm(\n    weights=weights,\n    normalize=True  # Automatically normalize weights to sum to 1\n)\nmerged_model = algorithm.run(model_pool)\n</code></pre>"},{"location":"get_started/basic_examples/import_and_use_merging_algorithms/#3-task-arithmetic","title":"3. Task Arithmetic","text":"<p>Enables task arithmetic operations with a scaling factor:</p> <pre><code>from fusion_bench.method import TaskArithmeticAlgorithm\n\nalgorithm = TaskArithmeticAlgorithm(\n    scaling_factor=0.3,  # Controls the strength of task vectors\n)\n\n# Create multiple models with the same architecture\nmodels = {\n    # To compute the task vectors, we need a pretrained model\n    \"_pretrained_\": create_mlp(768, 3072, 768),\n    \"model_1\": create_mlp(768, 3072, 768),\n    \"model_2\": create_mlp(768, 3072, 768),\n    \"model_3\": create_mlp(768, 3072, 768)\n}\nmodel_pool = BaseModelPool(models)\n\nmerged_model = algorithm.run(model_pool)\n</code></pre>"},{"location":"get_started/basic_examples/merge_llm/","title":"Merge Large Language Models using SLERP","text":"<p>This tutorial demonstrates how to merge Large Language Models (LLMs) using the SLERP (Spherical Linear Interpolation) <sup>1</sup> algorithm - a sophisticated model fusion technique that interpolates between two models along the surface of a high-dimensional sphere, preserving the geometric properties of the parameter space.</p> <p>SLERP is particularly effective for merging language models because it maintains the angular relationships between model parameters, which can be crucial for preserving semantic representations and learned behaviors. Unlike simple linear interpolation (LERP), SLERP follows a curved path on the sphere's surface, ensuring consistent interpolation speed and avoiding potential distortions.</p>"},{"location":"get_started/basic_examples/merge_llm/#standalone-yaml-configuration","title":"\ud83d\udd27 Standalone YAML Configuration","text":"<p>This example uses the following configuration that demonstrates merging LLMs using SLERP:</p> config/_get_started/llm_slerp.yaml<pre><code>_target_: fusion_bench.programs.FabricModelFusionProgram\n_recursive_: false\nmethod:\n  _target_: fusion_bench.method.SlerpForCausalLM\n  t: 0.5\nmodelpool:\n  _target_: fusion_bench.modelpool.CausalLMPool\n  models:\n    model_1: ibivibiv/alpaca-dragon-72b-v1\n    model_2: moreh/MoMo-72B-lora-1.8.7-DPO\n  tokenizer: ibivibiv/alpaca-dragon-72b-v1\n  enable_lazy_loading: true # load model as LazyStateDict\n</code></pre> <ol> <li>Program Configuration: Specifies <code>FabricModelFusionProgram</code> to handle the fusion workflow</li> <li> <p>Method Configuration: Uses <code>SlerpForCausalLM</code> with a scaling factor (t parameter), whose default value is set as 0.5. The option names in the configuration file are the same as those in the code.</p> <p><code>SlerpForCausalLM.__init__()</code></p> <p>Initialize the SlerpForCausalLM algorithm.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>float</code>)           \u2013            <p>The interpolation parameter. Must be in the range [0, 1].       t=0 returns the first model, t=1 returns the second model,       t=0.5 provides balanced interpolation.</p> </li> <li> <code>DOT_THRESHOLD</code>               (<code>float</code>, default:                   <code>0.9995</code> )           \u2013            <p>The threshold for the dot product of normalized vectors.                            When the absolute dot product exceeds this threshold,                            vectors are considered nearly collinear and linear                            interpolation (LERP) is used instead of SLERP for                            numerical stability. Defaults to 0.9995.</p> </li> <li> <code>epsilon</code>               (<code>float</code>, default:                   <code>1e-08</code> )           \u2013            <p>Small value used for numerical stability to avoid                      division by zero during vector normalization.                      Defaults to 1e-8.</p> </li> <li> <code>model_save_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Path where the merged model should be saved.                                      If None, the model is not saved to disk.                                      Defaults to None.</p> </li> <li> <code>show_pbar</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to display a progress bar during the interpolation                       process. Useful for debugging or monitoring progress with                       large models. Defaults to False.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments passed to the parent BaseAlgorithm class.</p> </li> </ul> Source code in <code>fusion_bench/method/slerp/slerp.py</code> <pre><code>def __init__(\n    self,\n    t: float,\n    DOT_THRESHOLD: float = 0.9995,\n    epsilon: float = 1e-8,\n    model_save_path: Optional[str] = None,\n    show_pbar: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the SlerpForCausalLM algorithm.\n\n    Args:\n        t (float): The interpolation parameter. Must be in the range [0, 1].\n                  t=0 returns the first model, t=1 returns the second model,\n                  t=0.5 provides balanced interpolation.\n        DOT_THRESHOLD (float, optional): The threshold for the dot product of normalized vectors.\n                                       When the absolute dot product exceeds this threshold,\n                                       vectors are considered nearly collinear and linear\n                                       interpolation (LERP) is used instead of SLERP for\n                                       numerical stability. Defaults to 0.9995.\n        epsilon (float, optional): Small value used for numerical stability to avoid\n                                 division by zero during vector normalization.\n                                 Defaults to 1e-8.\n        model_save_path (Optional[str], optional): Path where the merged model should be saved.\n                                                 If None, the model is not saved to disk.\n                                                 Defaults to None.\n        show_pbar (bool, optional): Whether to display a progress bar during the interpolation\n                                  process. Useful for debugging or monitoring progress with\n                                  large models. Defaults to False.\n        **kwargs: Additional keyword arguments passed to the parent BaseAlgorithm class.\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre> </li> <li> <p>Model Pool: Contains exactly two LLMs to be merged using spherical interpolation</p> </li> </ol>"},{"location":"get_started/basic_examples/merge_llm/#running-the-example","title":"\ud83d\ude80 Running the Example","text":"<p>Execute the SLERP fusion with the following command:</p> <pre><code>fusion_bench --config-path $PWD/config/_get_started --config-name llm_slerp\n</code></pre>"},{"location":"get_started/basic_examples/merge_llm/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>You can experiment with different interpolation factors by overriding the configuration:</p> <pre><code># Favor the first model more (closer to model_1)\nfusion_bench --config-path $PWD/config/_get_started --config-name llm_slerp \\\n    method.t=0.3\n\n# Balanced interpolation (default)\nfusion_bench --config-path $PWD/config/_get_started --config-name llm_slerp \\\n    method.t=0.5\n\n# Favor the second model more (closer to model_2)\nfusion_bench --config-path $PWD/config/_get_started --config-name llm_slerp \\\n    method.t=0.7\n</code></pre>"},{"location":"get_started/basic_examples/merge_llm/#debugging-configuration-vs-code","title":"\ud83d\udc1b Debugging Configuration (VS Code)","text":".vscode/launch.json<pre><code>{\n    \"name\": \"llm_slerp\",\n    \"type\": \"debugpy\",\n    \"request\": \"launch\",\n    \"module\": \"fusion_bench.scripts.cli\",\n    \"args\": [\n        \"--config-path\",\n        \"${workspaceFolder}/config/_get_started\",\n        \"--config-name\",\n        \"llm_slerp\"\n    ],\n    \"console\": \"integratedTerminal\",\n    \"justMyCode\": true,\n    \"env\": {\n        \"HYDRA_FULL_ERROR\": \"1\"\n    }\n}\n</code></pre> <ol> <li> <p>SLERP For Model Merging \u2013 A Primer https://www.coinfeeds.ai/ai-blog/slerp-model-merging-primer\u00a0\u21a9</p> </li> </ol>"},{"location":"get_started/basic_examples/parallel_clip_ensemble/","title":"Parallel CLIP Ensemble","text":"<p>This tutorial demonstrates how to create and evaluate a parallel ensemble of CLIP (Contrastive Language-Image Pre-training) models using device mapping for efficient multi-GPU inference. Unlike model fusion techniques that merge parameters, ensemble methods maintain separate models and aggregate their predictions at inference time.</p> <p>The ensemble approach averages predictions from multiple fine-tuned models:</p> \\[ y_{ensemble} = \\frac{1}{N} \\sum_{i=1}^{N} f_i(x) \\] <p>where \\( y_{ensemble} \\) is the ensemble prediction, \\( N \\) is the number of models, and \\( f_i(x) \\) is the prediction from the i-th model.</p>"},{"location":"get_started/basic_examples/parallel_clip_ensemble/#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>Parallel Execution: Models run simultaneously on different GPUs using <code>torch.jit.fork</code></li> <li>Device Mapping: Distribute models across multiple devices for memory efficiency</li> <li>Automatic Synchronization: Outputs are automatically moved to the same device for aggregation</li> </ul>"},{"location":"get_started/basic_examples/parallel_clip_ensemble/#python-implementation","title":"\ud83d\udd27 Python Implementation","text":"<p>Here's a complete example demonstrating parallel ensemble evaluation:</p> examples/ensemble/parallel_ensemble.py<pre><code>import os\n\nimport lightning as L\nfrom hydra import compose, initialize\n\nfrom fusion_bench import instantiate\nfrom fusion_bench.method import SimpleEnsembleAlgorithm\nfrom fusion_bench.modelpool import CLIPVisionModelPool\nfrom fusion_bench.scripts.cli import _get_default_config_path\nfrom fusion_bench.taskpool import CLIPVisionModelTaskPool\nfrom fusion_bench.utils.rich_utils import setup_colorlogging\n\nsetup_colorlogging()\n\nfabric = L.Fabric(accelerator=\"auto\", devices=1)\n\n# Load configuration using Hydra\nwith initialize(\n    version_base=None,\n    config_path=os.path.relpath(\n        _get_default_config_path(), start=os.path.dirname(__file__)\n    ),\n):\n    cfg = compose(\n        config_name=\"fabric_model_fusion\",\n        overrides=[\n            \"modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8\",\n            \"taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\",\n        ],\n    )\n    modelpool: CLIPVisionModelPool = instantiate(cfg.modelpool)\n    taskpool: CLIPVisionModelTaskPool = instantiate(cfg.taskpool, move_to_device=False)\n    taskpool.fabric = fabric\n\n# Hard-coded device map and algorithm instantiation for 8 models\ndevice_map = {\n    0: \"cuda:0\",\n    1: \"cuda:0\",\n    2: \"cuda:0\",\n    3: \"cuda:0\",\n    4: \"cuda:1\",\n    5: \"cuda:1\",\n    6: \"cuda:1\",\n    7: \"cuda:1\",\n}\nalgorithm = SimpleEnsembleAlgorithm(device_map=device_map)\nensemble = algorithm.run(modelpool)\n\nreport = taskpool.evaluate(ensemble)\nprint(report)\n</code></pre>"},{"location":"get_started/basic_examples/parallel_clip_ensemble/#yaml-configuration","title":"\ud83d\udd27 YAML Configuration","text":"<p>Alternatively, you can use the ensemble method configurations:</p> config/method/ensemble/simple_ensemble.yaml<pre><code># =============================================================================\n# FusionBench Method Configuration: Simple Ensemble\n# =============================================================================\n# Averages model predictions uniformly.\n#\n# device_map: leave null for single device or provide a mapping for multi-device setups.\n# =============================================================================\n_target_: fusion_bench.method.SimpleEnsembleAlgorithm\ndevice_map: null # Set to null for single device, or specify mapping\n</code></pre>"},{"location":"get_started/basic_examples/parallel_clip_ensemble/#running-the-example","title":"\ud83d\ude80 Running the Example","text":"<p>Execute the parallel ensemble evaluation:</p> <pre><code>cd examples/ensemble\npython parallel_ensemble.py\n</code></pre>"},{"location":"get_started/basic_examples/structured_configs/","title":"Structured Config Groups","text":"<p>Due to the modular design of FusionBench, it employs a powerful structured configuration system based on Hydra, which allows for modular, composable, and hierarchical configuration management. This document explains how to understand and use the structured config groups in FusionBench.</p>"},{"location":"get_started/basic_examples/structured_configs/#overview","title":"Overview","text":"<p>The configuration system is organized into several key groups, each serving a specific purpose in the model fusion pipeline:</p> <ul> <li>Method: Defines the fusion algorithm and its hyperparameters</li> <li>ModelPool: Manages the models involved in the fusion process</li> <li>TaskPool: Specifies evaluation tasks and datasets</li> <li>Hydra: Controls Hydra framework settings</li> <li>Fabric: PyTorch Lightning Fabric configurations</li> </ul>"},{"location":"get_started/basic_examples/structured_configs/#configuration-directory-structure","title":"Configuration Directory Structure","text":"<pre><code>config/\n\u251c\u2500\u2500 method/            # Fusion algorithms\n\u251c\u2500\u2500 modelpool/         # Model pool configurations\n\u251c\u2500\u2500 taskpool/          # Task pool configurations\n\u251c\u2500\u2500 hydra/             # Hydra framework settings\n\u251c\u2500\u2500 fabric/            # Fabric configurations\n\u2514\u2500\u2500 *.yaml             # Top-level configuration files\n</code></pre>"},{"location":"get_started/basic_examples/structured_configs/#core-configuration-groups","title":"Core Configuration Groups","text":""},{"location":"get_started/basic_examples/structured_configs/#1-method-configuration-method","title":"1. Method Configuration (<code>method/</code>)","text":"<p>The method configuration defines which fusion algorithm to use and its hyperparameters. Each method configuration file specifies:</p> <ul> <li><code>_target_</code>: The Python class implementing the algorithm</li> <li>Algorithm-specific parameters</li> </ul> <p>Example: Simple Average (hyperparameter-free) <pre><code>_target_: fusion_bench.method.SimpleAverageAlgorithm\n</code></pre></p> <p>Example: Task Arithmetic <pre><code>_target_: fusion_bench.method.TaskArithmeticAlgorithm\nscaling_factor: 0.3\n</code></pre></p>"},{"location":"get_started/basic_examples/structured_configs/#2-modelpool-configuration-modelpool","title":"2. ModelPool Configuration (<code>modelpool/</code>)","text":"<p>ModelPool configurations define the collection of models to be fused, including:</p> <ul> <li>Base/pretrained models</li> <li>Fine-tuned models for specific tasks</li> <li>Model preprocessors and processors</li> </ul> <p>Example: CLIP Vision Model Pool <pre><code>_target_: fusion_bench.modelpool.CLIPVisionModelPool\n_recursive_: False\nprocessor: openai/clip-vit-base-patch32\nmodels:\n  _pretrained_: openai/clip-vit-base-patch32\n  sun397: tanganke/clip-vit-base-patch32_sun397\n  stanford-cars: tanganke/clip-vit-base-patch32_stanford-cars\nplatform: hf\n</code></pre></p>"},{"location":"get_started/basic_examples/structured_configs/#3-taskpool-configuration-taskpool","title":"3. TaskPool Configuration (<code>taskpool/</code>)","text":"<p>TaskPool configurations specify the evaluation tasks and their datasets:</p> <ul> <li>Test datasets for evaluation</li> <li>Task-specific processors</li> </ul> <p>Example: CLIP Vision Task Pool <pre><code>_target_: fusion_bench.taskpool.CLIPVisionModelTaskPool\ntest_datasets:\n  sun397:\n    _target_: datasets.load_dataset\n    path: tanganke/sun397\n    split: test\n  stanford-cars:\n    _target_: datasets.load_dataset\n    path: tanganke/stanford_cars\n    split: test\nclip_model: openai/clip-vit-base-patch32\nprocessor: openai/clip-vit-base-patch32\n</code></pre></p>"},{"location":"get_started/basic_examples/structured_configs/#5-hydra-configuration-hydra","title":"5. Hydra Configuration (<code>hydra/</code>)","text":"<p>Controls Hydra framework behavior:</p> config/hydra/default.yaml<pre><code>defaults:\n  - override help: fusion_bench_help\n  - override job_logging: rich_logging\nrun:\n  dir: ${path.log_dir}\nsweep:\n  dir: ${path.log_dir}\n  subdir: ${hydra.job.num}\njob:\n  env_set:\n    HYDRA_FULL_ERROR: ${oc.env:HYDRA_FULL_ERROR,1}\noutput_subdir: \"\"\n</code></pre>"},{"location":"get_started/basic_examples/structured_configs/#complete-configuration-example","title":"Complete Configuration Example","text":"<p>Here's a complete configuration that combines all groups:</p> config/_get_started/clip_task_arithmetic.yaml<pre><code>_target_: fusion_bench.programs.FabricModelFusionProgram\n_recursive_: false\nmethod:\n  _target_: fusion_bench.method.TaskArithmeticAlgorithm\n  scaling_factor: 0.7\nmodelpool:\n  _target_: fusion_bench.modelpool.CLIPVisionModelPool\n  models:\n    _pretrained_: openai/clip-vit-base-patch32\n    sun397: tanganke/clip-vit-base-patch32_sun397\n    stanford-cars: tanganke/clip-vit-base-patch32_stanford-cars\ntaskpool:\n  _target_: fusion_bench.taskpool.CLIPVisionModelTaskPool\n  test_datasets:\n    sun397:\n      _target_: datasets.load_dataset\n      path: tanganke/sun397\n      split: test\n    stanford-cars:\n      _target_: datasets.load_dataset\n      path: tanganke/stanford_cars\n      split: test\n  clip_model: openai/clip-vit-base-patch32\n  processor: openai/clip-vit-base-patch32\n</code></pre>"},{"location":"get_started/basic_examples/structured_configs/#using-structured-configs","title":"Using Structured Configs","text":""},{"location":"get_started/basic_examples/structured_configs/#1-with-hydra-defaults","title":"1. With Hydra Defaults","text":"<p>You can use Hydra's defaults system to compose configurations:</p> <pre><code>defaults:\n  - method: simple_average\n  - modelpool: CLIPVisionModelPool/clip-vit-base-patch32_TA8\n  - taskpool: CLIPVisionModelTaskPool/clip-vit-classification_TA8.yaml\n  - _self_\n\n_target_: fusion_bench.programs.FabricModelFusionProgram\n</code></pre>"},{"location":"get_started/basic_examples/structured_configs/#2-command-line-overrides","title":"2. Command Line Overrides","text":"<p>Override configuration values from the command line:</p> <pre><code>fusion_bench method=task_arithmetic method.scaling_factor=0.5\n</code></pre>"},{"location":"get_started/basic_examples/structured_configs/#3-config-groups","title":"3. Config Groups","text":"<p>Specify different config groups:</p> <pre><code>fusion_bench \\\n  method=adamerging/clip \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=clip-vit-base-patch32_robustness_corrupted\n</code></pre>"},{"location":"get_started/basic_examples/structured_configs/#advanced-features","title":"Advanced Features","text":""},{"location":"get_started/basic_examples/structured_configs/#recursive-configuration","title":"Recursive Configuration","text":"<p>Control recursive instantiation with <code>_recursive_</code>:</p> <pre><code>_target_: fusion_bench.modelpool.CLIPVisionModelPool\n_recursive_: False  # Prevent recursive instantiation\n</code></pre>"},{"location":"get_started/basic_examples/structured_configs/#conditional-configurations","title":"Conditional Configurations","text":"<p>Leverage Hydra's conditional configuration syntax to handle advanced scenarios, such as dynamically selecting configurations based on specific conditions:</p> <pre><code>defaults:\n  - /dataset/image_classification/train@train_datasets:\n      - sun397\n      - stanford-cars\n      - resisc45\n</code></pre> <p>This structured approach makes FusionBench configurations highly flexible, maintainable, and easy to experiment with different combinations of methods, models, and tasks.</p>"},{"location":"get_started/intermediate_examples/","title":"Intermediate Examples","text":"<ul> <li> <p>Customize Algorithm</p> <p>Learn how to implement new algorithms in FusionBench.</p> <p> Read More</p> </li> <li> <p>Assess Model Performance During Algorithm Execution</p> <p>Learn how to monitor and analyze your model's performance as algorithms run.</p> <p> Read More</p> </li> <li> <p>Select Logger for Experiment Tracking</p> <p>Learn how to select and configure different loggers (TensorBoard, CSV, Wandb, etc.) for tracking your experiments.</p> <p> Read More</p> </li> </ul>"},{"location":"get_started/intermediate_examples/assess_model_performance_within_algorithm/","title":"Assess Model Performance During Algorithm Execution","text":"<p>This tutorial demonstrates how to evaluate model performance during the merging process using FusionBench's TaskPool system. You'll learn how to integrate evaluation at different stages of your algorithm, monitor performance throughout the merging process, and save intermediate results for analysis.</p>"},{"location":"get_started/intermediate_examples/assess_model_performance_within_algorithm/#overview","title":"\ud83c\udfaf Overview","text":"<p>During model merging, it's often valuable to assess how performance changes as models are incrementally merged. This can help you:</p> <ul> <li>Monitor Progress: Track how performance evolves during merging</li> <li>Early Stopping: Stop merging if performance starts degrading</li> <li>Compare Strategies: Evaluate different merging orders or parameters</li> <li>Debug Issues: Identify when and why merging performance drops</li> <li>Research Insights: Understand the dynamics of model fusion</li> </ul>"},{"location":"get_started/intermediate_examples/assess_model_performance_within_algorithm/#taskpool-integration","title":"\ud83c\udfd7\ufe0f TaskPool Integration","text":""},{"location":"get_started/intermediate_examples/assess_model_performance_within_algorithm/#understanding-taskpools","title":"Understanding TaskPools","text":"<p>TaskPools in FusionBench manage evaluation datasets and provide standardized interfaces for assessing model performance. The most common is <code>CLIPVisionModelTaskPool</code> for vision models:</p> <pre><code>from fusion_bench.taskpool import CLIPVisionModelTaskPool\nfrom copy import deepcopy\n\n# Access taskpool from the program context\ntaskpool = self._program.taskpool  # Available in algorithm classes\n</code></pre>"},{"location":"get_started/intermediate_examples/assess_model_performance_within_algorithm/#basic-evaluation-pattern","title":"Basic Evaluation Pattern","text":"<p>Here's the fundamental pattern for evaluating during algorithm execution:</p> <pre><code>import torch\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom fusion_bench import BaseAlgorithm\nfrom fusion_bench.taskpool import CLIPVisionModelTaskPool\nfrom fusion_bench.utils.json import save_to_json\n\nclass EvaluatingMergingAlgorithm(BaseAlgorithm):\n\n    def __init__(self, evaluate_on_every_step: bool = True, **kwargs):\n        super().__init__(**kwargs)\n        self.evaluate_on_every_step = evaluate_on_every_step\n\n    @torch.no_grad()\n    def run(self, modelpool):\n        # Access the taskpool from the program\n        taskpool = self._program.taskpool\n\n        # Store original test datasets for restoration\n        original_test_datasets = deepcopy(taskpool._test_datasets)\n\n        model_names = modelpool.model_names\n        merged_model = modelpool.load_model(model_names[0])\n\n        # Evaluate initial model\n        if self.evaluate_on_every_step:\n            report = self._evaluate_model(taskpool, merged_model, model_names[0], step=0)\n\n        # Iterative merging with evaluation\n        for step, model_name in enumerate(model_names[1:], 1):\n            # Load and merge next model\n            next_model = modelpool.load_model(model_name)\n            merged_model = self._merge_models(merged_model, next_model)\n\n            # Evaluate merged model\n            if self.evaluate_on_every_step:\n                # Update taskpool to include models merged so far\n                current_models = model_names[:step + 1]\n                report = self._evaluate_model(\n                    taskpool, merged_model, current_models, step\n                )\n\n        # Restore original taskpool state\n        taskpool._test_datasets = original_test_datasets\n        taskpool._is_setup = False\n\n        return merged_model\n\n    def _evaluate_model(self, taskpool, model, model_names, step):\n        \"\"\"Evaluate model and save results.\"\"\"\n        # Reset taskpool setup to reconfigure with new datasets\n        taskpool._is_setup = False\n\n        # Configure taskpool for current set of models\n        if isinstance(model_names, list):\n            # Multiple models - evaluate on their respective datasets\n            current_datasets = {\n                name: taskpool._test_datasets[name] \n                for name in model_names \n                if name in taskpool._test_datasets\n            }\n        else:\n            # Single model\n            current_datasets = {model_names: taskpool._test_datasets[model_names]}\n\n        # Update taskpool configuration\n        from omegaconf import DictConfig\n        taskpool._test_datasets = DictConfig(current_datasets)\n\n        # Run evaluation\n        report = taskpool.evaluate(deepcopy(model))\n\n        # Save results\n        if hasattr(self, 'log_dir') and self.log_dir:\n            save_path = Path(self.log_dir) / f\"report_{step}.json\"\n            save_to_json(report, save_path)\n\n        return report\n\n    def _merge_models(self, model1, model2):\n        \"\"\"Implement your merging logic here.\"\"\"\n        # This is a placeholder - implement your actual merging algorithm\n        pass\n</code></pre>"},{"location":"get_started/intermediate_examples/assess_model_performance_within_algorithm/#real-world-example-opcm-algorithm","title":"\ud83d\udd0d Real-World Example: OPCM Algorithm","text":"<p>The Orthogonal Projection-based Continual Merging (OPCM) algorithm provides an excellent example of evaluation during merging. For more information, refer to the OPCM implementation.</p>"},{"location":"get_started/intermediate_examples/customize_algorithm/","title":"Customize Algorithm","text":"<p>This tutorial demonstrates how to implement a new model merging algorithm in FusionBench. You'll learn the essential components, interfaces, and best practices for creating custom algorithms that integrate seamlessly with the FusionBench framework.</p>"},{"location":"get_started/intermediate_examples/customize_algorithm/#overview","title":"\ud83c\udfaf Overview","text":"<p>FusionBench follows a modular design where algorithms inherit from <code>BaseAlgorithm</code> and implement the required interfaces. This allows you to:</p> <ul> <li>Create custom merging strategies</li> <li>Leverage existing utilities and mixins</li> <li>Integrate with the configuration system</li> <li>Add profiling and monitoring capabilities</li> </ul>"},{"location":"get_started/intermediate_examples/customize_algorithm/#algorithm-structure","title":"\ud83c\udfd7\ufe0f Algorithm Structure","text":""},{"location":"get_started/intermediate_examples/customize_algorithm/#basic-template","title":"Basic Template","text":"<p>Here's the minimal structure for a custom algorithm:</p> <pre><code>import torch\nfrom typing import Union, Dict\nfrom torch import nn\n\nfrom fusion_bench.method import BaseAlgorithm\nfrom fusion_bench.modelpool import BaseModelPool\n\n\nclass CustomMergingAlgorithm(BaseAlgorithm):\n    \"\"\"\n    Custom algorithm implementation.\n\n    This algorithm demonstrates how to implement a new merging strategy.\n    \"\"\"\n\n    # Configuration mapping for YAML serialization\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"custom_param\": \"custom_param\",\n        \"another_param\": \"another_param\",\n    }\n\n    def __init__(self, custom_param: float = 0.5, another_param: bool = True):\n        \"\"\"\n        Initialize the algorithm with custom parameters.\n\n        Args:\n            custom_param: Example parameter for your algorithm\n            another_param: Another example parameter\n        \"\"\"\n        super().__init__()\n        self.custom_param = custom_param\n        self.another_param = another_param\n\n    def run(self, modelpool: Union[BaseModelPool, Dict[str, nn.Module]]) -&gt; nn.Module:\n        \"\"\"\n        Implement your custom merging logic here.\n\n        Args:\n            modelpool: Collection of models to merge\n\n        Returns:\n            Merged model\n        \"\"\"\n        # Convert dict to BaseModelPool if needed\n        if isinstance(modelpool, dict):\n            modelpool = BaseModelPool(modelpool)\n\n        # Your custom merging logic goes here\n        models = modelpool.models()\n        merged_model = ...\n\n        return merged_model\n</code></pre>"},{"location":"get_started/intermediate_examples/customize_algorithm/#file-organization","title":"\ud83d\udcc1 File Organization","text":"<p>Organize your algorithm files following FusionBench conventions:</p> <pre><code>fusion_bench/method/\n\u251c\u2500\u2500 your_algorithm/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 your_algorithm.py\n\u2514\u2500\u2500 __init__.py  # Register your algorithm here (optional)\n</code></pre>"},{"location":"get_started/intermediate_examples/customize_algorithm/#optional-register-in-main-initpy","title":"(Optional) Register in Main init.py","text":"<p>Add your algorithm to the main method registry:</p> fusion_bench/method/__init__.py<pre><code>_import_structure = {\n    # ... existing algorithms ...\n    \"your_algorithm\": [\"YourAlgorithm\"],\n    # ... rest of the algorithms ...\n}\n\nif TYPE_CHECKING:\n    from .your_algorithm import YourAlgorithm\n</code></pre>"},{"location":"get_started/intermediate_examples/customize_algorithm/#configuration-integration","title":"\u2699\ufe0f Configuration Integration","text":"<p>Integrate your algorithm with the FusionBench configuration system by creating a YAML configuration file:</p> config/method/your_algorithm.yaml<pre><code>_target_: fusion_bench.method.YourAlgorithm\ncustom_param: 0.5\nanother_param: true\n</code></pre>"},{"location":"get_started/intermediate_examples/customize_algorithm/#advanced-features","title":"\ud83d\udd0c Advanced Features","text":""},{"location":"get_started/intermediate_examples/customize_algorithm/#adding-profiling-support","title":"Adding Profiling Support","text":"<p>Use the <code>SimpleProfilerMixin</code> for performance monitoring:</p> <pre><code>from fusion_bench.mixins.simple_profiler import SimpleProfilerMixin\n\nclass YourAlgorithm(BaseAlgorithm, SimpleProfilerMixin):\n\n    def run(self, modelpool):\n        with self.profile(\"initialization\"):\n            # Initialization code\n            pass\n\n        with self.profile(\"model_loading\"):\n            # Model loading code\n            pass\n\n        with self.profile(\"parameter_merging\"):\n            # Merging logic\n            pass\n\n        # Print timing summary\n        self.print_profile_summary()\n</code></pre>"},{"location":"get_started/intermediate_examples/customize_algorithm/#adding-logging","title":"Adding Logging","text":"<p>Include proper logging for debugging and monitoring:</p> <pre><code>import logging\n\nlog = logging.getLogger(__name__)\n\nclass YourAlgorithm(BaseAlgorithm):\n\n    def run(self, modelpool):\n        log.info(f\"Starting merge with {len(modelpool.model_names)} models\")\n        log.debug(f\"Model names: {modelpool.model_names}\")\n\n        # Algorithm logic...\n\n        log.info(\"Merge completed successfully\")\n</code></pre>"},{"location":"get_started/intermediate_examples/customize_algorithm/#parameter-validation","title":"Parameter Validation","text":"<p>Add robust parameter validation:</p> <pre><code>class YourAlgorithm(BaseAlgorithm):\n\n    def __init__(self, param1: float, param2: int):\n        super().__init__()\n\n        # Validate parameters\n        if not 0 &lt;= param1 &lt;= 1:\n            raise ValueError(f\"param1 must be in [0, 1], got {param1}\")\n\n        if param2 &lt;= 0:\n            raise ValueError(f\"param2 must be positive, got {param2}\")\n\n        self.param1 = param1\n        self.param2 = param2\n</code></pre>"},{"location":"get_started/intermediate_examples/customize_algorithm/#usage-examples","title":"\ud83d\ude80 Usage Examples","text":""},{"location":"get_started/intermediate_examples/customize_algorithm/#command-line-usage","title":"Command Line Usage","text":"<pre><code># Use your custom algorithm\nfusion_bench \\\n    method=&lt;path_to_your_algorithm_config&gt; \\\n    modelpool=modelpool_config \\\n    taskpool=taskpool_config\n</code></pre>"},{"location":"get_started/intermediate_examples/customize_algorithm/#programmatic-usage-use-without-cli","title":"Programmatic Usage (Use without CLI)","text":"<pre><code>from fusion_bench.method.your_algorithm import YourAlgorithm\nfrom fusion_bench.modelpool import BaseModelPool\n\n# Create your algorithm\nalgorithm = YourAlgorithm(\n    custom_param=0.5,\n    another_param=True\n)\n\n# Apply to your models\nmerged_model = algorithm.run(your_modelpool)\n</code></pre>"},{"location":"get_started/intermediate_examples/customize_algorithm/#next-steps","title":"\ud83d\udd17 Next Steps","text":"<ul> <li>Explore existing algorithms for more implementation patterns</li> <li>Learn about model pools for advanced model management</li> <li>Check out evaluation strategies to assess your merged models</li> <li>Contribute your algorithm back to the FusionBench community!</li> </ul>"},{"location":"get_started/intermediate_examples/select_logger/","title":"Select Logger for Experiment Tracking","text":"<p>This tutorial demonstrates how to select and configure different loggers in FusionBench for experiment tracking. By default, FusionBench uses TensorBoard for logging, which is great for local development and quick experiments, but you can easily switch to other logging backends such as CSV, Weights &amp; Biases (wandb), MLFlow, or SwanLab.</p>"},{"location":"get_started/intermediate_examples/select_logger/#overview","title":"\ud83c\udfaf Overview","text":"<p>FusionBench integrates with Lightning Fabric's logging system, which provides a unified interface for various experiment tracking tools. This allows you to:</p> <ul> <li>Track metrics and hyperparameters across different experiments</li> <li>Compare results between different fusion algorithms</li> <li>Monitor training progress in real-time</li> <li>Store experiment artifacts and configurations</li> </ul>"},{"location":"get_started/intermediate_examples/select_logger/#available-loggers","title":"\ud83d\udccb Available Loggers","text":"<p>FusionBench supports the following loggers out of the box, the configuration files for which are located in <code>config/fabric/loggers/</code>:</p> Logger Description Configuration File Use Case TensorBoard Default logger, built-in visualization <code>tensorboard_logger.yaml</code> Local development, quick experiments Weights &amp; Biases Cloud-based experiment tracking <code>wandb_logger.yaml</code> Team collaboration, advanced visualization MLFlow Open-source ML lifecycle platform <code>mlflow_logger.yaml</code> Model registry, experiment comparison SwanLab Experiment tracking and visualization <code>swandb_logger.yaml</code> Alternative to wandb CSV Lightweight file-based logging <code>csv_logger.yaml</code> Simple tracking, automated pipelines"},{"location":"get_started/intermediate_examples/select_logger/#configuration-methods","title":"\u2699\ufe0f Configuration Methods","text":"<p>There are two ways to select a logger in FusionBench:</p>"},{"location":"get_started/intermediate_examples/select_logger/#command-line-override-recommended","title":"Command Line Override (Recommended)","text":"<p>The simplest way is to override the logger configuration via the command line:</p> <pre><code># Use CSV logger\nfusion_bench \\\n    --config-name fabric_model_fusion \\\n    fabric/loggers=csv_logger \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    method=simple_average\n\n# Use Weights &amp; Biases logger\nfusion_bench \\\n    --config-name fabric_model_fusion \\\n    fabric/loggers=wandb_logger \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    method=simple_average\n\n# Use SwanLab logger\nfusion_bench \\\n    --config-name fabric_model_fusion \\\n    fabric/loggers=swandb_logger \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    method=simple_average\n</code></pre>"},{"location":"get_started/intermediate_examples/select_logger/#logger-specific-configuration","title":"\ud83d\udd27 Logger-Specific Configuration","text":"<p>Each logger has its own configuration file located in <code>config/fabric/loggers/</code>. Here's how to customize them:</p>"},{"location":"get_started/intermediate_examples/select_logger/#tensorboard-logger","title":"TensorBoard Logger","text":"config/fabric/loggers/tensorboard_logger.yaml<pre><code>_target_: lightning.fabric.loggers.TensorBoardLogger\n# the logs directory would be `root_dir/name/version_X`\n# for example, `outputs/logs/lightning_logs/version_0` and `outputs/logs/lightning_logs/version_1` by default\n\n# root directory for all logging\nroot_dir: ${path.log_dir}\n# the name of the experiment\nname: \"\"\nversion: \"\"\nsub_dir: null\ndefault_hp_metric: false\n</code></pre> <p>Customization example:</p> <pre><code>fusion_bench \\\n    --config-name fabric_model_fusion \\\n    fabric/loggers=tensorboard_logger \\\n    fabric.loggers.name=my_experiment \\\n    fabric.loggers.version=v1 \\\n    method=simple_average\n</code></pre>"},{"location":"get_started/intermediate_examples/select_logger/#csv-logger","title":"CSV Logger","text":"config/fabric/loggers/csv_logger.yaml<pre><code>_target_: lightning.fabric.loggers.CSVLogger\n# the logs directory would be `root_dir/name/version_X`\n# for example, `outputs/logs/lightning_logs/version_0` and `outputs/logs/lightning_logs/version_1` by default\n\n# root directory for all logging\nroot_dir: ${path.log_dir}\n# the name of the experiment\nname: \"\"\nversion: \"\"\nprefix: \"\"\nflush_logs_every_n_steps: 100\n</code></pre> <p>Customization example:</p> <pre><code>fusion_bench \\\n    --config-name fabric_model_fusion \\\n    fabric/loggers=csv_logger \\\n    fabric.loggers.flush_logs_every_n_steps=50 \\\n    method=task_arithmetic\n</code></pre>"},{"location":"get_started/intermediate_examples/select_logger/#weights-biases-logger","title":"Weights &amp; Biases Logger","text":"config/fabric/loggers/wandb_logger.yaml<pre><code># https://lightning.ai/docs/fabric/2.4.0/guide/loggers/wandb.html#weights-and-biases\n_target_: wandb.integration.lightning.fabric.WandbLogger\nproject: ${hydra:job.config_name}\nsave_dir: ${path.log_dir}\n</code></pre> <p>Prerequisites: Install wandb and login:</p> <pre><code>pip install wandb\nwandb login\n</code></pre> <p>Customization example:</p> <pre><code>fusion_bench \\\n    --config-name fabric_model_fusion \\\n    fabric/loggers=wandb_logger \\\n    fabric.loggers.project=my_fusion_project \\\n    fabric.loggers.name=experiment_001 \\\n    method=ties_merging\n</code></pre>"},{"location":"get_started/intermediate_examples/select_logger/#swanlab-logger","title":"SwanLab Logger","text":"config/fabric/loggers/swandb_logger.yaml<pre><code>#https://github.com/SwanHubX/SwanLab/blob/main/swanlab/integration/pytorch_lightning.py\n_target_: swandb.integration.pytorch_lightning.SwanLabLogger\nproject: ${hydra:job.config_name}\ndescription: \"SwanLab logger with FusionBench\"\nsave_dir: ${path.log_dir}\n</code></pre> <p>Prerequisites: Install swanlab:</p> <pre><code>pip install swanlab\n</code></pre>"},{"location":"get_started/intermediate_examples/select_logger/#programmatic-usage","title":"\ud83d\udc0d Programmatic Usage","text":"<p>You can also configure loggers programmatically when using FusionBench as a library:</p> <pre><code>import os\nimport lightning as L\nfrom lightning.fabric.loggers import CSVLogger\nfrom hydra import compose, initialize\n\nfrom fusion_bench import instantiate\nfrom fusion_bench.method import SimpleAverageAlgorithm\nfrom fusion_bench.modelpool import CLIPVisionModelPool\nfrom fusion_bench.taskpool import CLIPVisionModelTaskPool\nfrom fusion_bench.scripts.cli import _get_default_config_path\nfrom fusion_bench.utils.rich_utils import setup_colorlogging\n\nsetup_colorlogging()\n\n# Option 1: Create a logger directly\ncsv_logger = CSVLogger(\n    root_dir=\"outputs/logs\",\n    name=\"my_experiment\",\n    flush_logs_every_n_steps=50\n)\n\nfabric = L.Fabric(\n    accelerator=\"auto\",\n    devices=1,\n    loggers=csv_logger\n)\nfabric.launch()\n\n# Load configuration using Hydra\nwith initialize(\n    version_base=None,\n    config_path=os.path.relpath(\n        _get_default_config_path(), start=os.path.dirname(__file__)\n    ),\n):\n    cfg = compose(\n        config_name=\"fabric_model_fusion\",\n        overrides=[\n            \"modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8\",\n            \"taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\",\n        ],\n    )\n    modelpool: CLIPVisionModelPool = instantiate(cfg.modelpool)\n    taskpool: CLIPVisionModelTaskPool = instantiate(cfg.taskpool, move_to_device=False)\n    taskpool.fabric = fabric\n\n# Run the fusion algorithm\nalgorithm = SimpleAverageAlgorithm()\nmerged_model = algorithm.run(modelpool)\n\n# Evaluate the merged model\nreport = taskpool.evaluate(merged_model)\nprint(report)\n</code></pre> <p>For Weights &amp; Biases:</p> <pre><code>from lightning.fabric.loggers import WandbLogger\n\n# Create wandb logger with custom configuration\nwandb_logger = WandbLogger(\n    project=\"my_fusion_project\",\n    name=\"experiment_001\",\n    save_dir=\"outputs/logs\",\n    log_model=True  # Log model checkpoints\n)\n\nfabric = L.Fabric(\n    accelerator=\"auto\",\n    devices=1,\n    loggers=wandb_logger\n)\nfabric.launch()\n\n# Continue with your fusion workflow...\n</code></pre>"},{"location":"get_started/intermediate_examples/select_logger/#accessing-logger-in-your-algorithm","title":"\ud83d\udcca Accessing Logger in Your Algorithm","text":"<p>If you're implementing a custom algorithm using <code>LightningFabricMixin</code>, you can access the logger to log custom metrics:</p> <pre><code>from fusion_bench.method import BaseAlgorithm\nfrom fusion_bench.mixins.lightning_fabric import LightningFabricMixin\n\nclass MyCustomAlgorithm(BaseAlgorithm, LightningFabricMixin):\n\n    def run(self, modelpool):\n        # Log scalar metrics\n        self.fabric.log(\"my_metric\", 0.95)\n\n        # Log multiple metrics at once\n        self.fabric.log_dict({\n            \"accuracy\": 0.95,\n            \"loss\": 0.05,\n            \"f1_score\": 0.93\n        })\n\n        # Log hyperparameters\n        self.log_hyperparams()\n\n        # Access the underlying logger for advanced features\n        logger = self.fabric.logger\n\n        # For TensorBoard logger specifically\n        if hasattr(self, 'tensorboard_summarywriter'):\n            writer = self.tensorboard_summarywriter\n            # Log custom data (images, histograms, etc.)\n            # writer.add_image(...)\n            # writer.add_histogram(...)\n\n        # Your fusion logic here...\n        merged_model = ...\n\n        return merged_model\n</code></pre>"},{"location":"get_started/intermediate_examples/select_logger/#next-steps","title":"\ud83c\udf93 Next Steps","text":"<ul> <li>Learn how to customize algorithms with logging capabilities</li> <li>Explore advanced program customization</li> <li>Check out other intermediate examples</li> </ul>"},{"location":"guides/docker/","title":"Build and Run Fusion Benchmarks in a Docker Container","text":""},{"location":"guides/docker/#build-image","title":"Build Image","text":"<p>Build the image using the following command:</p> <pre><code># Using the mirror\ndocker build -t fusion_bench .\n</code></pre> <p>If you want to use the default Docker Hub, you can omit the MIRROR argument or set it to an empty value:</p> <pre><code># Using the default Docker Hub\ndocker build --build-arg MIRROR=docker.io -t fusion_bench .\n</code></pre>"},{"location":"guides/docker/#run-container","title":"Run Container","text":"<p>Test the container using the following command:</p> <pre><code>docker run --gpus all -it --rm fusion_bench\n</code></pre>"},{"location":"guides/docs/","title":"\ud83d\udcda Contributing to Documentation","text":"<p>We welcome contributions to the FusionBench documentation! This guide will help you understand how to contribute effectively to our documentation built with MkDocs, Material theme, and mkdocstrings.</p>"},{"location":"guides/docs/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"guides/docs/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or later</li> <li>Git</li> </ul>"},{"location":"guides/docs/#setting-up-local-development","title":"Setting Up Local Development","text":"<ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/tanganke/fusion_bench.git\ncd fusion_bench\n</code></pre> </li> <li> <p>Install documentation dependencies:</p> <pre><code>pip install -e \".[docs]\"\n</code></pre> </li> <li> <p>Serve the documentation locally:</p> <pre><code>mkdocs serve\n# or add `--dirty` to only rebuild changed files\nmkdocs serve --dirty\n</code></pre> <p>The documentation will be available at <code>http://localhost:8000</code> with live reload enabled.</p> <p>For external access (useful for containers or remote development):</p> <pre><code>mkdocs serve -a 0.0.0.0:8000\n</code></pre> </li> </ol>"},{"location":"guides/docs/#documentation-structure","title":"\ud83d\udcc1 Documentation Structure","text":"<pre><code>docs/\n\u251c\u2500\u2500 README.md      # Home page\n\u251c\u2500\u2500 algorithms/    # Algorithm documentation\n\u251c\u2500\u2500 api/           # API reference (auto-generated)\n\u251c\u2500\u2500 cli/           # CLI documentation\n\u251c\u2500\u2500 guides/        # User guides and tutorials\n\u251c\u2500\u2500 modelpool/     # Model pool documentation\n\u251c\u2500\u2500 taskpool/      # Task pool documentation\n\u251c\u2500\u2500 css/           # Custom stylesheets\n\u251c\u2500\u2500 javascripts/   # Custom JavaScript\n\u2514\u2500\u2500 images/        # Documentation images\n</code></pre>"},{"location":"guides/docs/#writing-guidelines","title":"\u270d\ufe0f Writing Guidelines","text":""},{"location":"guides/docs/#markdown-standards","title":"Markdown Standards","text":"<ul> <li>Use ATX-style headers (<code>#</code>, <code>##</code>, <code>###</code>, etc.)</li> <li> <p>Use code fences with language specification:</p> <pre><code>def example_function():\n    return \"Hello, World!\"\n</code></pre> </li> <li> <p>Use admonitions for important notes:</p> </li> </ul> ExamplesSource <p>Important</p> <p>This is an important note.</p> <p>Caution</p> <p>This requires careful attention.</p> <p>Pro Tip</p> <p>This is a helpful tip.</p> <pre><code>!!! note \"Important\"\n    This is an important note.\n\n!!! warning \"Caution\"\n    This requires careful attention.\n\n!!! tip \"Pro Tip\"\n    This is a helpful tip.\n</code></pre>"},{"location":"guides/docs/#api-documentation","title":"API Documentation","text":"<p>Our API documentation is auto-generated using mkdocstrings. To document code:</p> <ol> <li> <p>Write comprehensive docstrings:</p> <pre><code>def example_function(param1: str, param2: int = 10) -&gt; str:\n    \"\"\"\n    Brief description of the function.\n\n    Args:\n        param1: Description of the first parameter.\n        param2: Description of the second parameter. Defaults to 10.\n\n    Returns:\n        Description of the return value.\n\n    Example:\n        ```python\n        result = example_function(\"hello\", 5)\n        print(result)  # Output: \"hello5\"\n        ```\n    \"\"\"\n    return param1 + str(param2)\n</code></pre> </li> <li> <p>Add API pages in <code>docs/api/</code> directory:</p> <pre><code># Module Name\n\nBrief description of the module.\n\n::: fusion_bench.module_name\n</code></pre> </li> </ol>"},{"location":"guides/docs/#mathematical-expressions","title":"Mathematical Expressions","text":"<p>Use MathJax for mathematical notation:</p> ExamplesSource <p>Inline math: \\(E = mc^2\\)</p> <p>Block math:</p> \\[\\frac{\\partial L}{\\partial w} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i) x_i\\] <pre><code>Inline math: $E = mc^2$\n\nBlock math:\n\n$$\\frac{\\partial L}{\\partial w} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i) x_i$$\n</code></pre> <p>Thank you for contributing to FusionBench documentation! \ud83d\ude80</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/","title":"Image Classification with CLIP Models using <code>HFCLIPClassifier</code>","text":""},{"location":"guides/clip_vit/HFCLIPClassifier/#introduction","title":"Introduction","text":"<p>The <code>HFCLIPClassifier</code> class provides a wrapper around the CLIP (Contrastive Language-Image Pre-training) model for image classification tasks. It supports zero-shot learning and can be fine-tuned for specific classification tasks.</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#basic-steps","title":"Basic Steps","text":""},{"location":"guides/clip_vit/HFCLIPClassifier/#importing-required-modules","title":"Importing Required Modules","text":"<p>First, we need to import the necessary modules for our CLIP-based image classification task:</p> <pre><code>import torch\nfrom transformers import CLIPModel, CLIPProcessor\nfrom fusion_bench.models.hf_clip import HFCLIPClassifier\nfrom torch.utils.data import DataLoader\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#loading-clip-model-and-processor","title":"Loading CLIP Model and Processor","text":"<pre><code>clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#initializing-hfclipclassifier","title":"Initializing HFCLIPClassifier","text":"<pre><code>classifier = HFCLIPClassifier(clip_model, processor)\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#setting-up-the-classification-task","title":"Setting Up the Classification Task","text":"<p>After initializing the classifier, we need to set up the classification task by defining class names and optionally, custom text templates.  The text encoder of CLIP model is used to encode the class names into text embeddings, which are then used to compute the logits for each class.</p> <pre><code>class_names = [\"cat\", \"dog\", \"bird\", \"fish\", \"horse\"]\nclassifier.set_classification_task(class_names)\n</code></pre> <p>By default, <code>set_classification_task</code> uses the following templates:</p> <pre><code>default_templates = [\n    lambda c: f\"a photo of a {c}\",\n]\n</code></pre> <p>You can also use custom templates:</p> <pre><code>custom_templates = [\n    lambda c: f\"a photo of a {c}\",\n    lambda c: f\"an image containing a {c}\",\n]\nclassifier.set_classification_task(class_names, templates=custom_templates)\n</code></pre> <p>Below is the code for <code>set_classification_task</code> and <code>forward</code> method of <code>HFCLIPClassifier</code>:</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.models.hf_clip.HFCLIPClassifier","title":"<code>HFCLIPClassifier</code>","text":"<p>               Bases: <code>Module</code></p> <p>A classifier based on the CLIP (Contrastive Language-Image Pre-training) model.</p> <p>This class wraps a CLIP model and provides functionality for image classification using zero-shot learning. It allows setting a classification task with custom class names and text templates.</p> <p>Attributes:</p> <ul> <li> <code>clip_model</code>               (<code>CLIPModel</code>)           \u2013            <p>The underlying CLIP model.</p> </li> <li> <code>processor</code>               (<code>CLIPProcessor</code>)           \u2013            <p>The CLIP processor for preparing inputs.</p> </li> <li> <code>zeroshot_weights</code>               (<code>Tensor</code>)           \u2013            <p>Computed text embeddings for zero-shot classification.</p> </li> <li> <code>classnames</code>               (<code>List[str]</code>)           \u2013            <p>List of class names for the current classification task.</p> </li> <li> <code>templates</code>               (<code>List[Callable[[str], str]]</code>)           \u2013            <p>List of template functions for generating text prompts.</p> </li> </ul> Source code in <code>fusion_bench/models/hf_clip.py</code> <pre><code>class HFCLIPClassifier(nn.Module):\n    \"\"\"\n    A classifier based on the CLIP (Contrastive Language-Image Pre-training) model.\n\n    This class wraps a CLIP model and provides functionality for image classification\n    using zero-shot learning. It allows setting a classification task with custom\n    class names and text templates.\n\n    Attributes:\n        clip_model (CLIPModel): The underlying CLIP model.\n        processor (CLIPProcessor): The CLIP processor for preparing inputs.\n        zeroshot_weights (Tensor): Computed text embeddings for zero-shot classification.\n        classnames (List[str]): List of class names for the current classification task.\n        templates (List[Callable[[str], str]]): List of template functions for generating text prompts.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        clip_model: CLIPModel,\n        processor: CLIPProcessor,\n        extra_module=None,\n    ):\n        \"\"\"\n        Initialize the HFCLIPClassifier.\n\n        Args:\n            clip_model (CLIPModel): The CLIP model to use for classification.\n            processor (CLIPProcessor): The CLIP processor for preparing inputs.\n        \"\"\"\n        super().__init__()\n        # we only fine-tune the vision model\n        clip_model.visual_projection.requires_grad_(False)\n        clip_model.text_model.requires_grad_(False)\n        clip_model.text_projection.requires_grad_(False)\n        clip_model.logit_scale.requires_grad_(False)\n\n        self.clip_model = clip_model\n        self.processor = processor\n        self.register_buffer(\n            \"zeroshot_weights\",\n            None,\n            persistent=False,\n        )\n\n        self.extra_module = extra_module\n\n    @property\n    def text_model(self):\n        \"\"\"Get the text model component of CLIP.\"\"\"\n        return self.clip_model.text_model\n\n    @property\n    def vision_model(self):\n        \"\"\"Get the vision model component of CLIP.\"\"\"\n        return self.clip_model.vision_model\n\n    def set_classification_task(\n        self,\n        classnames: List[str],\n        templates: List[Callable[[str], str]] = default_templates,\n    ):\n        \"\"\"\n        Set up the zero-shot classification task.\n\n        This method computes text embeddings for the given class names using the\n        provided templates. These embeddings are then used for classification.\n\n        Args:\n            classnames (List[str]): List of class names for the classification task.\n            templates (List[Callable[[str], str]], optional): List of template functions\n                for generating text prompts. Defaults to `default_templates`, i.e.\n                [\"a photo of a {classname}\"].\n        \"\"\"\n        processor = self.processor\n\n        self.classnames = classnames\n        self.templates = templates\n\n        with torch.no_grad():\n            zeroshot_weights = []\n            for classname in classnames:\n                text = [template(classname) for template in templates]\n                inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n                inputs = {\n                    k: v.to(get_device(self.text_model)) for k, v in inputs.items()\n                }\n                embeddings = self.text_model(**inputs)[1]\n                embeddings = self.clip_model.text_projection(embeddings)\n\n                # normalize embeddings\n                embeddings = embeddings / embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                embeddings = embeddings.mean(dim=0)\n                embeddings = embeddings / embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                zeroshot_weights.append(embeddings)\n\n            zeroshot_weights = torch.stack(zeroshot_weights, dim=0)\n\n        self.zeroshot_weights = zeroshot_weights\n\n    def forward(\n        self,\n        images: Tensor,\n        return_image_embeds=False,\n        return_dict=False,\n        task_name=None,\n    ):\n        \"\"\"\n        Perform forward pass for zero-shot image classification.\n\n        This method computes image embeddings for the input images and calculates\n        the similarity with the pre-computed text embeddings to produce classification logits.\n\n        Args:\n            images (Tensor): Input images to classify.\n            return_image_embeds (bool): Whether to return the image embeddings.\n            return_dict (bool): Whether to return a dictionary with logits and image embeddings.\n            task_name (Optional[str]): The name of the task.\n\n        Returns:\n            Tensor: Classification logits for each input image.\n\n        Raises:\n            ValueError: If the classification task hasn't been set using set_classification_task.\n        \"\"\"\n        if self.zeroshot_weights is None:\n            raise ValueError(\"Must set classification task before forward pass\")\n        text_embeds = self.zeroshot_weights\n\n        image_embeds = self.get_image_features(images)\n        # normalize embeddings\n        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n        if (\n            hasattr(self.vision_model, \"is_surgery_model\")\n            and self.vision_model.is_surgery_model\n        ):\n            # Dealing with the surgery model, for more details, please refer to:\n            # (ICML 2024) Yang, et.al. Representation Surgery for Multi-Task Model Merging\n            # https://arxiv.org/abs/2402.02705\n            self.vision_model: \"SurgeryModelWrapper\" = self.vision_model\n            image_embeds, _, _ = self.vision_model.compute_surgery_features(\n                image_embeds, dataset_name=task_name\n            )\n\n        # cosine similarity\n        logit_scale = self.clip_model.logit_scale.exp()\n        logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n        logits_per_image = logits_per_text.t()\n\n        if return_dict:\n            ret = {\"logits\": logits_per_image}\n            if return_image_embeds:\n                ret.update({\"image_embeds\": image_embeds})\n            return ret\n        else:\n            if return_image_embeds:\n                return logits_per_image, image_embeds\n            else:\n                return logits_per_image\n\n    def get_image_features(self, images: Tensor) -&gt; Tensor:\n        \"\"\"\n        Compute the image embeddings.\n\n        Returns:\n            image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n            applying the projection layer to the pooled output of [`CLIPVisionModel`].\n        \"\"\"\n\n        image_embeds = self.vision_model(images)\n        if isinstance(image_embeds, Tensor):\n            pass\n        elif isinstance(image_embeds, BaseModelOutputWithPooling):\n            image_embeds = image_embeds[1]\n        elif isinstance(image_embeds, dict) and \"pooler_output\" in image_embeds:\n            image_embeds = image_embeds[\"pooler_output\"]\n        else:\n            raise ValueError(\"Unsupported output type from vision model outputs\")\n        image_embeds = self.clip_model.visual_projection(image_embeds)\n        return image_embeds\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.models.hf_clip.HFCLIPClassifier.set_classification_task","title":"<code>set_classification_task(classnames, templates=default_templates)</code>","text":"<p>Set up the zero-shot classification task.</p> <p>This method computes text embeddings for the given class names using the provided templates. These embeddings are then used for classification.</p> <p>Parameters:</p> <ul> <li> <code>classnames</code>               (<code>List[str]</code>)           \u2013            <p>List of class names for the classification task.</p> </li> <li> <code>templates</code>               (<code>List[Callable[[str], str]]</code>, default:                   <code>default_templates</code> )           \u2013            <p>List of template functions for generating text prompts. Defaults to <code>default_templates</code>, i.e. [\"a photo of a {classname}\"].</p> </li> </ul> Source code in <code>fusion_bench/models/hf_clip.py</code> <pre><code>def set_classification_task(\n    self,\n    classnames: List[str],\n    templates: List[Callable[[str], str]] = default_templates,\n):\n    \"\"\"\n    Set up the zero-shot classification task.\n\n    This method computes text embeddings for the given class names using the\n    provided templates. These embeddings are then used for classification.\n\n    Args:\n        classnames (List[str]): List of class names for the classification task.\n        templates (List[Callable[[str], str]], optional): List of template functions\n            for generating text prompts. Defaults to `default_templates`, i.e.\n            [\"a photo of a {classname}\"].\n    \"\"\"\n    processor = self.processor\n\n    self.classnames = classnames\n    self.templates = templates\n\n    with torch.no_grad():\n        zeroshot_weights = []\n        for classname in classnames:\n            text = [template(classname) for template in templates]\n            inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n            inputs = {\n                k: v.to(get_device(self.text_model)) for k, v in inputs.items()\n            }\n            embeddings = self.text_model(**inputs)[1]\n            embeddings = self.clip_model.text_projection(embeddings)\n\n            # normalize embeddings\n            embeddings = embeddings / embeddings.norm(p=2, dim=-1, keepdim=True)\n\n            embeddings = embeddings.mean(dim=0)\n            embeddings = embeddings / embeddings.norm(p=2, dim=-1, keepdim=True)\n\n            zeroshot_weights.append(embeddings)\n\n        zeroshot_weights = torch.stack(zeroshot_weights, dim=0)\n\n    self.zeroshot_weights = zeroshot_weights\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.models.hf_clip.HFCLIPClassifier.forward","title":"<code>forward(images, return_image_embeds=False, return_dict=False, task_name=None)</code>","text":"<p>Perform forward pass for zero-shot image classification.</p> <p>This method computes image embeddings for the input images and calculates the similarity with the pre-computed text embeddings to produce classification logits.</p> <p>Parameters:</p> <ul> <li> <code>images</code>               (<code>Tensor</code>)           \u2013            <p>Input images to classify.</p> </li> <li> <code>return_image_embeds</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the image embeddings.</p> </li> <li> <code>return_dict</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return a dictionary with logits and image embeddings.</p> </li> <li> <code>task_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>          \u2013            <p>Classification logits for each input image.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the classification task hasn't been set using set_classification_task.</p> </li> </ul> Source code in <code>fusion_bench/models/hf_clip.py</code> <pre><code>def forward(\n    self,\n    images: Tensor,\n    return_image_embeds=False,\n    return_dict=False,\n    task_name=None,\n):\n    \"\"\"\n    Perform forward pass for zero-shot image classification.\n\n    This method computes image embeddings for the input images and calculates\n    the similarity with the pre-computed text embeddings to produce classification logits.\n\n    Args:\n        images (Tensor): Input images to classify.\n        return_image_embeds (bool): Whether to return the image embeddings.\n        return_dict (bool): Whether to return a dictionary with logits and image embeddings.\n        task_name (Optional[str]): The name of the task.\n\n    Returns:\n        Tensor: Classification logits for each input image.\n\n    Raises:\n        ValueError: If the classification task hasn't been set using set_classification_task.\n    \"\"\"\n    if self.zeroshot_weights is None:\n        raise ValueError(\"Must set classification task before forward pass\")\n    text_embeds = self.zeroshot_weights\n\n    image_embeds = self.get_image_features(images)\n    # normalize embeddings\n    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n    if (\n        hasattr(self.vision_model, \"is_surgery_model\")\n        and self.vision_model.is_surgery_model\n    ):\n        # Dealing with the surgery model, for more details, please refer to:\n        # (ICML 2024) Yang, et.al. Representation Surgery for Multi-Task Model Merging\n        # https://arxiv.org/abs/2402.02705\n        self.vision_model: \"SurgeryModelWrapper\" = self.vision_model\n        image_embeds, _, _ = self.vision_model.compute_surgery_features(\n            image_embeds, dataset_name=task_name\n        )\n\n    # cosine similarity\n    logit_scale = self.clip_model.logit_scale.exp()\n    logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n    logits_per_image = logits_per_text.t()\n\n    if return_dict:\n        ret = {\"logits\": logits_per_image}\n        if return_image_embeds:\n            ret.update({\"image_embeds\": image_embeds})\n        return ret\n    else:\n        if return_image_embeds:\n            return logits_per_image, image_embeds\n        else:\n            return logits_per_image\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#preparing-your-dataset","title":"Preparing Your Dataset","text":"<p>Create a custom dataset class that loads and preprocesses your images:</p> <pre><code>from torchvision import transforms\nfrom PIL import Image\n\nclass SimpleDataset(torch.utils.data.Dataset):\n    def __init__(self, image_paths: List[str], labels: List[int]):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n        ])\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert('RGB')\n        image = self.transform(image)\n        return image, self.labels[idx]\n\n# Create DataLoader\ndataset = SimpleDataset(image_paths, labels)  # Replace with your data\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n</code></pre> <p>You can also use <code>fusion_bench.dataset.clip_dataset.CLIPDataset</code> or <code>fusion_bench.dataset.image_dataset.TransformedImageDataset</code> to prepare your dataset. Here is examples of using <code>fusion_bench.dataset.clip_dataset.CLIPDataset</code> and <code>fusion_bench.dataset.image_dataset.TransformedImageDataset</code> to prepare your dataset:</p> <pre><code>from fusion_bench.dataset.clip_dataset import CLIPDataset\n\ndataset = CLIPDataset(dataset, processor)\n</code></pre> <pre><code>from fusion_bench.dataset.image_dataset import TransformedImageDataset\n\ndataset = TransformedImageDataset(dataset, transform)\n</code></pre> <p>Where <code>dataset</code> is your original dataset and <code>transform</code> is the transform you want to apply to the images. Below is the reference code for these two classes:</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.clip_dataset.CLIPDataset","title":"<code>CLIPDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A dataset class for CLIP models that converts a dataset of dictionaries or tuples into a format suitable for CLIP processing.</p> <p>This class wraps an existing dataset and applies CLIP preprocessing to the images. It expects each item in the dataset to be either a dictionary with 'image' and 'label' keys, or a tuple/list of (image, label).</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The original dataset to wrap.</p> </li> <li> <code>processor</code>               (<code>CLIPProcessor</code>, default:                   <code>None</code> )           \u2013            <p>The CLIP processor for preparing inputs. If None, no preprocessing is applied and raw images are returned.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>dataset</code>           \u2013            <p>The wrapped dataset.</p> </li> <li> <code>processor</code>               (<code>CLIPProcessor</code>)           \u2013            <p>The CLIP processor used for image preprocessing.</p> </li> </ul> Source code in <code>fusion_bench/dataset/clip_dataset.py</code> <pre><code>class CLIPDataset(torch.utils.data.Dataset):\n    \"\"\"\n    A dataset class for CLIP models that converts a dataset of dictionaries or tuples\n    into a format suitable for CLIP processing.\n\n    This class wraps an existing dataset and applies CLIP preprocessing to the images.\n    It expects each item in the dataset to be either a dictionary with 'image' and 'label' keys,\n    or a tuple/list of (image, label).\n\n    Args:\n        dataset: The original dataset to wrap.\n        processor (CLIPProcessor): The CLIP processor for preparing inputs. If None, no preprocessing is applied and raw images are returned.\n\n    Attributes:\n        dataset: The wrapped dataset.\n        processor (CLIPProcessor): The CLIP processor used for image preprocessing.\n    \"\"\"\n\n    def __init__(self, dataset: Dataset, processor: Optional[CLIPProcessor] = None):\n        self.dataset = dataset\n        self.processor = processor\n\n    def __len__(self):\n        \"\"\"Returns the number of items in the dataset.\"\"\"\n        return len(self.dataset)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[torch.Tensor, int]:\n        \"\"\"\n        Retrieves and processes an item from the dataset.\n\n        Args:\n            idx (int): The index of the item to retrieve.\n\n        Returns:\n            tuple: A tuple containing the processed image tensor and the label.\n\n        Raises:\n            ValueError: If the item is neither a dictionary nor a tuple/list of length 2.\n        \"\"\"\n        item = self.dataset[idx]\n        if isinstance(item, dict):\n            item = item\n        elif isinstance(item, (tuple, list)):\n            assert len(item) == 2, \"Each item should be a tuple or list of length 2\"\n            item = {\"image\": item[0], \"label\": item[1]}\n        else:\n            raise ValueError(\"Each item should be a dictionary or a tuple of length 2\")\n        image = item[\"image\"]\n        if self.processor is not None:\n            if isinstance(self.processor, (ProcessorMixin, BaseImageProcessor)):\n                # Apply the processor to the image to get the input tensor\n                inputs = self.processor(images=[image], return_tensors=\"pt\")[\n                    \"pixel_values\"\n                ][0]\n            elif callable(self.processor):\n                inputs = self.processor(image)\n            else:\n                raise ValueError(\n                    \"The processor should be a CLIPProcessor or a callable function\"\n                )\n        else:\n            # if processor is None, return the raw image directly\n            inputs = image\n        # convert boolean label to int, this is for the case when the label is a binary classification task\n        if isinstance(item[\"label\"], bool):\n            item[\"label\"] = 1 if item[\"label\"] else 0\n        return inputs, item[\"label\"]\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.clip_dataset.CLIPDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves and processes an item from the dataset.</p> <p>Parameters:</p> <ul> <li> <code>idx</code>               (<code>int</code>)           \u2013            <p>The index of the item to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code> (              <code>Tuple[Tensor, int]</code> )          \u2013            <p>A tuple containing the processed image tensor and the label.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the item is neither a dictionary nor a tuple/list of length 2.</p> </li> </ul> Source code in <code>fusion_bench/dataset/clip_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Tuple[torch.Tensor, int]:\n    \"\"\"\n    Retrieves and processes an item from the dataset.\n\n    Args:\n        idx (int): The index of the item to retrieve.\n\n    Returns:\n        tuple: A tuple containing the processed image tensor and the label.\n\n    Raises:\n        ValueError: If the item is neither a dictionary nor a tuple/list of length 2.\n    \"\"\"\n    item = self.dataset[idx]\n    if isinstance(item, dict):\n        item = item\n    elif isinstance(item, (tuple, list)):\n        assert len(item) == 2, \"Each item should be a tuple or list of length 2\"\n        item = {\"image\": item[0], \"label\": item[1]}\n    else:\n        raise ValueError(\"Each item should be a dictionary or a tuple of length 2\")\n    image = item[\"image\"]\n    if self.processor is not None:\n        if isinstance(self.processor, (ProcessorMixin, BaseImageProcessor)):\n            # Apply the processor to the image to get the input tensor\n            inputs = self.processor(images=[image], return_tensors=\"pt\")[\n                \"pixel_values\"\n            ][0]\n        elif callable(self.processor):\n            inputs = self.processor(image)\n        else:\n            raise ValueError(\n                \"The processor should be a CLIPProcessor or a callable function\"\n            )\n    else:\n        # if processor is None, return the raw image directly\n        inputs = image\n    # convert boolean label to int, this is for the case when the label is a binary classification task\n    if isinstance(item[\"label\"], bool):\n        item[\"label\"] = 1 if item[\"label\"] else 0\n    return inputs, item[\"label\"]\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.clip_dataset.CLIPDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of items in the dataset.</p> Source code in <code>fusion_bench/dataset/clip_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Returns the number of items in the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.image_dataset.TransformedImageDataset","title":"<code>TransformedImageDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A dataset class for image classification tasks that applies a transform to images.</p> <p>This class wraps an existing dataset and applies a specified transform to the images. It expects each item in the dataset to be either a dictionary with 'image' and 'label' keys, or a tuple/list of (image, label).</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The original dataset to wrap.</p> </li> <li> <code>transform</code>               (<code>Callable</code>)           \u2013            <p>A function/transform to apply on the image.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>dataset</code>           \u2013            <p>The wrapped dataset.</p> </li> <li> <code>transform</code>               (<code>Callable</code>)           \u2013            <p>The transform to be applied to the images.</p> </li> </ul> Source code in <code>fusion_bench/dataset/image_dataset.py</code> <pre><code>class TransformedImageDataset(Dataset):\n    \"\"\"\n    A dataset class for image classification tasks that applies a transform to images.\n\n    This class wraps an existing dataset and applies a specified transform to the images.\n    It expects each item in the dataset to be either a dictionary with 'image' and 'label' keys,\n    or a tuple/list of (image, label).\n\n    Args:\n        dataset: The original dataset to wrap.\n        transform (Callable): A function/transform to apply on the image.\n\n    Attributes:\n        dataset: The wrapped dataset.\n        transform (Callable): The transform to be applied to the images.\n    \"\"\"\n\n    def __init__(self, dataset: Dataset, transform: Callable):\n        super().__init__()\n        self.dataset = dataset\n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"Returns the number of items in the dataset.\"\"\"\n        return len(self.dataset)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[Any, Any]:\n        \"\"\"\n        Retrieves and processes an item from the dataset.\n\n        Args:\n            idx (int): The index of the item to retrieve.\n\n        Returns:\n            tuple: A tuple containing the processed image and the label.\n\n        Raises:\n            ValueError: If the item is neither a dictionary nor a tuple/list of length 2.\n        \"\"\"\n        item = self.dataset[idx]\n        if isinstance(item, dict):\n            item = item\n        elif isinstance(item, (tuple, list)):\n            assert len(item) == 2, \"Each item should be a tuple or list of length 2\"\n            item = {\"image\": item[0], \"label\": item[1]}\n        else:\n            raise ValueError(\"Each item should be a dictionary or a tuple of length 2\")\n        image = item[\"image\"]\n        inputs = self.transform(image)\n        return inputs, item[\"label\"]\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.image_dataset.TransformedImageDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves and processes an item from the dataset.</p> <p>Parameters:</p> <ul> <li> <code>idx</code>               (<code>int</code>)           \u2013            <p>The index of the item to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code> (              <code>Tuple[Any, Any]</code> )          \u2013            <p>A tuple containing the processed image and the label.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the item is neither a dictionary nor a tuple/list of length 2.</p> </li> </ul> Source code in <code>fusion_bench/dataset/image_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Retrieves and processes an item from the dataset.\n\n    Args:\n        idx (int): The index of the item to retrieve.\n\n    Returns:\n        tuple: A tuple containing the processed image and the label.\n\n    Raises:\n        ValueError: If the item is neither a dictionary nor a tuple/list of length 2.\n    \"\"\"\n    item = self.dataset[idx]\n    if isinstance(item, dict):\n        item = item\n    elif isinstance(item, (tuple, list)):\n        assert len(item) == 2, \"Each item should be a tuple or list of length 2\"\n        item = {\"image\": item[0], \"label\": item[1]}\n    else:\n        raise ValueError(\"Each item should be a dictionary or a tuple of length 2\")\n    image = item[\"image\"]\n    inputs = self.transform(image)\n    return inputs, item[\"label\"]\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.image_dataset.TransformedImageDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of items in the dataset.</p> Source code in <code>fusion_bench/dataset/image_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Returns the number of items in the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#inference","title":"Inference","text":"<p>Perform inference on your dataset:</p> <pre><code>classifier.eval()\nwith torch.no_grad():\n    for images, labels in dataloader:\n        logits = classifier(images)\n        predictions = torch.argmax(logits, dim=1)\n        # Process predictions as needed\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fine-tuning-optional","title":"Fine-tuning (Optional)","text":"<p>If you want to fine-tune the model:</p> <pre><code>optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-5)\ncriterion = torch.nn.CrossEntropyLoss()\n\nclassifier.train()\nfor epoch in range(num_epochs):\n    for images, labels in dataloader:\n        optimizer.zero_grad()\n        logits = classifier(images)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guides/clip_vit/HFCLIPClassifier/#custom-templates","title":"Custom Templates","text":"<p>You can provide custom templates when setting up the classification task:</p> <pre><code>custom_templates = [\n    lambda c: f\"a photo of a {c}\",\n    lambda c: f\"an image containing a {c}\",\n]\nclassifier.set_classification_task(class_names, templates=custom_templates)\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#accessing-model-components","title":"Accessing Model Components","text":"<p>You can access the text and vision models directly:</p> <pre><code>text_model = classifier.text_model\nvision_model = classifier.vision_model\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#working-with-zero-shot-weights","title":"Working with Zero-shot Weights","text":"<p>After setting the classification task, you can access the zero-shot weights:</p> <pre><code>zeroshot_weights = classifier.zeroshot_weights\n</code></pre> <p>These weights represent the text embeddings for each class and can be used for further analysis or custom processing.</p> <p>Remember to adjust the code according to your specific dataset and requirements. This documentation provides a comprehensive guide for using the <code>HFCLIPClassifier</code> for image classification tasks with CLIP models.</p>"},{"location":"guides/clip_vit/classification_templates/","title":"CLIP Template Factory Documentation","text":""},{"location":"guides/clip_vit/classification_templates/#overview","title":"Overview","text":"<p><code>CLIPTemplateFactory</code> is a class designed to facilitate the dynamic creation and management of dataset templates for use with CLIP models. It serves as a factory class that allows users to retrieve class names and templates for various datasets, register new datasets, and obtain a list of all available datasets.</p>"},{"location":"guides/clip_vit/classification_templates/#usage-example","title":"Usage Example","text":"<pre><code>from fusion_bench.tasks.clip_classification import CLIPTemplateFactory\n\n# List all available datasets\navailable_datasets = CLIPTemplateFactory.get_available_datasets()\nprint(available_datasets)\n</code></pre> <p>get class names and templates for image classification</p> <pre><code>classnames, templates = CLIPTemplateFactory.get_classnames_and_templates(\"cifar10\")\n# classnames: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n# templates is a list functions, `templates[0](classnames[0])` will return 'a photo of a airplane.'\n\n# or you can use the `get_classnames_and_templates` function\nfrom fusion_bench.tasks.clip_classification import get_classnames_and_templates\n\nclassnames, templates = get_classnames_and_templates(\"cifar10\")\n</code></pre> <p>or you can register a new dataset</p> <pre><code>CLIPTemplateFactory.register_dataset(\n    \"new_dataset\",\n    dataset_info={\n        \"module\": \"module_name\",\n        \"classnames\": \"classnames\",\n        \"templates\": \"templates\"\n    }\n)\n# Retrieve class names and templates for a registered dataset\n# this is equivalent to:\n# &gt;&gt;&gt; from module_name import classnames, templates\nclassnames, templates = CLIPTemplateFactory.get_classnames_and_templates(\"new_dataset\")\n\n# or pass the classnames and templates directly\nCLIPTemplaetFactory.register_dataset(\n    \"new_dataset\",\n    classnames=[\"class1\", \"class2\", \"class3\"],\n    templates=[\n        lambda x: f\"a photo of a {x}.\",\n        lambda x: f\"a picture of a {x}.\",\n        lambda x: f\"an image of a {x}.\"\n    ]\n)\n</code></pre>"},{"location":"guides/clip_vit/classification_templates/#reference","title":"Reference","text":"<p>For detailed API documentation, see fusion_bench.tasks.clip_classification.CLIPTemplateFactory in the API reference.</p>"},{"location":"guides/clip_vit/finetune/","title":"Fine-Tune Your Own Vision Transformer","text":"<p>In this guide, we will show you how to fine-tune your own Vision Transformer (ViT) model on a custom dataset using <code>fusion_bench</code> CLI.  FusionBench provides a simple and easy-to-use interface to fine-tune clip vision transformer in a single-task learning setting or traditional multi-task learning setting.</p>"},{"location":"guides/clip_vit/finetune/#basic-examples","title":"Basic Examples","text":""},{"location":"guides/clip_vit/finetune/#single-task-learning","title":"Single-Task Learning","text":"<p>Refer to <code>examples/clip_finetune/clip_finetune.sh</code> for a complete example of fine-tuning a CLIP-ViT model, including full fine-tuning, lora fine-tuning and linearized lora fine-tuning.</p>"},{"location":"guides/clip_vit/finetune/#multi-task-learning","title":"Multi-Task Learning","text":"<p>Fine-tune CLIP-ViT-B/32:</p> <pre><code>fusion_bench \\\n    method=clip_finetune \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_mtl \\\n    taskpool=dummy\n</code></pre> <p>Fine-tune CLIP-ViT-L/14 on eight GPUs with a per-device per-task batch size of 2.</p> <pre><code>fusion_bench \\\n    fabric.devices=8 \\\n    method=clip_finetune \\\n        method.batch_size=2 \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_mtl \\\n        modelpool.models._pretrained_.pretrained_model_name_or_path=openai/clip-vit-large-patch14 \\\n    taskpool=dummy\n</code></pre> <p>This will save the state dict of the vision model (<code>transformers.models.clip.CLIPVisionModel.CLIPVisionTransformer</code>) to the log directory. Subsequently, we can use <code>fusion_bench/scripts/clip/convert_checkpoint.py</code> to convert the state dict to a HuggingFace model (<code>CLIPVisionModel</code>).</p> method configurationmodel pool configuration config/method/clip_finetune.yaml<pre><code>name: clip_finetune\n\nseed: 42\n\nlearning_rate: 1e-5\nnum_steps: 4000\n\nbatch_size: 32\nnum_workers: 4\n\nsave_interval: 500\n</code></pre> config/modelpool/CLIPVisionModelPool/clip-vit-base-patch32_mtl.yaml<pre><code>defaults:\n  - CLIPVisionModelPool@: _template\n  - /model/clip-vit@models:\n      - clip-vit-base-patch32\n  - /dataset/image_classification/train@train_datasets: the_eight_tasks\n</code></pre> <pre><code># or CLIP-ViT-L/14, add option: --model openai/clip-vit-large-patch14\npython fusion_bench/scripts/clip/convert_checkpoint.py \\\n    --checkpoint /path/to/checkpoint \\\n    --output /path/to/output\n</code></pre> <p>After converting the checkpoint, you can use FusionBench to evaluate the model. For example, you can use the following command to evaluate the model on the eight tasks documented here.</p> <pre><code>path_to_clip_model=/path/to/converted/output\nfusion_bench method=dummy \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_individual \\\n    modelpool.models._pretrained_.pretrained_model_name_or_path=\"'${path_to_clip_model}'\" \\\n  taskpool=clip-vit-classification_TA8\n</code></pre>"},{"location":"guides/clip_vit/finetune/#single-task-learning_1","title":"Single-Task Learning","text":"<p>Simply remove some of the datasets from the <code>train_datasets</code> field in the model pool configuration.</p>"},{"location":"guides/clip_vit/finetune/#references","title":"References","text":""},{"location":"guides/clip_vit/finetune/#fusion_bench.method.classification.clip_finetune.ImageClassificationFineTuningForCLIP","title":"<code>ImageClassificationFineTuningForCLIP</code>","text":"<p>               Bases: <code>CLIPClassificationMixin</code>, <code>SimpleProfilerMixin</code>, <code>ModelFusionAlgorithm</code></p> <p>A class for fine-tuning CLIP models for image classification tasks.</p> Source code in <code>fusion_bench/method/classification/clip_finetune.py</code> <pre><code>class ImageClassificationFineTuningForCLIP(\n    CLIPClassificationMixin,\n    SimpleProfilerMixin,\n    ModelFusionAlgorithm,\n):\n    \"\"\"\n    A class for fine-tuning CLIP models for image classification tasks.\n    \"\"\"\n\n    def run(self, modelpool: CLIPVisionModelPool):\n        \"\"\"\n        Executes the fine-tuning process.\n\n        Args:\n            modelpool (CLIPVisionModelPool): The modelpool is responsible for loading the pre-trained model and training datasets.\n\n        Returns:\n            VisionModel: The fine-tuned vision model.\n        \"\"\"\n        self.modelpool = to_modelpool(modelpool)\n        config = self.config\n        self.log_hyperparams(config, filename=\"method_config.yaml\")\n        self.finetune_method = \"fine-tune\"\n\n        L.seed_everything(config.seed)\n\n        task_names = modelpool.train_dataset_names\n        with self.profile(\"setup model and optimizer\"):\n            processor, classifier, optimizer, lr_scheduler = self.setup_model()\n\n            if config.state_dict_load_path is not None:\n                self.fabric.load(\n                    config.state_dict_load_path,\n                    {\"vision_model\": classifier.clip_model.vision_model},\n                )\n                if config.skip_training:\n                    return classifier.clip_model.vision_model\n\n            self.setup_zero_shot_classification_head(\n                clip_processor=processor,\n                clip_model=classifier.clip_model,\n                task_names=task_names,\n            )\n\n            self.fabric.setup(classifier, optimizer)\n\n        with self.profile(\"setup data\"):\n            train_datasets = [\n                CLIPDataset(modelpool.load_train_dataset(task_name), processor)\n                for task_name in task_names\n            ]\n            train_dataloaders = [\n                DataLoader(\n                    dataset,\n                    shuffle=True,\n                    batch_size=config.batch_size,\n                    num_workers=config.num_workers,\n                )\n                for dataset in train_datasets\n            ]\n            train_dataloaders = self.fabric.setup_dataloaders(*train_dataloaders)\n            if not isinstance(train_dataloaders, (list, tuple)):\n                train_dataloaders = [train_dataloaders]\n            train_dataloader_iters = [\n                iter(InfiniteDataLoader(loader)) for loader in train_dataloaders\n            ]\n\n        # train\n        for step_idx in tqdm(\n            range(config.num_steps),\n            desc=self.finetune_method,\n            disable=not self.fabric.is_global_zero,\n            dynamic_ncols=True,\n        ):\n            optimizer.zero_grad()\n            loss = 0\n            for task, loader in zip(task_names, train_dataloader_iters):\n                with self.profile(\"data loading\"):\n                    batch = next(loader)\n                    images, labels = batch\n                with self.profile(\"forward\"):\n                    classifier.zeroshot_weights = self.zeroshot_weights[task]\n                    logits = classifier(images)\n                loss = loss + nn.functional.cross_entropy(logits, labels)\n\n            with self.profile(\"backward\"):\n                self.fabric.backward(loss)\n            with self.profile(\"optimizer step\"):\n                optimizer.step()\n                lr_scheduler.step()\n\n            metrics = {\"train/loss\": loss}\n\n            self.fabric.log_dict(metrics, step=step_idx)\n\n            if (step_idx + 1) % config.save_interval == 0:\n                save_path = os.path.join(\n                    self.log_dir, \"checkpoints\", f\"step={step_idx}.ckpt\"\n                )\n                self.save_model(classifier, save_path)\n\n        if config.state_dict_save_path is not None:\n            self.save_model(classifier, config.state_dict_save_path)\n        self.print_profile_summary()\n        return classifier.clip_model.vision_model\n\n    def save_model(\n        self,\n        model: HFCLIPClassifier | CLIPModel | CLIPVisionModel | CLIPVisionTransformer,\n        save_path: str,\n    ):\n        \"\"\"\n        Save the vision model to the specified path.\n\n        Args:\n            model (Union[HFCLIPClassifier, CLIPModel, CLIPVisionModel, CLIPVisionTransformer]): The model to save.\n            save_path (str): The path to save the model.\n        \"\"\"\n        if isinstance(model, HFCLIPClassifier):\n            vision_model = model.clip_model.vision_model\n        elif isinstance(model, CLIPModel):\n            vision_model = model.vision_model\n        elif isinstance(model, CLIPVisionModel):\n            vision_model = model.vision_model\n        elif isinstance(model, CLIPVisionTransformer):\n            vision_model = model\n        else:\n            raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n        save_dir = os.path.dirname(save_path)\n        if save_dir and not os.path.exists(save_dir):\n            os.makedirs(save_dir, exist_ok=True)\n        self.fabric.save(save_path, {\"vision_model\": vision_model})\n\n    def setup_model(self):\n        \"\"\"\n        Sets up the model, optimizer, and learning rate scheduler.\n\n        This method initializes the CLIP model, applies LoRA if specified, and configures the optimizer and learning rate scheduler.\n\n        Returns:\n            Tuple: A tuple containing the processor, classifier, optimizer, and learning rate scheduler.\n        \"\"\"\n        config = self.config\n        modelpool = self.modelpool\n\n        clip_model: CLIPModel = modelpool.load_clip_model(\"_pretrained_\")\n        processor = modelpool.load_processor()\n\n        self.finetune_method = \"full fine-tune\"\n        if config.use_lora or config.use_l_lora:\n            self.finetune_method = \"lora fine-tune\"\n            lora_config = LoraConfig(\n                **OmegaConf.to_container(\n                    config.lora_config, resolve=True, enum_to_str=True\n                )\n            )\n            clip_model.vision_model = get_peft_model(\n                clip_model.vision_model, lora_config\n            )\n\n            if config.use_l_lora:\n                # http://arxiv.org/abs/2310.04742\n                # Anke Tang et al. Parameter Efficient Multi-task Model Fusion with Partial Linearization. ICLR 2024.\n                self.finetune_method = \"l-lora fine-tune\"\n                print(\"Linearizing Lora Layers\")\n                linearize_lora_model_(clip_model.vision_model)\n\n        classifier = HFCLIPClassifier(clip_model, processor=processor)\n\n        if self.fabric.is_global_zero:\n            print(\"=== Model Summary (For Vision Model Only) ===\")\n            print_parameters(classifier.clip_model.vision_model)\n        # configure optimizers\n        optimizer = torch.optim.Adam(\n            [\n                p\n                for p in classifier.clip_model.vision_model.parameters()\n                if p.requires_grad\n            ],\n            lr=config.learning_rate,\n            weight_decay=config.weight_decay,\n        )\n        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer=optimizer, T_max=config.num_steps\n        )\n\n        return processor, classifier, optimizer, lr_scheduler\n</code></pre>"},{"location":"guides/clip_vit/finetune/#fusion_bench.method.classification.clip_finetune.ImageClassificationFineTuningForCLIP.run","title":"<code>run(modelpool)</code>","text":"<p>Executes the fine-tuning process.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code>               (<code>CLIPVisionModelPool</code>)           \u2013            <p>The modelpool is responsible for loading the pre-trained model and training datasets.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>VisionModel</code>          \u2013            <p>The fine-tuned vision model.</p> </li> </ul> Source code in <code>fusion_bench/method/classification/clip_finetune.py</code> <pre><code>def run(self, modelpool: CLIPVisionModelPool):\n    \"\"\"\n    Executes the fine-tuning process.\n\n    Args:\n        modelpool (CLIPVisionModelPool): The modelpool is responsible for loading the pre-trained model and training datasets.\n\n    Returns:\n        VisionModel: The fine-tuned vision model.\n    \"\"\"\n    self.modelpool = to_modelpool(modelpool)\n    config = self.config\n    self.log_hyperparams(config, filename=\"method_config.yaml\")\n    self.finetune_method = \"fine-tune\"\n\n    L.seed_everything(config.seed)\n\n    task_names = modelpool.train_dataset_names\n    with self.profile(\"setup model and optimizer\"):\n        processor, classifier, optimizer, lr_scheduler = self.setup_model()\n\n        if config.state_dict_load_path is not None:\n            self.fabric.load(\n                config.state_dict_load_path,\n                {\"vision_model\": classifier.clip_model.vision_model},\n            )\n            if config.skip_training:\n                return classifier.clip_model.vision_model\n\n        self.setup_zero_shot_classification_head(\n            clip_processor=processor,\n            clip_model=classifier.clip_model,\n            task_names=task_names,\n        )\n\n        self.fabric.setup(classifier, optimizer)\n\n    with self.profile(\"setup data\"):\n        train_datasets = [\n            CLIPDataset(modelpool.load_train_dataset(task_name), processor)\n            for task_name in task_names\n        ]\n        train_dataloaders = [\n            DataLoader(\n                dataset,\n                shuffle=True,\n                batch_size=config.batch_size,\n                num_workers=config.num_workers,\n            )\n            for dataset in train_datasets\n        ]\n        train_dataloaders = self.fabric.setup_dataloaders(*train_dataloaders)\n        if not isinstance(train_dataloaders, (list, tuple)):\n            train_dataloaders = [train_dataloaders]\n        train_dataloader_iters = [\n            iter(InfiniteDataLoader(loader)) for loader in train_dataloaders\n        ]\n\n    # train\n    for step_idx in tqdm(\n        range(config.num_steps),\n        desc=self.finetune_method,\n        disable=not self.fabric.is_global_zero,\n        dynamic_ncols=True,\n    ):\n        optimizer.zero_grad()\n        loss = 0\n        for task, loader in zip(task_names, train_dataloader_iters):\n            with self.profile(\"data loading\"):\n                batch = next(loader)\n                images, labels = batch\n            with self.profile(\"forward\"):\n                classifier.zeroshot_weights = self.zeroshot_weights[task]\n                logits = classifier(images)\n            loss = loss + nn.functional.cross_entropy(logits, labels)\n\n        with self.profile(\"backward\"):\n            self.fabric.backward(loss)\n        with self.profile(\"optimizer step\"):\n            optimizer.step()\n            lr_scheduler.step()\n\n        metrics = {\"train/loss\": loss}\n\n        self.fabric.log_dict(metrics, step=step_idx)\n\n        if (step_idx + 1) % config.save_interval == 0:\n            save_path = os.path.join(\n                self.log_dir, \"checkpoints\", f\"step={step_idx}.ckpt\"\n            )\n            self.save_model(classifier, save_path)\n\n    if config.state_dict_save_path is not None:\n        self.save_model(classifier, config.state_dict_save_path)\n    self.print_profile_summary()\n    return classifier.clip_model.vision_model\n</code></pre>"},{"location":"guides/clip_vit/finetune/#fusion_bench.method.classification.clip_finetune.ImageClassificationFineTuningForCLIP.save_model","title":"<code>save_model(model, save_path)</code>","text":"<p>Save the vision model to the specified path.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Union[HFCLIPClassifier, CLIPModel, CLIPVisionModel, CLIPVisionTransformer]</code>)           \u2013            <p>The model to save.</p> </li> <li> <code>save_path</code>               (<code>str</code>)           \u2013            <p>The path to save the model.</p> </li> </ul> Source code in <code>fusion_bench/method/classification/clip_finetune.py</code> <pre><code>def save_model(\n    self,\n    model: HFCLIPClassifier | CLIPModel | CLIPVisionModel | CLIPVisionTransformer,\n    save_path: str,\n):\n    \"\"\"\n    Save the vision model to the specified path.\n\n    Args:\n        model (Union[HFCLIPClassifier, CLIPModel, CLIPVisionModel, CLIPVisionTransformer]): The model to save.\n        save_path (str): The path to save the model.\n    \"\"\"\n    if isinstance(model, HFCLIPClassifier):\n        vision_model = model.clip_model.vision_model\n    elif isinstance(model, CLIPModel):\n        vision_model = model.vision_model\n    elif isinstance(model, CLIPVisionModel):\n        vision_model = model.vision_model\n    elif isinstance(model, CLIPVisionTransformer):\n        vision_model = model\n    else:\n        raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n    save_dir = os.path.dirname(save_path)\n    if save_dir and not os.path.exists(save_dir):\n        os.makedirs(save_dir, exist_ok=True)\n    self.fabric.save(save_path, {\"vision_model\": vision_model})\n</code></pre>"},{"location":"guides/clip_vit/finetune/#fusion_bench.method.classification.clip_finetune.ImageClassificationFineTuningForCLIP.setup_model","title":"<code>setup_model()</code>","text":"<p>Sets up the model, optimizer, and learning rate scheduler.</p> <p>This method initializes the CLIP model, applies LoRA if specified, and configures the optimizer and learning rate scheduler.</p> <p>Returns:</p> <ul> <li> <code>Tuple</code>          \u2013            <p>A tuple containing the processor, classifier, optimizer, and learning rate scheduler.</p> </li> </ul> Source code in <code>fusion_bench/method/classification/clip_finetune.py</code> <pre><code>def setup_model(self):\n    \"\"\"\n    Sets up the model, optimizer, and learning rate scheduler.\n\n    This method initializes the CLIP model, applies LoRA if specified, and configures the optimizer and learning rate scheduler.\n\n    Returns:\n        Tuple: A tuple containing the processor, classifier, optimizer, and learning rate scheduler.\n    \"\"\"\n    config = self.config\n    modelpool = self.modelpool\n\n    clip_model: CLIPModel = modelpool.load_clip_model(\"_pretrained_\")\n    processor = modelpool.load_processor()\n\n    self.finetune_method = \"full fine-tune\"\n    if config.use_lora or config.use_l_lora:\n        self.finetune_method = \"lora fine-tune\"\n        lora_config = LoraConfig(\n            **OmegaConf.to_container(\n                config.lora_config, resolve=True, enum_to_str=True\n            )\n        )\n        clip_model.vision_model = get_peft_model(\n            clip_model.vision_model, lora_config\n        )\n\n        if config.use_l_lora:\n            # http://arxiv.org/abs/2310.04742\n            # Anke Tang et al. Parameter Efficient Multi-task Model Fusion with Partial Linearization. ICLR 2024.\n            self.finetune_method = \"l-lora fine-tune\"\n            print(\"Linearizing Lora Layers\")\n            linearize_lora_model_(clip_model.vision_model)\n\n    classifier = HFCLIPClassifier(clip_model, processor=processor)\n\n    if self.fabric.is_global_zero:\n        print(\"=== Model Summary (For Vision Model Only) ===\")\n        print_parameters(classifier.clip_model.vision_model)\n    # configure optimizers\n    optimizer = torch.optim.Adam(\n        [\n            p\n            for p in classifier.clip_model.vision_model.parameters()\n            if p.requires_grad\n        ],\n        lr=config.learning_rate,\n        weight_decay=config.weight_decay,\n    )\n    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer=optimizer, T_max=config.num_steps\n    )\n\n    return processor, classifier, optimizer, lr_scheduler\n</code></pre>"},{"location":"guides/fusion_bench/","title":"Fusion Module Documentation","text":"<ul> <li>method: Implements different methods and algorithms for model training, evaluation, and other tasks. This includes base algorithms, ensemble methods, model recombination techniques, and more.  Read More</li> <li>modelpool: Manages different model pools. This module includes classes and functions for handling various models, including sequence-to-sequence language models, CLIP models, GPT-2 models, and models specific to the NYU Depth V2 dataset.  Read More</li> <li>taskpool: Manages different task pools. This module includes classes and functions for handling various tasks, such as image classification with CLIP, text generation with FLAN-T5, and tasks specific to the NYU Depth V2 dataset.  Read More</li> <li>models: Contains model definitions and utilities. This module includes implementations of different models, parameter management, input/output handling, and utility functions for model operations.</li> <li>tasks: Defines various tasks. This module includes implementations of different tasks, such as classification, text generation, and specific tasks for models like CLIP and FLAN-T5.</li> <li>dataset: Handles various datasets used in the project. This module includes dataset loaders and preprocessors for different types of data such as images, text, and specific datasets like NYU Depth V2.</li> <li>metrics: Defines metrics for evaluating models. This module includes specific metrics for different tasks and datasets, such as NYU Depth V2 and text-to-image generation.</li> <li>optim: Defines optimization algorithms. This module includes custom optimization algorithms used for training models.</li> <li>mixins: Provides mixin classes for additional functionalities. These mixins can be used to extend the capabilities of other classes, such as integrating with Lightning Fabric, providing live updates with the Rich library, and simple profiling.<ul> <li><code>LightningFabricMixin</code>: A mixin class for integrating Lightning Fabric into a project.  Read More</li> <li><code>SimpleProfilerMixin</code>: Adding profiling capabilities to your Python classes.  Read More</li> </ul> </li> <li>constants: Contains constant values and configurations used throughout the project.</li> <li>scripts: Contains scripts for various tasks. This includes command-line interface scripts, training scripts, and other utility scripts for managing and running different tasks.</li> <li>utils: Provides utility functions and classes. This module includes general utilities for data handling, device management, logging, timing, and other common operations needed throughout the project.</li> </ul>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/","title":"LightningFabricMixin","text":""},{"location":"guides/fusion_bench/mixins/lightning_fabric/#reference","title":"Reference","text":""},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric","title":"<code>lightning_fabric</code>","text":""},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.LightningFabricMixin","title":"<code>LightningFabricMixin</code>","text":"<p>A mixin class for integrating Lightning Fabric into a project.</p> <p>This class provides methods to initialize and manage a Lightning Fabric instance for distributed computing, including setup with optional logging, device management for tensors and modules, and hyperparameter logging. It leverages the Lightning framework to facilitate distributed training and inference across multiple devices and nodes, with support for custom logging via TensorBoard.</p> <p>Attributes:</p> <ul> <li>_fabric (L.Fabric): The Lightning Fabric instance used for distributed computing.</li> </ul> <p>Note:</p> <p>This mixin is designed to be used with classes that require distributed computing capabilities and wish to leverage the Lightning Fabric for this purpose. It assumes the presence of a <code>config</code> attribute or parameter in the consuming class for configuration.</p> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>class LightningFabricMixin:\n    \"\"\"\n    A mixin class for integrating Lightning Fabric into a project.\n\n    This class provides methods to initialize and manage a Lightning Fabric instance for distributed computing,\n    including setup with optional logging, device management for tensors and modules, and hyperparameter logging.\n    It leverages the Lightning framework to facilitate distributed training and inference across multiple devices\n    and nodes, with support for custom logging via TensorBoard.\n\n    Attributes:\n\n    - _fabric (L.Fabric): The Lightning Fabric instance used for distributed computing.\n\n    Note:\n\n    This mixin is designed to be used with classes that require distributed computing capabilities and wish to\n    leverage the Lightning Fabric for this purpose. It assumes the presence of a `config` attribute or parameter\n    in the consuming class for configuration.\n    \"\"\"\n\n    _fabric_instance: L.Fabric = None\n\n    def setup_lightning_fabric(self, config: DictConfig):\n        \"\"\"\n        Initializes and launches the Lightning Fabric with optional logging.\n\n        This method sets up the Lightning Fabric for distributed computing based on the provided configuration. If a fabric\n        configuration is not found, it logs a warning and exits. Optionally, if a fabric logger configuration is provided,\n        it initializes a TensorBoardLogger with the specified settings.\n\n        Expected configuration keys:\n        - fabric: The configuration for the Lightning Fabric.\n        - fabric.loggers: The configuration for the TensorBoardLogger.\n        \"\"\"\n        if self._fabric_instance is None:\n            if config.get(\"fabric\", None) is None:\n                log.warning(\"No fabric configuration found. use default settings.\")\n                self._fabric_instance = L.Fabric()\n            else:\n                self._fabric_instance = instantiate(config.fabric)\n            if not _is_using_cli():  # if not using cli, launch the fabric\n                self._fabric_instance.launch()\n            # Set the log directory in config if it is not already set\n            if (\n                self.log_dir is not None\n                and hasattr(config, \"log_dir\")\n                and config.get(\"log_dir\", None) is None\n            ):\n                if self._fabric_instance.is_global_zero:\n                    log.info(f\"Setting log_dir to {self.log_dir}\")\n                config.log_dir = self.log_dir\n\n    @property\n    def fabric(self):\n        if self._fabric_instance is None:\n            self.setup_lightning_fabric(getattr(self, \"config\", DictConfig({})))\n        return self._fabric_instance\n\n    @fabric.setter\n    def fabric(self, instance: L.Fabric):\n        self._fabric_instance = instance\n\n    @property\n    def log_dir(self):\n        \"\"\"\n        Retrieves the log directory from the fabric's logger.\n        \"\"\"\n        if self.fabric is not None and len(self.fabric._loggers) &gt; 0:\n            log_dir = self.fabric.logger.log_dir\n\n            # Special handling for SwanLabLogger to get the correct log directory\n            if (\n                log_dir is None\n                and self.fabric.logger.__class__.__name__ == \"SwanLabLogger\"\n            ):\n                log_dir = self.fabric.logger.save_dir or self.fabric.logger._logdir\n\n            assert log_dir is not None, \"log_dir should not be None\"\n            if self.fabric.is_global_zero and not os.path.exists(log_dir):\n                os.makedirs(log_dir, exist_ok=True)\n            return log_dir\n        else:\n            return None\n\n    def to_device(self, obj: TensorOrModule) -&gt; TensorOrModule:\n        \"\"\"\n        Moves a tensor or module to the proper device.\n\n        Args:\n            obj (TensorOrModule): The tensor or module to move to the device.\n\n        Returns:\n            TensorOrModule: the same type of object as the input, moved to the device.\n        \"\"\"\n        return self.fabric.to_device(obj)\n\n    @rank_zero_only\n    def log_hyperparams(\n        self,\n        config: Optional[DictConfig] = None,\n        save_dir: Optional[str] = None,\n        filename: str = \"config.yaml\",\n    ):\n        R\"\"\"\n        Logs the hyperparameters and saves the configuration to a YAML file.\n        The YAML file is saved in the log directory by default with the name `config.yaml`, or in the specified save directory `save_dir`.\n\n        Args:\n            config (Optional[DictConfig]): The configuration to log and save. If not provided, the class's `config` attribute is used.\n            save_dir (Optional[str]): The directory in which to save the configuration file. If not provided, the log directory is used.\n            filename (str): The name of the configuration file. Default is `config.yaml`.\n        \"\"\"\n        if config is None:\n            config = self.config\n        if save_dir is None:\n            save_dir = self.log_dir\n        self.fabric.logger.log_hyperparams(\n            OmegaConf.to_container(config, resolve=True, enum_to_str=True)\n        )\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir, exist_ok=True)\n        OmegaConf.save(\n            config,\n            os.path.join(self.log_dir if save_dir is None else save_dir, filename),\n        )\n\n    @property\n    def tensorboard_summarywriter(\n        self,\n    ) -&gt; \"lightning.fabric.loggers.tensorboard.SummaryWriter\":\n        if isinstance(self.fabric.logger, TensorBoardLogger):\n            return self.fabric.logger.experiment\n        else:\n            raise AttributeError(\"the logger is not a TensorBoardLogger.\")\n\n    @property\n    def is_debug_mode(self):\n        if hasattr(self, \"config\") and self.config.get(\"fast_dev_run\", False):\n            return True\n        elif hasattr(self, \"_program\") and self._program.config.get(\n            \"fast_dev_run\", False\n        ):\n            return True\n        else:\n            return False\n\n    def log(self, name: str, value: Any, step: Optional[int] = None):\n        \"\"\"\n        Logs the metric to the fabric's logger.\n        \"\"\"\n        self.fabric.log(name, value, step=step)\n\n    def log_dict(self, metrics: dict, step: Optional[int] = None):\n        \"\"\"\n        Logs the metrics to the fabric's logger.\n        \"\"\"\n        self.fabric.log_dict(metrics, step=step)\n\n    def log_optimizer_lr(\n        self,\n        optimizer: torch.optim.Optimizer,\n        step: Optional[int] = None,\n        name_template: str = \"train/lr_group_{0}\",\n    ):\n        \"\"\"\n        Logs the learning rate of the optimizer to the fabric's logger.\n        \"\"\"\n        for i, param_group in enumerate(optimizer.param_groups):\n            self.fabric.log(name_template.format(i), param_group[\"lr\"], step=step)\n</code></pre>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.LightningFabricMixin.log_dir","title":"<code>log_dir</code>  <code>property</code>","text":"<p>Retrieves the log directory from the fabric's logger.</p>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.LightningFabricMixin.log","title":"<code>log(name, value, step=None)</code>","text":"<p>Logs the metric to the fabric's logger.</p> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>def log(self, name: str, value: Any, step: Optional[int] = None):\n    \"\"\"\n    Logs the metric to the fabric's logger.\n    \"\"\"\n    self.fabric.log(name, value, step=step)\n</code></pre>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.LightningFabricMixin.log_dict","title":"<code>log_dict(metrics, step=None)</code>","text":"<p>Logs the metrics to the fabric's logger.</p> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>def log_dict(self, metrics: dict, step: Optional[int] = None):\n    \"\"\"\n    Logs the metrics to the fabric's logger.\n    \"\"\"\n    self.fabric.log_dict(metrics, step=step)\n</code></pre>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.LightningFabricMixin.log_hyperparams","title":"<code>log_hyperparams(config=None, save_dir=None, filename='config.yaml')</code>","text":"<p>Logs the hyperparameters and saves the configuration to a YAML file. The YAML file is saved in the log directory by default with the name <code>config.yaml</code>, or in the specified save directory <code>save_dir</code>.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>Optional[DictConfig]</code>, default:                   <code>None</code> )           \u2013            <p>The configuration to log and save. If not provided, the class's <code>config</code> attribute is used.</p> </li> <li> <code>save_dir</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The directory in which to save the configuration file. If not provided, the log directory is used.</p> </li> <li> <code>filename</code>               (<code>str</code>, default:                   <code>'config.yaml'</code> )           \u2013            <p>The name of the configuration file. Default is <code>config.yaml</code>.</p> </li> </ul> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>@rank_zero_only\ndef log_hyperparams(\n    self,\n    config: Optional[DictConfig] = None,\n    save_dir: Optional[str] = None,\n    filename: str = \"config.yaml\",\n):\n    R\"\"\"\n    Logs the hyperparameters and saves the configuration to a YAML file.\n    The YAML file is saved in the log directory by default with the name `config.yaml`, or in the specified save directory `save_dir`.\n\n    Args:\n        config (Optional[DictConfig]): The configuration to log and save. If not provided, the class's `config` attribute is used.\n        save_dir (Optional[str]): The directory in which to save the configuration file. If not provided, the log directory is used.\n        filename (str): The name of the configuration file. Default is `config.yaml`.\n    \"\"\"\n    if config is None:\n        config = self.config\n    if save_dir is None:\n        save_dir = self.log_dir\n    self.fabric.logger.log_hyperparams(\n        OmegaConf.to_container(config, resolve=True, enum_to_str=True)\n    )\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir, exist_ok=True)\n    OmegaConf.save(\n        config,\n        os.path.join(self.log_dir if save_dir is None else save_dir, filename),\n    )\n</code></pre>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.LightningFabricMixin.log_optimizer_lr","title":"<code>log_optimizer_lr(optimizer, step=None, name_template='train/lr_group_{0}')</code>","text":"<p>Logs the learning rate of the optimizer to the fabric's logger.</p> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>def log_optimizer_lr(\n    self,\n    optimizer: torch.optim.Optimizer,\n    step: Optional[int] = None,\n    name_template: str = \"train/lr_group_{0}\",\n):\n    \"\"\"\n    Logs the learning rate of the optimizer to the fabric's logger.\n    \"\"\"\n    for i, param_group in enumerate(optimizer.param_groups):\n        self.fabric.log(name_template.format(i), param_group[\"lr\"], step=step)\n</code></pre>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.LightningFabricMixin.setup_lightning_fabric","title":"<code>setup_lightning_fabric(config)</code>","text":"<p>Initializes and launches the Lightning Fabric with optional logging.</p> <p>This method sets up the Lightning Fabric for distributed computing based on the provided configuration. If a fabric configuration is not found, it logs a warning and exits. Optionally, if a fabric logger configuration is provided, it initializes a TensorBoardLogger with the specified settings.</p> <p>Expected configuration keys: - fabric: The configuration for the Lightning Fabric. - fabric.loggers: The configuration for the TensorBoardLogger.</p> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>def setup_lightning_fabric(self, config: DictConfig):\n    \"\"\"\n    Initializes and launches the Lightning Fabric with optional logging.\n\n    This method sets up the Lightning Fabric for distributed computing based on the provided configuration. If a fabric\n    configuration is not found, it logs a warning and exits. Optionally, if a fabric logger configuration is provided,\n    it initializes a TensorBoardLogger with the specified settings.\n\n    Expected configuration keys:\n    - fabric: The configuration for the Lightning Fabric.\n    - fabric.loggers: The configuration for the TensorBoardLogger.\n    \"\"\"\n    if self._fabric_instance is None:\n        if config.get(\"fabric\", None) is None:\n            log.warning(\"No fabric configuration found. use default settings.\")\n            self._fabric_instance = L.Fabric()\n        else:\n            self._fabric_instance = instantiate(config.fabric)\n        if not _is_using_cli():  # if not using cli, launch the fabric\n            self._fabric_instance.launch()\n        # Set the log directory in config if it is not already set\n        if (\n            self.log_dir is not None\n            and hasattr(config, \"log_dir\")\n            and config.get(\"log_dir\", None) is None\n        ):\n            if self._fabric_instance.is_global_zero:\n                log.info(f\"Setting log_dir to {self.log_dir}\")\n            config.log_dir = self.log_dir\n</code></pre>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.LightningFabricMixin.to_device","title":"<code>to_device(obj)</code>","text":"<p>Moves a tensor or module to the proper device.</p> <p>Parameters:</p> <ul> <li> <code>obj</code>               (<code>TensorOrModule</code>)           \u2013            <p>The tensor or module to move to the device.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TensorOrModule</code> (              <code>TensorOrModule</code> )          \u2013            <p>the same type of object as the input, moved to the device.</p> </li> </ul> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>def to_device(self, obj: TensorOrModule) -&gt; TensorOrModule:\n    \"\"\"\n    Moves a tensor or module to the proper device.\n\n    Args:\n        obj (TensorOrModule): The tensor or module to move to the device.\n\n    Returns:\n        TensorOrModule: the same type of object as the input, moved to the device.\n    \"\"\"\n    return self.fabric.to_device(obj)\n</code></pre>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.get_policy","title":"<code>get_policy(*args)</code>","text":"<p>Get the policy from the provided list of policy names.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of policy names.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set</code> (              <code>set</code> )          \u2013            <p>A set of policy objects.</p> </li> </ul> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>def get_policy(*args: str) -&gt; set:\n    \"\"\"\n    Get the policy from the provided list of policy names.\n\n    Args:\n        *args (str): A list of policy names.\n\n    Returns:\n        set: A set of policy objects.\n    \"\"\"\n    return {import_object(arg) for arg in args}\n</code></pre>"},{"location":"guides/fusion_bench/mixins/pyinstrument_profiling/","title":"PyinstrumentProfilerMixin","text":"<p>The <code>PyinstrumentProfilerMixin</code> provides statistical profiling capabilities using the pyinstrument library. This mixin allows you to easily profile code execution time and identify performance bottlenecks in your applications.</p>"},{"location":"guides/fusion_bench/mixins/pyinstrument_profiling/#installation","title":"Installation","text":"<p>First, install pyinstrument:</p> <pre><code>pip install pyinstrument\n</code></pre>"},{"location":"guides/fusion_bench/mixins/pyinstrument_profiling/#usage","title":"Usage","text":""},{"location":"guides/fusion_bench/mixins/pyinstrument_profiling/#basic-usage","title":"Basic Usage","text":"<pre><code>from fusion_bench.mixins import PyinstrumentProfilerMixin\n\nclass MyClass(PyinstrumentProfilerMixin):\n    def expensive_computation(self):\n        # Profile a specific code block\n        with self.profile(\"computation\"):\n            # Your expensive code here\n            result = sum(i**2 for i in range(1000000))\n\n        # Print profiling summary\n        self.print_profile_summary(\"Performance Report\")\n\n        # Save detailed HTML report\n        self.save_profile_report(\"profile.html\")\n\n        return result\n</code></pre>"},{"location":"guides/fusion_bench/mixins/pyinstrument_profiling/#manual-profiling-control","title":"Manual Profiling Control","text":"<pre><code>class MyClass(PyinstrumentProfilerMixin):\n    def run_analysis(self):\n        # Start profiling manually\n        self.start_profile(\"analysis\")\n\n        # Your code here\n        self.step1()\n        self.step2()\n        self.step3()\n\n        # Stop profiling\n        self.stop_profile(\"analysis\")\n\n        # Print results\n        self.print_profile_summary()\n</code></pre>"},{"location":"guides/fusion_bench/mixins/pyinstrument_profiling/#multiple-output-formats","title":"Multiple Output Formats","text":"<pre><code>class MyClass(PyinstrumentProfilerMixin):\n    def analyze_performance(self):\n        with self.profile(\"analysis\"):\n            # Your code here\n            pass\n\n        # Save in different formats\n        self.save_profile_report(\"report.html\", format=\"html\")\n        self.save_profile_report(\"report.txt\", format=\"text\")\n</code></pre>"},{"location":"guides/fusion_bench/mixins/pyinstrument_profiling/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.mixins.PyinstrumentProfilerMixin: The mixin class that provides the profiling functionality.</li> </ul>"},{"location":"guides/fusion_bench/mixins/simple_profiler/","title":"SimpleProfilerMixin","text":"<p>The <code>SimpleProfilerMixin</code> is a powerful and easy-to-use tool for adding profiling capabilities to your Python classes. This mixin is particularly useful for developers who need to optimize performance in their applications or want to gain insights into the time consumption of different parts of their code. By incorporating <code>SimpleProfilerMixin</code> into your classes, you can easily track and analyze the execution time of various operations, helping you identify bottlenecks and optimize your code effectively.</p>"},{"location":"guides/fusion_bench/mixins/simple_profiler/#reference","title":"Reference","text":""},{"location":"guides/fusion_bench/mixins/simple_profiler/#fusion_bench.mixins.simple_profiler.SimpleProfilerMixin","title":"<code>SimpleProfilerMixin</code>","text":"<p>A mixin class that provides simple profiling capabilities using Lightning's SimpleProfiler.</p> <p>This mixin allows for easy profiling of code blocks using a context manager or manual start/stop methods. It measures the execution time of named actions and provides a summary of the profiling results. Unlike statistical profilers, this provides precise timing measurements for specific code blocks.</p> Note <p>This mixin uses Lightning's SimpleProfiler which measures wall-clock time for named actions. It's suitable for timing discrete operations rather than detailed function-level profiling.</p> <p>Examples:</p> <pre><code>class MyClass(SimpleProfilerMixin):\n    def do_something(self):\n        with self.profile(\"data_loading\"):\n            # Load data here\n            data = load_data()\n\n        with self.profile(\"model_training\"):\n            # Train model here\n            model.train(data)\n\n        # Print the profiling summary\n        self.print_profile_summary(\"Training Profile\")\n</code></pre> <p>Attributes:</p> <ul> <li> <code>_profiler</code>               (<code>SimpleProfiler</code>)           \u2013            <p>An instance of the SimpleProfiler class used for profiling.</p> </li> </ul> Source code in <code>fusion_bench/mixins/simple_profiler.py</code> <pre><code>class SimpleProfilerMixin:\n    \"\"\"\n    A mixin class that provides simple profiling capabilities using Lightning's SimpleProfiler.\n\n    This mixin allows for easy profiling of code blocks using a context manager or manual\n    start/stop methods. It measures the execution time of named actions and provides\n    a summary of the profiling results. Unlike statistical profilers, this provides\n    precise timing measurements for specific code blocks.\n\n    Note:\n        This mixin uses Lightning's SimpleProfiler which measures wall-clock time\n        for named actions. It's suitable for timing discrete operations rather than\n        detailed function-level profiling.\n\n    Examples:\n        ```python\n        class MyClass(SimpleProfilerMixin):\n            def do_something(self):\n                with self.profile(\"data_loading\"):\n                    # Load data here\n                    data = load_data()\n\n                with self.profile(\"model_training\"):\n                    # Train model here\n                    model.train(data)\n\n                # Print the profiling summary\n                self.print_profile_summary(\"Training Profile\")\n        ```\n\n    Attributes:\n        _profiler (SimpleProfiler): An instance of the SimpleProfiler class used for profiling.\n    \"\"\"\n\n    _profiler: SimpleProfiler = None\n\n    @property\n    def profiler(self) -&gt; SimpleProfiler:\n        \"\"\"\n        Get the SimpleProfiler instance, creating it if necessary.\n\n        Returns:\n            SimpleProfiler: The profiler instance used for timing measurements.\n        \"\"\"\n        # Lazy initialization of the profiler instance\n        if self._profiler is None:\n            self._profiler = SimpleProfiler()\n        return self._profiler\n\n    @contextmanager\n    def profile(self, action_name: str) -&gt; Generator:\n        \"\"\"\n        Context manager for profiling a code block.\n\n        This context manager automatically starts profiling when entering the block\n        and stops profiling when exiting the block (even if an exception occurs).\n\n        Args:\n            action_name: A descriptive name for the action being profiled.\n                        This name will appear in the profiling summary.\n\n        Yields:\n            str: The action name that was provided.\n\n        Example:\n\n        ```python\n        with self.profile(\"data_processing\"):\n            # Process data here\n            result = process_large_dataset()\n        ```\n        \"\"\"\n        try:\n            self.start_profile(action_name)\n            yield action_name\n        finally:\n            self.stop_profile(action_name)\n\n    def start_profile(self, action_name: str):\n        \"\"\"\n        Start profiling for a named action.\n\n        This method begins timing for the specified action. You must call\n        stop_profile() with the same action name to complete the measurement.\n\n        Args:\n            action_name: A descriptive name for the action being profiled.\n                        This name will appear in the profiling summary.\n\n        Example:\n            ```python\n            self.start_profile(\"model_inference\")\n            result = model.predict(data)\n            self.stop_profile(\"model_inference\")\n            ```\n        \"\"\"\n        self.profiler.start(action_name)\n\n    def stop_profile(self, action_name: str):\n        \"\"\"\n        Stop profiling for a named action.\n\n        This method ends timing for the specified action that was previously\n        started with start_profile().\n\n        Args:\n            action_name: The name of the action to stop profiling.\n                        Must match the name used in start_profile().\n\n        Example:\n            ```python\n            self.start_profile(\"data_loading\")\n            data = load_data()\n            self.stop_profile(\"data_loading\")\n            ```\n        \"\"\"\n        self.profiler.stop(action_name)\n\n    @rank_zero_only\n    def print_profile_summary(self, title: Optional[str] = None):\n        \"\"\"\n        Print a summary of all profiled actions.\n\n        This method outputs a formatted summary showing the timing information\n        for all actions that have been profiled. The output includes action names\n        and their execution times.\n\n        Args:\n            title: Optional title to print before the profiling summary.\n                  If provided, this will be printed as a header.\n\n        Note:\n            This method is decorated with @rank_zero_only, meaning it will only\n            execute on the main process in distributed training scenarios.\n\n        Example:\n            ```python\n            # After profiling some actions\n            self.print_profile_summary(\"Training Performance Summary\")\n            ```\n        \"\"\"\n        if title is not None:\n            print(title)\n        print(self.profiler.summary())\n\n    def __del__(self):\n        \"\"\"\n        Cleanup when the object is destroyed.\n\n        Ensures that the profiler instance is properly cleaned up to prevent\n        memory leaks when the mixin instance is garbage collected.\n        \"\"\"\n        if self._profiler is not None:\n            del self._profiler\n            self._profiler = None\n</code></pre>"},{"location":"guides/fusion_bench/mixins/simple_profiler/#fusion_bench.mixins.simple_profiler.SimpleProfilerMixin.profiler","title":"<code>profiler</code>  <code>property</code>","text":"<p>Get the SimpleProfiler instance, creating it if necessary.</p> <p>Returns:</p> <ul> <li> <code>SimpleProfiler</code> (              <code>SimpleProfiler</code> )          \u2013            <p>The profiler instance used for timing measurements.</p> </li> </ul>"},{"location":"guides/fusion_bench/mixins/simple_profiler/#fusion_bench.mixins.simple_profiler.SimpleProfilerMixin.__del__","title":"<code>__del__()</code>","text":"<p>Cleanup when the object is destroyed.</p> <p>Ensures that the profiler instance is properly cleaned up to prevent memory leaks when the mixin instance is garbage collected.</p> Source code in <code>fusion_bench/mixins/simple_profiler.py</code> <pre><code>def __del__(self):\n    \"\"\"\n    Cleanup when the object is destroyed.\n\n    Ensures that the profiler instance is properly cleaned up to prevent\n    memory leaks when the mixin instance is garbage collected.\n    \"\"\"\n    if self._profiler is not None:\n        del self._profiler\n        self._profiler = None\n</code></pre>"},{"location":"guides/fusion_bench/mixins/simple_profiler/#fusion_bench.mixins.simple_profiler.SimpleProfilerMixin.print_profile_summary","title":"<code>print_profile_summary(title=None)</code>","text":"<p>Print a summary of all profiled actions.</p> <p>This method outputs a formatted summary showing the timing information for all actions that have been profiled. The output includes action names and their execution times.</p> <p>Parameters:</p> <ul> <li> <code>title</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional title to print before the profiling summary.   If provided, this will be printed as a header.</p> </li> </ul> Note <p>This method is decorated with @rank_zero_only, meaning it will only execute on the main process in distributed training scenarios.</p> Example <pre><code># After profiling some actions\nself.print_profile_summary(\"Training Performance Summary\")\n</code></pre> Source code in <code>fusion_bench/mixins/simple_profiler.py</code> <pre><code>@rank_zero_only\ndef print_profile_summary(self, title: Optional[str] = None):\n    \"\"\"\n    Print a summary of all profiled actions.\n\n    This method outputs a formatted summary showing the timing information\n    for all actions that have been profiled. The output includes action names\n    and their execution times.\n\n    Args:\n        title: Optional title to print before the profiling summary.\n              If provided, this will be printed as a header.\n\n    Note:\n        This method is decorated with @rank_zero_only, meaning it will only\n        execute on the main process in distributed training scenarios.\n\n    Example:\n        ```python\n        # After profiling some actions\n        self.print_profile_summary(\"Training Performance Summary\")\n        ```\n    \"\"\"\n    if title is not None:\n        print(title)\n    print(self.profiler.summary())\n</code></pre>"},{"location":"guides/fusion_bench/mixins/simple_profiler/#fusion_bench.mixins.simple_profiler.SimpleProfilerMixin.profile","title":"<code>profile(action_name)</code>","text":"<p>Context manager for profiling a code block.</p> <p>This context manager automatically starts profiling when entering the block and stops profiling when exiting the block (even if an exception occurs).</p> <p>Parameters:</p> <ul> <li> <code>action_name</code>               (<code>str</code>)           \u2013            <p>A descriptive name for the action being profiled.         This name will appear in the profiling summary.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>Generator</code> )          \u2013            <p>The action name that was provided.</p> </li> </ul> <p>Example:</p> <pre><code>with self.profile(\"data_processing\"):\n    # Process data here\n    result = process_large_dataset()\n</code></pre> Source code in <code>fusion_bench/mixins/simple_profiler.py</code> <pre><code>@contextmanager\ndef profile(self, action_name: str) -&gt; Generator:\n    \"\"\"\n    Context manager for profiling a code block.\n\n    This context manager automatically starts profiling when entering the block\n    and stops profiling when exiting the block (even if an exception occurs).\n\n    Args:\n        action_name: A descriptive name for the action being profiled.\n                    This name will appear in the profiling summary.\n\n    Yields:\n        str: The action name that was provided.\n\n    Example:\n\n    ```python\n    with self.profile(\"data_processing\"):\n        # Process data here\n        result = process_large_dataset()\n    ```\n    \"\"\"\n    try:\n        self.start_profile(action_name)\n        yield action_name\n    finally:\n        self.stop_profile(action_name)\n</code></pre>"},{"location":"guides/fusion_bench/mixins/simple_profiler/#fusion_bench.mixins.simple_profiler.SimpleProfilerMixin.start_profile","title":"<code>start_profile(action_name)</code>","text":"<p>Start profiling for a named action.</p> <p>This method begins timing for the specified action. You must call stop_profile() with the same action name to complete the measurement.</p> <p>Parameters:</p> <ul> <li> <code>action_name</code>               (<code>str</code>)           \u2013            <p>A descriptive name for the action being profiled.         This name will appear in the profiling summary.</p> </li> </ul> Example <pre><code>self.start_profile(\"model_inference\")\nresult = model.predict(data)\nself.stop_profile(\"model_inference\")\n</code></pre> Source code in <code>fusion_bench/mixins/simple_profiler.py</code> <pre><code>def start_profile(self, action_name: str):\n    \"\"\"\n    Start profiling for a named action.\n\n    This method begins timing for the specified action. You must call\n    stop_profile() with the same action name to complete the measurement.\n\n    Args:\n        action_name: A descriptive name for the action being profiled.\n                    This name will appear in the profiling summary.\n\n    Example:\n        ```python\n        self.start_profile(\"model_inference\")\n        result = model.predict(data)\n        self.stop_profile(\"model_inference\")\n        ```\n    \"\"\"\n    self.profiler.start(action_name)\n</code></pre>"},{"location":"guides/fusion_bench/mixins/simple_profiler/#fusion_bench.mixins.simple_profiler.SimpleProfilerMixin.stop_profile","title":"<code>stop_profile(action_name)</code>","text":"<p>Stop profiling for a named action.</p> <p>This method ends timing for the specified action that was previously started with start_profile().</p> <p>Parameters:</p> <ul> <li> <code>action_name</code>               (<code>str</code>)           \u2013            <p>The name of the action to stop profiling.         Must match the name used in start_profile().</p> </li> </ul> Example <pre><code>self.start_profile(\"data_loading\")\ndata = load_data()\nself.stop_profile(\"data_loading\")\n</code></pre> Source code in <code>fusion_bench/mixins/simple_profiler.py</code> <pre><code>def stop_profile(self, action_name: str):\n    \"\"\"\n    Stop profiling for a named action.\n\n    This method ends timing for the specified action that was previously\n    started with start_profile().\n\n    Args:\n        action_name: The name of the action to stop profiling.\n                    Must match the name used in start_profile().\n\n    Example:\n        ```python\n        self.start_profile(\"data_loading\")\n        data = load_data()\n        self.stop_profile(\"data_loading\")\n        ```\n    \"\"\"\n    self.profiler.stop(action_name)\n</code></pre>"},{"location":"guides/nlp/question_answering/","title":"Question Answering","text":""},{"location":"guides/nlp/question_answering/#key-concepts","title":"Key Concepts","text":""},{"location":"guides/nlp/question_answering/#overlapping-tokens","title":"Overlapping Tokens","text":"<p>Overlapping tokens are segments of text that are repeated between consecutive chunks when a long text needs to be split into smaller pieces due to model's maximum token limit.</p> <p>Here's a detailed explanation:</p> <ol> <li> <p>Why we need overlapping:</p> <ul> <li>When a text is too long for the model's context window (max_length)</li> <li>To maintain continuity and context between chunks</li> <li>To avoid losing information that might be split between chunks</li> </ul> </li> <li> <p>Key parameters in the code:</p> <ul> <li>max_length: Maximum number of tokens allowed</li> <li>stride: Number of overlapping tokens between chunks</li> <li>return_overflowing_tokens: Tells tokenizer to return multiple chunks</li> <li>truncation=\"only_second\": Only truncates the context, not the question</li> </ul> </li> </ol> <p>Let's illustrate with an example:</p> <p>Suppose we have a text: \"The quick brown fox jumps over the lazy sleeping dog\". The tokenization might look like this:</p> <pre><code>Chunk 1: [The quick brown fox jumps over]\n                    \u2193 overlap \u2193\nChunk 2:            [brown fox jumps over the lazy]\n                                \u2193 overlap \u2193\nChunk 3:                        [jumps over the lazy sleeping dog]\n</code></pre> <p>Real-world example with actual tokens:</p> <pre><code>from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\nquestion = \"What did the fox do?\"\ncontext = \"The quick brown fox jumps over the lazy sleeping dog. It was a beautiful sunny day.\"\n\ntokenized = tokenizer(\n    question,\n    context,\n    max_length=16,\n    truncation=\"only_second\",\n    return_overflowing_tokens=True,\n    stride=4\n)\n\n# Print the decoded tokens for each chunk\nfor encoding in tokenized[\"input_ids\"]:\n    print(tokenizer.decode(encoding))\n</code></pre>"},{"location":"guides/nlp/question_answering/#offset-mapping","title":"Offset Mapping","text":"<p>Offset mapping is a feature that provides the character-level mapping between the original text and the tokenized output. It returns a list of tuples (start, end) where:</p> <ul> <li>start: starting character position in the original text</li> <li>end: ending character position in the original text</li> </ul> <p>Here's a detailed breakdown:</p> <ol> <li> <p>Structure of offset_mapping:</p> <pre><code>[(0, 0),    # [CLS] token - special token, maps to nothing\n(0, 3),     # \"how\" - maps to characters 0-3 in original text\n(4, 8),     # \"many\" - maps to characters 4-8\n...]\n</code></pre> </li> <li> <p>Special tokens mapping:</p> <ul> <li>[CLS], [SEP], [PAD]: represented as (0, 0)</li> <li>These tokens don't correspond to any actual text in the input</li> </ul> </li> <li> <p>Usage example:</p> <pre><code># Example showing how to use offset_mapping\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ntext = \"How many cats?\"\ntokenized = tokenizer(text, return_offsets_mapping=True)\n\nfor token_id, offset in zip(tokenized[\"input_ids\"], tokenized[\"offset_mapping\"]):\n    token = tokenizer.decode([token_id])\n    start, end = offset\n    original_text = text[start:end] if start != end else \"[SPECIAL]\"\n    print(f\"Token: {token}, Offset: {offset}, Original text: {original_text}\")\n</code></pre> </li> </ol> <p>Main purposes of offset_mapping:</p> <ol> <li> <p>Answer span location:</p> <ul> <li>Helps locate exact position of answers in QA tasks</li> <li>Maps token positions back to original text positions</li> </ul> </li> <li> <p>Token-text alignment:</p> <ul> <li>Enables precise tracking of which parts of original text correspond to which tokens</li> <li>Useful for tasks requiring character-level precision</li> </ul> </li> <li> <p>Handling overlapping chunks:</p> <ul> <li>Helps maintain correct position information when text is split into chunks</li> <li>Essential for combining predictions from multiple chunks</li> </ul> </li> </ol> <p>Common operations with offset_mapping: <pre><code># Finding original text for a token\ndef get_original_text(text, offset):\n    start, end = offset\n    return text[start:end] if start != end else \"[SPECIAL]\"\n\n# Finding token position for a text span\ndef find_token_position(offset_mapping, char_start, char_end):\n    for idx, (start, end) in enumerate(offset_mapping):\n        if start == char_start and end == char_end:\n            return idx\n    return None\n</code></pre></p> <p>This feature is particularly important in Question Answering tasks where you need to:</p> <ul> <li>Map predicted token positions back to original text</li> <li>Handle answer spans across multiple chunks</li> <li>Maintain precise position information for answer extraction</li> </ul>"},{"location":"guides/nlp/question_answering/#overflow_to_sample_mapping","title":"overflow_to_sample_mapping","text":"<p><code>overflow_to_sample_mapping</code> is an index list that maps each feature in the overflowing tokens back to its original sample. It's particularly useful when processing multiple examples with overflow.</p> <p>Here's a detailed explanation:</p> <ul> <li>When a text is split into multiple chunks due to length</li> <li>Each chunk needs to be traced back to its original example</li> <li><code>overflow_to_sample_mapping</code> provides this tracking mechanism</li> </ul> <p>Here's a comprehensive example:</p> <pre><code>from transformers import AutoTokenizer\nimport pandas as pd\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Multiple examples\nexamples = {\n    \"question\": [\n        \"What is the capital?\",\n        \"Who won the game?\"\n    ],\n    \"context\": [\n        \"Paris is the capital of France. It is known for the Eiffel Tower. The city has many historic monuments.\" * 5,  # Made longer by repeating\n        \"The Lakers won the game against the Bulls. It was a close match.\" * 2\n    ]\n}\n\n# Tokenize with overflow\ntokenized_examples = []\nfor q, c in zip(examples[\"question\"], examples[\"context\"]):\n    tokenized = tokenizer(\n        q,\n        c,\n        max_length=50,  # Small max_length for demonstration\n        stride=10,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n        truncation=\"only_second\"\n    )\n    tokenized_examples.append(tokenized)\n\n# Let's see how many chunks each example was split into\nfor i, tokenized in enumerate(tokenized_examples):\n    print(f\"\\nExample {i}:\")\n    print(f\"Number of chunks: {len(tokenized['input_ids'])}\")\n    print(f\"Overflow to sample mapping: {tokenized.overflow_to_sample_mapping}\")\n</code></pre> <p>This might output something like:</p> <pre><code>Example 0:\nNumber of chunks: 4\nOverflow to sample mapping: [0, 0, 0, 0]  # All chunks belong to first example\n\nExample 1:\nNumber of chunks: 2\nOverflow to sample mapping: [0, 0]  # All chunks belong to first example\n</code></pre> <p>Practical Use Case:</p> <pre><code>def prepare_train_features(examples):\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=384,\n        stride=128,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context\n    sample_mapping = tokenized_examples.overflow_to_sample_mapping\n\n    # For each feature, we need to know from which example it came from\n    for i, sample_idx in enumerate(sample_mapping):\n        # Get the example's original question\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_start = sequence_ids.index(1)  # Find where context starts\n\n        # Set example ID for this feature\n        tokenized_examples[i][\"example_id\"] = examples[\"id\"][sample_idx]\n\n        # Set offset mappings for answer spans\n        tokenized_examples[i][\"offset_mapping\"] = [\n            (o if sequence_ids[k] == 1 else None)\n            for k, o in enumerate(tokenized_examples[i][\"offset_mapping\"])\n        ]\n\n    return tokenized_examples\n</code></pre> <p>Key Benefits:</p> <ol> <li> <p>Tracking Features: </p> <ul> <li>Maps each feature back to its source example</li> <li>Maintains relationship between chunks and original data</li> </ul> </li> <li> <p>Data Processing:</p> <ul> <li>Helps in maintaining example-level information</li> <li>Essential for combining predictions from multiple chunks</li> </ul> </li> <li> <p>Batch Processing:</p> <ul> <li>Enables proper batching of features</li> <li>Maintains data integrity during training</li> </ul> </li> </ol> <p>Common Use Pattern:</p> <pre><code># Example of using overflow_to_sample_mapping in a training loop\nfor i, sample_idx in enumerate(tokenized_examples.overflow_to_sample_mapping):\n    # Get original example ID\n    original_example_id = examples[\"id\"][sample_idx]\n\n    # Get original answer\n    original_answer = examples[\"answers\"][sample_idx]\n\n    # Process feature while maintaining connection to original example\n    process_feature(tokenized_examples[i], original_example_id, original_answer)\n</code></pre> <p>This feature is particularly important in Question Answering tasks where:</p> <ul> <li>Long contexts need to be split into multiple chunks</li> <li>Each chunk needs to be processed separately</li> <li>Results need to be combined while maintaining reference to original examples</li> </ul>"},{"location":"guides/resnet/image_classification_finetune/","title":"Fine-tuning ResNet Models for Image Classification","text":"<p>This guide demonstrates how to fine-tune ResNet models for image classification tasks using FusionBench. The framework supports various ResNet architectures on different datasets.</p>"},{"location":"guides/resnet/image_classification_finetune/#quick-start","title":"Quick Start","text":""},{"location":"guides/resnet/image_classification_finetune/#training-a-resnet-18-model-on-cifar-10","title":"Training a ResNet-18 Model on CIFAR-10","text":"<pre><code>fusion_bench --config-name model_fusion \\\n    path.log_dir=outputs/resnet18/cifar10 \\\n    method=classification/image_classification_finetune \\\n    modelpool=ResNetForImageClassfication/transformers/resnet18_cifar10\n</code></pre> <p>This command will start fine-tuning a ResNet-18 model on the CIFAR-10 dataset using all available GPUs.  The global batch size is determined by the number of GPUs multiplied by the batch size per GPU (specified as <code>method.dataloader_kwargs.batch_size</code>). The training outputs, including checkpoints and logs, will be saved in the specified directory (in this example, <code>outputs/resnet18/cifar10</code>).</p>"},{"location":"guides/resnet/image_classification_finetune/#testing-the-fine-tuned-model","title":"Testing the Fine-tuned Model","text":"<p>After training, test your model using the saved checkpoint:</p> <pre><code>fusion_bench --config-name model_fusion \\\n    method=classification/image_classification_finetune_test \\\n    method.checkpoint_path=&lt;path_to_your_checkpoint&gt; \\\n    modelpool=ResNetForImageClassfication/transformers/resnet18_cifar10\n</code></pre> <p>Example with actual checkpoint path:</p> <pre><code>fusion_bench --config-name model_fusion \\\n    method=classification/image_classification_finetune_test \\\n    method.checkpoint_path=\"outputs/resnet18/cifar10/version_0/checkpoints/epoch\\=9-step\\=1960.ckpt\" \\\n    modelpool=ResNetForImageClassfication/transformers/resnet18_cifar10\n</code></pre>"},{"location":"guides/resnet/image_classification_finetune/#training-configuration","title":"Training Configuration","text":""},{"location":"guides/resnet/image_classification_finetune/#default-training-parameters","title":"Default Training Parameters","text":"<p>The default training configuration (<code>config/method/classification/image_classification_finetune.yaml</code>) includes:</p> config/method/classification/image_classification_finetune.yaml<pre><code>_target_: fusion_bench.method.classification.ImageClassificationFineTuning\nmax_epochs: 10\nmax_steps: null\nlabel_smoothing: 0\noptimizer:\n  _target_: torch.optim.SGD\n  lr: 0.001\n  momentum: 0.9\n  weight_decay: 1e-4\nlr_scheduler:\n  _target_: torch.optim.lr_scheduler.CosineAnnealingLR\n  T_max: ${..max_epochs}\ndataloader_kwargs:\n  batch_size: 256 # batch size per GPU\n  num_workers: 8\n  pin_memory: true\n</code></pre>"},{"location":"guides/resnet/image_classification_finetune/#customizing-training-parameters","title":"Customizing Training Parameters","text":"<p>You can override any training parameter from the command line:</p> <pre><code># Custom learning rate and batch size\nfusion_bench --config-name model_fusion \\\n    method=classification/image_classification_finetune \\\n    method.optimizer.lr=0.01 \\\n    method.dataloader_kwargs.batch_size=128 \\\n    modelpool=ResNetForImageClassfication/transformers/resnet18_cifar10\n\n# Different optimizer (Adam)\nfusion_bench --config-name model_fusion \\\n    method=classification/image_classification_finetune \\\n    method.optimizer._target_=torch.optim.Adam \\\n    method.optimizer.lr=0.001 \\\n    modelpool=ResNetForImageClassfication/transformers/resnet18_cifar10\n\n# Step-based training instead of epoch-based\nfusion_bench --config-name model_fusion \\\n    method=classification/image_classification_finetune \\\n    method.max_epochs=null \\\n    method.max_steps=5000 \\\n    modelpool=ResNetForImageClassfication/transformers/resnet18_cifar10\n</code></pre>"},{"location":"guides/resnet/image_classification_finetune/#testing-configuration","title":"Testing Configuration","text":"<p>The testing configuration (<code>config/method/classification/image_classification_finetune_test.yaml</code>) includes:</p> config/method/classification/image_classification_finetune_test.yaml<pre><code>_target_: fusion_bench.method.classification.ImageClassificationFineTuning_Test\ncheckpoint_path: null\ndataloader_kwargs:\n  batch_size: 256\n  num_workers: 4\n  pin_memory: true\n</code></pre>"},{"location":"guides/resnet/image_classification_finetune/#testing-options","title":"Testing Options","text":"<pre><code># Test with checkpoint\nfusion_bench --config-name model_fusion \\\n    method=classification/image_classification_finetune_test \\\n    method.checkpoint_path=\"path/to/checkpoint.ckpt\" \\\n    modelpool=ResNetForImageClassfication/transformers/resnet18_cifar10\n\n# Test without checkpoint (using pretrained weights)\nfusion_bench --config-name model_fusion \\\n    method=classification/image_classification_finetune_test \\\n    modelpool=ResNetForImageClassfication/transformers/resnet18_cifar10\n</code></pre>"},{"location":"guides/resnet/image_classification_finetune/#monitoring-training","title":"Monitoring Training","text":"<p>View training progress using TensorBoard:</p> <pre><code>cd outputs\ntensorboard --logdir .\n</code></pre> <p>The following metrics are logged: - Training and validation loss - Top-1 and Top-5 accuracy - Learning rate schedule - Device statistics (GPU utilization, memory usage)</p>"},{"location":"modelpool/","title":"Model Pool Module","text":"<p>A modelpool is a collection of models that are utilized in the process of model fusion. In the context of straightforward model fusion techniques, like averaging, only models with the same architecture are used. While for more complex methods, such as AdaMerging <sup>1</sup>, each model is paired with a unique set of unlabeled test data. This data is used during the test-time adaptation phase.</p>"},{"location":"modelpool/#configuration-structure","title":"Configuration Structure","text":"<p>Starting from version 0.2, modelpools use Hydra-based configuration with the <code>_target_</code> field to specify the class to instantiate. A modelpool configuration file typically contains the following fields:</p>"},{"location":"modelpool/#core-fields","title":"Core Fields","text":"<ul> <li><code>_target_</code>: The fully qualified class name of the modelpool (e.g., <code>fusion_bench.modelpool.CLIPVisionModelPool</code>)</li> <li><code>models</code>: A dictionary of model configurations where each key is the model name and the value is the model configuration:<ul> <li>Special model names: <code>_pretrained_</code> refers to the base/pretrained model</li> <li>Each model configuration should contain <code>_target_</code> field specifying how to load the model</li> <li>Additional parameters can be passed to the model loading function</li> </ul> </li> </ul>"},{"location":"modelpool/#dataset-fields-optional","title":"Dataset Fields (Optional)","text":"<p>For model fusion techniques that require datasets:</p> <ul> <li><code>train_datasets</code>: Dictionary of training dataset configurations</li> <li><code>val_datasets</code>: Dictionary of validation dataset configurations  </li> <li><code>test_datasets</code>: Dictionary of testing dataset configurations</li> </ul> <p>Each dataset configuration should contain:</p> <ul> <li><code>_target_</code>: The loading function (e.g., <code>datasets.load_dataset</code>)</li> <li>Additional parameters for the dataset loading function</li> </ul>"},{"location":"modelpool/#additional-model-specific-fields","title":"Additional Model-Specific Fields","text":"<p>Different modelpool types may include additional configuration fields:</p> <ul> <li><code>processor</code>: For vision models, configuration for image preprocessors or tokenizers</li> <li><code>tokenizer</code>: For language models, tokenizer configuration</li> <li><code>model_kwargs</code>: Additional arguments passed to model loading functions</li> <li><code>base_model</code>: Base model identifier used as a reference for other models</li> </ul>"},{"location":"modelpool/#configuration-examples","title":"Configuration Examples","text":""},{"location":"modelpool/#basic-clip-vision-model-pool","title":"Basic CLIP Vision Model Pool","text":"<pre><code>_target_: fusion_bench.modelpool.CLIPVisionModelPool\nbase_model: openai/clip-vit-base-patch32\nmodels:\n  _pretrained_:\n    _target_: transformers.CLIPVisionModel.from_pretrained\n    pretrained_model_name_or_path: ${...base_model}\n  finetuned_model:\n    _target_: transformers.CLIPVisionModel.from_pretrained\n    pretrained_model_name_or_path: path/to/finetuned/model\nprocessor:\n  _target_: transformers.CLIPProcessor.from_pretrained\n  pretrained_model_name_or_path: ${..base_model}\n</code></pre>"},{"location":"modelpool/#causal-language-model-pool","title":"Causal Language Model Pool","text":"<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nbase_model: decapoda-research/llama-7b-hf\nmodels:\n  _pretrained_:\n    _target_: transformers.LlamaForCausalLM.from_pretrained\n    pretrained_model_name_or_path: ${...base_model}\n  math_model:\n    _target_: transformers.LlamaForCausalLM.from_pretrained\n    pretrained_model_name_or_path: path/to/math/model\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer:\n  _target_: transformers.AutoTokenizer.from_pretrained\n  pretrained_model_name_or_path: ${..base_model}\n</code></pre>"},{"location":"modelpool/#model-pool-with-datasets","title":"Model Pool with Datasets","text":"<pre><code>_target_: fusion_bench.modelpool.CLIPVisionModelPool\nbase_model: openai/clip-vit-base-patch32\nmodels:\n  _pretrained_:\n    _target_: transformers.CLIPVisionModel.from_pretrained\n    pretrained_model_name_or_path: ${...base_model}\ntrain_datasets:\n  eurosat:\n    _target_: datasets.load_dataset\n    path: tanganke/eurosat\n    split: train\n  cars:\n    _target_: datasets.load_dataset\n    path: tanganke/stanford_cars\n    split: train\nprocessor:\n  _target_: transformers.CLIPProcessor.from_pretrained\n  pretrained_model_name_or_path: ${..base_model}\n</code></pre>"},{"location":"modelpool/#usage","title":"Usage","text":""},{"location":"modelpool/#creating-a-modelpool","title":"Creating a ModelPool","text":"<p>Starting from v0.2, modelpools can be created directly or through Hydra configuration:</p> <pre><code># Create from configuration file\nfrom fusion_bench.utils import instantiate\nfrom omegaconf import OmegaConf\n\nconfig = OmegaConf.load(\"path/to/modelpool/config.yaml\")\nmodelpool = instantiate(config)\n\n# Create directly\nfrom fusion_bench.modelpool import CLIPVisionModelPool\nmodelpool = CLIPVisionModelPool(\n    models={\n        \"_pretrained_\": {\n            \"_target_\": \"transformers.CLIPVisionModel.from_pretrained\", \n            \"pretrained_model_name_or_path\": \"openai/clip-vit-base-patch32\"\n        }\n    }\n)\n</code></pre>"},{"location":"modelpool/#loading-models","title":"Loading Models","text":"<p>Models are loaded on-demand when requested:</p> <pre><code># Load a specific model\nmodel = modelpool.load_model('_pretrained_')\n\n# Load pretrained model (if available)\nmodel = modelpool.load_pretrained_model()\n\n# Load pretrained model or first available model\nmodel = modelpool.load_pretrained_or_first_model()\n\n# Iterate over all models\nfor model_name, model in modelpool.named_models():\n    print(f\"Processing {model_name}\")\n</code></pre>"},{"location":"modelpool/#model-pool-properties","title":"Model Pool Properties","text":"<pre><code># Check if pretrained model exists\nif modelpool.has_pretrained:\n    print(\"Pretrained model available\")\n\n# Get model names (excluding special models like _pretrained_)\nmodel_names = modelpool.model_names\n\n# Get all model names (including special models)  \nall_names = modelpool.all_model_names\n\n# Get number of models\nnum_models = len(modelpool)\n</code></pre>"},{"location":"modelpool/#working-with-datasets","title":"Working with Datasets","text":"<p>If datasets are configured, you can access them similarly:</p> <pre><code># Load datasets\ntrain_dataset = modelpool.load_train_dataset('eurosat')\nval_dataset = modelpool.load_val_dataset('eurosat')\ntest_dataset = modelpool.load_test_dataset('eurosat')\n\n# Get dataset names\ntrain_names = modelpool.train_dataset_names\nval_names = modelpool.val_dataset_names\ntest_names = modelpool.test_dataset_names\n</code></pre>"},{"location":"modelpool/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.modelpool.BaseModelPool</li> </ul> <ol> <li> <p>AdaMerging: Adaptive Model Merging for Multi-Task Learning. http://arxiv.org/abs/2310.02575\u00a0\u21a9</p> </li> </ol>"},{"location":"modelpool/clip_vit/","title":"CLIP-ViT Models for Open Vocabulary Image Classification","text":"<p>This document provides comprehensive information about CLIP-ViT models for open vocabulary image classification, including model implementation details, usage instructions, and experimental results. The <code>CLIPVisionModelPool</code> class manages collections of pre-trained and fine-tuned CLIP Vision models for open vocabulary image classification tasks.</p>"},{"location":"modelpool/clip_vit/#classification-head-initialization","title":"Classification Head Initialization","text":"<p>FusionBench employs CLIP's zero-shot classification approach. The classification head is constructed by:</p> <ol> <li>Generating text embeddings for each class name using predefined templates (e.g., \"a photo of a {class}\")</li> <li>Computing text embeddings using CLIP's text encoder for all class-template combinations</li> <li>Averaging embeddings across templates to obtain final class representations</li> <li>These text embeddings serve as the classification weights</li> </ol> <p>Implementation details can be found at <code>CLIPClassificationMixin.setup_zero_shot_classification_head</code>.</p>"},{"location":"modelpool/clip_vit/#the-eight-tasks","title":"The Eight Tasks","text":"<p>The most common eight tasks used in the research community are SUN397, Cars, RESISC45, EuroSAT, SVHN, GTSRB, MNIST, and DTD. These tasks cover a wide range of domains, including natural images, satellite images, and digit recognition. You can download the datasets from this HuggingFace Collection or using the <code>datasets</code> library as follows:</p> <pre><code>from datasets import load_dataset\n\n# take `gtsrb` as an example\ndataset = load_dataset(\"tanganke/gtsrb\")\n\ntrain_dataset = dataset[\"train\"]\ntest_dataset = dataset[\"test\"]\n</code></pre> <p>The authors of Task Arithmetic have fine-tuned the CLIP-ViT models from the open_clip library on these eight tasks and provide the models publicly on Google Drive.  However, these models rely on a specific version of the open_clip library. </p> <p>To make experiments more convenient and avoid dependency on a specific library version, we have re-trained these models and made them publicly available on the HuggingFace Model Hub. We use the Adam Optimizer with a fixed learning rate of 1e-5 over 4000 training steps (batch_size=32). Only the vision encoder is fine-tuned, while the text encoder remains fixed to preserve the open-vocabulary property of the model.</p> <ul> <li>fine-tuned CLIP-ViT-B/32 models</li> <li>fine-tuned CLIP-ViT-B/16 models</li> <li>fine-tuned CLIP-ViT-L/14 models</li> </ul> <p>To use these models, you can load them from the Transformers library as follows:</p>"},{"location":"modelpool/clip_vit/#direct-model-usage","title":"Direct Model Usage","text":"<p>Load vision backbone directly:</p> <pre><code>from transformers import CLIPVisionModel\n\n# load the CLIP-ViT-B/32 model, take `gtsrb` as an example\nvision_model = CLIPVisionModel.from_pretrained('tanganke/clip-vit-base-patch32_gtsrb')\n</code></pre> <p>Substitute the vision encoder of CLIP:</p> <pre><code>from transformers import CLIPProcessor, CLIPModel\n\n# load pre-trained CLIP model\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n# substitute the vision model with the fine-tuned one\nclip_model.vision_model.load_state_dict(vision_model.vision_model.state_dict())\n</code></pre>"},{"location":"modelpool/clip_vit/#using-clipvisionmodelpool","title":"Using <code>CLIPVisionModelPool</code>","text":"<p>For more convenient model management, use the CLIPVisionModelPool:</p> <pre><code>from fusion_bench.modelpool import CLIPVisionModelPool\nfrom omegaconf import DictConfig\n\n# Initialize model pool\nmodelpool = CLIPVisionModelPool(\n    models={\n        \"_pretrained_\": \"openai/clip-vit-base-patch32\",\n        \"gtsrb\": \"tanganke/clip-vit-base-patch32_gtsrb\"\n    },\n    processor=\"openai/clip-vit-base-patch32\"\n)\n\n# Load models through the pool\nvision_model = modelpool.load_model(\"gtsrb\")\nprocessor = modelpool.load_processor()\n\n# Or load complete CLIP model\nclip_model = modelpool.load_clip_model(\"_pretrained_\")\n</code></pre> <p>The <code>CLIPVisionModelPool</code> class provides a convenient way to manage multiple CLIP Vision models, and the models are automatically downloaded from the Hugging Face Model Hub when needed. Alternatively, you can configure a model pool to automatically download models from ModelScope. This is useful if you prefer to use the ModelScope platform, which is popular in China and provides access to a wide range of models.</p> <pre><code>from fusion_bench.modelpool import CLIPVisionModelPool\nfrom omegaconf import DictConfig\n\n# Initialize model pool\nmodelpool = CLIPVisionModelPool(\n    models={\n        \"_pretrained_\": \"openai-mirror/clip-vit-base-patch32\",\n        \"gtsrb\": \"tanganke/clip-vit-base-patch32_gtsrb\"\n    },\n    processor=\"openai-mirror/clip-vit-base-patch32\",\n    platform=\"modelscope\"\n)\n\n# Load models through the pool\nvision_model = modelpool.load_model(\"gtsrb\")\nprocessor = modelpool.load_processor()\n\n# Or load complete CLIP model\nclip_model = modelpool.load_clip_model(\"gtsrb\")\n</code></pre>"},{"location":"modelpool/clip_vit/#performance-of-the-fine-tuned-models","title":"Performance of the Fine-tuned Models","text":"<p>evaluate the fine-tuned CLIP-ViT-B/32 models on the eight tasks:</p> <pre><code># evaluate singlue fine-tuned models\nfor task in sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd\ndo\n    fusion_bench method=dummy \\\n        modelpool=CLIPVisionModelPool/clip-vit-base-patch32_individual \\\n            modelpool.models._pretrained_.pretrained_model_name_or_path=tanganke/clip-vit-base-patch32_${task} \\\n        taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n        report_save_path=\"outputs/ViT-B-32/single-task/clip-vit-base-patch32_${task}.json\"\ndone\n</code></pre> <p>evaluate the fine-tuned CLIP-ViT-L/14 models on the eight tasks:</p> <pre><code># assume you have eight GPUs, and you can evaluate the models on the eight tasks in parallel\ntasks=(sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd)\nCUDA_DEVICES=(0 1 2 3 4 5 6 7)  # List of CUDA devices to use\n\nfor i in \"${!CUDA_DEVICES[@]}\"; do\n    task=${tasks[$i]}\n    CUDA_VISIBLE_DEVICES=${CUDA_DEVICES[$i]} fusion_bench method=dummy \\\n        modelpool=CLIPVisionModelPool/clip-vit-large-patch14_individual \\\n            modelpool.models._pretrained_.pretrained_model_name_or_path=tanganke/clip-vit-large-patch14_${task} \\\n        taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n            taskpool.clip_model=openai/clip-vit-large-patch14 \\\n        report_save_path=\"outputs/ViT-L-14/single-task/clip-vit-large-patch14_${task}.json\" &amp;\ndone\n</code></pre> Performance of the fine-tuned CLIP-ViT-B/32 modelsPerformance of the fine-tuned CLIP-ViT-B/16 modelsPerformance of the fine-tuned CLIP-ViT-L/14 models Model SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average Pre-trained 63.2 59.8 60.7 46.0 31.6 32.5 48.3 43.9 48.2 SUN397 75.0 47.0 54.3 46.5 28.3 26.4 44.3 41.6 45.4 Cars 56.6 78.3 50.9 38.4 30.2 30.6 49.7 41.8 47.1 RESISC45 52.0 47.2 95.2 56.9 23.9 24.3 39.7 35.9 46.9 EuroSAT 49.0 39.9 33.5 99.0 11.8 22.9 33.8 35.5 40.7 SVHN 40.5 36.3 18.9 9.8 97.3 27.3 81.8 23.2 41.9 GTSRB 36.8 33.0 20.6 21.3 41.2 98.9 30.9 23.9 38.3 MNIST 50.3 40.0 31.3 17.7 50.1 19.3 99.6 30.7 42.4 DTD 54.6 51.3 36.9 25.0 28.9 21.8 47.3 79.7 43.2 Model SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average SUN397 78.9 56.2 58.9 46.6 42.7 39.9 59.3 40.8 52.9 Cars 62.2 85.9 60.8 48.7 47.1 44.8 61.6 43.2 56.8 RESISC45 60.5 57.8 96.6 65.7 28.4 35.6 71.5 39.0 56.9 EuroSAT 58.3 59.2 37.4 99.0 40.5 38.9 57.4 37.7 53.6 SVHN 57.6 55.4 42.8 19.6 97.6 32.6 90.0 33.1 53.6 GTSRB 54.0 50.5 25.3 13.2 52.0 99.0 56.9 33.9 48.1 MNIST 58.7 52.4 47.0 23.6 65.0 27.6 99.7 37.7 51.5 DTD 57.7 58.1 53.5 43.0 44.2 36.2 70.4 82.3 55.7 Model SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average Pre-trained 68.3 77.8 71.0 58.9 58.4 50.6 76.4 55.5 64.6 SUN397 82.8 68.4 58.1 49.9 55.0 46.3 79.5 52.8 61.6 Cars 67.8 92.9 68.7 56.4 51.7 47.7 80.5 55.6 65.2 RESISC45 65.6 69.0 97.4 64.3 38.3 46.6 77.7 49.9 63.6 EuroSAT 65.2 69.0 40.6 99.2 33.4 45.6 73.5 47.1 59.2 SVHN 66.4 69.0 54.0 19.7 97.9 48.7 92.2 50.1 62.3 GTSRB 63.4 64.8 38.7 19.6 71.0 99.2 75.1 45.8 59.7 MNIST 56.0 49.8 53.5 26.6 48.2 33.1 99.8 47.1 51.7 DTD 66.8 75.3 65.5 43.7 49.5 45.0 68.5 85.5 62.5"},{"location":"modelpool/clip_vit/#the-20-task-model-collections","title":"The 20-Task Model Collections","text":"<p>In addition to the eight tasks, we have also fine-tuned CLIP-ViT-B/32 models on 12 additional image classification tasks, resulting in a total of 20 tasks. These additional tasks include:</p> <ul> <li>oxford_flowers102: Oxford 102 Flower dataset with 102 flower categories</li> <li>pcam: PatchCamelyon dataset for histopathologic cancer detection</li> <li>fer2013: Facial Expression Recognition 2013 dataset</li> <li>oxford-iiit-pet: Oxford-IIIT Pet dataset with 37 pet breeds</li> <li>stl10: STL-10 dataset with 10 classes</li> <li>cifar100: CIFAR-100 dataset with 100 fine-grained classes</li> <li>cifar10: CIFAR-10 dataset with 10 classes</li> <li>food101: Food-101 dataset with 101 food categories</li> <li>fashion_mnist: Fashion-MNIST dataset with 10 fashion categories</li> <li>emnist_letters: EMNIST Letters dataset</li> <li>kmnist: Kuzushiji-MNIST dataset</li> <li>rendered-sst2: Rendered Stanford Sentiment Treebank v2 dataset</li> </ul> Performance of the fine-tuned CLIP-ViT-B/32 modelsPerformance of the fine-tuned CLIP-ViT-B/16 modelsPerformance of the fine-tuned CLIP-ViT-L/14 models Models sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd oxford_flowers102 pcam fer2013 oxford-iiit-pet stl10 cifar100 cifar10 food101 fashion_mnist emnist_letters kmnist rendered-sst2 Pre-trained 63.18 59.58 60.27 45.00 31.63 32.53 48.26 44.20 66.45 60.64 41.25 83.32 97.12 63.72 89.83 82.36 63.01 11.98 9.95 58.65 Fine-Tuned 74.86 78.52 95.14 99.07 97.27 98.91 99.58 79.68 88.55 87.96 71.61 92.45 97.55 88.38 97.60 88.41 94.75 95.62 98.23 71.28 Models sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd oxford_flowers102 pcam fer2013 oxford-iiit-pet stl10 cifar100 cifar10 food101 fashion_mnist emnist_letters kmnist rendered-sst2 Pre-Trained 65.54 64.68 66.38 54.11 51.99 43.45 51.73 45.00 71.31 54.02 46.39 88.44 98.25 66.33 90.77 87.01 67.30 12.44 11.21 60.57 Fine-Tuned 78.92 85.90 96.56 99.00 97.61 98.99 99.70 82.34 94.88 90.55 72.76 94.49 98.15 88.78 98.28 91.87 94.53 95.28 98.10 75.73 Models sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd oxford_flowers102 pcam fer2013 oxford-iiit-pet stl10 cifar100 cifar10 food101 fashion_mnist emnist_letters kmnist rendered-sst2 Pre-Trained 68.22 77.86 71.33 61.19 58.43 50.52 76.31 55.53 79.25 51.21 49.96 93.21 99.36 75.05 95.59 91.18 66.96 12.34 9.71 68.92 Fine-Tuned 82.76 92.77 97.38 99.11 97.92 99.24 99.76 85.48 97.67 91.13 75.93 95.75 99.23 93.00 99.13 94.77 95.28 95.43 98.30 80.45"},{"location":"modelpool/clip_vit/#clipvisionmodelpool-implementation","title":"CLIPVisionModelPool Implementation","text":"<p>The <code>CLIPVisionModelPool</code> class extends the base <code>BaseModelPool</code> class and provides specialized functionality for managing CLIP Vision models from Hugging Face Transformers library.</p>"},{"location":"modelpool/clip_vit/#key-features","title":"Key Features","text":"<ul> <li>Multi-platform Support: Supports both Hugging Face (<code>hf</code>) and ModelScope (<code>modelscope</code>) platforms.</li> <li>Model Loading: Handles CLIPVisionModel and CLIPModel loading with automatic path resolution</li> <li>Dataset Integration: Built-in support for loading train/validation/test datasets via the <code>datasets</code> library.</li> <li>Processor Management: Manages CLIPProcessor instances for consistent image preprocessing.</li> <li>Model Persistence: Save and load models with proper state preservation.</li> </ul>"},{"location":"modelpool/clip_vit/#class-configuration","title":"Class Configuration","text":"<p>The CLIPVisionModelPool accepts the following parameters:</p> <ul> <li><code>models</code> (DictConfig): Configuration mapping model names to their paths or configurations</li> <li><code>processor</code> (Optional[DictConfig]): Configuration for the CLIP processor (defaults to corresponding CLIP model processor)</li> <li><code>platform</code> (Literal[\"hf\", \"huggingface\", \"modelscope\"]): Platform for model/dataset loading (default: \"hf\")</li> <li><code>train_datasets</code>, <code>val_datasets</code>, <code>test_datasets</code> (Optional[DictConfig]): Dataset configurations</li> </ul>"},{"location":"modelpool/clip_vit/#model-pool-configuration","title":"Model Pool Configuration","text":"<p>To use these models from our FusionBench library, you can specify the modelpool configuration file as follows:</p> config/modelpool/CLIPVisionModelPool/clip-vit-base-patch32_TA8.yaml<pre><code># eight image classification tasks defined in task arithmetic paper\ndefaults:\n  - /dataset/image_classification/train@train_datasets:\n      - sun397\n      - stanford-cars\n      - resisc45\n      - eurosat\n      - svhn\n      - gtsrb\n      - mnist\n      - dtd\n  - /dataset/image_classification/test@test_datasets:\n      - sun397\n      - stanford-cars\n      - resisc45\n      - eurosat\n      - svhn\n      - gtsrb\n      - mnist\n      - dtd\n  - _self_\n_target_: fusion_bench.modelpool.CLIPVisionModelPool\n_recursive_: False\nprocessor: openai/clip-vit-base-patch32\nmodels:\n  _pretrained_: openai/clip-vit-base-patch32\n  sun397: tanganke/clip-vit-base-patch32_sun397\n  stanford-cars: tanganke/clip-vit-base-patch32_stanford-cars\n  resisc45: tanganke/clip-vit-base-patch32_resisc45\n  eurosat: tanganke/clip-vit-base-patch32_eurosat\n  svhn: tanganke/clip-vit-base-patch32_svhn\n  gtsrb: tanganke/clip-vit-base-patch32_gtsrb\n  mnist: tanganke/clip-vit-base-patch32_mnist\n  dtd: tanganke/clip-vit-base-patch32_dtd\nplatform: hf\n</code></pre> <p>The configuration uses YAML's inheritance feature with the defaults key. It inherits from a template (<code>_template.yaml</code>) and overrides specific values. Some values are set to <code>???</code> or null, indicating that they need to be specified or can be optionally set when using this configuration. This configuration structure allows for modular and reusable setups, making it easier to manage different model configurations within the FusionBench library.</p> config/modelpool/CLIPVisionModelPool/_template.yaml<pre><code>_usage_: |\n  defaults:\n    - CLIPVisionModelPool@: _template\n_target_: fusion_bench.modelpool.CLIPVisionModelPool\n_recursive_: False\nmodels: ???\ntrain_datasets: null\ntest_datasets: null\nprocessor: openai/clip-vit-base-patch32\n</code></pre> <p>The type of the modelpool is <code>fusion_bench.modelpool.CLIPVisionModelPool</code>.</p>"},{"location":"modelpool/clip_vit/#special-model-handling","title":"Special Model Handling","text":"<p>The modelpool recognizes special model names:</p> <ul> <li><code>_pretrained_</code>: Reserved name for the base pretrained model</li> <li>All other names are treated as task-specific fine-tuned models</li> <li>Use <code>has_pretrained</code> property to check for pretrained model availability</li> <li>Access model lists via <code>all_model_names</code> (includes special) or <code>model_names</code> (excludes special)</li> </ul>"},{"location":"modelpool/clip_vit/#basic-usage-example","title":"Basic Usage Example","text":"<pre><code>from fusion_bench.modelpool import CLIPVisionModelPool\nfrom omegaconf import DictConfig\n\n# Configure the model pool\nconfig = DictConfig({\n    \"models\": {\n        \"_pretrained_\": \"openai/clip-vit-base-patch32\",\n        \"sun397\": \"tanganke/clip-vit-base-patch32_sun397\",\n        \"cars\": \"tanganke/clip-vit-base-patch32_stanford-cars\"\n    },\n    \"processor\": \"openai/clip-vit-base-patch32\",\n})\n\n# Initialize the model pool\nmodelpool = CLIPVisionModelPool(config.models, processor=config.processor)\n\n# Load models\npretrained_model = modelpool.load_model(\"_pretrained_\")\nsun397_model = modelpool.load_model(\"sun397\")\n\n# Load processor\nprocessor = modelpool.load_processor()\n\n# Load complete CLIP model\nclip_model = modelpool.load_clip_model(\"_pretrained_\")\n\n# Check available models\nprint(f\"All models: {modelpool.all_model_names}\")\nprint(f\"Task-specific models: {modelpool.model_names}\")\nprint(f\"Has pretrained model: {modelpool.has_pretrained}\")\n</code></pre>"},{"location":"modelpool/clip_vit/#advanced-configuration-example","title":"Advanced Configuration Example","text":"<pre><code>from fusion_bench.modelpool import CLIPVisionModelPool\n\n# Advanced configuration with custom loading parameters\nmodelpool = CLIPVisionModelPool(\n    models={\n        \"_pretrained_\": {\n            \"_target_\": \"transformers.CLIPVisionModel.from_pretrained\",\n            \"pretrained_model_name_or_path\": \"openai/clip-vit-base-patch32\",\n            \"torch_dtype\": \"auto\",\n            \"device_map\": \"cuda:0\"\n        },\n        \"sun397\": \"tanganke/clip-vit-base-patch32_sun397\"\n    },\n    processor={\n        \"_target_\": \"transformers.CLIPProcessor.from_pretrained\", \n        \"pretrained_model_name_or_path\": \"openai/clip-vit-base-patch32\"\n    },\n    train_datasets={\n        \"sun397\": \"tanganke/sun397\",\n        \"cars\": \"tanganke/stanford-cars\"\n    },\n)\n\n# Load dataset\ntrain_dataset = modelpool.load_train_dataset(\"sun397\")\n</code></pre>"},{"location":"modelpool/clip_vit/#working-with-modelscope-platform","title":"Working with ModelScope Platform","text":"<pre><code># Use ModelScope platform (popular in China)\nmodelpool = CLIPVisionModelPool(\n    models={\n        \"_pretrained_\": \"openai-community/clip-vit-base-patch32\",\n        \"custom_task\": \"your-modelscope-username/custom-model\"\n    },\n    platform=\"modelscope\"\n)\n\n# Models and datasets will be loaded from ModelScope hub\nmodel = modelpool.load_model(\"_pretrained_\")\n</code></pre>"},{"location":"modelpool/clip_vit/#lora-and-l-lora-models","title":"LoRA and L-LoRA Models","text":"<p>We have fine-tuned CLIP-ViT-B/16 models on the eight image classification tasks using LoRA (Low-Rank Adaptation) and L-LoRA (Linearized LoRA) methods.</p>"},{"location":"modelpool/clip_vit/#training-configuration","title":"Training Configuration","text":"<ul> <li>Target modules: <code>q_proj</code> and <code>v_proj</code> layers</li> <li>Learning rate: 1e-5 using Adam optimizer</li> <li>Training steps: 2000 steps</li> <li>Fine-tuning script: Available at <code>examples/clip_finetune/clip_finetune.sh</code></li> </ul>"},{"location":"modelpool/clip_vit/#available-model-collections","title":"Available Model Collections","text":"<ul> <li>CLIP-ViT-B/16 on the eight image classification tasks (LoRA)</li> <li>CLIP-ViT-B/16 on eight image classification tasks (L-LoRA)</li> </ul>"},{"location":"modelpool/clip_vit/#loading-lora-models","title":"Loading LoRA Models","text":"<p>Load LoRA models using PEFT (see load_lora_vision_model_hf):</p> <pre><code>from transformers import CLIPVisionModel\nfrom peft import PeftModel\n\n# Load base model\nbase_model = CLIPVisionModel.from_pretrained('openai/clip-vit-base-patch16').vision_model\n\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(base_model, peft_model_id)\n</code></pre>"},{"location":"modelpool/clip_vit/#loading-l-lora-models","title":"Loading L-LoRA Models","text":"<p>Load L-LoRA models using the specialized loader (refer to load_l_lora_vision_model_hf):</p> <pre><code>from fusion_bench.models.linearized.vision_model import load_l_lora_vision_model_hf\n\n# Load L-LoRA model\nmodel = load_l_lora_vision_model_hf(\n    base_model_name='openai/clip-vit-base-patch16',\n    lora_model_name=lora_model_id\n)\n</code></pre>"},{"location":"modelpool/clip_vit/#integration-with-clipvisionmodelpool","title":"Integration with CLIPVisionModelPool","text":"<p>The CLIPVisionModelPool can be configured to work with LoRA/L-LoRA models by specifying appropriate model configurations:</p> <pre><code>from fusion_bench.modelpool import CLIPVisionModelPool\n\n# Configure pool for LoRA models\nmodelpool = CLIPVisionModelPool(\n    models={\n        \"_pretrained_\": \"openai/clip-vit-base-patch16\",\n        \"sun397_lora\": {\n            \"_target_\": \"fusion_bench.models.linearized.vision_model.load_lora_vision_model_hf\",\n            \"base_model_name\": \"openai/clip-vit-base-patch16\",\n            \"lora_model_name\": \"tanganke/clip-vit-base-patch16_sun397_lora\"\n        }\n    }\n)\n</code></pre> Performance of the fine-tuned CLIP-ViT-B/16 models (LoRA-16)Performance of the fine-tuned CLIP-ViT-B/16 models (L-LoRA-16) Model SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average SUN397 70.8 64.8 66.7 55.4 51.8 44.0 52.2 46.0 56.5 Cars 65.8 72.3 65.7 54.5 52.3 44.1 54.1 45.3 56.8 RESISC45 66.2 64.6 88.9 65.4 51.8 43.6 54.7 45.6 60.1 EuroSAT 65.6 64.6 59.4 97.1 48.2 43.6 60.5 46.0 60.6 SVHN 65.5 64.1 65.3 39.1 93.2 45.5 83.0 45.1 62.6 GTSRB 65.5 63.9 64.2 28.6 56.9 91.0 71.3 45.5 60.9 MNIST 65.3 64.1 65.7 51.9 57.6 46.6 98.4 45.2 61.9 DTD 64.5 64.2 61.0 49.1 54.2 44.2 68.0 67.9 59.1 Model SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average SUN397 69.0 65.0 66.7 56.0 52.6 44.0 53.3 45.4 56.5 Cars 65.8 69.7 65.7 54.4 52.0 43.7 52.6 45.3 56.2 RESISC45 65.9 64.3 83.6 66.3 51.7 43.4 51.9 45.6 59.1 EuroSAT 65.5 64.8 64.2 95.4 50.7 43.8 58.4 45.9 61.1 SVHN 65.3 64.4 65.2 46.6 90.1 45.8 80.0 45.4 62.9 GTSRB 65.5 64.4 64.2 43.8 59.5 78.6 72.6 45.2 61.7 MNIST 65.3 64.5 65.0 53.3 57.6 45.6 96.4 45.5 61.7 DTD 65.7 64.7 65.9 54.5 51.6 44.4 58.2 56.2 57.7 <p> </p>"},{"location":"modelpool/clip_vit/#usage-examples","title":"Usage Examples","text":"<p>This section demonstrates various ways to use CLIP-ViT models for open vocabulary image classification with different fusion methods using the <code>fusion_bench</code> command line interface and the CLIPVisionModelPool API.</p>"},{"location":"modelpool/clip_vit/#model-information-inspection","title":"Model Information Inspection","text":"<p>Inspect basic information about CLIP-ViT models including parameter counts:</p> <pre><code># Inspect CLIP-ViT-B/32 model\nfusion_bench \\\n  method=dummy \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_individual \\\n  taskpool=dummy  # dummy task reports basic model information (e.g., parameter count)\n\n# Output:\n# {'model_info': {'trainable_params': 87456000, 'all_params': 87456000, 'trainable_percentage': 1.0}}\n\n# Inspect CLIP-ViT-L/14 model\nfusion_bench \\\n  method=dummy \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_individual \\\n  taskpool=dummy\n\n# Output:\n# {'model_info': {'trainable_params': 303179776, 'all_params': 303179776, 'trainable_percentage': 1.0}}\n</code></pre>"},{"location":"modelpool/clip_vit/#programmatic-model-information","title":"Programmatic Model Information","text":"<p>You can also inspect models programmatically using the CLIPVisionModelPool:</p> <pre><code>from fusion_bench.modelpool import CLIPVisionModelPool\n\n# Initialize model pool\nmodelpool = CLIPVisionModelPool(\n    models={\"clip_model\": \"openai/clip-vit-base-patch32\"}\n)\n\n# Load and inspect model\nmodel = modelpool.load_model(\"clip_model\")\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\nprint(f\"Model architecture: {model}\")\n</code></pre>"},{"location":"modelpool/clip_vit/#single-model-evaluation","title":"Single Model Evaluation","text":"<p>evaluate a single CLIP-ViT-B/32 model on the eight downstream tasks:</p> <pre><code>path_to_clip_model=\"tanganke/clip-vit-base-patch32_sun397\"\n\nfusion_bench \\\n  method=dummy \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_individual \\\n    modelpool.models._pretrained_.pretrained_model_name_or_path=\"'${path_to_clip_model}'\" \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>Here:</p> <ul> <li>The <code>dummy</code> method is a special method used to skip the model merging process, it loads the pre-trained model in the modelpool and return the model without any modification (or the first model when a model with the name <code>_pretrained_</code> does not exist in modelpool), see dummy method for more information. </li> <li>The <code>CLIPVisionModelPool/clip-vit-base-patch32_individual</code> modelpool contains a single model. By passing argument <code>modelpool.models.0.path=...</code>, we override the path of the model with the specified path. config/modelpool/CLIPVisionModelPool/clip-vit-base-patch32_individual.yaml<pre><code>_target_: fusion_bench.modelpool.CLIPVisionModelPool\n_recursive_: False\nmodels:\n  _pretrained_: openai/clip-vit-base-patch32\nprocessor: ${.models._pretrained_}\n</code></pre></li> <li>The <code>CLIPVisionModelTaskPool/clip-vit-classification_TA8</code> taskpool is used to evaluate the model on the eight tasks.   if <code>$path_to_clip_model</code> is not specified, the pre-trained model from HuggingFace will be used by default. config/taskpool/CLIPVisionModelTaskPool/clip-vit-classification_TA8.yaml<pre><code>defaults:\n  - CLIPVisionModelTaskPool@: _template\n  - /dataset/image_classification/test@test_datasets:\n      - sun397\n      - stanford-cars\n      - resisc45\n      - eurosat\n      - svhn\n      - gtsrb\n      - mnist\n      - dtd\n</code></pre></li> </ul> <p>Use a for loop to evaluate multiple CLIP-ViT-B/32 model on the eight tasks, and save reports to json files:</p> <pre><code>for task in sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd\ndo\n    fusion_bench method=dummy \\\n        modelpool=CLIPVisionModelPool/clip-vit-base-patch32_individual \\\n            modelpool.models._pretrained_.pretrained_model_name_or_path=tanganke/clip-vit-base-patch32_${task} \\\n        taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n        report_save_path=\"outputs/ViT-B-32/single-task/clip-vit-base-patch32_${task}.json\"\ndone\n</code></pre> <p>evaluate the CLIP-ViT-L/14 model on the eight tasks</p> <pre><code>fusion_bench method=dummy \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_individual \\\n    modelpool.models._pretrained_.pretrained_model_name_or_path=\"'${path_to_clip_model}'\" \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#simple-averaging","title":"Simple Averaging","text":"<p>merge CLIP-ViT-B/32 models using simple average and evaluate on the eight tasks</p> <pre><code>fusion_bench \\\n  method=simple_average \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8_model_only \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \n\n# results\n{\n    \"svhn\": {\"accuracy\": 0.6451674699783325, \"loss\": 1.128771424293518},\n    \"stanford_cars\": {\"accuracy\": 0.625668466091156, \"loss\": 1.135254979133606},\n    \"resisc45\": {\"accuracy\": 0.7079365253448486, \"loss\": 0.9697789549827576},\n    \"eurosat\": {\"accuracy\": 0.7685185074806213, \"loss\": 0.6301173567771912},\n    \"gtsrb\": {\"accuracy\": 0.5494061708450317, \"loss\": 1.492265224456787},\n    \"mnist\": {\"accuracy\": 0.8626000285148621, \"loss\": 0.5933865308761597},\n    \"dtd\": {\"accuracy\": 0.5090425610542297, \"loss\": 1.79731023311615},\n    \"sun397\": {\"accuracy\": 0.6543576717376709, \"loss\": 1.1993952989578247},\n}\n</code></pre> <p>merge CLIP-ViT-L/14 models using simple average and evaluate on the eight tasks</p> <pre><code>fusion_bench method=simple_average \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8_model_only \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#fisher-merging","title":"Fisher Merging","text":"<p>merge CLIP-ViT-B/32 models using Fisher Merging and evaluate on the eight tasks</p> <pre><code>fusion_bench \\\n  method=fisher_merging/clip_fisher_merging \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>merge CLIP-ViT-L/14 models using Fisher Merging and evaluate on the eight tasks</p> <pre><code>fusion_bench \\\n  method=fisher_merging/clip_fisher_merging \\\n    method.dataloader_kwargs.batch_size=8 method.dataloader_kwargs.num_workers=4 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#regmean","title":"RegMean","text":"<p>merge CLIP-ViT-B/32 models using RegMean and evaluate on the eight tasks</p> <pre><code>fusion_bench method=regmean/clip_regmean \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>For CLIP-ViT-L/14 models:</p> <pre><code>fusion_bench \\\n  method=regmean/clip_regmean \\\n    method.dataloader_kwargs.batch_size=8 method.dataloader_kwargs.num_workers=4 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#regmean_1","title":"RegMean++","text":"<p>Run and evaluate the RegMean++ algorithm on eight image classification tasks:</p> <pre><code>for model in clip-vit-base-patch32 clip-vit-base-patch16 clip-vit-large-patch14\ndo\n  fusion_bench \\\n      method=regmean_plusplus/clip_regmean_plusplus \\\n      modelpool=CLIPVisionModelPool/${model}_TA8 \\\n      taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n        taskpool.base_model=openai/${model} \\\n      report_save_path=outputs/${model}_TA8_regmean_plusplus.json\ndone\n</code></pre>"},{"location":"modelpool/clip_vit/#task-arithmetic","title":"Task Arithmetic","text":"<p>merge CLIP-ViT-B/32 models using task arithmetic and evaluate on the eight tasks</p> <pre><code>fusion_bench method=task_arithmetic method.scaling_factor=0.3\\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n\n# results\n{\n    \"svhn\": {\"accuracy\": 0.77927166223526, \"loss\": 0.7050645351409912},\n    \"stanford_cars\": {\"accuracy\": 0.5565228462219238, \"loss\": 1.4873239994049072},\n    \"resisc45\": {\"accuracy\": 0.6487301588058472, \"loss\": 1.3709946870803833},\n    \"eurosat\": {\"accuracy\": 0.7674074172973633, \"loss\": 0.6550557017326355},\n    \"gtsrb\": {\"accuracy\": 0.6850356459617615, \"loss\": 1.2349143028259277},\n    \"mnist\": {\"accuracy\": 0.9606999754905701, \"loss\": 0.1570172756910324},\n    \"dtd\": {\"accuracy\": 0.471808522939682, \"loss\": 2.1495635509490967},\n    \"sun397\": {\"accuracy\": 0.571083128452301, \"loss\": 1.7016042470932007},\n}\n</code></pre> <pre><code># or use a for loop to try different scaling factors \n# and save the results to different files\nfor scaling_factor in 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\ndo\n  fusion_bench \\\n    method=task_arithmetic method.scaling_factor=$scaling_factor \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    report_save_path=outputs/clip-vit-base-patch32_TA8_task_arithmetic_scaling_factor_${scaling_factor}.json\ndone\n</code></pre> <p>merge CLIP-ViT-L/14 models using task arithmetic and evaluate on the eight tasks</p> <pre><code>fusion_bench method=task_arithmetic method.scaling_factor=0.3\\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#ties-merging","title":"Ties-Merging","text":"<p>merge CLIP-ViT-B/32 models using Ties-Merging and evaluate on the eight tasks</p> <pre><code>fusion_bench method=ties_merging method.scaling_factor=0.3 method.threshold=20 \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <pre><code># or use a for loop to try different scaling factors\n# and save the results to different files\nfor scaling_factor in 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\ndo\n  fusion_bench \\\n    method=ties_merging method.scaling_factor=$scaling_factor method.threshold=20 \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    report_save_path=outputs/clip-vit-base-patch32_TA8_ties_merging_scaling_factor_${scaling_factor}.json\ndone\n</code></pre> <p>merge CLIP-ViT-L/14 models using Ties-Merging and evaluate on the eight tasks</p> <pre><code>fusion_bench method=ties_merging method.scaling_factor=0.3 method.threshold=20 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#adamerging","title":"AdaMerging","text":"<p>merge CLIP-ViT-B/32 models using task-wise AdaMerging and evaluate on the eight tasks, and save the merging weights by specifying the <code>method.save_merging_weights</code> parameter</p> <pre><code>fusion_bench \\\n  method=adamerging/clip \\\n    method.name=clip_task_wise_adamerging \\\n    method.save_merging_weights=outputs/clip-vit-base-patch32_TA8_task_wise_adamerging_weights.pt \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>merge CLIP-ViT-L/14 models using task-wise AdaMerging and evaluate on the eight tasks, and save the merging weights by specifying the <code>method.save_merging_weights</code> parameter. Here we split the training process into two stages, the first stage is to train the merging weights, and the second stage is to evaluate the model with the learned merging weights.</p> <pre><code># learn the merging weights.\n# the per-device batch size is 4, and the total batch size is 4*4=16\nfusion_bench print_config=false \\\n  method=adamerging/clip \\\n    method.name=clip_task_wise_adamerging \\\n    method.save_merging_weights=outputs/clip-vit-large-patch14_TA8_task_wise_adamerging_weights.pt \\\n    method.devices=4 method.batch_size=4 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=dummy # dummy taskpool is used to skip the evaluation process\n\n# by specifying the learned merging weights, we skip the training process and directly evaluate the model\nfusion_bench print_config=false \\\n  method=adamerging/clip \\\n    method.name=clip_task_wise_adamerging \\\n    method.weights=outputs/clip-vit-large-patch14_TA8_task_wise_adamerging_weights.pt \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre> <p>merge CLIP-ViT-B/32 models using layer-wise AdaMerging and evaluate on the eight tasks</p> <pre><code>fusion_bench \\\n    method=adamerging/clip \\\n        method.name=clip_layer_wise_adamerging \\\n        method.save_merging_weights=merging_weights.pt \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    fabric.loggers.root_dir=outputs/logs/ViT-B-32 \\\n    fabric.loggers.name=clip_layer_wise_adamerging_adamerging\n</code></pre> <p>merge CLIP-ViT-L/14 models using layer-wise AdaMerging and evaluate on the eight tasks</p> <pre><code># learn the merging weights.\n# the per-device batch size is 4, and the total batch size is 4*4=16\nfusion_bench print_config=false \\\n  method=adamerging/clip \\\n    method.name=clip_layer_wise_adamerging \\\n    method.save_merging_weights=outputs/clip-vit-large-patch14_TA8_layer_wise_adamerging_weights.pt \\\n    method.devices=4 method.batch_size=4 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=dummy # dummy taskpool is used to skip the evaluation process\n\n# by specifying the learned merging weights, we skip the training process and directly evaluate the model\nfusion_bench \\\n  method=adamerging/clip \\\n    method.name=clip_layer_wise_adamerging \\\n    method.weights=outputs/clip-vit-large-patch14_TA8_layer_wise_adamerging_weights.pt \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#weight-ensembling-moe","title":"Weight-Ensembling MoE","text":"<p>fuse CLIP-ViT-B/32 models using Weight-Ensembling Mixture of Experts and evaluate on the eight tasks</p> <pre><code>fusion_bench \\\n  method=weight_ensembling_moe \\\n    method.name=clip_weight_ensembling_moe \\\n    method.use_grad_accumulate=false \\\n    method.save_checkpoint=outputs/clip-vit-base-patch32_TA8_weight_ensembling_moe_checkpoint.ckpt \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>fuse CLIP-ViT-L/14 models using Weight-Ensembling Mixture of Experts and evaluate on the eight tasks</p> <pre><code># merge eight CLIP-ViT-L/14 models using WE MoE, fine-tune the routers\nfusion_bench print_config=false \\\n  method=weight_ensembling_moe \\\n    method.name=clip_weight_ensembling_moe \\\n    method.use_grad_accumulate=true \\\n    method.save_checkpoint=outputs/clip-vit-large-patch14_TA8_weight_ensembling_moe_checkpoint.ckpt \\\n    method.batch_size=4 method.devices=4 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=dummy &amp;&amp;\n\n# load the checkpoint and evaluate the model\nfusion_bench \\\n  method=weight_ensembling_moe \\\n    method.name=clip_weight_ensembling_moe \\\n    method.checkpoint=outputs/clip-vit-large-patch14_TA8_weight_ensembling_moe_checkpoint.ckpt \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#experimental-results","title":"Experimental Results","text":"<p>We provide the experimental results of the CLIP-ViT models for open vocabulary image classification on the eight tasks in the following table.</p> <p>Hyperparameters not fully optimized</p> <p>The hyperparameters used in these merging methods are not fully optimized and should be considered as preliminary results only. We welcome any discoveries of more effective parameters and would be grateful for your contributions to help us improve our results.</p> <p>Please note that some model merging paper results were obtained using OpenCLIP models, which may show discrepancies with the results presented here. In such cases, the results reported in the original papers should be considered authoritative.</p> Table: Multi-task model merging methods using CLIP-ViT-B/32 models.Table: Multi-task model merging methods using CLIP-ViT-B/16 models.Table: Multi-task model merging methods using CLIP-ViT-L/14 models. Method SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average Reference Results Pre-trained 63.2 59.8 60.7 46.0 31.6 32.5 48.2 43.9 48.2 Fine-tuned (STL) 75.0 78.3 95.2 99.0 97.3 98.9 99.6 79.7 90.3 Traditional MTL 72.3 76.6 92.2 97.9 95.5 97.7 99.3 77.7 88.6 Model Ensemble Simple Ensemble 64.9 63.5 75.7 93.7 85.7 73.8 93.8 55.1 75.8 Model Merging Simple Averaging 65.4 62.6 70.8 76.9 64.5 54.9 86.3 50.9 66.5 Fisher Merging 66.7 64.0 72.2 91.6 69.0 64.3 83.5 53.7 70.6 RegMean 68.6 70.0 84.6 95.4 92.6 83.4 98.4 66.1 82.4 RegMean++ 69.3 70.5 86.7 96.1 94.1 90.4 99.0 68.7 84.4 Task Arithmetic (\\(\\lambda=0.3\\)) 57.1 55.7 64.9 76.7 77.9 68.5 96.1 47.2 68.0 Concrete Task Arithmetic (\\(\\lambda=0.3\\)) 64.2 63.3 75.6 94.1 90.3 82.9 98.0 52.5 77.6 Ties-Merging (\\(\\lambda=0.3\\)) 67.1 64.2 74.1 76.8 77.7 69.4 94.1 54.0 72.2 Task-wise AdaMerging (\\(\\lambda=0.3\\)) 58.6 56.9 69.8 82.4 70.3 58.9 97.2 55.3 68.7 Layer-wise AdaMerging (\\(\\lambda=0.3\\)) 67.9 71.3 83.5 92.7 87.4 92.9 98.2 67.0 82.6 Concrete Layer-wise AdaMerging (\\(\\lambda=0.3\\)) 69.1 72.7 85.9 94.7 91.3 95.7 98.7 66.8 84.4 WUDI-Merging 68.0 72.5 85.0 94.6 94.8 94.9 99.3 66.6 84.5 Model Mixing Efficient Weight-Ensembling MoE (\\(90\\%\\)) 74.3 76.3 92.7 97.9 96.1 98.6 99.5 77.8 89.1 Weight-Ensembling MoE 73.7 76.8 93.4 98.2 96.8 98.2 99.6 76.6 89.2 Method SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average Reference Results Pre-trained 65.5 64.6 66.3 54.1 51.9 43.4 51.7 44.9 55.3 Fine-tuned (STL) 78.9 85.9 96.6 99.0 97.6 99.0 99.7 82.3 92.3 Model Merging Simple Averaging 68.7 69.0 75.0 83.2 74.9 62.5 93.7 51.1 72.3 Fisher Merging 70.8 71.8 76.2 93.4 77.4 61.2 90.7 52.3 74.2 RegMean 72.6 78.8 89.2 96.3 94.9 90.0 98.8 67.9 86.0 RegMean++ 72.8 78.9 89.3 97.3 96.0 93.0 99.1 71.0 87.2 Task Arithmetic (\\(\\lambda=0.3\\)) 65.9 68.3 75.4 84.5 88.8 81.9 98.0 53.9 77.1 Ties-Merging (\\(\\lambda=0.3\\)) 70.6 71.2 79.8 87.5 83.2 76.2 96.4 55.4 77.5 Layer-wise AdaMerging (\\(\\lambda=0.3\\)) 70.6 79.6 86.1 93.6 93.5 95.4 98.1 62.9 85.0 Model Mixing Efficient Weight-Ensembling MoE (\\(90\\%\\)) 77.7 85.0 94.9 98.2 97.2 98.9 99.5 81.4 91.6 Weight-Ensembling MoE 77.2 85.0 94.8 98.3 97.3 98.9 99.6 80.8 91.5 Method SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average Reference Results Pre-trained 68.3 77.8 71.0 58.9 58.4 50.6 76.4 55.5 64.6 Fine-tuned (STL) 82.8 92.9 97.4 99.2 97.9 99.2 99.8 85.5 94.3 Traditional MTL 79.0 89.3 94.5 98.4 96.4 98.1 99.4 83.7 92.4 Model Merging Simple Averaging 72.5 81.5 82.2 90.0 81.6 74.0 96.6 61.8 80.0 Fisher Merging 70.6 79.4 84.1 98.1 74.7 85.0 89.5 61.0 80.3 RegMean 76.9 89.8 93.0 97.5 96.3 94.1 98.7 77.0 90.4 RegMean++ 77.2 89.6 92.8 97.5 96.9 96.3 99.2 78.4 91.0 Task Arithmetic (\\(\\lambda=0.3\\)) 72.0 79.0 80.5 86.0 87.5 83.5 98.0 58.8 80.7 Ties-Merging (\\(\\lambda=0.3\\)) 74.7 83.3 86.4 91.3 89.7 85.2 97.8 63.9 84.0 Task-wise AdaMerging (\\(\\lambda=0.3\\)) 75.8 80.1 77.2 83.6 68.4 93.5 93.1 69.0 80.1 Layer-wise AdaMerging (\\(\\lambda=0.3\\)) 78.1 90.7 90.8 96.5 94.8 97.5 98.6 81.3 91.0 Model Mixing Efficient Weight-Ensembling MoE (\\(90\\%\\)) 81.5 92.0 96.0 97.8 97.7 99.1 99.5 84.1 93.5 Weight-Ensembling MoE 81.5 92.3 96.5 98.8 97.6 99.4 99.6 84.5 93.8"},{"location":"modelpool/clip_vit/#scope","title":"Scope","text":""},{"location":"modelpool/clip_vit/#task-vector-cosine-similarity","title":"Task Vector Cosine Similarity","text":"<p>Compute the cosine similarities between the task vectors and save the results to a CSV file.</p> <pre><code># CLIP-ViT-B/32 models\nfusion_bench \\\n  method=task_vector_cos_similarity \\\n    method.save_to_csv='outputs/clip-vit-base-patch32_cos.csv' \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=dummy  # do not evaluate the model\n\n# CLIP-ViT-L/14 models\nfusion_bench \\\n  method=task_vector_cos_similarity \\\n    method.save_to_csv='outputs/clip-vit-large-patch14_cos.csv' \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  tsakpool=dummy\n</code></pre> Cosine similarity matrices of task vectors for CLIP-ViT-B/32 and CLIP-ViT-L/14 models."},{"location":"modelpool/clip_vit/#generalization-and-robustness-evaluation","title":"Generalization and Robustness Evaluation","text":"<p>You can also evaluate the generalization and robustness of different multi-task model fusion methods by change the configurations.</p> <p>Instruction for running the generalization experiments:</p> <pre><code>fusion_bench \\\n    method=... \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_generalization_exp1 # or `clip-vit-base-patch32_generalization_exp2`\n</code></pre> <p>Instruction for running the robustness experiments:</p> <pre><code># corription can be one of the following values: \n# contrast, gaussian_noise, impulse_noise, jpeg_compression, motion_blur, pixelate, spatter\n# or pass `taskpool=clip-vit-base-patch32_robustness_clean` to evaluate the model on clean data\ncorruption=contrast\nfusion_bench \\\n    --config-name clip-vit-base-patch32_robustness_corrupted \\\n    corruption=${corruption} \\\n    method=... \\\n</code></pre> <p>Below is an example of different types of corruptions:</p>  An example of corruption data visualization, in which the corruption image generation method refers to Hendrycks &amp; Dietterich (2019) <sup>1</sup>."},{"location":"modelpool/clip_vit/#experimental-results_1","title":"Experimental Results","text":"<p>Hyperparameters not fully optimized</p> <p>The hyperparameters used in these merging methods are not fully optimized and should be considered as preliminary results only. We welcome any discoveries of more effective parameters and would be grateful for your contributions to help us improve our results.</p> <p>Please note that some model merging paper results were obtained using OpenCLIP models, which may show discrepancies with the results presented here. In such cases, the results reported in the original papers should be considered authoritative.</p> Table: Results of the generalization experiments (Exp1).Table: Results of the generalization experiments (Exp2). Seen Tasks Unseen Tasks Method SUN397 Cars RESISC45 DTD SVHN GTSRB Avg. MNIST EuroSAT Avg. Pre-trained 63.2 59.9 60.6 43.9 23.5 30.4 46.9 47.6 45.6 46.6 Fisher Merging 65.5 67.2 78.2 57.6 84.2 75.9 71.4 71.8 49.4 60.6 RegMean 69.5 70.8 88.7 67.2 95.2 89.4 80.1 82.9 44.6 63.8 RegMean++ 69.8 70.8 90.2 70.3 95.5 93.2 81.6 81.3 44.1 62.7 Task Arithmetic 64.3 63.0 73.2 54.9 84.7 79.5 69.9 75.5 42.6 59.1 Ties-Merging 68.3 65.5 76.9 54.9 75.4 72.0 68.9 73.1 47.3 60.2 Layer-wise AdaMerging 68.4 71.9 87.9 69.1 92.2 93.8 80.5 77.7 47.3 62.5 Weight-Ensembling MoE 75.4 77.5 94.3 77.0 96.8 98.7 86.6 78.3 44.0 61.1 Seen Tasks Unseen Tasks Method SUN397 Cars GTSRB EuroSAT DTD MNIST Avg. RESISC45 SVHN Avg. Pre-trained 63.2 59.9 30.4 45.6 43.9 47.6 48.4 60.6 23.5 40.1 Fisher Merging 68.1 67.4 67.2 86.4 58.6 81.6 71.5 60.2 42.5 51.3 RegMean 70.4 71.9 89.3 97.6 69.8 98.8 83.0 49.4 49.0 49.2 RegMean++ 70.6 71.4 94.2 96.8 70.5 99.2 83.8 50.8 54.8 52.8 Task Arithmetic 65.2 63.6 76.1 87.1 56.4 94.2 73.8 52.4 45.2 48.8 Ties-Merging 68.2 65.9 70.0 81.2 56.0 89.0 71.7 60.3 47.3 53.8 Layer-wise AdaMerging 69.8 72.4 95.5 95.1 70.7 98.1 83.6 48.7 60.7 54.7 Weight-Ensembling MoE 74.3 78.1 98.8 98.7 75.1 99.5 87.4 47.3 51.3 49.3 <p>Table: Results of the robustness experiments (\\(\\lambda=0.3\\)).</p> Method Cars EuroSAT RESISC45 GTSRB Avg. Cars EuroSAT RESISC45 GTSRB Avg. Clean Test set Motion Blur Fisher Merging 66.0 92.7 83.7 78.7 80.3 60.7 57.6 81.7 78.4 69.6 RegMean 73.1 97.2 91.2 95.1 89.1 70.8 71.3 88.7 87.9 79.7 RegMean++ 73.7 96.7 91.9 96.6 89.7 72.6 71.2 89.9 93.6 81.8 Task Arithmetic 64.6 91.8 80.2 74.8 77.9 62.4 59.2 78.5 63.3 65.9 Ties-Merging 65.2 83.3 78.1 67.4 73.5 64.4 53.9 76.4 57.1 62.9 Layer-wise AdaMerging 75.2 94.3 87.6 96.7 88.5 72.4 72.7 85.3 94.3 81.2 Weight-Ensembling MoE 77.4 98.9 94.4 99.0 92.4 76.5 74.2 93.7 97.4 85.5 Impulse Noise Gaussian Noise Fisher Merging 61.5 50.0 74.7 52.6 59.7 61.6 48.1 76.0 51.3 59.3 RegMean 68.1 54.2 85.1 69.2 69.1 69.6 42.8 87.1 69.8 67.3 RegMean++ 69.3 48.6 85.8 76.2 70.0 71.4 39.9 88.2 74.2 68.4 Task Arithmetic 59.8 53.3 72.3 45.0 57.6 61.5 52.5 75.0 50.1 59.8 Ties-Merging 60.2 45.6 69.8 38.3 53.5 61.8 47.3 73.1 42.3 56.1 Layer-wise AdaMerging 69.2 40.0 79.6 83.3 68.0 70.0 53.3 82.1 80.0 71.4 Weight-Ensembling MoE 75.1 9.7 91.5 91.8 67.0 76.5 9.6 92.7 88.7 66.8 Pixelate Spatter Fisher Merging 2.2 34.0 17.0 63.2 29.1 61.4 64.2 74.6 47.3 61.9 RegMean 2.3 38.1 17.1 90.9 37.1 68.5 64.0 84.6 83.9 75.2 RegMean++ 2.2 37.9 17.2 94.2 37.9 69.7 60.3 84.2 89.3 75.9 Task Arithmetic 2.3 33.2 19.1 65.6 30.0 61.0 62.5 72.8 57.0 63.3 Ties-Merging 3.3 31.8 18.0 58.5 27.9 61.3 52.9 70.3 48.1 58.2 Layer-wise AdaMerging 1.3 52.9 21.0 91.0 41.5 68.4 55.9 78.3 92.3 73.7 Weight-Ensembling MoE 0.5 11.6 2.3 97.5 28.0 75.1 9.7 91.4 96.3 68.1 Contrast JPEG Compression Fisher Merging 63.8 58.4 75.5 70.4 67.0 66.3 67.6 82.6 58.9 68.8 RegMean 70.7 62.9 87.1 91.5 78.0 72.4 76.6 91.1 83.4 80.9 RegMean++ 72.1 63.0 87.7 95.5 79.6 73.5 76.3 91.8 89.6 82.8 Task Arithmetic 62.3 55.7 75.3 70.8 66.0 63.9 66.1 80.1 61.0 67.8 Ties-Merging 64.2 52.4 74.8 63.5 63.7 65.0 59.5 77.9 53.2 63.9 Layer-wise AdaMerging 73.1 67.4 83.0 96.2 79.9 72.9 70.7 86.3 90.6 80.1 Weight-Ensembling MoE 77.2 34.7 93.1 98.4 75.9 77.3 61.0 94.1 95.7 82.0"},{"location":"modelpool/clip_vit/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.modelpool.clip_vision.CLIPVisionModelPool</li> </ul>"},{"location":"modelpool/clip_vit/#model-configuration-schema","title":"Model Configuration Schema","text":"<p>The CLIPVisionModelPool supports flexible model configurations:</p> <pre><code># Simple string configuration\nmodels = {\n    \"model_name\": \"huggingface/model-id\"\n}\n\n# Advanced configuration with custom parameters\nmodels = {\n    \"model_name\": {\n        \"_target_\": \"transformers.CLIPVisionModel.from_pretrained\",\n        \"pretrained_model_name_or_path\": \"huggingface/model-id\",\n        \"torch_dtype\": \"auto\",\n        \"device_map\": \"auto\"\n    }\n}\n\n# Mixed configuration\nmodels = {\n    \"_pretrained_\": \"openai/clip-vit-base-patch32\",\n    \"sun397\": \"tanganke/clip-vit-base-patch32_sun397\",\n    \"custom_model\": {\n        \"_target_\": \"some.custom.loader\",\n        \"custom_param\": \"value\"\n    }\n}\n</code></pre>"},{"location":"modelpool/clip_vit/#platform-support","title":"Platform Support","text":"<p>The modelpool supports multiple platforms for model and dataset loading:</p> <ul> <li>Hugging Face Hub (<code>platform=\"hf\"</code>): Default platform for loading models and datasets</li> <li>ModelScope (<code>platform=\"modelscope\"</code>): Alternative platform popular in China</li> </ul> <pre><code># Hugging Face platform (default, platform=\"hf\" or \"huggingface\")\nmodelpool = CLIPVisionModelPool(models, platform=\"hf\")\n\n# ModelScope platform\nmodelpool = CLIPVisionModelPool(models, platform=\"modelscope\")\n</code></pre> <ol> <li> <p>Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019.\u00a0\u21a9</p> </li> </ol>"},{"location":"modelpool/flan-t5_generation/","title":"Flan-T5 Models for Text Generation","text":""},{"location":"modelpool/flan-t5_generation/#model-information","title":"Model Information","text":"<p>Prompt-based fine-tuned Flan-T5 models on GLUE benchmark tasks. The models are fine-tuned in a text-to-text setting, and the prompt templates are provided below. The source code for the prompt templates can be found in the repository.</p> fusion_bench/tasks/flan_t5_text_generation/glue_prompt_templates.py<pre><code>cola = {\n    \"description\": \"template used by GLUE-CoLA\",\n    \"input_text\": \"Indicate if the following sentence is grammatically correct or not: \\\"{sentence}\\\". Answere 'acceptable' or 'unacceptable'.\",\n    \"target_text\": {\"0\": \"unacceptable\", \"1\": \"acceptable\"},\n}\nmnli = {\n    \"input_text\": \"Does the premise: '{premise}' logically imply, contradict, or is neutral to the hypothesis: '{hypothesis}'? Answere with 'entailment', 'contradiction', or 'neutral'.\",\n    \"target_text\": {\"0\": \"entailment\", \"1\": \"neutral\", \"2\": \"contradiction\"},\n}\nmrpc = {\n    \"input_text\": \"Are the following sentences '{sentence1}' and '{sentence2}' conveying the same meaning? Answere with 'yes' or 'no'.\",\n    \"target_text\": {\"0\": \"no\", \"1\": \"yes\"},\n}\nqnli = {\n    \"input_text\": \"Given the context: '{sentence}', does the question '{question}' have an answer based on the information provided? Answer with 'yes' or 'no'.\",\n    \"target_text\": {\"0\": \"yes\", \"1\": \"no\"},\n}\nqqp = {\n    \"input_text\": \"Do the questions '{question1}' and '{question2}' have the same intent? Answere with 'yes' or 'no'.\",\n    \"target_text\": {\"0\": \"no\", \"1\": \"yes\"},\n}\nrte = {\n    \"description\": \"Template used by GLUE-RTE\",\n    \"input_text\": \"Does the text: '{sentence1}' entail that '{sentence2}' is true? Provide 'yes' or 'no'.\",\n    \"target_text\": {\"0\": \"yes\", \"1\": \"no\"},\n}\nsst2 = {\n    \"input_text\": \"Given the sentence '{sentence}', determine the sentiment. Is it positive or negative?\",\n    \"target_text\": {\"0\": \"negative\", \"1\": \"positive\"},\n}\nstsb = {\n    \"input_text\": \"Consider the sentences '{sentence1}' and '{sentence2}'. On a scale from 1 (completely different) to 5 (completely similar), rate the similarity.\",\n    \"target_text\": \"{:.1f}\",\n}\n</code></pre>"},{"location":"modelpool/flan-t5_generation/#flan-t5-base","title":"Flan-T5-base","text":""},{"location":"modelpool/flan-t5_generation/#full-fine-tuned-models","title":"Full Fine-tuned Models","text":"<p>full fine-tuned Flan-T5-base models on tasks from GLUE benchmark</p> Model GLUE-COLA GLUE-MNLI GLUE-MRPC GLUE-QNLI GLUE-QQP GLUE-RTE GLUE-SST2 GLUE-STSB Average Pre-trained 69.127517 56.454407 76.225490 88.449570 82.119713 80.144404 91.169725 62.190453 75.735160 GLUE-COLA 74.976031 37.208355 72.794118 87.552627 80.415533 76.895307 91.399083 63.583974 73.103128 GLUE-MNLI 65.867689 83.413143 75.735294 89.236683 82.616869 77.978339 90.596330 66.215025 78.957422 GLUE-MRPC 63.374880 48.293428 87.500000 85.831960 81.100668 72.563177 88.073394 76.062875 75.350048 GLUE-QNLI 68.744008 39.246052 75.490196 91.488193 81.291120 78.339350 91.628440 68.200428 74.303474 GLUE-QQP 59.060403 50.412634 73.774510 88.339740 85.369775 81.227437 90.825688 75.948390 75.619822 GLUE-RTE 65.388303 51.115639 69.607843 88.705839 80.774178 85.920578 90.252294 68.944418 75.088636 GLUE-SST2 67.785235 53.958227 76.470588 87.772286 83.415780 80.505415 93.577982 63.612718 75.887279 GLUE-STSB 69.319271 49.302089 76.470588 88.962109 81.662132 77.617329 90.137615 88.695433 77.770821"},{"location":"modelpool/flan-t5_generation/#lora-fine-tuned-models-r16","title":"LoRA Fine-tuned Models (r=16)","text":"<p>LoRA fine-tuned (r=16) Flan-T5-base models on tasks from GLUE benchmark:</p> Model GLUE-COLA GLUE-MNLI GLUE-MRPC GLUE-QNLI GLUE-QQP GLUE-RTE GLUE-SST2 GLUE-STSB Average Pre-trained 69.1 56.5 76.2 88.4 82.1 80.1 91.2 62.2 75.7 GLUE-COLA 69.1 39.9 75.2 89.1 81.1 81.9 90.7 54.0 GLUE-MNLI 69.4 82.7 73.8 89.3 82.0 79.4 90.9 68.1 GLUE-MRPC 64.0 44.9 85.5 82.6 81.0 69.0 88.6 73.6 GLUE-QNLI 68.9 52.7 76.7 90.9 82.8 79.8 91.5 68.9 GLUE-QQP 65.0 54.6 75.7 89.0 84.0 81.6 90.7 75.3 GLUE-RTE 64.9 51.8 69.4 89.2 79.8 84.5 90.6 70.1 GLUE-SST2 68.3 56.6 76.0 88.5 83.4 79.8 92.9 62.6 GLUE-STSB 65.7 1.7 67.4 89.3 80.1 79.8 90.8 87.4"},{"location":"modelpool/flan-t5_generation/#flan-t5-large","title":"Flan-T5-Large","text":""},{"location":"modelpool/flan-t5_generation/#lora-fine-tuned-models-r16_1","title":"LoRA Fine-tuned Models (r=16)","text":"<p>LoRA fine-tuned (r=16) Flan-T5-large models on tasks from GLUE benchmark:</p> Model GLUE-COLA GLUE-MNLI GLUE-MRPC GLUE-QNLI GLUE-QQP GLUE-RTE GLUE-SST2 GLUE-STSB Average Pretrained 73.7 56.6 82.4 91.1 85.5 85.6 94.3 87.5 82.1 GLUE-COLA 80.2 53.9 81.4 90.8 84.5 84.1 93.9 87.1 GLUE-MNLI 73.7 88.5 77.9 92.4 85.2 87.7 94.4 86.7 GLUE-MRPC 75.6 52.6 89.2 92.6 84.4 86.3 94.3 86.3 GLUE-QNLI 73.5 54.5 82.8 94.4 85.8 85.2 93.7 87.1 GLUE-QQP 74.0 53.8 82.8 92.5 87.2 85.6 94.5 88.3 GLUE-RTE 75.6 57.5 69.9 92.8 83.8 91.7 94.6 86.0 GLUE-SST2 73.6 55.3 82.1 91.6 85.5 85.2 95.2 86.9 GLUE-STSB 73.4 39.3 82.1 92.6 86.1 83.4 94.0 90.9"},{"location":"modelpool/flan-t5_generation/#basic-examples","title":"Basic Examples","text":""},{"location":"modelpool/flan-t5_generation/#inverstigate-model-information","title":"Inverstigate Model Information","text":"<p>Load pre-trained Flan-T5-base model and print the model information</p> <pre><code>fusion_bench \\\n    modelpool=Seq2SeqLMPool/flan-t5-base_glue \\\n    method=dummy taskpool=dummy\n# {'model_info': {'trainable_params': 247577856, 'all_params': 247577856, 'trainable_percentage': 1.0}}\n</code></pre> <p>Load pre-trained Flan-T5-large model and print the model information</p> <pre><code>fusion_bench \\\n    modelpool=Seq2SeqLMPool/flan-t5-large_glue_lora16 \\\n    method=dummy taskpool=dummy\n</code></pre>"},{"location":"modelpool/flan-t5_generation/#evaluate-single-model","title":"Evaluate Single Model","text":"<p>Evaluate the pre-trained Flan-T5-base model on GLUE tasks</p> <pre><code>fusion_bench \\\n    method=dummy \\\n    modelpool=Seq2SeqLMPool/flan-t5-base_individual \\\n        modelpool.models._pretrained_.pretrained_model_name_or_path=google/flan-t5-base \\\n    taskpool=flan-t5_glue_text_generation \\\n    report_save_path=outputs/flan-t5-base/pretrained.json\n</code></pre> <p>or evaluate the fine-tuned Flan-T5-base model on GLUE tasks</p> <pre><code>for task in cola mnli mrpc qnli qqp rte sst2 stsb; do\n    fusion_bench \\\n        method=dummy \\\n        modelpool=Seq2SeqLMPool/flan-t5-base_individual \\\n            modelpool.models._pretrained_.pretrained_model_name_or_path=tanganke/flan-t5-base_glue-${task} \\\n        taskpool=flan-t5_glue_text_generation \\\n        report_save_path=outputs/flan-t5-base/glue-$task.json\ndone\n</code></pre>"},{"location":"modelpool/flan-t5_generation/#simple-average","title":"Simple Average","text":"<p>Merge the Flan-T5 models on GLUE tasks using simple average and evaluate on the Flan-T5 text generation task</p> <pre><code># for full fine-tuned models\nfusion_bench \\\n    method=simple_average \\\n    modelpool=Seq2SeqLMPool/flan-t5-base_glue \\\n    taskpool=flan-t5_glue_text_generation\n\n# or using the LoRA fine-tuned models\nfusion_bench \\\n    method=simple_average \\\n    modelpool=Seq2SeqLMPool/flan-t5-base_glue_lora16 \\\n    taskpool=flan-t5_glue_text_generation\n</code></pre>"},{"location":"modelpool/flan-t5_generation/#task-arithmetic","title":"Task Arithmetic","text":"<p>Merge the Flan-T5 models on GLUE tasks using task arithmetic and evaluate on the Flan-T5 text generation task, with scaling factor from 0.0 to 1.0</p> <pre><code># full fine-tuned models with scaling factor set to 0.3\nfusion_bench \\\n    method=task_arithmetic \\\n        method.scaling_factor=0.3 \\\n    modelpool=Seq2SeqLMPool/flan-t5-base_glue \\\n    taskpool=flan-t5_glue_text_generation\n\n# use a for loop to evaluate the performance of task arithmetic with different scaling factors (LoRA fine-tuned models)\nfor scaling_factor in $(seq 0.0 0.1 1.0)\ndo\n    fusion_bench \\\n        method=task_arithmetic \\\n            method.scaling_factor=$scaling_factor \\\n        modelpool=Seq2SeqLMPool/flan-t5-base_glue_lora16 \\\n        taskpool=flan-t5_glue_text_generation\ndone\n</code></pre> scaling_coef glue-cola glue-mnli glue-mrpc glue-qnli glue-qqp glue-rte glue-sst2 glue-stsb Average 0.0 69.1 56.5 76.2 88.4 82.1 80.1 91.2 62.2 75.7 0.1 69.5 59.8 78.7 89.7 83.6 80.5 91.1 70.7 78.0 0.2 69.3 59.0 78.7 90.1 83.8 79.1 91.5 72.9 78.1 0.3 68.8 55.2 78.7 89.8 83.7 79.1 91.5 72.4 77.4 0.4 68.1 31.3 77.7 88.7 83.3 78.7 91.2 68.9 73.5 0.5 66.0 2.2 78.7 86.3 82.6 78.0 90.4 74.2 0.6 5.9 0.0 78.4 81.2 81.6 74.4 88.2 74.9 0.7 0.0 0.0 77.0 74.1 79.8 66.1 70.8 74.7 0.8 0.0 0.0 74.0 67.5 77.0 62.1 8.1 72.5 0.9 0.0 0.0 66.7 60.5 71.7 58.5 0.0 71.6 1.0 0.0 0.0 46.3 50.6 56.3 52.3 0.0 69.8"},{"location":"modelpool/flan-t5_generation/#ties-merging","title":"Ties-Merging","text":"<p>or using ties-merging</p> <pre><code># for full fine-tuned models with scaling factor set to 0.3\nfusion_bench \\\n    method=ties_merging \\\n        method.scaling_factor=0.3 \\\n    modelpool=Seq2SeqLMPool/flan-t5-base_glue \\\n    taskpool=flan-t5_glue_text_generation\n\n# use a for loop to evaluate the performance of ties-merging with different scaling factors (LoRA fine-tuned models)\nfor scaling_factor in $(seq 0.0 0.1 1.0)\ndo\n    fusion_bench \\\n        method=ties_merging \\\n            method.scaling_factor=$scaling_factor \\\n        modelpool=Seq2SeqLMPool/flan-t5-base_glue_lora16 \\\n        taskpool=flan-t5_glue_text_generation\ndone\n</code></pre>"},{"location":"modelpool/flan-t5_generation/#experimental-results","title":"Experimental Results","text":"<p>Flan-T5-Base models:</p> Table: Multi-task model merging methods using Flan-T5-Base (full fine-tuned) models.Table: Multi-task model merging methods using Flan-T5-Base (LoRA r=16) models. Method CoLA MNLI MRPC QNLI QQP RTE SST-2 STSB Average Reference Results Pre-trained 69.1 56.5 76.2 88.4 82.1 80.1 91.2 62.2 75.7 Fine-tuned (STL) 75.0 83.4 87.5 91.5 85.4 85.9 93.6 88.7 86.4 Model Merging Methods Simple Average 69.1 62.6 79.4 89.8 83.9 81.2 91.7 73.2 78.9 Task Arithmetic (\\(\\lambda=0.3\\)) 70.5 57.8 78.4 90.2 83.6 80.5 92.3 77.8 78.9 Ties-Merging (\\(\\lambda=0.3\\)) 70.3 65.0 78.9 90.2 83.5 81.6 91.7 78.3 79.9 Method CoLA MNLI MRPC QNLI QQP RTE SST-2 STSB Average Reference Results Pre-trained 69.1 56.5 76.2 88.4 82.1 80.1 91.2 62.2 75.7 Fine-tuned (STL) 69.1 82.7 85.5 90.9 84.0 84.4 92.9 87.4 84.6 Model Merging Methods Simple Average 69.7 59.7 78.9 90.1 83.8 90.5 91.2 72.0 78.2 Task Arithmetic (\\(\\lambda=0.3\\)) 68.8 55.2 78.7 89.8 83.7 79.1 91.5 72.4 77.4 Ties-Merging (\\(\\lambda=0.3\\)) 68.3 56.3 79.4 89.8 83.7 79.4 91.6 71.2 77.5 <p>Flan-T5-Large models:</p> Table: Multi-task model merging methods using Flan-T5-Large (LoRA r=16) models Method CoLA MNLI MRPC QNLI QQP RTE SST-2 STSB Average Reference Results Pre-trained 73.7 56.6 82.4 91.1 85.5 85.6 94.3 87.5 82.1 Fine-tuned (STL) 80.2 88.5 89.2 94.4 87.2 91.7 95.2 90.9 89.6 Model Merging Methods Simple Average 74.6 84.3 84.1 92.8 86.3 87.4 94.8 88.0 86.5 Task Arithmetic (\\(\\lambda=0.3\\)) 76.9 85.4 85.3 93.9 85.8 88.1 95.2 87.8 87.3 Ties-Merging (\\(\\lambda=0.3\\)) 77.1 85.1 86.3 93.9 86.0 87.7 95.1 88.0 87.4"},{"location":"modelpool/flan-t5_generation/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.modelpool.Seq2SeqLMPool</li> <li>fusion_bench.modelpool.SequenceClassificationModelPool</li> <li>fusion_bench.modelpool.PeftModelForSeq2SeqLMPool</li> </ul>"},{"location":"modelpool/gpt2_classification/","title":"GPT-2 Models for Text Classification","text":"<p>Here we provide a series of GPT-2 models fine-tuned for text classification tasks.</p>"},{"location":"modelpool/gpt2_classification/#the-seven-tasks-from-glue-benchmark","title":"The Seven Tasks from GLUE Benchmark","text":"<p>We provide seven GPT-2 models fine-tuned on the following tasks from GLUE Benchmark: CoLA, SST-2, MRPC, QQP, MNLI, RTE, and QNLI. These models are fine-tuned with the learning rate of 5e-5 for 3 epochs. The models are available on HuggingFace as Pytorch models.</p> <p>Evaluation results of these single-task models on the GLUE Benchmark are as follows:</p> Model CoLA MNLI MRPC QNLI QQP RTE SST-2 Avg. CoLA 76.8 32.8 68.4 50.4 39.2 48.0 51.0 52.4 MNLI 59.5 82.1 33.8 46.5 24.9 57.4 40.5 49.2 MRPC 30.8 25.9 80.4 47.1 65.9 49.1 49.1 49.8 QNLI 58.7 38.9 30.6 88.3 39.9 48.7 47.0 50.3 QQP 31.4 25.7 62.3 45.0 89.6 49.1 49.1 50.3 RTE 52.8 47.7 37.5 53.5 33.7 65.3 54.9 49.3 SST-2 51.8 32.9 40.2 49.8 56.8 44.4 91.2 52.4"},{"location":"modelpool/gpt2_classification/#model-pool-configuration","title":"Model Pool Configuration","text":"<p>To use these models from our FusionBench library, you can specify the modelpool configuration file as follows:</p> config/modelpool/gpt-2_glue.yaml<pre><code>type: HF_GPT2ForSequenceClassification\nmodels:\n  - name: _pretrained_\n    path: gpt2\n  - name: cola\n    path: tanganke/gpt2_cola\n  - name: mnli\n    path: tanganke/gpt2_mnli\n  - name: mrpc\n    path: tanganke/gpt2_mrpc\n  - name: qnli\n    path: tanganke/gpt2_qnli\n  - name: qqp\n    path: tanganke/gpt2_qqp\n  - name: rte\n    path: tanganke/gpt2_rte\n  - name: sst2\n    path: tanganke/gpt2_sst2\n</code></pre>"},{"location":"modelpool/gpt2_classification/#basic-examples","title":"Basic Examples","text":"<p>Here are some basic examples of using our CLI tool <code>fusion_bench</code> to merge the GPT-2 models.</p>"},{"location":"modelpool/gpt2_classification/#simple-ensemble","title":"Simple Ensemble","text":"<p>construct an ensemble of GPT-2 models using simple ensemble and evaluate on the seven tasks</p> <pre><code>fusion_bench method=simple_ensemble \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\n</code></pre>"},{"location":"modelpool/gpt2_classification/#simpleaverage","title":"SimpleAverage","text":"<p>merge GPT-2 models using simple average and evluate on the seven tasks</p> <pre><code>fusion_bench method=simple_average \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\n</code></pre>"},{"location":"modelpool/gpt2_classification/#fisher-merging","title":"Fisher merging","text":"<p>merge GPT-2 models using Fisher Merging and evluate the merged model</p> <pre><code>fusion_bench \\\n  method=fisher_merging/gpt2_fisher_merging \\\n    method.batch_size=8 method.num_fisher_examples=512 \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\n</code></pre>"},{"location":"modelpool/gpt2_classification/#regmean","title":"RegMean","text":"<p>merge GPT-2 models using RegMean and evaluate the merged model</p> <pre><code>fusion_bench \\\n  method=regmean/gpt2_regmean \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\n</code></pre>"},{"location":"modelpool/gpt2_classification/#task-arithmetic","title":"Task Arithmetic","text":"<p>merge using Task Arithmetic on the seven tasks</p> <pre><code># set the scaling factor to 0.3\nfusion_bench \\\n  method=task_arithmetic \\\n    method.scaling_factor=0.3 \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\n\n# or run the following script to evaluate the model with different scaling factors,\n# and save the results to different files\n# or \"for scaling_factor in $(seq 0 0.1 1.0)\", I use the following for loop for better readability for readers who are not familiar with bash\nfor scaling_factor in 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 \ndo\nfusion_bench report_save_path=outputs/gpt2_glue_task_arithmetic_scaling_factor_${scaling_factor}.json \\\n  method=task_arithmetic \\\n    method.scaling_factor=${scaling_factor} \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\ndone\n</code></pre> <p>After running the above commands, you will get the following results:</p> <p>Table: Task Arithmetic with different scaling factors</p> scaling_coef cola mnli mrpc qnli qqp rte sst2 Avg. 0.0 0.308725 0.330107 0.313725 0.491671 0.63166 0.527076 0.509174 0.444591 0.1 0.426654 0.501375 0.367647 0.556654 0.739105 0.494585 0.509174 0.513599 0.2 0.658677 0.585532 0.698529 0.602599 0.785258 0.472924 0.669725 0.639035 0.3 0.682646 0.639837 0.718137 0.669046 0.807915 0.462094 0.792431 0.68173 0.4 0.690316 0.673867 0.70098 0.702178 0.817067 0.472924 0.819954 0.696755 0.5 0.68744 0.685583 0.696078 0.704924 0.81818 0.472924 0.836009 0.700163 0.6 0.688399 0.680998 0.678922 0.700531 0.808978 0.472924 0.850917 0.697381 0.7 0.684564 0.665003 0.669118 0.702361 0.789612 0.480144 0.853211 0.692002 0.8 0.677852 0.619154 0.659314 0.673989 0.748776 0.501805 0.819954 0.671549 0.9 0.644295 0.503515 0.654412 0.540912 0.637942 0.487365 0.78555 0.607713 1.0 0.627996 0.411004 0.54902 0.496614 0.478234 0.530686 0.71445 0.544"},{"location":"modelpool/gpt2_classification/#ties-merging","title":"Ties-Merging","text":"<p>merge using Ties-Merging on the seven tasks</p> <pre><code>fusion_bench \\\n  method=ties_merging \\\n    method.scaling_factor=0.3 \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\\\n\n# or run the following script to evaluate the model with different scaling factors,\n# and save the results to different files\nfor scaling_factor in 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\ndo fusion_bench report_save_path=outputs/gpt2_glue_ties_merging_scaling_factor_${scaling_factor}.json \\\n  method=ties_merging \\\n    method.scaling_factor=${scaling_factor} \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\ndone\n</code></pre> scaling_coef cola mnli mrpc qnli qqp rte sst2 Avg. 0.0 0.308725 0.330107 0.313725 0.491671 0.63166 0.527076 0.509174 0.444591 0.1 0.348035 0.45624 0.328431 0.542559 0.70554 0.523466 0.509174 0.487635 0.2 0.489933 0.589913 0.416667 0.596559 0.788647 0.501805 0.510321 0.556264 0.3 0.646213 0.648497 0.632353 0.641406 0.810611 0.516245 0.618119 0.644778 0.4 0.670182 0.691594 0.669118 0.683141 0.821815 0.490975 0.736239 0.680438 0.5 0.681687 0.710036 0.678922 0.696504 0.82466 0.476534 0.77867 0.69243 0.6 0.683605 0.713805 0.683824 0.695589 0.823967 0.476534 0.817661 0.699284 0.7 0.685523 0.700968 0.64951 0.689365 0.816893 0.487365 0.829128 0.694107 0.8 0.686481 0.68538 0.64951 0.693209 0.801608 0.483755 0.837156 0.691014 0.9 0.684564 0.650229 0.671569 0.69687 0.775587 0.516245 0.833716 0.689826 1.0 0.667306 0.576566 0.661765 0.645616 0.72372 0.490975 0.822248 0.655456"},{"location":"modelpool/gpt2_classification/#experimental-results","title":"Experimental Results","text":"<p>Table: Multi-task model merging methods using GPT-2 models</p> Method CoLA MNLI MRPC QNLI QQP RTE SST-2 Avg. Fine-tuned (STL) 76.8 82.1 80.4 88.3 89.6 65.3 91.2 82.0 Model Merging Simple Average 55.0 55.1 51.0 57.6 76.7 44.8 52.5 56.1 Fisher Merging 54.8 58.0 39.5 63.3 81.5 49.1 64.7 58.7 RegMean 61.7 70.4 65.4 69.7 78.8 56.0 79.7 68.8 Task Arithmetic (\\(\\lambda=0.5\\)) 68.7 68.6 69.6 70.5 81.8 47.3 83.6 70.0 Ties-Merging (\\(\\lambda=0.6\\)) 68.4 71.4 68.4 69.6 82.4 47.7 81.8 70.0"},{"location":"modelpool/nyuv2/","title":"Scene Understanding on NYUv2 tasks","text":""},{"location":"modelpool/nyuv2/#examples","title":"Examples","text":""},{"location":"modelpool/nyuv2/#simple-average","title":"Simple Average","text":"<pre><code>fusion_bench --config-name nyuv2_config \\\n    method=simple_average\n</code></pre>"},{"location":"modelpool/nyuv2/#task-arithmetic","title":"Task Arithmetic","text":"<pre><code>fusion_bench --config-name nyuv2_config \\\n    method=task_arithmetic \\\n        method.scaling_factor=0.3\n</code></pre>"},{"location":"modelpool/nyuv2/#ties-merging","title":"Ties-Merging","text":"<pre><code>fusion_bench --config-name nyuv2_config \\\n    method=ties_merging \\\n        method.scaling_factor=0.3\n</code></pre>"},{"location":"modelpool/nyuv2/#experimental-results","title":"Experimental Results","text":"Method Segmentation (mIoU \\(\\uparrow\\)) Segmentation (Pix Acc \\(\\uparrow\\)) Depth Estimation (Abs Err \\(\\downarrow\\)) Depth Estimation (Rel Err \\(\\downarrow\\)) Normal (Mean \\(\\downarrow\\)) Single-Task Learning Segmentation 52.0 73.8 242.8 88.7 82.8 Depth Estimation 2.3 6.2 42.5 17.7 82.8 Normal 2.0 4.9 264.0 98.1 24.7 Multi-Task Model Fusion Methods Weight Averaging 39.0 67.0 55.1 22.7 30.4 Task Arithmetic (\\(\\lambda=0.3\\)) 33.6 63.3 56.3 23.2 31.3 Ties-Merging (\\(\\lambda=0.3\\)) 36.3 61.7 60.5 24.5 33.1"},{"location":"modelpool/llm/","title":"Large Language Models (Causal LMs)","text":"<p>The <code>CausalLMPool</code> class provides a unified interface for managing and loading causal language models from the Hugging Face Transformers library with flexible configuration options.</p>"},{"location":"modelpool/llm/#configuration","title":"Configuration","text":"<p>The <code>CausalLMPool</code> can be configured using YAML files. Here are the main configuration options:</p>"},{"location":"modelpool/llm/#basic-configuration","title":"Basic Configuration","text":"<pre><code>_target_: fusion_bench.modelpool.CausalLMPool # (1)\nmodels:\n  _pretrained_: path_to_pretrained_model # (2)\n  model_a: path_to_model_a\n  model_b: path_to_model_b\nmodel_kwargs: # (3)\n  torch_dtype: bfloat16  # or float16, float32, etc.\ntokenizer: path_to_tokenizer # (4)\n</code></pre> <ol> <li><code>_target_</code> indicates the modelpool class to be instantiated.</li> <li><code>_pretrained_</code>, <code>model_a</code>, and <code>model_b</code> indicates the name of the model to be loaded, if a plain string is given as the value, it will be passed to <code>AutoModelForCausalLM.from_pretrained</code> to load the model.</li> <li><code>model_kwargs</code> is a dictionary of keyword arguments to be passed to <code>AutoModelForCausalLM.from_pretrained</code>, can be overridden by passing kwargs to <code>modelpool.load_model</code> function.</li> <li><code>tokenizer</code> indicates the tokenizer to be loaded, if a plain string, it will be passed to <code>AutoTokenizer.from_pretrained</code>.</li> </ol> <p>Special Model Names in FusionBench</p> <p>Names starting and ending with \"_\" are reserved for special purposes in FusionBench. For example, <code>_pretrained_</code> is a special model name in FusionBench, it is used to specify the pre-trained model to be loaded and pre-trained model can be loaded by calling <code>modelpool.load_pretrained_model()</code> or <code>modelpool.load_model(\"_pretrained_\")</code>.</p>"},{"location":"modelpool/llm/#basic-usage","title":"Basic Usage","text":""},{"location":"modelpool/llm/#information-about-the-model-pool","title":"Information about the Model Pool","text":"<p>Get all the model names in the model pool except the special model names:</p> <pre><code>&gt;&gt;&gt; modelpool.model_names\n['model_a', 'model_b']\n</code></pre> <p>Check if a pre-trained model is in the model pool:</p> <pre><code>&gt;&gt;&gt; modelpool.has_pretrained\nTrue\n</code></pre> <p>Get all the model names in the model pool, including the special model names:</p> <pre><code>&gt;&gt;&gt; modelpool.all_model_names\n['_pretrained_', 'model_a', 'model_b']\n</code></pre>"},{"location":"modelpool/llm/#loading-and-saving-models-and-tokenizers","title":"Loading and Saving Models and Tokenizers","text":"<p>Load a model from the model pool by model name:</p> <pre><code>&gt;&gt;&gt; model_a = modelpool.load_model(\"model_a\")\n</code></pre> <p>Load a model from the model pool and pass/override additional arguments to the model constructor:</p> <pre><code>&gt;&gt;&gt; model_a_fp32 = modelpool.load_model(\"model_a\", torch_dtype=\"float32\")\n</code></pre> <p>Load the pre-trained model from the model pool:</p> <pre><code>&gt;&gt;&gt; pretrained_model = modelpool.load_pretrained_model()\n# or equivalently\n&gt;&gt;&gt; pretrained_model = modelpool.load_model(\"_pretrained_\")\n</code></pre> <p>Load the pre-trained model or the first model in the model pool:</p> <pre><code># if there is a pre-trained model in the model pool, then it will be loaded\n# otherwise, the first model in the model pool will be loaded\n&gt;&gt;&gt; model = modelpool.load_pretrained_or_first_model()\n</code></pre> <p>Load the tokenizer from the model pool:</p> <pre><code>&gt;&gt;&gt; tokenizer = modelpool.load_tokenizer()\n</code></pre> <p>Save a model with tokenizer:</p> <pre><code># Save model with tokenizer\n&gt;&gt;&gt; modelpool.save_model(\n    model=model,\n    path=\"path/to/save\",\n    save_tokenizer=True,\n    push_to_hub=False\n)\n</code></pre>"},{"location":"modelpool/llm/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can also use more detailed configuration with explicit model and tokenizer settings:</p> <pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_:\n    _target_: transformers.AutoModelForCausalLM # (1)\n    pretrained_model_name_or_path: path_to_pretrained_model\n  model_a:\n    _target_: transformers.AutoModelForCausalLM\n    pretrained_model_name_or_path: path_to_model_a\ntokenizer:\n  _target_: transformers.AutoTokenizer # (2)\n  pretrained_model_name_or_path: path_to_tokenizer\nmodel_kwargs:\n  torch_dtype: bfloat16\n</code></pre> <ol> <li><code>_target_</code> indicates the model class to be loaded, if a plain string is given as the value, it will be passed to <code>AutoModelForCausalLM.from_pretrained</code> to load the model.     By setting <code>_target_</code>, you can use a custom model class or function to load the model.     For example, you can use <code>load_peft_causal_lm</code> to load a PEFT model.</li> <li><code>_target_</code> indicates the tokenizer class to be loaded, if a plain string is given as the value, it will be passed to <code>AutoTokenizer.from_pretrained</code> to load the tokenizer.     By setting <code>_target_</code>, you can use a custom tokenizer class or function to load the tokenizer.</li> </ol>"},{"location":"modelpool/llm/#working-with-peft-models","title":"Working with PEFT Models","text":"<pre><code>from fusion_bench.modelpool.causal_lm import load_peft_causal_lm\n\n# Load a PEFT model\nmodel = load_peft_causal_lm(\n    base_model_path=\"path/to/base/model\",\n    peft_model_path=\"path/to/peft/model\",\n    torch_dtype=\"bfloat16\",\n    is_trainable=True,\n    merge_and_unload=False\n)\n</code></pre>"},{"location":"modelpool/llm/#configuration-examples","title":"Configuration Examples","text":""},{"location":"modelpool/llm/#single-model-configuration","title":"Single Model Configuration","text":"config/modelpool/CausalLMPool/single_llama_model.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\n_recursive_: false\n# each model should have a name and a path, and the model is loaded from the path\n# this is equivalent to `AutoModelForCausalLM.from_pretrained(path)`\nmodels:\n  _pretrained_:\n    _target_: transformers.LlamaForCausalLM.from_pretrained\n    pretrained_model_name_or_path: ${...base_model}\nmodel_kwargs:\n  torch_dtype: float16\ntokenizer:\n  _target_: transformers.AutoTokenizer.from_pretrained\n  pretrained_model_name_or_path: ${..base_model}\nbase_model: decapoda-research/llama-7b-hf\n</code></pre>"},{"location":"modelpool/llm/#multiple-models-configuration","title":"Multiple Models Configuration","text":"<p>Here we use models from MergeBench as an example.</p> gemma-2-2bgemma-2-2b-itgemma-2-9bgemma-2-9b-itLlama-3.1-8BLlama-3.1-8B-InstructLlama-3.2-3BLlama-3.2-3B-Instruct config/modelpool/CausalLMPool/mergebench/gemma-2-2b.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: google/gemma-2-2b\n  instruction: MergeBench/gemma-2-2b_instruction\n  math: MergeBench/gemma-2-2b_math\n  coding: MergeBench/gemma-2-2b_coding\n  multilingual: MergeBench/gemma-2-2b_multilingual\n  safety: MergeBench/gemma-2-2b_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: google/gemma-2-2b\n</code></pre> config/modelpool/CausalLMPool/mergebench/gemma-2-2b-it.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: google/gemma-2-2b-it\n  instruction: MergeBench/gemma-2-2b-it_instruction\n  math: MergeBench/gemma-2-2b-it_math\n  coding: MergeBench/gemma-2-2b-it_coding\n  multilingual: MergeBench/gemma-2-2b-it_multilingual\n  safety: MergeBench/gemma-2-2b-it_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: google/gemma-2-2b-it\n</code></pre> config/modelpool/CausalLMPool/mergebench/gemma-2-9b.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: google/gemma-2-9b\n  instruction: MergeBench/gemma-2-9b_instruction\n  math: MergeBench/gemma-2-9b_math\n  coding: MergeBench/gemma-2-9b_coding\n  multilingual: MergeBench/gemma-2-9b_multilingual\n  safety: MergeBench/gemma-2-9b_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: google/gemma-2-9b\n</code></pre> config/modelpool/CausalLMPool/mergebench/gemma-2-9b-it.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: google/gemma-2-9b-it\n  instruction: MergeBench/gemma-2-9b-it_instruction\n  math: MergeBench/gemma-2-9b-it_math\n  coding: MergeBench/gemma-2-9b-it_coding\n  multilingual: MergeBench/gemma-2-9b-it_multilingual\n  safety: MergeBench/gemma-2-9b-it_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: google/gemma-2-9b-it\n</code></pre> config/modelpool/CausalLMPool/mergebench/Llama-3.1-8B.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: meta-llama/Llama-3.1-8B\n  instruction: MergeBench/Llama-3.1-8B_instruction\n  math: MergeBench/Llama-3.1-8B_math\n  coding: MergeBench/Llama-3.1-8B_coding\n  multilingual: MergeBench/Llama-3.1-8B_multilingual\n  safety: MergeBench/Llama-3.1-8B_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: meta-llama/Llama-3.1-8B\n</code></pre> config/modelpool/CausalLMPool/mergebench/Llama-3.1-8B-Instruct.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: meta-llama/Llama-3.1-8B-Instruct\n  instruction: MergeBench/Llama-3.1-8B-Instruct_instruction\n  math: MergeBench/Llama-3.1-8B-Instruct_math\n  coding: MergeBench/Llama-3.1-8B-Instruct_coding\n  multilingual: MergeBench/Llama-3.1-8B-Instruct_multilingual\n  safety: MergeBench/Llama-3.1-8B-Instruct_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: meta-llama/Llama-3.1-8B-Instruct\n</code></pre> config/modelpool/CausalLMPool/mergebench/Llama-3.2-3B.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: meta-llama/Llama-3.2-3B\n  instruction: MergeBench/Llama-3.2-3B_instruction\n  math: MergeBench/Llama-3.2-3B_math\n  coding: MergeBench/Llama-3.2-3B_coding\n  multilingual: MergeBench/Llama-3.2-3B_multilingual\n  safety: MergeBench/Llama-3.2-3B_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: meta-llama/Llama-3.2-3B\n</code></pre> config/modelpool/CausalLMPool/mergebench/Llama-3.2-3B-Instruct.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: meta-llama/Llama-3.2-3B-Instruct\n  instruction: MergeBench/Llama-3.2-3B-Instruct_instruction\n  math: MergeBench/Llama-3.2-3B-Instruct_math\n  coding: MergeBench/Llama-3.2-3B-Instruct_coding\n  multilingual: MergeBench/Llama-3.2-3B-Instruct_multilingual\n  safety: MergeBench/Llama-3.2-3B-Instruct_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: meta-llama/Llama-3.2-3B-Instruct\n</code></pre>"},{"location":"modelpool/llm/#merge-large-language-models-with-fusionbench","title":"Merge Large Language Models with FusionBench","text":"<p>Merge gemma-2b models with simple average:</p> <pre><code>fusion_bench method=simple_average modelpool=CausalLMPool/mergebench/gemma-2-2b\n</code></pre> <p>Merge gemma-2b models with Task Arithmetic:</p> <pre><code>fusion_bench method=task_arithmetic modelpool=CausalLMPool/mergebench/gemma-2-2b\n</code></pre> <p>Merge Llama-3.1-8B models with Ties-Merging:</p> <pre><code>fusion_bench method=ties_merging modelpool=CausalLMPool/mergebench/Llama-3.1-8B\n</code></pre> <p>Merge Llama-3.1-8B-Instruct models with Dare-Ties, with 70% sparsity:</p> <pre><code>fusion_bench method=dare/ties_merging method.sparsity_ratio=0.7 modelpool=CausalLMPool/mergebench/Llama-3.1-8B-Instruct\n</code></pre>"},{"location":"modelpool/llm/#special-features","title":"Special Features","text":""},{"location":"modelpool/llm/#causallmbackbonepool","title":"CausalLMBackbonePool","text":"<p>The <code>CausalLMBackbonePool</code> is a specialized version of <code>CausalLMPool</code> that returns only the transformer layers of the model. This is useful when you need to work with the model's backbone architecture directly.</p> <pre><code>from fusion_bench.modelpool import CausalLMBackbonePool\n\nbackbone_pool = CausalLMBackbonePool.from_config(config)\nlayers = backbone_pool.load_model(\"model_a\")  # Returns model.layers\n</code></pre>"},{"location":"modelpool/llm/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.modelpool.CausalLMPool</li> <li>fusion_bench.modelpool.CausalLMBackbonePool</li> <li>fusion_bench.modelpool.causal_lm.load_peft_causal_lm</li> </ul>"},{"location":"modelpool/llm/Qwen2.5/","title":"Qwen2.5","text":"<p>Qwen2.5 is a series of large language models developed by Alibaba Cloud's Qwen team. These models are designed to excel in various natural language processing tasks including text generation, code completion, mathematical reasoning, and more. The Qwen2.5 series offers models of different sizes to accommodate various computational requirements and use cases.</p> <p>The following table shows the architecture details and licensing information for all Qwen2.5 open-weight models:</p> Models Layers Heads (Q / KV) Tie Embedding Context / Generation Length License 0.5B 24 14 / 2 Yes 32K / 8K Apache 2.0 1.5B 28 12 / 2 Yes 32K / 8K Apache 2.0 3B 36 16 / 2 Yes 32K / 8K Qwen Research 7B 28 28 / 4 No 128K / 8K Apache 2.0 14B 48 40 / 8 No 128K / 8K Apache 2.0 32B 64 40 / 8 No 128K / 8K Apache 2.0 72B 80 64 / 8 No 128K / 8K Qwen"},{"location":"modelpool/llm/Qwen2.5/#qwen25-15b-models","title":"Qwen2.5-1.5B Models","text":"<p>In FusionBench, we provide several pre-configured model pools for Qwen2.5-1.5B models that are commonly used for model fusion experiments.  These configurations include base models and their fine-tuned variants specialized for different domains.</p> <p>This configuration includes the base model along with instruction, math, and code variants:</p> config/modelpool/CausalLMPool/Qwen2.5-1.5B_three_models.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\n_recursive_: false\nenable_lazy_loading: true\nmodels:\n  _pretrained_: Qwen/Qwen2.5-1.5B\n  math: Qwen/Qwen2.5-Math-1.5B\n  code: Qwen/Qwen2.5-Coder-1.5B\n  instruction: Qwen/Qwen2.5-1.5B-Instruct\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: Qwen/Qwen2.5-1.5B\n</code></pre> <p>This configuration focuses specifically on mathematical and coding capabilities:</p> config/modelpool/CausalLMPool/Qwen2.5-1.5B_math_and_code.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\n_recursive_: false\nenable_lazy_loading: true\nmodels:\n  _pretrained_: Qwen/Qwen2.5-1.5B\n  math: Qwen/Qwen2.5-Math-1.5B\n  code: Qwen/Qwen2.5-Coder-1.5B\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: Qwen/Qwen2.5-1.5B\n</code></pre>"},{"location":"modelpool/llm/Qwen2.5/#model-fusion-experiments","title":"Model Fusion Experiments","text":""},{"location":"modelpool/llm/Qwen2.5/#simple-average","title":"Simple Average","text":"<p>Merge all three specialized models using simple parameter averaging:</p> <pre><code>fusion_bench path.log_dir=outputs/Qwen2.5-1.5B/three_models/simple_average \\\n    method=linear/simple_average_for_causallm \\\n    modelpool=CausalLMPool/Qwen2.5-1.5B_three_models\n</code></pre> <p>Example for evaluating the merged model using lm-eval-harness on gsm8k and gsm8k_cot tasks:</p> <pre><code>scripts/lm_eval/evaluate_task.sh \\\n    outputs/Qwen2.5-1.5B/three_models/simple_average/checkpoint \\\n    --tasks 'gsm8k,gsm8k_cot' --output_path outputs/lm_eval\n</code></pre> <p>Merge math and code models using simple parameter averaging:</p> <pre><code>fusion_bench path.log_dir=outputs/Qwen2.5-1.5B/math_and_code/simple_average \\\n    method=linear/simple_average_for_causallm \\\n    modelpool=CausalLMPool/Qwen2.5-1.5B_math_and_code\n</code></pre>"},{"location":"modelpool/llm/Qwen2.5/#task-arithmetic","title":"Task Arithmetic","text":"<p>Merge all three specialized models using task arithmetic:</p> <pre><code>scaling_factor=0.8\nfusion_bench path.log_dir=outputs/Qwen2.5-1.5B/three_models/task_arithmetic/${scaling_factor} \\\n    method=linear/task_arithmetic_for_causallm \\\n    method.scaling_factor=${scaling_factor} \\\n    modelpool=CausalLMPool/Qwen2.5-1.5B_three_models\n</code></pre>"},{"location":"modelpool/llm/Qwen2.5/#ties-merging","title":"Ties-Merging","text":"<p>Merge all three specialized models using TIES merging:</p> <pre><code>scaling_factor=0.8\nfusion_bench path.log_dir=outputs/Qwen2.5-1.5B/three_models/ties_merging/${scaling_factor} \\\n    method=linear/ties_merging_for_causallm \\\n    method.scaling_factor=${scaling_factor} \\\n    modelpool=CausalLMPool/Qwen2.5-1.5B_three_models\n</code></pre>"},{"location":"modelpool/llm/Qwen2.5/#citation","title":"Citation","text":"<p>If you use Qwen2.5 models in your research, please cite:</p> <pre><code>@misc{qwen2025qwen25technicalreport,\n      title={Qwen2.5 Technical Report}, \n      author={Qwen and : and An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},\n      year={2025},\n      eprint={2412.15115},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.15115}, \n}\n</code></pre>"},{"location":"modelpool/llm/gemma-2/","title":"Gemma-2","text":""},{"location":"modelpool/llm/gemma-2/#gemma-2-2b-models","title":"Gemma-2-2B Models","text":"<p>This configuration includes the base model and specialized fine-tuned variants from MergeBench:</p> config/modelpool/CausalLMPool/mergebench/gemma-2-2b.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: google/gemma-2-2b\n  instruction: MergeBench/gemma-2-2b_instruction\n  math: MergeBench/gemma-2-2b_math\n  coding: MergeBench/gemma-2-2b_coding\n  multilingual: MergeBench/gemma-2-2b_multilingual\n  safety: MergeBench/gemma-2-2b_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: google/gemma-2-2b\n</code></pre> <p>This configuration focuses on instruction-tuned variants:</p> config/modelpool/CausalLMPool/mergebench/gemma-2-2b-it.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: google/gemma-2-2b-it\n  instruction: MergeBench/gemma-2-2b-it_instruction\n  math: MergeBench/gemma-2-2b-it_math\n  coding: MergeBench/gemma-2-2b-it_coding\n  multilingual: MergeBench/gemma-2-2b-it_multilingual\n  safety: MergeBench/gemma-2-2b-it_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: google/gemma-2-2b-it\n</code></pre>"},{"location":"modelpool/llm/gemma-2/#model-fusion-experiments","title":"Model Fusion Experiments","text":""},{"location":"modelpool/llm/gemma-2/#simple-average","title":"Simple Average","text":"<pre><code>fusion_bench path.log_dir=outputs/gemma-2-2b/simple_average \\\n    method=linear/simple_average_for_causallm \\\n    modelpool=CausalLMPool/mergebench/gemma-2-2b\n</code></pre>"},{"location":"modelpool/llm/gemma-2/#gemma-2-9b-models","title":"Gemma-2-9B Models","text":"<p>This configuration includes the base model and specialized fine-tuned variants from MergeBench:</p> config/modelpool/CausalLMPool/mergebench/gemma-2-9b.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: google/gemma-2-9b\n  instruction: MergeBench/gemma-2-9b_instruction\n  math: MergeBench/gemma-2-9b_math\n  coding: MergeBench/gemma-2-9b_coding\n  multilingual: MergeBench/gemma-2-9b_multilingual\n  safety: MergeBench/gemma-2-9b_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: google/gemma-2-9b\n</code></pre> <p>This configuration focuses on instruction-tuned variants:</p> config/modelpool/CausalLMPool/mergebench/gemma-2-9b-it.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: google/gemma-2-9b-it\n  instruction: MergeBench/gemma-2-9b-it_instruction\n  math: MergeBench/gemma-2-9b-it_math\n  coding: MergeBench/gemma-2-9b-it_coding\n  multilingual: MergeBench/gemma-2-9b-it_multilingual\n  safety: MergeBench/gemma-2-9b-it_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: google/gemma-2-9b-it\n</code></pre>"},{"location":"modelpool/llm/llama-2/","title":"LLaMA-2","text":"<p>LLaMA-2 is representing a significant advancement in open-source language modeling. </p>"},{"location":"modelpool/llm/llama-2/#llama-2-7b-models","title":"LLaMA-2-7B Models","text":"<p>The LLaMA-2-7B model family provides a good balance between performance and computational efficiency.  In FusionBench, we offer pre-configured model pools that include various specialized variants for different domains.</p> <p>This configuration includes the base LLaMA-2-7B model along with specialized variants for chat, mathematics, and coding:</p> config/modelpool/CausalLMPool/llama-7b_3-models_v1.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\n_recursive_: false\nenable_lazy_loading: true\nmodels:\n  _pretrained_: meta-llama/Llama-2-7b-hf\n  chat: meta-llama/Llama-2-7b-chat-hf\n  math: WizardLMTeam/WizardMath-7B-V1.0\n  code: codellama/CodeLlama-7b-hf\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: meta-llama/Llama-2-7b-hf\n</code></pre>"},{"location":"modelpool/llm/llama-2/#model-fusion-experiments","title":"Model Fusion Experiments","text":""},{"location":"modelpool/llm/llama-2/#simple-average","title":"Simple Average","text":"<p>Merge all models using simple parameter averaging:</p> <pre><code>fusion_bench path.log_dir=outputs/llama-2/3-models_v1/simple_average \\\n    method=linear/simple_average_for_causallm \\\n    modelpool=CausalLMPool/llama-7b_3-models_v1\n</code></pre>"},{"location":"modelpool/llm/llama-2/#citation","title":"Citation","text":"<p>If you use LLaMA-2 models in your research, please cite:</p> <pre><code>@article{touvron2023llama,\n  title={Llama 2: Open foundation and fine-tuned chat models},\n  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},\n  journal={arXiv preprint arXiv:2307.09288},\n  year={2023}\n}\n</code></pre>"},{"location":"modelpool/llm/llama-3/","title":"LLaMA-3","text":""},{"location":"modelpool/llm/llama-3/#llama-31-8b","title":"Llama-3.1-8B","text":"<p>This configuration includes the pretrained base model along with domain-specific fine-tuned models from MergeBench:</p> config/modelpool/CausalLMPool/mergebench/Llama-3.1-8B.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: meta-llama/Llama-3.1-8B\n  instruction: MergeBench/Llama-3.1-8B_instruction\n  math: MergeBench/Llama-3.1-8B_math\n  coding: MergeBench/Llama-3.1-8B_coding\n  multilingual: MergeBench/Llama-3.1-8B_multilingual\n  safety: MergeBench/Llama-3.1-8B_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: meta-llama/Llama-3.1-8B\n</code></pre> <p>This configuration focuses on instruction-tuned variants:</p> config/modelpool/CausalLMPool/mergebench/Llama-3.1-8B-Instruct.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: meta-llama/Llama-3.1-8B-Instruct\n  instruction: MergeBench/Llama-3.1-8B-Instruct_instruction\n  math: MergeBench/Llama-3.1-8B-Instruct_math\n  coding: MergeBench/Llama-3.1-8B-Instruct_coding\n  multilingual: MergeBench/Llama-3.1-8B-Instruct_multilingual\n  safety: MergeBench/Llama-3.1-8B-Instruct_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: meta-llama/Llama-3.1-8B-Instruct\n</code></pre>"},{"location":"modelpool/llm/llama-3/#model-fusion-experiments","title":"Model Fusion Experiments","text":""},{"location":"modelpool/llm/llama-3/#simple-average","title":"Simple Average","text":"<pre><code>fusion_bench path.log_dir=outputs/llama-3.1-8b/simple_average \\\n    method=linear/simple_average_for_causallm \\\n    modelpool=CausalLMPool/mergebench/Llama-3.1-8B\n</code></pre>"},{"location":"modelpool/llm/llama-3/#llama-32-3b","title":"Llama-3.2-3B","text":"<p>This configuration includes the pretrained base model along with domain-specific fine-tuned models from MergeBench:</p> config/modelpool/CausalLMPool/mergebench/Llama-3.2-3B.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: meta-llama/Llama-3.2-3B\n  instruction: MergeBench/Llama-3.2-3B_instruction\n  math: MergeBench/Llama-3.2-3B_math\n  coding: MergeBench/Llama-3.2-3B_coding\n  multilingual: MergeBench/Llama-3.2-3B_multilingual\n  safety: MergeBench/Llama-3.2-3B_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: meta-llama/Llama-3.2-3B\n</code></pre> <p>This configuration focuses on instruction-tuned variants:</p> config/modelpool/CausalLMPool/mergebench/Llama-3.2-3B-Instruct.yaml<pre><code>_target_: fusion_bench.modelpool.CausalLMPool\nmodels:\n  _pretrained_: meta-llama/Llama-3.2-3B-Instruct\n  instruction: MergeBench/Llama-3.2-3B-Instruct_instruction\n  math: MergeBench/Llama-3.2-3B-Instruct_math\n  coding: MergeBench/Llama-3.2-3B-Instruct_coding\n  multilingual: MergeBench/Llama-3.2-3B-Instruct_multilingual\n  safety: MergeBench/Llama-3.2-3B-Instruct_safety\nmodel_kwargs:\n  torch_dtype: bfloat16\ntokenizer: meta-llama/Llama-3.2-3B-Instruct\n</code></pre>"},{"location":"modelpool/llm/llama-3/#model-fusion-experiments_1","title":"Model Fusion Experiments","text":""},{"location":"modelpool/llm/llama-3/#simple-average_1","title":"Simple Average","text":"<pre><code>fusion_bench path.log_dir=outputs/llama-3.2-3b/simple_average \\\n    method=linear/simple_average_for_causallm \\\n    modelpool=CausalLMPool/mergebench/Llama-3.2-3B\n</code></pre>"},{"location":"readinglist/","title":"Reading Lists","text":"<p>Info</p> <p>working in progress. Any suggestions are welcome.</p> <p>I've been compiling a comprehensive list of papers and resources that have been instrumental in my research journey.  This collection is designed to serve as a valuable starting point for those interested in delving into the field of deep model fusion. If you have any suggestions for papers to add, please feel free to raise an issue or submit a pull request.</p> <p>Note</p> <p>Meaning of the symbols in the list:</p> <ul> <li> Highly recommended</li> <li> LLaMA model-related or Mistral-related work</li> <li> Code available on GitHub</li> <li> models or datasets available on Hugging Face</li> </ul>"},{"location":"readinglist/#survey-papers","title":"Survey Papers","text":"<ul> <li> <p>      E. Yang et al., \u201cModel Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities.\u201d arXiv, Aug. 14, 2024.</p> Quote <p></p> </li> <li> <p> Yadav et al. A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning arXiv:2408.07057</p> </li> <li>      W. Li, Y. Peng, M. Zhang, L. Ding, H. Hu, and L. Shen, \u201cDeep Model Fusion: A Survey.\u201d arXiv, Sep. 27, 2023. doi: 10.48550/arXiv.2309.15698.</li> <li>      H. Zheng et al., \u201cLearn From Model Beyond Fine-Tuning: A Survey.\u201d arXiv, Oct. 12, 2023.</li> </ul>"},{"location":"readinglist/#findings-on-model-fusion","title":"Findings on Model Fusion","text":"<ul> <li> Aakanksha et al. Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning arXiv:2410.10801</li> <li> Yadav et al. What Matters for Model Merging at Scale? arXiv:2410.03617</li> </ul>"},{"location":"readinglist/#model-ensemble","title":"Model Ensemble","text":"<ul> <li>Liu T Y, Soatto S. Tangent Model Composition for Ensembling and Continual Fine-tuning. arXiv, 2023.</li> <li> <p> Wan et al. Knowledge Fusion of Large Language Models arXiv:2401.10491</p> Quote <p></p> </li> <li> <p> Wan F, Yang Z, Zhong L, et al. FuseChat: Knowledge Fusion of Chat Models. arXiv, 2024.</p> Quote <p></p> </li> </ul>"},{"location":"readinglist/#model-merging","title":"Model Merging","text":""},{"location":"readinglist/#mode-connectivity","title":"Mode Connectivity","text":"<p>Mode connectivity is such an important concept in model merging that it deserves its own page.</p>"},{"location":"readinglist/#weight-interpolation","title":"Weight Interpolation","text":"<ul> <li> <p>Osowiechi et al. WATT: Weight Average Test-Time Adaptation of CLIP arXiv:2406.13875</p> Quote <p></p> </li> <li> <p>Jiang et al. ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning</p> Quote <p></p> </li> <li> <p>Chronopoulou et al. Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization arXiv:2311.09344</p> Quote <p></p> </li> <li> <p>      L. Yu, B. Yu, H. Yu, F. Huang, and Y. Li, \u201cLanguage Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch,\u201d Nov. 06, 2023, arXiv: arXiv:2311.03099. Available: http://arxiv.org/abs/2311.03099</p> </li> <li>       E. Yang et al., \u201cAdaMerging: Adaptive Model Merging for Multi-Task Learning,\u201d ICLR 2024, arXiv: arXiv:2310.02575. doi: 10.48550/arXiv.2310.02575.</li> <li> P. Yadav, D. Tam, L. Choshen, C. Raffel, and M. Bansal, \u201cResolving Interference When Merging Models,\u201d Jun. 02, 2023, arXiv: arXiv:2306.01708. Available: http://arxiv.org/abs/2306.01708</li> <li> Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard, \u201cTask Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models,\u201d May 30, 2023, arXiv: arXiv:2305.12827. doi: 10.48550/arXiv.2305.12827.</li> <li> G. Ilharco et al., \u201cEditing Models with Task Arithmetic,\u201d Mar. 31, 2023, arXiv: arXiv:2212.04089. doi: 10.48550/arXiv.2212.04089.</li> <li> <p>Tang et al. Towards Efficient Pareto Set Approximation via Mixture of Experts Based Model Fusion</p> Quote <p></p> </li> <li> <p>Rame et al. Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards arXiv:2306.04488</p> </li> <li> <p>Huang et al. LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition arXiv:2307.13269</p> Quote <p></p> </li> <li> <p>Wu et al. Pi-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation</p> Quote <p></p> </li> <li> <p>Chronopoulou et al. AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models arXiv:2302.07027</p> Quote <p></p> </li> <li> <p>Zimmer et al. Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging arXiv:2306.16788</p> Quote <p></p> </li> <li> <p> Wortsman et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time arXiv:2203.05482</p> </li> </ul>"},{"location":"readinglist/#alignment-based-methods","title":"Alignment-based Methods","text":"<ul> <li> <p>Kinderman et al. Foldable SuperNets: Scalable Merging of Transformers with Different Initializations and Tasks arXiv:2410.01483</p> Quote <p></p> </li> <li> <p>S. K. Ainsworth, J. Hayase, and S. Srinivasa, \u201cGit Re-Basin: Merging Models modulo Permutation Symmetries,\u201d ICLR 2023. Available: http://arxiv.org/abs/2209.04836</p> </li> <li>George Stoica, Daniel Bolya, Jakob Bjorner, Taylor Hearn, and Judy Hoffman, \u201cZipIt! Merging Models from Different Tasks without Training,\u201d May 04, 2023, arXiv: arXiv:2305.03053. Available: http://arxiv.org/abs/2305.03053</li> </ul>"},{"location":"readinglist/#subspace-based-methods","title":"Subspace-based Methods","text":"<ul> <li>Tang A, Shen L, Luo Y, et al. Concrete subspace learning based interference elimination for multi-task model fusion. arXiv preprint arXiv:2312.06173, 2023.</li> <li>       X. Yi, S. Zheng, L. Wang, X. Wang, and L. He, \u201cA safety realignment framework via subspace-oriented model fusion for large language models.\u201d arXiv, May 14, 2024. doi: 10.48550/arXiv.2405.09055.</li> <li> Wang K, Dimitriadis N, Ortiz-Jimenez G, et al. Localizing Task Information for Improved Model Merging and Compression. arXiv preprint arXiv:2405.07813, 2024.</li> </ul>"},{"location":"readinglist/#online-model-merging","title":"Online Model Merging","text":"<ul> <li> <p> Alexandrov el al. Mitigating Catastrophic Forgetting in Language Transfer via Model Merging arXiv:2407.08699</p> Quote <p> </p> </li> <li> <p> Lu et al. Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment arXiv:2405.17931</p> </li> <li>Izmailov et al. Averaging Weights Leads to Wider Optima and Better Generalization</li> <li>Kaddour et al. Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging arXiv:2209.14981</li> <li>Zhang et al. Lookahead Optimizer: k steps forward, 1 step back http://arxiv.org/abs/1907.08610</li> </ul>"},{"location":"readinglist/#model-mixingupscalingexpansion","title":"Model Mixing/Upscaling/Expansion","text":"<ul> <li> <p> Samragh et al. Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization arXiv:2409.12903</p> Quote <p></p> </li> <li> <p>Zhao et al. Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering arXiv:2409.16167</p> Quote <p></p> </li> <li> <p>Tang et al. SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models arXiv:2408.10174</p> Quote <p></p> </li> <li> <p>      C. Chen et al., \u201cModel Composition for Multimodal Large Language Models.\u201d arXiv, Feb. 20, 2024. doi: 10.48550/arXiv.2402.12750.</p> </li> <li>A. Tang, L. Shen, Y. Luo, N. Yin, L. Zhang, and D. Tao, \u201cMerging Multi-Task Models via Weight-Ensembling Mixture of Experts,\u201d Feb. 01, 2024, arXiv: arXiv:2402.00433. doi: 10.48550/arXiv.2402.00433.</li> <li> <p>       Zhenyi Lu et al., \"Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging\" 10.48550/arXiv.2406.15479</p> Quote <p></p> </li> <li> <p>  Tang A, Shen L, Luo Y, et al. SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models. arXiv, 2024.</p> </li> <li> <p> Kim et al. SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling arXiv:2312.15166</p> Quote <p></p> </li> <li> <p>Komatsuzaki et al. Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints arXiv:2212.05055</p> Quote <p></p> </li> </ul>"},{"location":"readinglist/#benchmarks","title":"Benchmarks","text":"<ul> <li>Tam et al. Realistic Evaluation of Model Merging for Compositional Generalization arXiv:2409.18314</li> <li> Tang et al. FusionBench: A Comprehensive Benchmark of Deep Model Fusion.</li> </ul>"},{"location":"readinglist/#libraries-and-tools","title":"Libraries and Tools","text":""},{"location":"readinglist/#fine-tuning-preparing-models-for-fusion","title":"Fine-tuning, Preparing models for fusion","text":"<ul> <li>     PyTorch Classification: A PyTorch library for training/fine-tuning models (CNN, ViT, CLIP) on image classification tasks</li> <li>      LLaMA Factory: A PyTorch library for fine-tuning LLMs</li> </ul>"},{"location":"readinglist/#model-fusion","title":"Model Fusion","text":"<ul> <li>      FusionBench: A Comprehensive Benchmark of Deep Model Fusion.</li> <li>       MergeKit: A PyTorch library for merging large language models.</li> </ul>"},{"location":"readinglist/#version-control","title":"Version Control","text":"<ul> <li>Kandpal et al. Git-Theta: A Git Extension for Collaborative Development of Machine Learning Models arXiv:2306.04529</li> </ul>"},{"location":"readinglist/#other-applications-of-model-fusion","title":"Other Applications of Model Fusion","text":""},{"location":"readinglist/#applications-in-reinforcement-learning-rl","title":"Applications in Reinforcement Learning (RL)","text":"<ul> <li>(Survey Paper) Song Y, Suganthan P N, Pedrycz W, et al. Ensemble reinforcement learning: A survey. Applied Soft Computing, 2023.</li> <li> Lee K, Laskin M, Srinivas A, et al. \u201cSunrise: A simple unified framework for ensemble learning in deep reinforcement learning\", ICML, 2021.</li> <li>Ren J, Li Y, Ding Z, et al. \u201cProbabilistic mixture-of-experts for efficient deep reinforcement learning\". arXiv:2104.09122, 2021.</li> <li> Celik O, Taranovic A, Neumann G. \u201cAcquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts\". arXiv preprint arXiv:2403.06966, 2024.</li> </ul>"},{"location":"readinglist/mode_connectivity/","title":"Mode-Connectivity","text":"<p>This is a collection of AWESOME things about recent researches about Mode Connectivity, mainly about the findings of Geometric Properties of learned neural networks.  Mode connectivity is an important property of the loss landscape of deep neural networks and is crucial for understanding weight interpolation-based model merging methods. For the detailed methods side, we include papers which try to rebasin the different learned models and how to directly induce a better model (generalization) from one-shot training. For the application side, downstream taks such as federated learning, continual learning and sparse neural network which may potentially benefit from the advances of the model connectivity research. Waited to be researched...</p>"},{"location":"readinglist/mode_connectivity/#geometric-properties","title":"Geometric Properties","text":""},{"location":"readinglist/mode_connectivity/#the-lmc","title":"The LMC","text":"<ul> <li>Topology and Geometry of Half-Rectified Network Optimization.</li> </ul> <p>C. Daniel Freeman, Joan Bruna [ICLR17]</p>"},{"location":"readinglist/mode_connectivity/#the-nonlienar-mc","title":"The Nonlienar MC","text":"<ul> <li>Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs. </li> </ul> <p>Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, Andrew Gordon Wilson [Neurips18][codes]</p> <ul> <li>Essentially No Barriers in Neural Network Energy Landscape. </li> </ul> <p>Felix Draxler, Kambis Veschgini, Manfred Salmhofer, Fred A. Hamprecht [ICML18][codes]</p> <ul> <li> Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling.</li> </ul> <p>Gregory W. Benton, Wesley J. Maddox, Sanae Lotfi, Andrew Gordon Wilson [ICML21][codes]</p>"},{"location":"readinglist/mode_connectivity/#findings","title":"Findings","text":"<ul> <li>Geometry of the Loss Landscape in Overparameterized Neural Networks: Symmetries and Invariances.</li> </ul> <p>Berfin \u015eim\u015fek, Fran\u00e7ois Ged, Arthur Jacot, Francesco Spadaro, Cl\u00e9ment Hongler, Wulfram Gerstner, Johanni Brea [ICML21][codes]</p> <ul> <li>(Initialisations) Random initialisations performing above chance and how to find them. </li> </ul> <p>Frederik Benzing, Simon Schug, Robert Meier, Johannes von Oswald, Yassir Akram, Nicolas Zucchet, Laurence Aitchison, Angelika Steger [NeurIPS22 OPT][codes]</p> <ul> <li>Linear mode connectivity and the lottery ticket hypothesis. </li> </ul> <p>Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, Michael Carbin [ICML20][codes]</p> <ul> <li>(Functional behaviors of end points) On Convexity and Linear Mode Connectivity in Neural Networks. </li> </ul> <p>David Yunis, Kumar Kshitij Patel, Pedro Henrique Pamplona Savarese, Gal Vardi, Jonathan Frankle, Matthew Walter, Karen Livescu, Michael Maire [NeurIPS22 OPT]</p> <ul> <li>Large Scale Structure of Neural Network Loss Landscapes.</li> </ul> <p>Stanislav Fort, Stanislaw Jastrzebski [NeurIPS19]</p> <ul> <li>Plateau in Monotonic Linear Interpolation -- A \"Biased\" View of Loss Landscape for Deep Networks. </li> </ul> <p>Xiang Wang, Annie N. Wang, Mo Zhou, Rong Ge [ICLR23]</p> <ul> <li>Analyzing Monotonic Linear Interpolation in Neural Network Loss Landscapes. </li> </ul> <p>James Lucas, Juhan Bae, Michael R. Zhang, Stanislav Fort, Richard Zemel, Roger Grosse [ICML21][codes]</p>"},{"location":"readinglist/mode_connectivity/#theory","title":"Theory","text":"<ul> <li>Explaining landscape connectivity of low-cost solutions for multilayer nets.</li> </ul> <p>Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Sanjeev Arora, Rong Ge [NeurIPS19]</p>"},{"location":"readinglist/mode_connectivity/#methods-for-rebasin","title":"Methods for rebasin","text":"<ul> <li>(Width, Depth)(Simulated Annealing) The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks.</li> </ul> <p>Rahim Entezari, Hanie Sedghi, Olga Saukh, Behnam Neyshabur [ICLR22][codes]</p> <ul> <li>Optimizing mode connectivity via neuron alignment.</li> </ul> <p>N. Joseph Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, Rongjie Lai [NeurIPS19][codes]</p> <ul> <li>\ud83d\udd25  (Three methods) Git Re-Basin: Merging Models modulo Permutation Symmetries. </li> </ul> <p>Samuel K. Ainsworth, Jonathan Hayase, Siddhartha Srinivasa [ICLR23][codes][pytorch]</p> <ul> <li>Re-basin via implicit Sinkhorn differentiation.</li> </ul> <p>Fidel A. Guerrero Pe\u00f1a, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric Granger, Marco Pedersoli [paper22]</p> <ul> <li>Linear Mode Connectivity of Deep Neural Networks via Permutation Invariance and Renormalization.</li> </ul> <p>Rahim Entezari, Hanie Sedghi, Olga Saukh, Behnam Neyshabur [ICLR23][codes]</p>"},{"location":"readinglist/mode_connectivity/#model-merging","title":"Model merging","text":"<ul> <li> [Stochastic Weight Averaging] Averaging Weights Leads to Wider Optima and Better Generalization. </li> </ul> <p>Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson [UAI18][codes]</p> <ul> <li>Subspace Inference for Bayesian Deep Learning.</li> </ul> <p>Pavel Izmailov, Wesley J. Maddox, Polina Kirichenko, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson. [UAI19][codes]</p> <ul> <li>Bayesian Nonparametric Federated Learning of Neural Networks.</li> </ul> <p>Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Trong Nghia Hoang, and Yasaman Khazaeni. [ICML19][codes]</p> <ul> <li>Model fusion via optimal transport.</li> </ul> <p>Singh, Sidak Pal and Jaggi, Martin [NeurIPS20][codes]</p> <ul> <li>[Averaging merge] Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. </li> </ul> <p>Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, Ludwig Schmidt [ICML22][codes]</p> <ul> <li>lo-fi: distributed fine-tuning without communication. </li> </ul> <p>Mitchell Wortsman, Suchin Gururangan, Shen Li, Ali Farhadi, Ludwig Schmidt, Micheal Rabbat, Ari S. Morcos [TMLR23]</p> <ul> <li> Learning Neural Network Subspaces.</li> </ul> <p>Mitchell Wortsman, Maxwell Horton, Carlos Guestrin, Ali Farhadi, Mohammad Rastegari [ICML21][codes]</p> <ul> <li>Robust fine-tuning of zero-shot models. </li> </ul> <p>Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo-Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, Ludwig Schmidt [CVPR22][codes]</p> <ul> <li>[Fisher merge] Merging Models with Fisher-Weighted Averaging.</li> </ul> <p>Michael Matena, Colin Raffel [NeurIPS22][codes]</p> <ul> <li>[Regression Mean merge] Dataless Knowledge Fusion by Merging Weights of Language Models. </li> </ul> <p>Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, Pengxiang Cheng [ICLR23]</p> <ul> <li>Wasserstein Barycenter-based Model Fusion and Linear Mode Connectivity of Neural Networks. </li> </ul> <p>Aditya Kumar Akash, Sixu Li, Nicol\u00e1s Garc\u00eda Trillos [paper23][codes]</p> <ul> <li>PopulAtion Parameter Averaging (PAPA).</li> </ul> <p>Alexia Jolicoeur-Martineau, Emy Gervais, Kilian Fatras, Yan Zhang, Simon Lacoste-Julien [paper23]</p> <ul> <li>ZipIt! Merging Models from Different Tasks without Training.</li> </ul> <p>George Stoica, Daniel Bolya, Jakob Bjorner, Taylor Hearn, and Judy Hoffman [paper23]</p>"},{"location":"readinglist/mode_connectivity/#pretrained-model-connectivity","title":"Pretrained model connectivity","text":"<ul> <li>What is being transferred in transfer learning? </li> </ul> <p>Behnam Neyshabur, Hanie Sedghi, Chiyuan Zhang [NeurIPS20][codes]</p> <ul> <li>Exploring Mode Connectivity for Pre-trained Language Models. </li> </ul> <p>Yujia Qin, Cheng Qian, Jing Yi, Weize Chen, Yankai Lin, Xu Han, Zhiyuan Liu, Maosong Sun, Jie Zhou [EMNLP22][codes]</p> <ul> <li>Knowledge is a Region in Weight Space for Fine-tuned Language Models. </li> </ul> <p>Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, Leshem Choshen [paper23]</p>"},{"location":"readinglist/mode_connectivity/#equivariant-network-design","title":"Equivariant Network Design","text":"<ul> <li>\ud83d\udc40 Equivariant Architectures for Learning in Deep Weight Spaces. </li> </ul> <p>Aviv Navon, Aviv Shamsian, Idan Achituve, Ethan Fetaya, Gal Chechik, Haggai Maron [paper23][codes]</p> <ul> <li>\ud83d\udc40 Permutation Equivariant Neural Functionals. </li> </ul> <p>Allan Zhou, Kaien Yang, Kaylee Burns, Yiding Jiang, Samuel Sokota, J. Zico Kolter, Chelsea Finn [paper23]</p>"},{"location":"readinglist/mode_connectivity/#related-paper","title":"Related paper","text":"<ul> <li>Entropy-SGD optimizes the prior of a PAC-Bayes bound: Generalization properties of Entropy-SGD and data-dependent priors. </li> </ul> <p>Gintare Karolina Dziugaite, Daniel Roy [ICML18]</p> <ul> <li>Sharpness-Aware Minimization for Efficiently Improving Generalization. </li> </ul> <p>Pierre Foret, Ariel Kleiner, Hossein Mobahi, Behnam Neyshabur [ICLR21][codes]</p> <ul> <li>Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization.</li> </ul> <p>Alexandre Ram\u00e9, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, L\u00e9on Bottou, David Lopez-Paz [paper23][codes]</p>"},{"location":"readinglist/mode_connectivity/#applications","title":"Applications","text":"<ul> <li>(FL) Connecting Low-Loss Subspace for Personalized Federated Learning. </li> </ul> <p>Seok-Ju Hahn, Minwoo Jeong, Junghye Lee [KDD22][codes]</p> <ul> <li>Linear Mode Connectivity in Multitask and Continual Learning. </li> </ul> <p>Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, Hassan Ghasemzadeh [ICLR21][codes]</p> <ul> <li>All Roads Lead to Rome? On Invariance of BERT Representations. </li> </ul> <p>Yuxin Ren, Qipeng Guo, Zhijing Jin, Shauli Ravfogel, Mrinmaya Sachan, Ryan Cotterell, Bernhard Sch\u00f6lkopf [TACL23]</p> <ul> <li>(Meta Learning) Subspace Learning for Effective Meta-Learning.</li> </ul> <p>Weisen Jiang, James Kwok, Yu Zhang [ICML22][codes]</p> <ul> <li>(Incremental Learning) Towards better plasticity-stability trade-off in incremental learning: a simple linear connector.</li> </ul> <p>Guoliang Lin, Hanlu Chu, Hanjiang Lai [CVPR22][codes]</p> <ul> <li> (Sparsity) Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask? </li> </ul> <p>Mansheej Paul, Feng Chen, Brett W. Larsen, Jonathan Frankle, Surya Ganguli, Gintare Karolina Dziugaite [ICLR23]</p> <ul> <li> Improving Ensemble Distillation With Weight Averaging and Diversifying Perturbation.</li> </ul> <p>Giung Nam, Hyungi Lee, Byeongho Heo, Juho Lee [ICML22][codes]</p> <ul> <li>LCS: Learning Compressible Subspaces for Efficient, Adaptive, Real-Time Network Compression at Inference Time</li> </ul> <p>Elvis Nunez, Maxwell Horton, Anish Prabhu, Anurag Ranjan, Ali Farhadi, Mohammad Rastegari[WACV23][codes]</p> <ul> <li>(OOD) Diverse Weight Averaging for Out-of-Distribution Generalization</li> </ul> <p>Alexandre Ram\u00e9, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Gallinari, Matthieu Cord[NeurIPS22][codes]</p> <ul> <li>Linear Connectivity Reveals Generalization Strategies. </li> </ul> <p>Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, Jo\u00e3o Sedoc, Naomi Saphra [ICLR23][codes]</p>"},{"location":"readinglist/mode_connectivity/#tools","title":"Tools","text":"<ul> <li>Loss landscapes. For loss landscape visualization and analysis.     [pypi]</li> </ul>"},{"location":"taskpool/","title":"Task Pool Module","text":"<p>A taskpool is a collection of tasks that can be used to evaluate the performance of merged models. Each task in the taskpool is typically associated with a dataset and evaluation metrics.</p>"},{"location":"taskpool/#configuration-structure","title":"Configuration Structure","text":"<p>Starting from version 0.2, taskpools use Hydra-based configuration with the <code>_target_</code> field to specify the class to instantiate. A taskpool configuration file typically contains the following fields:</p>"},{"location":"taskpool/#core-fields","title":"Core Fields","text":"<ul> <li><code>_target_</code>: The fully qualified class name of the taskpool (e.g., <code>fusion_bench.taskpool.CLIPVisionModelTaskPool</code>)</li> <li><code>test_datasets</code>: A dictionary of test dataset configurations where each key is the task name and the value is the dataset configuration</li> <li>Additional model-specific configuration fields (processor, tokenizer, etc.)</li> </ul>"},{"location":"taskpool/#common-configuration-fields","title":"Common Configuration Fields","text":"<p>Different taskpool types may include additional configuration fields:</p> <ul> <li><code>processor</code>: Configuration for data processors (e.g., image preprocessors, tokenizers)</li> <li><code>dataloader_kwargs</code>: Configuration for PyTorch DataLoader (batch_size, num_workers, etc.)</li> <li><code>fast_dev_run</code>: Boolean flag for quick development testing</li> <li><code>base_model</code>: Base model identifier used for loading processors and other components</li> </ul>"},{"location":"taskpool/#configuration-examples","title":"Configuration Examples","text":""},{"location":"taskpool/#clip-vision-model-task-pool","title":"CLIP Vision Model Task Pool","text":"<pre><code>_target_: fusion_bench.taskpool.CLIPVisionModelTaskPool\nbase_model: openai/clip-vit-base-patch32\nclip_model:\n  _target_: transformers.CLIPModel.from_pretrained\n  pretrained_model_name_or_path: ${..base_model}\nprocessor:\n  _target_: transformers.CLIPProcessor.from_pretrained\n  pretrained_model_name_or_path: ${..base_model}\ntest_datasets:\n  cifar10:\n    _target_: datasets.load_dataset\n    path: cifar10\n    split: test\n  cifar100:\n    _target_: datasets.load_dataset\n    path: cifar100\n    split: test\ndataloader_kwargs:\n  batch_size: 128\n  num_workers: 8\n</code></pre>"},{"location":"taskpool/#gpt-2-text-classification-task-pool","title":"GPT-2 Text Classification Task Pool","text":"<pre><code>_target_: fusion_bench.taskpool.GPT2TextClassificationTaskPool\ntest_datasets:\n  cola:\n    _target_: fusion_bench.taskpool.gpt2_text_classification.load_gpt2_dataset\n    name: cola\n    split: validation\n  sst2:\n    _target_: fusion_bench.taskpool.gpt2_text_classification.load_gpt2_dataset\n    name: sst2\n    split: validation\ntokenizer:\n  _target_: fusion_bench.modelpool.huggingface_gpt2_classification.load_gpt2_tokenizer\n  pretrained_model_name_or_path: gpt2\ndataloader_kwargs:\n  batch_size: 8\n  num_workers: 0\n</code></pre>"},{"location":"taskpool/#dummy-task-pool-for-debugging","title":"Dummy Task Pool (for debugging)","text":"<pre><code>_target_: fusion_bench.taskpool.DummyTaskPool\nmodel_save_path: null\n</code></pre>"},{"location":"taskpool/#usage","title":"Usage","text":""},{"location":"taskpool/#creating-a-taskpool","title":"Creating a TaskPool","text":"<p>Starting from v0.2, taskpools can be created directly or through Hydra configuration:</p> <pre><code># Create from configuration file\nfrom fusion_bench.utils import instantiate\nfrom omegaconf import OmegaConf\n\nconfig = OmegaConf.load(\"path/to/taskpool/config.yaml\")\ntaskpool = instantiate(config)\n\n# Create directly\nfrom fusion_bench.taskpool import CLIPVisionModelTaskPool\ntaskpool = CLIPVisionModelTaskPool(\n    test_datasets={\n        \"cifar10\": {\n            \"_target_\": \"datasets.load_dataset\",\n            \"path\": \"cifar10\",\n            \"split\": \"test\"\n        }\n    },\n    processor=\"openai/clip-vit-base-patch32\",\n    clip_model=\"openai/clip-vit-base-patch32\"\n)\n</code></pre>"},{"location":"taskpool/#evaluating-models","title":"Evaluating Models","text":"<p>The primary function of a taskpool is to evaluate models across multiple tasks:</p> <pre><code># Evaluate a model on all tasks in the taskpool\nreport = taskpool.evaluate(model)\n\n# The report structure:\n# {\n#     \"task_name\": {\n#         \"metric_name\": metric_value,\n#         ...\n#     },\n#     ...\n# }\n</code></pre>"},{"location":"taskpool/#integration-with-algorithms","title":"Integration with Algorithms","text":"<p>Taskpools can be used within fusion algorithms for evaluation during training:</p> <pre><code>class CustomAlgorithm(BaseAlgorithm):\n    def run(self, modelpool):\n        # Your fusion logic here\n        merged_model = self.fuse_models(modelpool)\n\n        # Evaluate if taskpool is available\n        if hasattr(self, '_program') and self._program.taskpool is not None:\n            report = self._program.taskpool.evaluate(merged_model)\n            print(f\"Evaluation results: {report}\")\n\n        return merged_model\n</code></pre>"},{"location":"taskpool/#implementation-details","title":"Implementation Details","text":"<ul> <li>fusion_bench.taskpool.BaseTaskPool</li> </ul>"},{"location":"taskpool/LlamaTestGenerationTaskPool/","title":"LlamaTestGenerationTaskPool","text":"<p>The <code>LlamaTestGenerationTaskPool</code> class is used to evaluate a language model on a set of prompts. It can also be used in an interactive mode for debugging purposes.</p>"},{"location":"taskpool/LlamaTestGenerationTaskPool/#references","title":"References","text":""},{"location":"taskpool/LlamaTestGenerationTaskPool/#fusion_bench.taskpool.llama.test_generation","title":"<code>test_generation</code>","text":""},{"location":"taskpool/LlamaTestGenerationTaskPool/#fusion_bench.taskpool.llama.test_generation.LlamaTestGenerationTaskPool","title":"<code>LlamaTestGenerationTaskPool</code>","text":"<p>               Bases: <code>BaseTaskPool</code></p> <p>This task pool is used to evaluate a language model on a set of prompts. For the purpose of debugging, it can also be used in an interactive mode.</p> Source code in <code>fusion_bench/taskpool/llama/test_generation.py</code> <pre><code>class LlamaTestGenerationTaskPool(BaseTaskPool):\n    \"\"\"\n    This task pool is used to evaluate a language model on a set of prompts.\n    For the purpose of debugging, it can also be used in an interactive mode.\n    \"\"\"\n\n    def __init__(\n        self,\n        test_prompts: List[str],\n        max_length: int = 1024,\n        temperature: float = 0.01,\n        top_p: float = 0.9,\n        iterative_mode: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            test_prompts (List[str]): A list of prompts to be used for testing the model.\n            max_length (int, optional): The maximum length of the generated text. Defaults to 1024.\n            temperature (float, optional): The sampling temperature for text generation. Defaults to 0.01.\n            top_p (float, optional): The cumulative probability for nucleus sampling. Defaults to 0.9.\n            iterative_mode (bool, optional): If True, enables interactive mode for debugging. Defaults to False.\n        \"\"\"\n        self.test_prompts = test_prompts\n        self.max_length = max_length\n        self.temperature = temperature\n        self.top_p = top_p\n        self.iterative_mode = iterative_mode\n        super().__init__(**kwargs)\n\n    def evaluate(\n        self,\n        model: Union[\"LlamaForCausalLM\", Any],\n        tokenizer: Optional[\"PreTrainedTokenizer\"] = None,\n    ):\n        if tokenizer is None:\n            if self._program is None:\n                log.error(\n                    \"`_program` is not set. This is probably happening when you are not runing the program via `fusion_bench` CLI.\"\n                    \"Please pass `tokenizer` to this function.\"\n                )\n            modelpool: \"CausalLMPool\" = self._program.modelpool\n            tokenizer = modelpool.load_tokenizer()\n\n        report = get_model_summary(model)\n        if self.test_prompts is not None:\n            for prompt_idx, prompt in enumerate(self.test_prompts):\n                print(f\"=== Generating text {prompt_idx+1}/{len(self.test_prompts)}\")\n                report[f\"conversation_{prompt_idx+1}\"] = self._generate_text(\n                    model, tokenizer, prompt\n                )\n\n        if self.iterative_mode:\n            for prompt_idx in itertools.count():\n                # Prompt for input\n                # print usage instructions\n                print(\"Enter a prompt to generate text. Type 'exit' to exit the loop.\")\n                prompt = input(\n                    f\"Enter a prompt, or type 'exit' to quit ({prompt_idx+1}): \"\n                )\n                if prompt == \"exit\":\n                    break\n                report[f\"iterative_conversation_{prompt_idx+1}\"] = self._generate_text(\n                    model, tokenizer, prompt\n                )\n\n        return report\n\n    def _generate_text(\n        self, model: \"LlamaForCausalLM\", tokenizer: \"PreTrainedTokenizer\", prompt: str\n    ) -&gt; dict:\n        \"\"\"\n        Generate text using the provided model and tokenizer for a given prompt.\n\n        This method generates text based on the given prompt using the specified model and tokenizer.\n        It prints the prompt and the generated response, and returns a dictionary containing the prompt,\n        response, wall time, number of characters, and number of tokens.\n\n        Args:\n            model: The language model to be used for text generation.\n            tokenizer: The tokenizer to be used for encoding and decoding text.\n            prompt (str): The input prompt for text generation.\n\n        Returns:\n            dict: A dictionary containing the following keys:\n                - \"prompt\" (str): The input prompt.\n                - \"response\" (str): The generated response.\n                - \"wall_time\" (float): The time taken to generate the response.\n                - \"num_chars\" (int): The number of characters in the generated response.\n                - \"num_tokens\" (int): The number of tokens in the generated response.\n        \"\"\"\n        print(prompt)\n        start_time = time.time()\n        outputs = generate_text(\n            model,\n            tokenizer=tokenizer,\n            prompt=prompt,\n            max_length=self.max_length,\n            temperature=self.temperature,\n            top_p=self.top_p,\n        )\n        print_bordered(\n            outputs[\"response\"], title=\"Generated Text\", code_style=\"markdown\"\n        )\n        print(\"\\n\")\n        return {\n            \"prompt\": prompt,\n            \"response\": outputs[\"response\"],\n            \"wall_time\": time.time() - start_time,\n            \"num_chars\": len(outputs[\"response\"]),\n            \"num_tokens\": outputs[\"num_tokens\"],\n        }\n</code></pre>"},{"location":"taskpool/LlamaTestGenerationTaskPool/#fusion_bench.taskpool.llama.test_generation.LlamaTestGenerationTaskPool.__init__","title":"<code>__init__(test_prompts, max_length=1024, temperature=0.01, top_p=0.9, iterative_mode=False, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>test_prompts</code>               (<code>List[str]</code>)           \u2013            <p>A list of prompts to be used for testing the model.</p> </li> <li> <code>max_length</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The maximum length of the generated text. Defaults to 1024.</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>The sampling temperature for text generation. Defaults to 0.01.</p> </li> <li> <code>top_p</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>The cumulative probability for nucleus sampling. Defaults to 0.9.</p> </li> <li> <code>iterative_mode</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, enables interactive mode for debugging. Defaults to False.</p> </li> </ul> Source code in <code>fusion_bench/taskpool/llama/test_generation.py</code> <pre><code>def __init__(\n    self,\n    test_prompts: List[str],\n    max_length: int = 1024,\n    temperature: float = 0.01,\n    top_p: float = 0.9,\n    iterative_mode: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        test_prompts (List[str]): A list of prompts to be used for testing the model.\n        max_length (int, optional): The maximum length of the generated text. Defaults to 1024.\n        temperature (float, optional): The sampling temperature for text generation. Defaults to 0.01.\n        top_p (float, optional): The cumulative probability for nucleus sampling. Defaults to 0.9.\n        iterative_mode (bool, optional): If True, enables interactive mode for debugging. Defaults to False.\n    \"\"\"\n    self.test_prompts = test_prompts\n    self.max_length = max_length\n    self.temperature = temperature\n    self.top_p = top_p\n    self.iterative_mode = iterative_mode\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"taskpool/LlamaTestGenerationTaskPool/#fusion_bench.taskpool.llama.test_generation.generate_text","title":"<code>generate_text(model, tokenizer, prompt, max_length=1024, temperature=0.01, top_p=0.9, device=None)</code>","text":"<p>Generate text using the loaded model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>LlamaForCausalLM</code>)           \u2013            <p>The loaded language model</p> </li> <li> <code>tokenizer</code>               (<code>PreTrainedTokenizer</code>)           \u2013            <p>The loaded tokenizer</p> </li> <li> <code>prompt</code>               (<code>str</code>)           \u2013            <p>Input prompt text</p> </li> <li> <code>max_length</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>Maximum length of generated sequence</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Controls randomness (higher = more random)</p> </li> <li> <code>top_p</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Nucleus sampling parameter</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>Generated text</p> </li> </ul> Source code in <code>fusion_bench/taskpool/llama/test_generation.py</code> <pre><code>def generate_text(\n    model: \"LlamaForCausalLM\",\n    tokenizer: \"PreTrainedTokenizer\",\n    prompt: str,\n    max_length: int = 1024,\n    temperature: float = 0.01,\n    top_p=0.9,\n    device: torch.device = None,\n):\n    \"\"\"\n    Generate text using the loaded model.\n\n    Args:\n        model: The loaded language model\n        tokenizer: The loaded tokenizer\n        prompt (str): Input prompt text\n        max_length (int): Maximum length of generated sequence\n        temperature (float): Controls randomness (higher = more random)\n        top_p (float): Nucleus sampling parameter\n\n    Returns:\n        str: Generated text\n    \"\"\"\n    if device is None:\n        device = get_device(model)\n\n    # Encode the prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n    # Move to GPU if available\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            temperature=temperature,\n            top_p=top_p,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n            do_sample=True,\n        )\n\n    # Decode and return the generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    response = generated_text[len(prompt) :]\n    return {\n        \"generated_text\": generated_text,\n        \"response\": response,\n        \"num_tokens\": len(outputs[0]) - len(inputs[\"input_ids\"][0]),\n    }\n</code></pre>"},{"location":"taskpool/clip_vit_classification/","title":"Image Classification Tasks for CLIP Models","text":""},{"location":"taskpool/clip_vit_classification/#clipvisionmodeltaskpool","title":"CLIPVisionModelTaskPool","text":"<p>The <code>CLIPVisionModelTaskPool</code> class is used to define image classification tasks for CLIP models. It provides methods to evaluate the performance of a given model on multiple datasets.</p>"},{"location":"taskpool/clip_vit_classification/#attributes","title":"Attributes","text":"<ul> <li><code>test_datasets</code>: A dictionary containing the test datasets.</li> <li><code>processor</code>: The processor used for preprocessing the input data. This is used to set up the classifier.</li> <li><code>data_processor</code>: The data processor used for processing the input data.</li> <li><code>clip_model</code>: The CLIP model used for evaluation.</li> <li><code>dataloader_kwargs</code>: Keyword arguments for the data loader.</li> <li><code>layer_wise_feature_save_path</code>: Path to save the layer-wise features.</li> <li><code>layer_wise_feature_first_token_only</code>: Boolean indicating whether to save only the first token of the features.</li> <li><code>layer_wise_feature_max_num</code>: Maximum number of features to save.</li> <li><code>fast_dev_run</code>: Boolean indicating whether to run in fast development mode.</li> </ul>"},{"location":"taskpool/clip_vit_classification/#methods","title":"Methods","text":"<ul> <li><code>setup()</code>: Sets up the processor, data processor, CLIP model, test datasets, and data loaders.</li> <li><code>evaluate(model)</code>: Evaluates the given model on the image classification task.</li> <li><code>on_task_evaluation_begin(classifier, task_name)</code>: Called at the beginning of task evaluation to set up hooks for saving layer-wise features.</li> <li><code>on_task_evaluation_end()</code>: Called at the end of task evaluation to save features and remove hooks.</li> </ul>"},{"location":"taskpool/clip_vit_classification/#configuration","title":"Configuration","text":"<p>The <code>CLIPVisionModelTaskPool</code> class can be configured using a YAML file. Here is an example configuration:</p> <pre><code>test_datasets:\n  dataset1: ...\n  dataset2: ...\nprocessor:\n  _target_: transformers.CLIPProcessor.from_pretrained\n  pretrained_model_name_or_path: openai/clip-vit-base-patch32\ndata_processor:\n  _target_: transformers.CLIPProcessor.from_pretrained\n  pretrained_model_name_or_path: openai/clip-vit-base-patch32\nclip_model:\n  _target_: transformers.CLIPModel.from_pretrained\n  pretrained_model_name_or_path: openai/clip-vit-base-patch32\ndataloader_kwargs:\n  batch_size: 32\n  num_workers: 4\nlayer_wise_feature_save_path: path/to/save/features\nlayer_wise_feature_first_token_only: true\nlayer_wise_feature_max_num: 1000\nfast_dev_run: false\n</code></pre>"},{"location":"taskpool/clip_vit_classification/#references","title":"References","text":"<p>For detailed API documentation, see fusion_bench.taskpool.CLIPVisionModelTaskPool in the API reference.</p>"},{"location":"taskpool/dummy/","title":"Dummy TaskPool","text":"<p>The <code>DummyTaskPool</code> is used for debugging purposes.  It inherits from the base <code>TaskPool</code> class.</p>"},{"location":"taskpool/dummy/#reference","title":"Reference","text":""},{"location":"taskpool/dummy/#fusion_bench.taskpool.dummy.DummyTaskPool","title":"<code>DummyTaskPool</code>","text":"<p>               Bases: <code>BaseTaskPool</code></p> <p>A lightweight task pool implementation for debugging and development workflows.</p> <p>This dummy task pool provides a minimal evaluation interface that focuses on model introspection rather than task-specific performance evaluation. It's designed for development scenarios where you need to test model fusion pipelines, validate architectures, or debug workflows without the overhead of running actual evaluation tasks.</p> The task pool is particularly useful when <ul> <li>You want to verify model fusion works correctly</li> <li>You need to check parameter counts after fusion</li> <li>You're developing new fusion algorithms</li> <li>You want to test infrastructure without expensive evaluations</li> </ul> Example <pre><code>&gt;&gt;&gt; taskpool = DummyTaskPool(model_save_path=\"/tmp/fused_model\")\n&gt;&gt;&gt; results = taskpool.evaluate(fused_model)\n&gt;&gt;&gt; print(f\"Model has {results['model_info']['trainable_params']} parameters\")\n</code></pre> Source code in <code>fusion_bench/taskpool/dummy.py</code> <pre><code>class DummyTaskPool(BaseTaskPool):\n    \"\"\"A lightweight task pool implementation for debugging and development workflows.\n\n    This dummy task pool provides a minimal evaluation interface that focuses on\n    model introspection rather than task-specific performance evaluation. It's\n    designed for development scenarios where you need to test model fusion\n    pipelines, validate architectures, or debug workflows without the overhead\n    of running actual evaluation tasks.\n\n    The task pool is particularly useful when:\n        - You want to verify model fusion works correctly\n        - You need to check parameter counts after fusion\n        - You're developing new fusion algorithms\n        - You want to test infrastructure without expensive evaluations\n\n    Example:\n        ```python\n        &gt;&gt;&gt; taskpool = DummyTaskPool(model_save_path=\"/tmp/fused_model\")\n        &gt;&gt;&gt; results = taskpool.evaluate(fused_model)\n        &gt;&gt;&gt; print(f\"Model has {results['model_info']['trainable_params']} parameters\")\n        ```\n    \"\"\"\n\n    def __init__(self, model_save_path: Optional[str] = None, **kwargs):\n        \"\"\"Initialize the dummy task pool with optional model saving capability.\n\n        Args:\n            model_save_path: Optional path where the evaluated model should be saved.\n                If provided, the model will be serialized and saved to this location\n                after evaluation using the separate_save utility. If None, no model\n                saving will be performed.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; # Create taskpool without saving\n            &gt;&gt;&gt; taskpool = DummyTaskPool()\n\n            &gt;&gt;&gt; # Create taskpool with model saving\n            &gt;&gt;&gt; taskpool = DummyTaskPool(model_save_path=\"/path/to/save/model.pth\")\n            ```\n        \"\"\"\n        super().__init__(**kwargs)\n        self.model_save_path = model_save_path\n\n    def evaluate(self, model):\n        \"\"\"Perform lightweight evaluation and analysis of the given model.\n\n        This method provides a minimal evaluation that focuses on model introspection\n        rather than task-specific performance metrics. It performs parameter analysis,\n        optionally saves the model, and returns a summary report.\n\n        The evaluation process includes:\n        1. Printing human-readable parameter information (rank-zero only)\n        2. Optionally saving the model if a save path was configured\n        3. Generating and returning a model summary report\n\n        Args:\n            model: The model to evaluate. Can be any PyTorch nn.Module including\n                fusion models, pre-trained models, or custom architectures.\n\n        Returns:\n            dict: A model summary report containing parameter statistics and\n                architecture information. See get_model_summary() for detailed\n                format specification.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; taskpool = DummyTaskPool(model_save_path=\"/tmp/model.pth\")\n            &gt;&gt;&gt; model = torch.nn.Linear(10, 5)\n            &gt;&gt;&gt; results = taskpool.evaluate(model)\n            &gt;&gt;&gt; print(f\"Trainable params: {results['model_info']['trainable_params']}\")\n            ```\n        \"\"\"\n        if rank_zero_only.rank == 0:\n            print_parameters(model, is_human_readable=True)\n\n            if self.model_save_path is not None:\n                with timeit_context(f\"Saving the model to {self.model_save_path}\"):\n                    separate_save(model, self.model_save_path)\n\n        return get_model_summary(model)\n</code></pre>"},{"location":"taskpool/dummy/#fusion_bench.taskpool.dummy.DummyTaskPool.evaluate","title":"<code>evaluate(model)</code>","text":"<p>Perform lightweight evaluation and analysis of the given model.</p> <p>This method provides a minimal evaluation that focuses on model introspection rather than task-specific performance metrics. It performs parameter analysis, optionally saves the model, and returns a summary report.</p> <p>The evaluation process includes: 1. Printing human-readable parameter information (rank-zero only) 2. Optionally saving the model if a save path was configured 3. Generating and returning a model summary report</p> <p>Parameters:</p> <ul> <li> <code>model</code>           \u2013            <p>The model to evaluate. Can be any PyTorch nn.Module including fusion models, pre-trained models, or custom architectures.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>A model summary report containing parameter statistics and architecture information. See get_model_summary() for detailed format specification.</p> </li> </ul> Example <pre><code>&gt;&gt;&gt; taskpool = DummyTaskPool(model_save_path=\"/tmp/model.pth\")\n&gt;&gt;&gt; model = torch.nn.Linear(10, 5)\n&gt;&gt;&gt; results = taskpool.evaluate(model)\n&gt;&gt;&gt; print(f\"Trainable params: {results['model_info']['trainable_params']}\")\n</code></pre> Source code in <code>fusion_bench/taskpool/dummy.py</code> <pre><code>def evaluate(self, model):\n    \"\"\"Perform lightweight evaluation and analysis of the given model.\n\n    This method provides a minimal evaluation that focuses on model introspection\n    rather than task-specific performance metrics. It performs parameter analysis,\n    optionally saves the model, and returns a summary report.\n\n    The evaluation process includes:\n    1. Printing human-readable parameter information (rank-zero only)\n    2. Optionally saving the model if a save path was configured\n    3. Generating and returning a model summary report\n\n    Args:\n        model: The model to evaluate. Can be any PyTorch nn.Module including\n            fusion models, pre-trained models, or custom architectures.\n\n    Returns:\n        dict: A model summary report containing parameter statistics and\n            architecture information. See get_model_summary() for detailed\n            format specification.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; taskpool = DummyTaskPool(model_save_path=\"/tmp/model.pth\")\n        &gt;&gt;&gt; model = torch.nn.Linear(10, 5)\n        &gt;&gt;&gt; results = taskpool.evaluate(model)\n        &gt;&gt;&gt; print(f\"Trainable params: {results['model_info']['trainable_params']}\")\n        ```\n    \"\"\"\n    if rank_zero_only.rank == 0:\n        print_parameters(model, is_human_readable=True)\n\n        if self.model_save_path is not None:\n            with timeit_context(f\"Saving the model to {self.model_save_path}\"):\n                separate_save(model, self.model_save_path)\n\n    return get_model_summary(model)\n</code></pre>"},{"location":"taskpool/flan-t5_generation/","title":"Flan-T5 Models for Text Generation Tasks","text":"<p>This task pool provides a set of text generation tasks from the GLUE benchmark for the Flan-T5 model.  Each task is associated with a dataset.  We report the exact match accuracy metric for CoLA, MNLI, MRPC, QNLI, QQP, RTE, and SST2, and spearman's rho for STSB.</p>"},{"location":"taskpool/flan-t5_generation/#references","title":"References","text":""},{"location":"taskpool/flan-t5_generation/#fusion_bench.compat.taskpool.flan_t5_glue_text_generation","title":"<code>flan_t5_glue_text_generation</code>","text":""},{"location":"taskpool/flan-t5_generation/#fusion_bench.compat.taskpool.flan_t5_glue_text_generation.FlanT5GLUETextGenerationTaskPool","title":"<code>FlanT5GLUETextGenerationTaskPool</code>","text":"<p>               Bases: <code>LightningFabricMixin</code>, <code>TaskPool</code></p> <p>A task pool for FlanT5 GLUE text generation tasks. This class manages the tasks and provides methods for loading and evaluating tasks.</p> Source code in <code>fusion_bench/compat/taskpool/flan_t5_glue_text_generation.py</code> <pre><code>class FlanT5GLUETextGenerationTaskPool(LightningFabricMixin, TaskPool):\n    \"\"\"\n    A task pool for FlanT5 GLUE text generation tasks.\n    This class manages the tasks and provides methods for loading and evaluating tasks.\n    \"\"\"\n\n    _tokenizer_instance = None\n\n    @property\n    def tokenizer(self):\n        \"\"\"\n        Returns the tokenizer. If it's not already initialized, it initializes it using the config's tokenizer.\n        \"\"\"\n        if self._tokenizer_instance is None:\n            self._tokenizer_instance = AutoTokenizer.from_pretrained(\n                self.config.tokenizer\n            )\n        return self._tokenizer_instance\n\n    def load_task(self, task_name_or_config: str | DictConfig):\n        \"\"\"\n        Loads a task given a task name or config. If the task name is in `CLASSIFICATION_TASKS`, it creates a `FlanT5GLUETextGenerationClassificationTask`.\n        If the task name is in `REGRESSION_TASKS`, it creates a `FlanT5GLUETextGenerationRegressionTask`. Otherwise, it raises a `ValueError`.\n        \"\"\"\n        if isinstance(task_name_or_config, str):\n            task_config = self.get_task_config(task_name_or_config)\n        else:\n            task_config = task_name_or_config\n\n        if task_config.name in CLASSIFICATION_TASKS:\n            task = FlanT5GLUETextGenerationClassificationTask(task_config)\n            task._taskpool = self\n            return task\n        elif task_config.name in REGRESSION_TASKS:\n            task = FlanT5GLUETextGenerationRegressionTask(task_config)\n            task._taskpool = self\n            return task\n        else:\n            raise ValueError(f\"Unknown task {task_config.name}\")\n\n    def evaluate(self, model: T5ForConditionalGeneration, name: str = None):\n        \"\"\"\n        Evaluate the model on the FlanT5 GLUE text generation tasks.\n\n        Args:\n            model (T5ForConditionalGeneration): The model to evaluate.\n            name (str, optional): The name of the model. Defaults to None. This is used to identify the model in the report.\n\n        Returns:\n            dict: A dictionary containing the evaluation results for each task.\n        \"\"\"\n        if not isinstance(model, T5ForConditionalGeneration):\n            log.warning(\n                f\"Model is not an instance of T5ForConditionalGeneration, but {type(model)}\"\n            )\n        report = {}\n        training_params, all_params = count_parameters(model)\n        report[\"model_info\"] = {\n            \"trainable_params\": training_params,\n            \"all_params\": all_params,\n            \"trainable_percentage\": training_params / all_params,\n        }\n        if name is not None:\n            report[\"model_info\"][\"name\"] = name\n        model = self.fabric.setup(model)\n        report.update(super().evaluate(model))\n        log.info(f\"evaluation report: {report}\")\n        return report\n</code></pre>"},{"location":"taskpool/flan-t5_generation/#fusion_bench.compat.taskpool.flan_t5_glue_text_generation.FlanT5GLUETextGenerationTaskPool.tokenizer","title":"<code>tokenizer</code>  <code>property</code>","text":"<p>Returns the tokenizer. If it's not already initialized, it initializes it using the config's tokenizer.</p>"},{"location":"taskpool/flan-t5_generation/#fusion_bench.compat.taskpool.flan_t5_glue_text_generation.FlanT5GLUETextGenerationTaskPool.evaluate","title":"<code>evaluate(model, name=None)</code>","text":"<p>Evaluate the model on the FlanT5 GLUE text generation tasks.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>T5ForConditionalGeneration</code>)           \u2013            <p>The model to evaluate.</p> </li> <li> <code>name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The name of the model. Defaults to None. This is used to identify the model in the report.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>A dictionary containing the evaluation results for each task.</p> </li> </ul> Source code in <code>fusion_bench/compat/taskpool/flan_t5_glue_text_generation.py</code> <pre><code>def evaluate(self, model: T5ForConditionalGeneration, name: str = None):\n    \"\"\"\n    Evaluate the model on the FlanT5 GLUE text generation tasks.\n\n    Args:\n        model (T5ForConditionalGeneration): The model to evaluate.\n        name (str, optional): The name of the model. Defaults to None. This is used to identify the model in the report.\n\n    Returns:\n        dict: A dictionary containing the evaluation results for each task.\n    \"\"\"\n    if not isinstance(model, T5ForConditionalGeneration):\n        log.warning(\n            f\"Model is not an instance of T5ForConditionalGeneration, but {type(model)}\"\n        )\n    report = {}\n    training_params, all_params = count_parameters(model)\n    report[\"model_info\"] = {\n        \"trainable_params\": training_params,\n        \"all_params\": all_params,\n        \"trainable_percentage\": training_params / all_params,\n    }\n    if name is not None:\n        report[\"model_info\"][\"name\"] = name\n    model = self.fabric.setup(model)\n    report.update(super().evaluate(model))\n    log.info(f\"evaluation report: {report}\")\n    return report\n</code></pre>"},{"location":"taskpool/flan-t5_generation/#fusion_bench.compat.taskpool.flan_t5_glue_text_generation.FlanT5GLUETextGenerationTaskPool.load_task","title":"<code>load_task(task_name_or_config)</code>","text":"<p>Loads a task given a task name or config. If the task name is in <code>CLASSIFICATION_TASKS</code>, it creates a <code>FlanT5GLUETextGenerationClassificationTask</code>. If the task name is in <code>REGRESSION_TASKS</code>, it creates a <code>FlanT5GLUETextGenerationRegressionTask</code>. Otherwise, it raises a <code>ValueError</code>.</p> Source code in <code>fusion_bench/compat/taskpool/flan_t5_glue_text_generation.py</code> <pre><code>def load_task(self, task_name_or_config: str | DictConfig):\n    \"\"\"\n    Loads a task given a task name or config. If the task name is in `CLASSIFICATION_TASKS`, it creates a `FlanT5GLUETextGenerationClassificationTask`.\n    If the task name is in `REGRESSION_TASKS`, it creates a `FlanT5GLUETextGenerationRegressionTask`. Otherwise, it raises a `ValueError`.\n    \"\"\"\n    if isinstance(task_name_or_config, str):\n        task_config = self.get_task_config(task_name_or_config)\n    else:\n        task_config = task_name_or_config\n\n    if task_config.name in CLASSIFICATION_TASKS:\n        task = FlanT5GLUETextGenerationClassificationTask(task_config)\n        task._taskpool = self\n        return task\n    elif task_config.name in REGRESSION_TASKS:\n        task = FlanT5GLUETextGenerationRegressionTask(task_config)\n        task._taskpool = self\n        return task\n    else:\n        raise ValueError(f\"Unknown task {task_config.name}\")\n</code></pre>"},{"location":"taskpool/gpt2_classification/","title":"GPT-2 Sequence Classification Tasks","text":"<p>This task pool provides a set of sequence classification tasks from the GLUE benchmark for the GPT-2 model.  Each task is associated with a dataset and the accuracy metric. The tasks are: CoLA, MNLI, MRPC, QNLI, QQP, RTE, and SST2.</p>"},{"location":"taskpool/gpt2_classification/#references","title":"References","text":""},{"location":"taskpool/gpt2_classification/#fusion_bench.taskpool.gpt2_text_classification","title":"<code>gpt2_text_classification</code>","text":""},{"location":"taskpool/gpt2_classification/#fusion_bench.taskpool.gpt2_text_classification.GPT2TextClassificationTaskPool","title":"<code>GPT2TextClassificationTaskPool</code>","text":"<p>               Bases: <code>BaseTaskPool</code>, <code>LightningFabricMixin</code></p> <p>A task pool for GPT2 text classification tasks. This class manages the tasks and provides methods for loading test dataset and evaluation.</p> Source code in <code>fusion_bench/taskpool/gpt2_text_classification.py</code> <pre><code>class GPT2TextClassificationTaskPool(BaseTaskPool, LightningFabricMixin):\n    \"\"\"\n    A task pool for GPT2 text classification tasks.\n    This class manages the tasks and provides methods for loading test dataset and evaluation.\n    \"\"\"\n\n    _config_mapping = BaseTaskPool._config_mapping | {\n        \"_test_datasets\": \"test_datasets\",\n        \"_tokenizer\": \"tokenizer\",\n        \"dataloader_kwargs\": \"dataloader_kwargs\",\n        \"fast_dev_run\": \"fast_dev_run\",\n    }\n\n    def __init__(\n        self,\n        test_datasets: DictConfig,\n        tokenizer: DictConfig,\n        dataloader_kwargs: DictConfig,\n        fast_dev_run: bool,\n        **kwargs,\n    ):\n        self._test_datasets = test_datasets\n        self._tokenizer = tokenizer\n        self.dataloader_kwargs = dataloader_kwargs\n        self.fast_dev_run = fast_dev_run\n        super().__init__(**kwargs)\n\n        self.setup()\n\n    def setup(self):\n        global tokenizer\n        self.tokenizer = tokenizer = instantiate(self._tokenizer)\n\n    def get_classifier(\n        self, task_name: str, model: GPT2Model\n    ) -&gt; GPT2ForSequenceClassification:\n        modelpool = self._program.modelpool\n        classifier = modelpool.load_classifier(task_name)\n        classifier.transformer = deepcopy(model)\n        return classifier\n\n    @torch.no_grad()\n    def evaluate_single_task(\n        self,\n        task_name: str,\n        model: GPT2Model,\n        test_loader: DataLoader,\n    ):\n        loss_metric = MeanMetric()\n        # load classifier and replace the backbone with the passed model\n        model: GPT2ForSequenceClassification = self.get_classifier(task_name, model)\n        accuracy = Accuracy(\"multiclass\", num_classes=model.num_labels)\n        model = self.fabric.setup(model)\n\n        if self.config.get(\"fast_dev_run\", False):\n            log.info(\"Running under fast_dev_run mode, evaluating on a single batch.\")\n            test_loader = itertools.islice(test_loader, 1)\n        else:\n            test_loader = test_loader\n\n        for batch in (\n            pbar := tqdm(\n                test_loader, desc=\"Evaluating\", leave=False, dynamic_ncols=True\n            )\n        ):\n            input_ids = batch[\"input_ids\"]\n            attention_mask = batch[\"attention_mask\"]\n            labels = batch[\"labels\"]\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            loss = F.cross_entropy(logits, labels)\n\n            accuracy(logits.detach().cpu(), labels.detach().cpu())\n            loss_metric.update(loss.detach().cpu())\n            pbar.set_postfix(\n                {\n                    \"accuracy\": accuracy.compute().item(),\n                    \"loss\": loss_metric.compute().item(),\n                }\n            )\n\n        acc = accuracy.compute().item()\n        loss = loss_metric.compute().item()\n        results = {\"accuracy\": acc, \"loss\": loss}\n        log.info(f\"Results for task {task_name}: {results}\")\n        return results\n\n    def get_test_dataloader(self, task_name: str):\n        dataset = instantiate(self._test_datasets[task_name])\n        dataloader_kwargs = {\n            \"shuffle\": False,\n        }\n        dataloader_kwargs.update(self.dataloader_kwargs)\n        dataloader = DataLoader(\n            dataset, collate_fn=default_data_collator, **dataloader_kwargs\n        )\n        if self.fabric is not None:\n            dataloader = self.fabric.setup_dataloaders(dataloader)\n        return dataloader\n\n    @override\n    def evaluate(self, model: GPT2Model, name: str = None):\n        \"\"\"Evaluate the model on the test datasets.\n\n        Args:\n            model (GPT2Model): The model to evaluate.\n            name (str, optional): The name of the model. Defaults to None. This is used to identify the model in the report.\n\n        Returns:\n            dict: A dictionary containing the evaluation results for each task.\n        \"\"\"\n        report = {}\n        if name is not None:\n            report[\"name\"] = name\n        for task_name in (pbar := tqdm(self._test_datasets, desc=\"Evaluating tasks\")):\n            pbar.set_description(f\"Evaluating task {task_name}\")\n            dataloader = self.get_test_dataloader(task_name)\n            result = self.evaluate_single_task(task_name, model, dataloader)\n            report[task_name] = result\n\n        # calculate the average accuracy and loss\n        if \"average\" not in report:\n            report[\"average\"] = {}\n            accuracies = [\n                value[\"accuracy\"]\n                for key, value in report.items()\n                if isinstance(value, dict) and \"accuracy\" in value\n            ]\n            if len(accuracies) &gt; 0:\n                average_accuracy = sum(accuracies) / len(accuracies)\n                report[\"average\"][\"accuracy\"] = average_accuracy\n            losses = [value[\"loss\"] for key, value in report.items() if \"loss\" in value]\n            if len(losses) &gt; 0:\n                average_loss = sum(losses) / len(losses)\n                report[\"average\"][\"loss\"] = average_loss\n\n        log.info(f\"Evaluation Result: {report}\")\n        return report\n</code></pre>"},{"location":"taskpool/gpt2_classification/#fusion_bench.taskpool.gpt2_text_classification.GPT2TextClassificationTaskPool.evaluate","title":"<code>evaluate(model, name=None)</code>","text":"<p>Evaluate the model on the test datasets.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>GPT2Model</code>)           \u2013            <p>The model to evaluate.</p> </li> <li> <code>name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The name of the model. Defaults to None. This is used to identify the model in the report.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>A dictionary containing the evaluation results for each task.</p> </li> </ul> Source code in <code>fusion_bench/taskpool/gpt2_text_classification.py</code> <pre><code>@override\ndef evaluate(self, model: GPT2Model, name: str = None):\n    \"\"\"Evaluate the model on the test datasets.\n\n    Args:\n        model (GPT2Model): The model to evaluate.\n        name (str, optional): The name of the model. Defaults to None. This is used to identify the model in the report.\n\n    Returns:\n        dict: A dictionary containing the evaluation results for each task.\n    \"\"\"\n    report = {}\n    if name is not None:\n        report[\"name\"] = name\n    for task_name in (pbar := tqdm(self._test_datasets, desc=\"Evaluating tasks\")):\n        pbar.set_description(f\"Evaluating task {task_name}\")\n        dataloader = self.get_test_dataloader(task_name)\n        result = self.evaluate_single_task(task_name, model, dataloader)\n        report[task_name] = result\n\n    # calculate the average accuracy and loss\n    if \"average\" not in report:\n        report[\"average\"] = {}\n        accuracies = [\n            value[\"accuracy\"]\n            for key, value in report.items()\n            if isinstance(value, dict) and \"accuracy\" in value\n        ]\n        if len(accuracies) &gt; 0:\n            average_accuracy = sum(accuracies) / len(accuracies)\n            report[\"average\"][\"accuracy\"] = average_accuracy\n        losses = [value[\"loss\"] for key, value in report.items() if \"loss\" in value]\n        if len(losses) &gt; 0:\n            average_loss = sum(losses) / len(losses)\n            report[\"average\"][\"loss\"] = average_loss\n\n    log.info(f\"Evaluation Result: {report}\")\n    return report\n</code></pre>"},{"location":"taskpool/lm_eval_harness/","title":"Language Model Evaluation Harness Task Pool","text":"<p>The <code>LMEvalHarnessTaskPool</code> is a task pool implementation that integrates the LM-Eval Harness library into the Fusion Bench framework. It allows you to evaluate language models on a wide range of standardized benchmarks and tasks.</p>"},{"location":"taskpool/lm_eval_harness/#usage","title":"Usage","text":""},{"location":"taskpool/lm_eval_harness/#basic-usage","title":"Basic Usage","text":"<pre><code>fusion_bench \\\n    method=dummy \\\n    modelpool=CausalLMPool/single_llama_model \\\n    taskpool=LMEvalHarnessTaskPool/lm_eval \\\n    taskpool.tasks=\"hellaswag,truthfulqa\"\n</code></pre>"},{"location":"taskpool/lm_eval_harness/#configuration-options","title":"Configuration Options","text":"<p>The <code>LMEvalHarnessTaskPool</code> supports the following configuration options:</p> Parameter Type Default Description <code>tasks</code> Union[str, List[str]] Required Comma-separated list of task names or a list of task names <code>apply_chat_template</code> bool False Whether to apply chat template to the prompts <code>include_path</code> Optional[str] None Additional path to include for external tasks <code>batch_size</code> int 1 Batch size for model evaluation <code>metadata</code> Optional[DictConfig] None Additional metadata to pass to task configs <code>verbosity</code> Optional[Literal[\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\"]] None Logging verbosity level <code>output_path</code> Optional[str] None Path to save evaluation results, if not specified, the results will be saved to <code>log_dir/lm_eval_results</code>, where <code>log_dir</code> is the directory controlled by lightning Fabric. <code>log_samples</code> bool False Whether to log individual samples"},{"location":"taskpool/lm_eval_harness/#example-configurations","title":"Example Configurations","text":"<p>Basic evaluation with multiple tasks:</p> <pre><code>fusion_bench \\\n    method=dummy \\\n    modelpool=CausalLMPool/single_llama_model \\\n    taskpool=LMEvalHarnessTaskPool/lm_eval \\\n    taskpool.tasks=\"hellaswag,truthfulqa\"\n</code></pre> <p>Here <code>dummy</code> method simply loads the pre-trained model or the first model in the model pool and does nothing else.</p>"},{"location":"taskpool/lm_eval_harness/#available-tasks","title":"Available Tasks","text":"<p>To see a complete list of available tasks, you can use: <pre><code>lm-eval --tasks list\n</code></pre></p>"},{"location":"taskpool/lm_eval_harness_cli/","title":"LM Evaluation Harness CLI Tool","text":""},{"location":"taskpool/lm_eval_harness_cli/#overview","title":"Overview","text":"<p>The <code>scripts/lm_eval/evaluate_task.sh</code> script is a comprehensive command-line tool for evaluating language models on various tasks using the LM Evaluation Harness framework. This script provides a convenient wrapper around the lm_eval library with additional features like GPU detection, automated result organization, and support for multiple inference backends.</p>"},{"location":"taskpool/lm_eval_harness_cli/#installation-requirements","title":"Installation Requirements","text":"<p>Before using the script, ensure you have the following dependencies installed:</p> <pre><code># Install LM Evaluation Harness\npip install -e '.[lm-eval-harness]'\n</code></pre>"},{"location":"taskpool/lm_eval_harness_cli/#basic-usage","title":"Basic Usage","text":""},{"location":"taskpool/lm_eval_harness_cli/#syntax","title":"Syntax","text":"<pre><code>./scripts/lm_eval/evaluate_task.sh MODEL --tasks TASK --output_path OUTPUT_DIR [OPTIONS...]\n</code></pre>"},{"location":"taskpool/lm_eval_harness_cli/#required-arguments","title":"Required Arguments","text":"<ul> <li><code>MODEL</code>: The model path or name to evaluate (positional argument)</li> <li><code>--tasks TASK</code>: The task(s) to evaluate on (single task or comma-separated list)</li> <li><code>--output_path OUTPUT_DIR</code>: Directory to save evaluation results</li> </ul>"},{"location":"taskpool/lm_eval_harness_cli/#optional-arguments","title":"Optional Arguments","text":"<ul> <li><code>--batch_size BATCH_SIZE</code>: Batch size for evaluation (default: auto)</li> <li><code>--use_vllm</code>: Enable vLLM for optimized inference (default: false)</li> <li><code>--help</code> or <code>-h</code>: Display help information</li> </ul>"},{"location":"taskpool/lm_eval_harness_cli/#examples","title":"Examples","text":""},{"location":"taskpool/lm_eval_harness_cli/#single-task-evaluation","title":"Single Task Evaluation","text":"<p>Evaluate a model on a single task with standard inference:</p> <pre><code>./scripts/lm_eval/evaluate_task.sh 'meta-llama/Llama-2-7b-hf' \\\n    --tasks 'hellaswag' \\\n    --output_path './outputs/lm_eval' \\\n    --batch_size 8 \\\n    --num_fewshot 5\n</code></pre>"},{"location":"taskpool/lm_eval_harness_cli/#multiple-tasks-evaluation","title":"Multiple Tasks Evaluation","text":"<p>Evaluate a model on multiple tasks simultaneously:</p> <pre><code>./scripts/lm_eval/evaluate_task.sh 'meta-llama/Llama-2-7b-hf' \\\n    --tasks 'gsm8k,gsm8k_cot,hellaswag' \\\n    --output_path './outputs/lm_eval' \\\n    --batch_size 8\n</code></pre>"},{"location":"taskpool/lm_eval_harness_cli/#using-vllm-for-optimized-inference","title":"Using vLLM for Optimized Inference","text":"<p>For faster inference with large models, use vLLM:</p> <pre><code>./scripts/lm_eval/evaluate_task.sh 'meta-llama/Llama-2-7b-hf' \\\n    --tasks 'lambada_openai' \\\n    --output_path './outputs/lm_eval' \\\n    --use_vllm \\\n    --batch_size auto\n</code></pre>"},{"location":"taskpool/lm_eval_harness_cli/#custom-vllm-configuration","title":"Custom vLLM Configuration","text":"<p>Override default vLLM parameters:</p> <pre><code>./scripts/lm_eval/evaluate_task.sh 'meta-llama/Llama-2-7b-hf' \\\n    --tasks 'lambada_openai' \\\n    --output_path './outputs/lm_eval' \\\n    --use_vllm \\\n    --model_args 'pretrained=meta-llama/Llama-2-7b-hf,tensor_parallel_size=2,gpu_memory_utilization=0.9'\n</code></pre>"},{"location":"taskpool/lm_eval_harness_cli/#configuration-options","title":"Configuration Options","text":""},{"location":"taskpool/lm_eval_harness_cli/#inference-backends","title":"Inference Backends","text":""},{"location":"taskpool/lm_eval_harness_cli/#standard-inference-default","title":"Standard Inference (Default)","text":"<p>When using standard inference, the script automatically configures:</p> <ul> <li>Model type: <code>hf</code> (Hugging Face transformers)</li> <li>Default model arguments: <code>pretrained=$MODEL,dtype=bfloat16,parallelize=True</code></li> </ul>"},{"location":"taskpool/lm_eval_harness_cli/#vllm-inference","title":"vLLM Inference","text":"<p>When <code>--use_vllm</code> is enabled, the script configures:</p> <ul> <li>Model type: <code>vllm</code></li> <li>Default parameters:</li> <li><code>tensor_parallel_size=1</code></li> <li><code>dtype=auto</code></li> <li><code>gpu_memory_utilization=0.8</code></li> <li><code>data_parallel_size=1</code></li> </ul>"},{"location":"taskpool/lm_eval_harness_cli/#output-structure","title":"Output Structure","text":"<p>Results are organized in the following directory structure:</p> <pre><code>OUTPUT_DIR/\n\u251c\u2500\u2500 TASK_NAME_1/\n\u2502   \u2514\u2500\u2500 MODEL_NAME__SANITIZED/\n\u2502       \u251c\u2500\u2500 results.json\n\u2502       \u2514\u2500\u2500 samples/\n\u2514\u2500\u2500 TASK_NAME_2/\n    \u2514\u2500\u2500 MODEL_NAME__SANITIZED/\n        \u251c\u2500\u2500 results.json\n        \u2514\u2500\u2500 samples/\n</code></pre> <p>Where: - <code>TASK_NAME</code> is the name of the evaluation task - <code>MODEL_NAME__SANITIZED</code> is the model name with slashes replaced by double underscores - <code>results.json</code> contains the evaluation metrics - <code>samples/</code> directory contains detailed sample-level results</p>"},{"location":"taskpool/lm_eval_harness_cli/#common-tasks","title":"Common Tasks","text":"<p>Here are some commonly used evaluation tasks:</p>"},{"location":"taskpool/lm_eval_harness_cli/#language-understanding","title":"Language Understanding","text":"<ul> <li><code>hellaswag</code>: Commonsense reasoning</li> <li><code>piqa</code>: Physical interaction reasoning</li> <li><code>winogrande</code>: Winograd schema challenge</li> </ul>"},{"location":"taskpool/lm_eval_harness_cli/#mathematical-reasoning","title":"Mathematical Reasoning","text":"<ul> <li><code>gsm8k</code>: Grade school math problems</li> <li><code>gsm8k_cot</code>: GSM8K with chain-of-thought</li> <li><code>math</code>: Mathematical problem solving</li> </ul>"},{"location":"taskpool/lm_eval_harness_cli/#reading-comprehension","title":"Reading Comprehension","text":"<ul> <li><code>lambada_openai</code>: Language modeling evaluation</li> <li><code>arc_easy</code>: Science questions (easy)</li> <li><code>arc_challenge</code>: Science questions (challenging)</li> </ul>"},{"location":"taskpool/lm_eval_harness_cli/#code-understanding","title":"Code Understanding","text":"<ul> <li><code>humaneval</code>: Code generation evaluation</li> <li><code>mbpp</code>: Python programming problems</li> </ul>"}]}