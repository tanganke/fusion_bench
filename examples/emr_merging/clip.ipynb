{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ed8f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "from fusion_bench import (\n",
    "    CLIPVisionModelPool,\n",
    "    CLIPVisionModelTaskPool,\n",
    "    get_default_config_path,\n",
    "    initialize_hydra_config,\n",
    "    instantiate,\n",
    ")\n",
    "from fusion_bench.models.hf_clip import HFCLIPClassifier\n",
    "from fusion_bench.tasks.clip_classification import (\n",
    "    get_classnames_and_templates,\n",
    "    get_num_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21e5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fabric = L.Fabric(accelerator=\"auto\", devices=1)\n",
    "fabric.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e67e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = initialize_hydra_config(\n",
    "    config_name=\"fabric_model_fusion\",\n",
    "    config_path=get_default_config_path(),\n",
    "    overrides=[\n",
    "        \"method=emr_merging/emr_merging\",\n",
    "        \"modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8_model_only\",\n",
    "        \"taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8.yaml\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a923c24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭────────────── Instantiate by calling class ──────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">fusion_bench</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">method</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">emr_merging</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">emr_merging</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">EMRMerging()</span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰──────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m─────────────\u001b[0m\u001b[32m Instantiate by calling class \u001b[0m\u001b[32m─────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49mfusion_bench\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mmethod\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49memr_merging\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49memr_merging\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mEMRMerging\u001b[0m\u001b[38;2;248;248;242;49m(\u001b[0m\u001b[38;2;248;248;242;49m)\u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰──────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭────────────────── Instantiate by calling class ───────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">fusion_bench</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">modelpool</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">clip_vision</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">modelpool</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">CLIPVisionModelPool(</span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    processor</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"openai/clip-vit-base-patch32\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    models</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"&lt;DictConfig object&gt;\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    platform</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"hf\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                                                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">)</span>                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰───────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m─────────────────\u001b[0m\u001b[32m Instantiate by calling class \u001b[0m\u001b[32m──────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49mfusion_bench\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mmodelpool\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mclip_vision\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mmodelpool\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mCLIPVisionModelPool\u001b[0m\u001b[38;2;248;248;242;49m(\u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mprocessor\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49mopenai/clip-vit-base-patch32\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mmodels\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49m<DictConfig object>\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mplatform\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49mhf\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                                                \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m)\u001b[0m                                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰───────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭─────────────────── Instantiate by calling class ────────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">fusion_bench</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">taskpool</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">clip_vision</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">taskpool</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">CLIPVisionModelTaskPool(</span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    test_datasets</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"&lt;DictConfig object&gt;\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    base_model</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"openai/clip-vit-base-patch32\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    clip_model</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"&lt;DictConfig object&gt;\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    processor</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"&lt;DictConfig object&gt;\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    data_processor</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"&lt;DictConfig object&gt;\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    dataloader_kwargs</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"&lt;DictConfig object&gt;\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    layer_wise_feature_save_path</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #ffffff\">None</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                              <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    layer_wise_feature_first_token_only</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #ffffff\">True</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    layer_wise_feature_max_num</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #ffffff\">1000</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">)</span>                                                                   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰─────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m──────────────────\u001b[0m\u001b[32m Instantiate by calling class \u001b[0m\u001b[32m───────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49mfusion_bench\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mtaskpool\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mclip_vision\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mtaskpool\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mCLIPVisionModelTaskPool\u001b[0m\u001b[38;2;248;248;242;49m(\u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mtest_datasets\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49m<DictConfig object>\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mbase_model\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49mopenai/clip-vit-base-patch32\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mclip_model\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49m<DictConfig object>\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mprocessor\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49m<DictConfig object>\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                                \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mdata_processor\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49m<DictConfig object>\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mdataloader_kwargs\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49m<DictConfig object>\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mlayer_wise_feature_save_path\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;102;217;239;49mNone\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                              \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mlayer_wise_feature_first_token_only\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;102;217;239;49mTrue\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mlayer_wise_feature_max_num\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;174;129;255;49m1000\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                                \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m)\u001b[0m                                                                   \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰─────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused argument: base_model=openai/clip-vit-base-patch32\n"
     ]
    }
   ],
   "source": [
    "algorithm = instantiate(config.method)\n",
    "modelpool: CLIPVisionModelPool = instantiate(config.modelpool)\n",
    "taskpool: CLIPVisionModelTaskPool = instantiate(config.taskpool)\n",
    "taskpool.fabric = fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c493aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595ca415e875432eb8d5bf97741216b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ca774a159941799895c2f17c1aa5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524391ca77ba4c4c9e7a5805acaa60f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a862fbaee6cf410c96518387dbe1c5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4951f38274244a1aa7dd40d836752c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b33d6306c1442199d784ee8fe17706b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa926aca41ac44d780a2b69c65b0f38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca73b60587e14699bbbf0fc58162e4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809118ce453645bcb4df1f1143882c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanganke/Documents/GitHub/fusion_bench/fusion_bench/method/emr_merging/utils.py:125: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.rescaler = nn.Parameter(torch.tensor(rescaler), requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "emr_model = algorithm.run(modelpool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64bf8eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMRModulatedModel(\n",
       "  (backbone): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "        (position_embedding): Embedding(50, 768)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (modulators): ModuleDict(\n",
       "    (sun397): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "    (stanford-cars): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "    (resisc45): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "    (eurosat): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "    (svhn): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "    (gtsrb): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "    (mnist): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "    (dtd): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "  )\n",
       "  (unified_task_vector): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b77ec22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭──────────────── Instantiate by calling function ─────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">transformers</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">processing_utils</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">ProcessorMixin</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">from_pretrained(</span>    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    pretrained_model_name_or_path</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"openai/clip-vit-base-patch32\"</span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">)</span>                                                                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰──────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m───────────────\u001b[0m\u001b[32m Instantiate by calling function \u001b[0m\u001b[32m────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49mtransformers\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mprocessing_utils\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mProcessorMixin\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mfrom_pretrained\u001b[0m\u001b[38;2;248;248;242;49m(\u001b[0m    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mpretrained_model_name_or_path\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49mopenai/clip-vit-base-patch32\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m)\u001b[0m                                                                \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰──────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭──────────────── Instantiate by calling function ─────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">transformers</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">processing_utils</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">ProcessorMixin</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">from_pretrained(</span>    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    pretrained_model_name_or_path</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"openai/clip-vit-base-patch32\"</span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">)</span>                                                                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰──────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m───────────────\u001b[0m\u001b[32m Instantiate by calling function \u001b[0m\u001b[32m────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49mtransformers\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mprocessing_utils\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mProcessorMixin\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mfrom_pretrained\u001b[0m\u001b[38;2;248;248;242;49m(\u001b[0m    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mpretrained_model_name_or_path\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49mopenai/clip-vit-base-patch32\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m)\u001b[0m                                                                \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰──────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not taskpool._is_setup:\n",
    "    taskpool.setup()\n",
    "\n",
    "classifier = HFCLIPClassifier(\n",
    "    taskpool.clip_model,\n",
    "    processor=taskpool.processor,\n",
    ")\n",
    "classifier.vision_model = emr_model\n",
    "classifier = fabric.to_device(classifier)\n",
    "results = {}\n",
    "for task_name in taskpool._test_datasets:\n",
    "    emr_model.set_task(task_name)\n",
    "    classnames, templates = get_classnames_and_templates(task_name)\n",
    "    classifier.set_classification_task(\n",
    "        classnames=classnames,\n",
    "        templates=templates,\n",
    "    )\n",
    "    result = taskpool._evaluate(\n",
    "        classifier,\n",
    "        test_loader=taskpool.test_dataloaders[task_name],\n",
    "        task_name=task_name,\n",
    "        num_classes=get_num_classes(task_name),\n",
    "    )\n",
    "    print(f\"Results for task {task_name}:\\n{result}\")\n",
    "    results[task_name] = result\n",
    "\n",
    "print(\"Final results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09edfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion_bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
