{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ed8f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "from fusion_bench import (\n",
    "    CLIPVisionModelPool,\n",
    "    CLIPVisionModelTaskPool,\n",
    "    get_default_config_path,\n",
    "    initialize_hydra_config,\n",
    "    instantiate,\n",
    ")\n",
    "from fusion_bench.models.hf_clip import HFCLIPClassifier\n",
    "from fusion_bench.tasks.clip_classification import (\n",
    "    get_classnames_and_templates,\n",
    "    get_num_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21e5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fabric = L.Fabric(accelerator=\"auto\", devices=1)\n",
    "fabric.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e67e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = initialize_hydra_config(\n",
    "    config_name=\"fabric_model_fusion\",\n",
    "    config_path=get_default_config_path(),\n",
    "    overrides=[\n",
    "        \"method=emr_merging/emr_merging\",\n",
    "        \"modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8_model_only\",\n",
    "        \"taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8.yaml\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a923c24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭────────────── Instantiate by calling class ──────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">fusion_bench</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">method</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">emr_merging</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">emr_merging</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">EMRMerging()</span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰──────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m─────────────\u001b[0m\u001b[32m Instantiate by calling class \u001b[0m\u001b[32m─────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49mfusion_bench\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mmethod\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49memr_merging\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49memr_merging\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mEMRMerging\u001b[0m\u001b[38;2;248;248;242;49m(\u001b[0m\u001b[38;2;248;248;242;49m)\u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰──────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭────────────────── Instantiate by calling class ───────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">fusion_bench</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">modelpool</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">clip_vision</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">modelpool</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">CLIPVisionModelPool(</span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    processor</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"openai/clip-vit-base-patch32\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    models</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"&lt;DictConfig object&gt;\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    platform</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"hf\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                                                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">)</span>                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰───────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m─────────────────\u001b[0m\u001b[32m Instantiate by calling class \u001b[0m\u001b[32m──────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49mfusion_bench\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mmodelpool\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mclip_vision\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mmodelpool\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mCLIPVisionModelPool\u001b[0m\u001b[38;2;248;248;242;49m(\u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mprocessor\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49mopenai/clip-vit-base-patch32\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mmodels\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49m<DictConfig object>\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mplatform\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49mhf\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                                                \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m)\u001b[0m                                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰───────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭─────────────────── Instantiate by calling class ────────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">fusion_bench</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">taskpool</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">clip_vision</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">taskpool</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">CLIPVisionModelTaskPool(</span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    test_datasets</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"&lt;DictConfig object&gt;\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    base_model</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"openai/clip-vit-base-patch32\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    clip_model</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"&lt;DictConfig object&gt;\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    processor</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"&lt;DictConfig object&gt;\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    data_processor</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"&lt;DictConfig object&gt;\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    dataloader_kwargs</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #ffffff\">\"&lt;DictConfig object&gt;\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    layer_wise_feature_save_path</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #ffffff\">None</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                              <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    layer_wise_feature_first_token_only</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #ffffff\">True</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">    layer_wise_feature_max_num</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #ffffff\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #ffffff\">1000</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">,</span>                                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #ffffff\">)</span>                                                                   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰─────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m──────────────────\u001b[0m\u001b[32m Instantiate by calling class \u001b[0m\u001b[32m───────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49mfusion_bench\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mtaskpool\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mclip_vision\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mtaskpool\u001b[0m\u001b[38;2;255;70;137;49m.\u001b[0m\u001b[38;2;248;248;242;49mCLIPVisionModelTaskPool\u001b[0m\u001b[38;2;248;248;242;49m(\u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mtest_datasets\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49m<DictConfig object>\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mbase_model\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49mopenai/clip-vit-base-patch32\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mclip_model\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49m<DictConfig object>\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mprocessor\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49m<DictConfig object>\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                                \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mdata_processor\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49m<DictConfig object>\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mdataloader_kwargs\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;230;219;116;49m<DictConfig object>\u001b[0m\u001b[38;2;230;219;116;49m\"\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mlayer_wise_feature_save_path\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;102;217;239;49mNone\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                              \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mlayer_wise_feature_first_token_only\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;102;217;239;49mTrue\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m    \u001b[0m\u001b[38;2;248;248;242;49mlayer_wise_feature_max_num\u001b[0m\u001b[38;2;255;70;137;49m=\u001b[0m\u001b[38;2;174;129;255;49m1000\u001b[0m\u001b[38;2;248;248;242;49m,\u001b[0m                                \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[38;2;248;248;242;49m)\u001b[0m                                                                   \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰─────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused argument: base_model=openai/clip-vit-base-patch32\n"
     ]
    }
   ],
   "source": [
    "algorithm = instantiate(config.method)\n",
    "modelpool: CLIPVisionModelPool = instantiate(config.modelpool)\n",
    "taskpool: CLIPVisionModelTaskPool = instantiate(config.taskpool)\n",
    "taskpool.fabric = fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c493aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8c7e5064024efda7dc4b3d1c324d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17111b1bd414b51892840f01564e747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139cee6f8a5f44de9ffbc6dd8ea905c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af698dc2e8404848ac0986f953b4a062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ab019cb6f940fc8f63e9459560d8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cd5811ff364f56ac5df85d092dfb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c733e06e081349bd9df285723c8aaa2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fc040302f24da8bb704c74c232b2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bf4662cd5e4b749e180b69e4e62602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emr_model = algorithm.run(modelpool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64bf8eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMRModulatedModel(\n",
       "  (backbone): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "        (position_embedding): Embedding(50, 768)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (modulators): ModuleDict(\n",
       "    (sun397): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "    (stanford-cars): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "    (resisc45): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "    (eurosat): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "    (svhn): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "    (gtsrb): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "    (mnist): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "    (dtd): EMRTaskModulator(\n",
       "      (mask): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       "    )\n",
       "  )\n",
       "  (unified_task_vector): ParameterDictModel(vision_model.embeddings.class_embedding: torch.Size([768]), vision_model.embeddings.patch_embedding.weight: torch.Size([768, 3, 32, 32]), vision_model.embeddings.position_embedding.weight: torch.Size([50, 768]), vision_model.pre_layrnorm.weight: torch.Size([768]), vision_model.pre_layrnorm.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.0.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.0.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.0.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.0.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.0.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.1.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.1.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.1.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.1.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.1.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.2.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.2.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.2.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.2.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.2.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.3.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.3.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.3.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.3.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.3.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.4.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.4.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.4.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.4.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.4.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.5.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.5.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.5.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.5.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.5.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.6.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.6.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.6.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.6.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.6.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.6.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.7.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.7.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.7.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.7.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.7.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.7.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.8.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.8.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.8.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.8.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.8.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.8.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.9.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.9.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.9.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.9.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.9.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.9.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.10.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.10.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.10.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.10.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.10.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.10.layer_norm2.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.k_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.k_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.v_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.v_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.q_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.q_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.self_attn.out_proj.weight: torch.Size([768, 768]), vision_model.encoder.layers.11.self_attn.out_proj.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm1.bias: torch.Size([768]), vision_model.encoder.layers.11.mlp.fc1.weight: torch.Size([3072, 768]), vision_model.encoder.layers.11.mlp.fc1.bias: torch.Size([3072]), vision_model.encoder.layers.11.mlp.fc2.weight: torch.Size([768, 3072]), vision_model.encoder.layers.11.mlp.fc2.bias: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.weight: torch.Size([768]), vision_model.encoder.layers.11.layer_norm2.bias: torch.Size([768]), vision_model.post_layernorm.weight: torch.Size([768]), vision_model.post_layernorm.bias: torch.Size([768]))\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b77ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not taskpool._is_setup:\n",
    "    taskpool.setup()\n",
    "\n",
    "classifier = HFCLIPClassifier(\n",
    "    taskpool.clip_model,\n",
    "    processor=taskpool.processor,\n",
    ")\n",
    "# do not use the classifier.vision_model here due to PyTorch limitation (see note in hf_clip.py)\n",
    "classifier.clip_model.vision_model = emr_model\n",
    "classifier = fabric.to_device(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8608007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15046195680, 15046195680)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(emr_model), id(classifier.clip_model.vision_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fad6c329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dae3082f9af4384a4c050c11e9e4f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating sun397:   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fusion_bench/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/envs/fusion_bench/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:949: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for task sun397:\n",
      "{'accuracy': 0.7096221446990967, 'loss': 1.107221007347107}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f084f647c894af5986518ac0b9206b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating stanford-cars:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for task stanford-cars:\n",
      "{'accuracy': 0.7560005187988281, 'loss': 0.8067706227302551}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b68be28e7c443edb3a40c05c1f3d159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating resisc45:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for task resisc45:\n",
      "{'accuracy': 0.918571412563324, 'loss': 0.3021736443042755}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f862c83bda74dbd95798f7df6109c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating eurosat:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for task eurosat:\n",
      "{'accuracy': 0.9762963056564331, 'loss': 0.06937872618436813}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d816bc559e46f9b274c94767d25a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating svhn:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for task svhn:\n",
      "{'accuracy': 0.9651198387145996, 'loss': 0.13929736614227295}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb11ac3a6b24ed4814ff2757e896f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating gtsrb:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for task gtsrb:\n",
      "{'accuracy': 0.9775930047035217, 'loss': 0.0880066528916359}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e1fcaecc9a4f1c9a25baa8ad1ff5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating mnist:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for task mnist:\n",
      "{'accuracy': 0.9950000047683716, 'loss': 0.023963619023561478}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4252b23d454f76b7fc1268994ee82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating dtd:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for task dtd:\n",
      "{'accuracy': 0.7218084931373596, 'loss': 1.063814401626587}\n",
      "Final results: {'sun397': {'accuracy': 0.7096221446990967, 'loss': 1.107221007347107}, 'stanford-cars': {'accuracy': 0.7560005187988281, 'loss': 0.8067706227302551}, 'resisc45': {'accuracy': 0.918571412563324, 'loss': 0.3021736443042755}, 'eurosat': {'accuracy': 0.9762963056564331, 'loss': 0.06937872618436813}, 'svhn': {'accuracy': 0.9651198387145996, 'loss': 0.13929736614227295}, 'gtsrb': {'accuracy': 0.9775930047035217, 'loss': 0.0880066528916359}, 'mnist': {'accuracy': 0.9950000047683716, 'loss': 0.023963619023561478}, 'dtd': {'accuracy': 0.7218084931373596, 'loss': 1.063814401626587}}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for task_name in taskpool._test_datasets:\n",
    "    emr_model.set_task(task_name)\n",
    "    classnames, templates = get_classnames_and_templates(task_name)\n",
    "    classifier.set_classification_task(\n",
    "        classnames=classnames,\n",
    "        templates=templates,\n",
    "    )\n",
    "    result = taskpool._evaluate(\n",
    "        classifier,\n",
    "        test_loader=taskpool.test_dataloaders[task_name],\n",
    "        task_name=task_name,\n",
    "        num_classes=get_num_classes(task_name),\n",
    "    )\n",
    "    print(f\"Results for task {task_name}:\\n{result}\")\n",
    "    results[task_name] = result\n",
    "\n",
    "print(\"Final results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0436c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion_bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
