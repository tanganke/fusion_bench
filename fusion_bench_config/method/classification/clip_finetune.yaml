name: clip_finetune
seed: 42
learning_rate: 1e-5
weight_decay: 0
num_steps: 4000
batch_size: 128
num_workers: 16
save_interval: 500
# if `state_dict_load_path` is not null, the training will be resumed from the state_dict_path
state_dict_load_path: null
# if `state_dict_save_path` is not null, the state_dict will be saved to the path after training
state_dict_save_path: null
# if `skip_training` is true, use with `state_dict_load_path` to skip training and only evaluate
skip_training: false
# === LoRA ===
use_lora: false
lora_config:
  r: 16
  lora_alpha: 32
  target_modules:
    - q_proj
    - v_proj
  lora_dropout: 0.1
  bias: none
# === L-LoRA ===
use_l_lora: false
