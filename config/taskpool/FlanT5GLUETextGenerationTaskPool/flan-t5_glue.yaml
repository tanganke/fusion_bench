_target_: fusion_bench.taskpool.flan_t5.FlanT5GLUETextGenerationTaskPool
_recursive_: false
tasks:
  glue-cola:
    _target_: fusion_bench.taskpool.flan_t5.FlanT5GLUETextGenerationClassificationTask
    name: cola
    split: validation
  glue-mnli:
    _target_: fusion_bench.taskpool.flan_t5.FlanT5GLUETextGenerationClassificationTask
    name: mnli
    split: validation_matched
  glue-mrpc:
    _target_: fusion_bench.taskpool.flan_t5.FlanT5GLUETextGenerationClassificationTask
    name: mrpc
    split: validation
  glue-qnli:
    _target_: fusion_bench.taskpool.flan_t5.FlanT5GLUETextGenerationClassificationTask
    name: qnli
    split: validation
  glue-qqp:
    _target_: fusion_bench.taskpool.flan_t5.FlanT5GLUETextGenerationClassificationTask
    name: qqp
    split: validation
  glue-rte:
    _target_: fusion_bench.taskpool.flan_t5.FlanT5GLUETextGenerationClassificationTask
    name: rte
    split: validation
  glue-sst2:
    _target_: fusion_bench.taskpool.flan_t5.FlanT5GLUETextGenerationClassificationTask
    name: sst2
    split: validation
  glue-stsb:
    _target_: fusion_bench.taskpool.flan_t5.FlanT5GLUETextGenerationRegressionTask
    name: stsb
    split: validation
# all flan-t5 models share the same tokenizer,
# so it is not necessary to change it when you evaluate other models,
# such as flan-t5-large, flan-t5-xxl
tokenizer: google/flan-t5-base
# cache directory for storing the preprocessed data
cache_dir: outputs/cache
batch_size: 32
num_workers: 4
fast_dev_run: ${fast_dev_run}
