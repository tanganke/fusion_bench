_target_: fusion_bench.modelpool.CausalLMPool
_recursive_: false
models:
  _pretrained_: # model name
    # 对应： { "_target_": "transformers.LlamaForCausalLM.from_pretrained", "pretrained_model_name_or_path": "MergeBench/Llama-3.2-3B_instruction" } -> return transformers.LlamaForCausalLM.from_pretrained(pretrained_model_name_or_path="MergeBench/Llama-3.2-3B_instruction")
    _target_: transformers.AutoModelForCausalLM.from_pretrained # callable function or class name
    pretrained_model_name_or_path: MergeBench/Llama-3.2-3B_instruction
  coding: # modelpool.load_model()
    _target_: transformers.AutoModelForCausalLM.from_pretrained
    pretrained_model_name_or_path: MergeBench/Llama-3.2-3B-Instruct_coding
  instruction:
    _target_: transformers.AutoModelForCausalLM.from_pretrained
    pretrained_model_name_or_path: MergeBench/Llama-3.2-3B-Instruct_instruction
  math:
    _target_: transformers.AutoModelForCausalLM.from_pretrained
    pretrained_model_name_or_path: MergeBench/Llama-3.2-3B-Instruct_math
  multilingual:
    _target_: transformers.AutoModelForCausalLM.from_pretrained
    pretrained_model_name_or_path: MergeBench/Llama-3.2-3B-Instruct_multilingual
  safety:
    _target_: transformers.AutoModelForCausalLM.from_pretrained
    pretrained_model_name_or_path: MergeBench/Llama-3.2-3B-Instruct_safety

# model_kwargs 是 transformers.AutoModelForCausalLM.from_pretrained 的默认传参
model_kwargs:
  torch_dtype: bfloat16

tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: MergeBench/Llama-3.2-3B_instruction
