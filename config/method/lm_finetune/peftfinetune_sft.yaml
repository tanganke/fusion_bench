_target_: ttt.method.FullFinetuneSFT
_recursive_: False

optimizer:
  _target_: torch.optim.AdamW
  fused: True
  weight_decay: 0.01
  lr: 5e-5

lr_scheduler:
  _target_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 5
  num_training_steps: _T_max_ # this will be replaced by the expected number of training steps

dataloader_kwargs:
  # per-gpu batch size
  batch_size: 1
  num_workers: 0
  pin_memory: True

peft_config:
  _target_: peft.LoraConfig
  task_type: peft.TaskType.CAUSAL_LM
  target_modules:
    - query
    - value
  r: 16
  lora_alpha: 16
  lora_dropout: 0
  bias: none

adapter_name: default
# whether to merge and unload the adapter after training
merge_and_unload: false

# Training hyperparameters
# if max_epochs=-1, max_steps will be used to determine the number of training steps
max_epochs: 3
max_steps: -1
max_steps_per_epoch: -1
accumulate_grad_batches: 1
lr_scheduler_interval: step
lr_scheduler_frequency: 1
# Checkpointing may be done by epoch or step, and at the end of training
# `checkpoint_save_interval` can be 'epoch' or 'step'
checkpoint_save_interval: epoch
checkpoint_save_frequency: 1
# Whether to use gradient clipping, and if so, the value and algorithm
gradient_clip_val: null
gradient_clip_algorithm: norm
save_optimizer_state: false
# save_full_model must be true when using shared FSDP
save_full_model: false
# Path to checkpoint to load from, used for resuming training
ckpt_path: null
